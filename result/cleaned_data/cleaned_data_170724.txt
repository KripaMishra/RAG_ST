With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers.
The toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler, and a runtime library to deploy your application.
Using built-in capabilities for distributing computations across multi-GPU configurations, scientists and researchers can develop applications that scale from single GPU workstations to cloud installations with thousands of GPUs.
EULA The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools.
If you do not agree with the terms and conditions of the license agreement, then do not download or use the software.
Installation Guides  Quick Start Guide This guide provides the minimal first-steps instructions for installation and verifying CUDA on a standard system.
Installation Guide Windows This guide discusses how to install and check for correct operation of the CUDA Development Tools on Microsoft Windows systems.
Installation Guide Linux This guide discusses how to install and check for correct operation of the CUDA Development Tools on GNU/Linux systems.
Programming Guides  Programming Guide This guide provides a detailed discussion of the CUDA programming model and programming interface.
It then describes the hardware implementation, and provides guidance on how to achieve maximum performance.
The appendices include a list of all CUDA-enabled devices, detailed description of all extensions to the C++ language, listings of supported mathematical functions, C++ features supported in host and device code, details on texture fetching, technical specifications of various devices, and concludes by introducing the low-level driver API.
Best Practices Guide This guide presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for CUDA-capable GPU architectures.
The intent is to provide guidelines for obtaining the best performance from NVIDIA GPUs using the CUDA Toolkit.
Maxwell Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Maxwell Architecture.
This document provides guidance to ensure that your software applications are compatible with Maxwell.
Pascal Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Pascal Architecture.
This document provides guidance to ensure that your software applications are compatible with Pascal.
Volta Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Volta Architecture.
This document provides guidance to ensure that your software applications are compatible with Volta.
Turing Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Turing Architecture.
This document provides guidance to ensure that your software applications are compatible with Turing.
NVIDIA Ampere GPU Architecture Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Ampere GPU Architecture.
This document provides guidance to ensure that your software applications are compatible with NVIDIA Ampere GPU architecture.
Hopper Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Hopper GPUs.
This document provides guidance to ensure that your software applications are compatible with Hopper architecture.
Ada Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Ada GPUs.
This document provides guidance to ensure that your software applications are compatible with Ada architecture.
Maxwell Tuning Guide Maxwell is NVIDIA’s 4th-generation architecture for CUDA compute applications.
Applications that follow the best practices for the Kepler architecture should typically see speedups on the Maxwell architecture without any code changes.
This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Maxwell architectural features.
Pascal Tuning Guide Pascal is NVIDIA’s 5th-generation architecture for CUDA compute applications.
Applications that follow the best practices for the Maxwell architecture should typically see speedups on the Pascal architecture without any code changes.
This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Pascal architectural features.
Volta Tuning Guide Volta is NVIDIA’s 6th-generation architecture for CUDA compute applications.
Applications that follow the best practices for the Pascal architecture should typically see speedups on the Volta architecture without any code changes.
This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Volta architectural features.
Turing Tuning Guide Turing is NVIDIA’s 7th-generation architecture for CUDA compute applications.
Applications that follow the best practices for the Pascal architecture should typically see speedups on the Turing architecture without any code changes.
This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Turing architectural features.
NVIDIA Ampere GPU Architecture Tuning Guide NVIDIA Ampere GPU Architecture is NVIDIA’s 8th-generation architecture for CUDA compute applications.
Applications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the NVIDIA Ampere GPU Architecture without any code changes.
This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging NVIDIA Ampere GPU Architecture’s features.
Hopper Tuning Guide Hopper GPU Architecture is NVIDIA’s 9th-generation architecture for CUDA compute applications.
Applications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the Hopper GPU Architecture without any code changes.
This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Hopper GPU Architecture’s features.
Ada Tuning Guide The NVIDIA® Ada GPU architecture is NVIDIA’s latest architecture for CUDA® compute applications.
The NVIDIA Ada GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere and Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA Ada architecture without any code changes.
This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Ada GPU architecture’s features.
PTX ISA This guide provides detailed instructions on the use of PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA).
Instead, use the NVIDIA Video Codec SDK ( https: developer.nvidia.com/nvidia-video-codec-sdk ).
PTX Interoperability This document shows how to write PTX that is ABI-compliant and interoperable with other CUDA code.
Inline PTX Assembly This document shows how to inline PTX (parallel thread execution) assembly language statements into CUDA code.
It describes available assembler statement parameters and constraints, and the document also provides a list of some pitfalls that you may encounter.
CUDA API References  CUDA Runtime API Fields in structures might appear in order that is different from the order of declaration.
CUDA Driver API Fields in structures might appear in order that is different from the order of declaration.
cuBLAS The cuBLAS library is an implementation of BLAS (Basic Linear Algebra Subprograms) on top of the NVIDIA CUDA runtime.
It allows the user to access the computational resources of NVIDIA Graphical Processing Unit (GPU), but does not auto-parallelize across multiple GPUs.
NVBLAS The NVBLAS library is a multi-GPUs accelerated drop-in BLAS (Basic Linear Algebra Subprograms) built on top of the NVIDIA cuBLAS Library.
nvJPEG The nvJPEG Library provides high-performance GPU accelerated JPEG decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications.
cuFile API Reference Guide The NVIDIA® GPUDirect® Storage cuFile API Reference Guide provides information about the preliminary version of the cuFile API reference guide that is used in applications and frameworks to leverage GDS technology and describes the intent, context, and operation of those APIs, which are part of the GDS technology.
The initial set of functionality in the library focuses on imaging and video processing and is widely applicable for developers in these areas.
NPP will evolve over time to encompass more of the compute heavy tasks in a variety of problem domains.
It accepts CUDA C++ source code in character string form and creates handles that can be used to obtain the PTX.
The PTX string generated by NVRTC can be loaded by cuModuleLoadData and cuModuleLoadDataEx, and linked with other modules by cuLinkAddData of the CUDA Driver API.
This facility can often provide optimizations and performance not possible in a purely offline static compilation.
PTX Compiler API References  PTX Compiler APIs This guide shows how to compile a PTX program into GPU assembly code using APIs provided by the static PTX Compiler library.
Miscellaneous  CUDA Demo Suite This document describes the demo applications shipped with the CUDA Demo Suite.
CUDA on WSL This guide is intended to help users get started with using NVIDIA CUDA on Windows Subsystem for Linux (WSL 2).
The guide covers installation and running CUDA applications and containers in this environment.
Multi-Instance GPU (MIG) This edition of the user guide describes the Multi-Instance GPU feature of the NVIDIA® A100 GPU.
CUDA Compatibility This document describes CUDA Compatibility, including CUDA Enhanced Compatibility and CUDA Forward Compatible Upgrade.
The CUDA Profiling Tools Interface (CUPTI) enables the creation of profiling and tracing tools that target CUDA applications.
GPUDirect RDMA A technology introduced in Kepler-class GPUs and CUDA 5.0, enabling a direct path for communication between the GPU and a third-party peer device on the PCI Express bus when the devices share the same upstream root complex using standard features of PCI Express.
This document introduces the technology and describes the steps necessary to enable a GPUDirect RDMA connection to NVIDIA GPUs within the Linux device driver model.
nvcc accepts a range of conventional compiler options, such as for defining macros and include/library paths, and for steering the compilation process.
CUDA-GDB The NVIDIA tool for debugging CUDA applications running on Linux and QNX, providing developers with a mechanism for debugging CUDA applications running on actual hardware.
Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Edition Nsight Eclipse Plugins Edition getting started guide Nsight Systems The documentation for Nsight Systems.
Nsight Compute The NVIDIA Nsight Compute is the next-generation interactive kernel profiler for CUDA applications.
It provides detailed performance metrics and API debugging via a user interface and command line tool.
White Papers  Floating Point and IEEE 754 A number of issues related to floating point accuracy and compliance are a frequent source of confusion on both CPUs and GPUs.
The purpose of this white paper is to discuss the most common issues related to NVIDIA GPUs and to supplement the documentation in the CUDA C++ Programming Guide.
Incomplete-LU and Cholesky Preconditioned Iterative Methods In this white paper we show how to use the cuSPARSE and cuBLAS libraries to achieve a 2x speedup over CPU in the incomplete-LU and Cholesky preconditioned iterative methods.
We focus on the Bi-Conjugate Gradient Stabilized and Conjugate Gradient iterative methods, that can be used to solve large sparse nonsymmetric and symmetric positive definite linear systems, respectively.
Also, we comment on the parallel sparse triangular solve, which is an essential building block in these algorithms.
Application Notes  CUDA for Tegra This application note provides an overview of NVIDIA® Tegra® memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the Tegra® integrated GPU (iGPU).
libdevice User’s Guide The libdevice library is an LLVM bitcode library that implements common functions for GPU kernels.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact © Copyright 2007-2024, NVIDIA Corporation & affiliates.
CUDA 12.5 Update 1 Release Notes v12.5 | PDF | Archive NVIDIA CUDA Toolkit Release Notes The Release Notes for the CUDA Toolkit.
CUDA 12.5 Update 1 Release Notes  The release notes for the NVIDIA® CUDA® Toolkit can be found online at https: docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html .
Note The release notes have been reorganized into two major sections: the general CUDA release notes, and the CUDA libraries release notes including historical information for 12.x releases. 1.1. CUDA Toolkit Major Component Versions  CUDA Components Starting with CUDA 11, the various components in the toolkit are versioned independently.
For more information various GPU products that are CUDA capable, visit https: developer.nvidia.com/cuda-gpus .
The CUDA driver is backward compatible, meaning that applications compiled against a particular version of the CUDA will continue to work on subsequent (later) driver releases.
More information on compatibility can be found at https: docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-and-upgrades .
Note : Starting with CUDA 11.0, the toolkit components are individually versioned, and the toolkit itself is versioned as shown in the table below.
CUDA minor version compatibility is described in detail in https: docs.nvidia.com/deploy/cuda-compatibility/index.html Table 2 CUDA Toolkit and Minimum Required Driver Version for CUDA Minor Version Compatibility  CUDA Toolkit Minimum Required Driver Version for CUDA Minor Version Compatibility* Linux x86_64 Driver Version Windows x86_64 Driver Version CUDA 12.x >=525.60.13 >=528.33 CUDA 11.8.x CUDA 11.7.x CUDA 11.6.x CUDA 11.5.x CUDA 11.4.x CUDA 11.3.x CUDA 11.2.x CUDA 11.1.x >=450.80.02 >=452.39 CUDA 11.0 (11.0.3) >=450.36.06** >=451.22** * Using a Minimum Required Version that is different from Toolkit Driver Version could be allowed in compatibility mode – please read the CUDA Compatibility Guide for details.
** CUDA 11.0 was released with an earlier driver version, but by upgrading to Tesla Recommended Drivers 450.80.02 (Linux) / 452.39 (Windows), minor version compatibility is possible across the CUDA 11.x family of toolkits.
The version of the development NVIDIA GPU Driver packaged in each CUDA Toolkit release is shown below.
Note that this driver is for development purposes and is not recommended for use in production with Tesla GPUs.
For running CUDA applications in production with Tesla GPUs, it is recommended to download the latest driver for Tesla GPUs from the NVIDIA driver downloads site at https: www.nvidia.com/drivers .
During the installation of the CUDA Toolkit, the installation of the NVIDIA driver may be skipped on Windows (when using the interactive or silent installation) or on Linux (by using meta packages).
For more information on customizing the install process on Windows, see https: docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html#install-cuda-software .
For meta packages on Linux, see https: docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-metas . 1.2. New Features  This section lists new general CUDA and CUDA compilers features.
1.2.1. General CUDA  In an upcoming CUDA release the NVIDIA Open GPU kernel module flavor will be the default and recommended installation option.
End-users with Maxwell, Pascal, or Volta GPUs may need to take action to install the NVIDIA proprietary kernel modules.
More details can be found here . 1.2.2. CUDA Compiler  For changes to PTX, refer to https: docs.nvidia.com/cuda/parallel-thread-execution/#ptx-isa-version-8-5 .
1.2.3. CUDA Developer Tools  For changes to nvprof and Visual Profiler, see the changelog .
For new features, improvements, and bug fixes in Nsight Visual Studio Edition, see the changelog .
For new features, improvements, and bug fixes in CUDA-GDB, see the changelog . 1.3. Resolved Issues  1.3.1.
CUDA Compiler  Resolved an issue found when trying sm89 ptx of FP8 gemm kernel compiled by 12.4 when run on an sm90 device.
Resolved an issue in which nvcc failed to compile any CUDA code when specifying C++20 with CUDA 12.5 and Visual Studio 2022 17.10.0.
Also added a new environment variable NVCC_REPORT_ALLERROR to emit error messages if the error is coming from a system header, instead of aborting the compiler.
Resolved a compiler issue that caused different results when compiling with the -G flag than without the flag.
Fixed the incorrect control flow transformation in the compiler caused by optimizations applied to multi-block loops.
Resolved issues seen when compiling cuBLASDx device functions, in some conditions leading to “Misaligned shared or local address”.
Fix to correct the calculation of write-after-read hazard latency. 1.4. Known Issues and Limitations  Runfile will not be supported for Amazon Linux 2023.
Launching Cooperative Group kernels with MPS is not supported on Tegra platforms. 1.5. Deprecated or Dropped Features  Features deprecated in the current release of the CUDA software still work in the current release, but their documentation may have been removed, and they will become officially unsupported in a future release.
We recommend that developers employ alternative solutions to these features in their software. 1.5.1. Deprecated or Dropped Architectures  NVIDIA CUDA support for the PowerPC architecture is removed in CUDA 12.5.
1.5.2. Deprecated Operating Systems  NVIDIA CUDA support for Red Hat Enterprise Linux 7 and CentOS 7 is removed in CUDA 12.5.
Support for Microsoft Windows 10 21H2 and Microsoft Windows 10 21H2 (SV1) is deprecated. 1.5.3. Deprecated Toolchains  CUDA Toolkit 12.4 deprecated support for the following host compilers: Microsoft Visual C/C++ (MSVC) 2017 All GCC versions prior to GCC 7.3 1.5.4.
It will be dropped in an upcoming release. 2. CUDA Libraries  This section covers CUDA Libraries release notes for 12.x releases.
CUDA Math Libraries toolchain uses C++11 features, and a C++11-compatible standard library (libstdc++ >= 20150422) is required on the host. 2.1. cuBLAS Library  2.1.1.
cuBLAS: Release 12.5 Update 1  New Features Performance improvement to matrix multiplication targeting large language models, specifically for small batch sizes on Hopper GPUs.
Known Issues The bias epilogue (without ReLU or GeLU) may be not supported on Hopper GPUs for strided batch cases.
Resolved Issues Under rare circumstances, executing SYMM/HEMM concurrently with GEMM on Hopper GPUs might have caused race conditions in the host code, which could lead to an Illegal Memory Access CUDA error.
cublasLtMatmul could produce an Illegal Instruction CUDA error on Pascal GPUs under the following conditions: batch is greater than 1, and beta is not equal to 0, and the computations are out-of-place (C != D). 2.1.2. cuBLAS: Release 12.5  New Features cuBLAS adds an experimental API to support mixed precision grouped batched GEMMs.
This enables grouped batched GEMMs with FP16 or BF16 inputs/outputs with the FP32 compute type.
Known Issues cublasLtMatmul ignores inputs to CUBLASLT_MATMUL_DESC_D_SCALE_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER if the elements of the respective matrix are not of FP8 types.
Resolved Issues cublasLtMatmul ignored the mismatch between the provided scale type and the implied by the documentation, assuming the latter.
For instance, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 and produce incorrect results.
cuBLAS SYMV failed for large n dimension: 131072 and above for ssymv, 92673 and above for csymv and dsymv, and 65536 and above for zsymv. 2.1.3. cuBLAS: Release 12.4 Update 1  Known Issues Setting a cuBLAS handle stream to cudaStreamPerThread and setting the workspace via cublasSetWorkspace will cause any subsequent cublasSetWorkspace calls to fail.
cublasLtMatmul ignores mismatches between the provided scale type and the scale type implied by the documentation and assumes the latter.
For example, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 which can produce incorrect results.
Resolved Issues cublasLtMatmul ignored the CUBLASLT_MATMUL_DESC_AMAX_D_POINTER for unsupported configurations instead of returning an error.
In particular, computing absolute maximum of D is currently supported only for FP8 Matmul when the output data type is also FP8 ( CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2 ).
Reduced host-side overheads for some of the cuBLASLt APIs: cublasLtMatmul() , cublasLtMatmulAlgoCheck() , and cublasLtMatmulAlgoGetHeuristic() .
cublasLtMatmul() and cublasLtMatmulAlgoGetHeuristic() could have resulted in floating point exceptions (FPE) on some Hopper-based GPUs, including Multi-Instance GPU (MIG).
The issue was introduced in cuBLAS 11.8. 2.1.4. cuBLAS: Release 12.4  New Features cuBLAS adds experimental APIs to support grouped batched GEMM for single precision and double precision.
Grouped batch mode allows you to concurrently solve GEMMs of different dimensions (m, n, k), leading dimensions (lda, ldb, ldc), transpositions (transa, transb), and scaling factors (alpha, beta).
Known Issues When the current context has been created using cuGreenCtxCreate() , cuBLAS does not properly detect the number of SMs available.
The user may provide the corrected SM count to cuBLAS using an API such as cublasSetSmCountTarget() .
BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE .
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU}_BGRAD could out-of-bound access the workspace.
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU} could produce illegal memory access if no workspace is provided.
When captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync .
However, as there is currently no support for memory nodes in child graphs or graphs launched from the device , attempts to capture cuBLAS routines in such scenarios may fail.
To avoid this issue, use the cublasSetWorkspace() function to provide user-owned workspace memory. 2.1.5. cuBLAS: Release 12.3 Update 1  New Features Improved performance of heuristics cache for workloads that have a high eviction rate.
Known Issues BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE .
You may encounter the following issues: (1) HER{,2,X,K,2K} may zero the imaginary part on the diagonal elements of the output matrix; and (2) HER{,2,X,K,2K}, SYR{,2,X,K,2K} and others may produce NaN resulting from performing computation on matrices A and B which would otherwise be skipped.
If strict compliance with BLAS is required, the user may manually check for alpha value before invoking the functions or switch to CUBLAS_POINTER_MODE_HOST .
Resolved Issues cuBLASLt matmul operations might have computed the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.
When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or CUDA Toolkit 12.3, matrix multiply descriptors initialized using cublasLtMatmulDescInit() sometimes did not respect attribute changes using cublasLtMatmulDescSetAttribute() .
Fixed creation of cuBLAS or cuBLASLt handles on Hopper GPUs under the Multi-Process Service (MPS).
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} might have returned incorrect results for the bias gradient. 2.1.6. cuBLAS: Release 12.3  New Features Improved performance on NVIDIA L40S Ada GPUs.
Known Issues cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.
When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or later, matrix multiply descriptors initialized using cublasLtMatmulDescInit() may not respect attribute changes using cublasLtMatmulDescSetAttribute() .
To workaround this issue, create the matrix multiply descriptor using cublasLtMatmulDescCreate() instead of cublasLtMatmulDescInit() . 2.1.7. cuBLAS: Release 12.2 Update 2  New Features cuBLASLt will now attempt to decompose problems that cannot be run by a single gemm kernel.
It does this by partitioning the problem into smaller chunks and executing the gemm kernel multiple times.
This improves functional coverage for very large m, n, or batch size cases and makes the transition from the cuBLAS API to the cuBLASLt API more reliable. 2.1.8. cuBLAS: Release 12.2  Known Issues cuBLAS initialization fails on Hopper architecture GPUs when MPS is in use with CUDA_MPS_ACTIVE_THREAD_PERCENTAGE set to a value less than 100%.
Some Hopper kernels produce incorrect results for batched matmuls with CUBLASLT_EPILOGUE_RELU_BIAS or CUBLASLT_EPILOGUE_GELU_BIAS and a non-zero CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE .
The kernels apply the first batch’s bias vector to all batches. 2.1.9. cuBLAS: Release 12.1 Update 1  New Features Support for FP8 on NVIDIA Ada GPUs.
This is useful in some rare cases where certain CPU instructions used by cuBLASLt heuristics negatively impact CPU performance.
Known Issues When creating a matrix layout using the cublasLtMatrixLayoutCreate() function, the object pointed at by cublasLtMatrixLayout_t is smaller than cublasLtMatrixLayoutOpaque_t (but enough to hold the internal structure).
As a result, the object should not be dereferenced or copied explicitly, as this might lead to out of bound accesses.
If one needs to serialize the layout or copy it, it is recommended to manually allocate an object of size sizeof(cublasLtMatrixLayoutOpaque_t) bytes, and initialize it using cublasLtMatrixLayoutInit() function.
The issue will be fixed in future releases by ensuring that cublasLtMatrixLayoutCreate() allocates at least sizeof(cublasLtMatrixLayoutOpaque_t) bytes. 2.1.10. cuBLAS: Release 12.0 Update 1  New Features Improved performance on NVIDIA H100 SXM and NVIDIA H100 PCIe GPUs.
Known Issues For optimal performance on NVIDIA Hopper architecture, cuBLAS needs to allocate a bigger internal workspace (64 MiB) than on the previous architectures (8 MiB).
A possible workaround is to set the CUBLAS_WORKSPACE_CONFIG environment variable to :32768:2 when running cuBLAS on NVIDIA Hopper architecture.
Resolved Issues Reduced cuBLAS host-side overheads caused by not using the cublasLt heuristics cache.
Added forward compatible single precision complex GEMM that does not require workspace. 2.1.11. cuBLAS: Release 12.0  New Features cublasLtMatmul now supports FP8 with a non-zero beta.
Added more Hopper-specific kernels for cublasLtMatmul with epilogues: CUBLASLT_EPILOGUE_BGRAD{A,B} CUBLASLT_EPILOGUE_{RELU,GELU}_AUX CUBLASLT_EPILOGUE_D{RELU,GELU} Improved Hopper performance on arm64-sbsa by adding Hopper kernels that were previously supported only on the x86_64 architecture for Windows and Linux.
Known Issues There are no forward compatible kernels for single precision complex gemms that do not require workspace.
Resolved Issues Fixed an issue on NVIDIA Ampere architecture and newer GPUs where cublasLtMatmul with epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} and a nontrivial reduction scheme (that is, not CUBLASLT_REDUCTION_SCHEME_NONE ) could return incorrect results for the bias gradient.
cublasLtMatmul for gemv-like cases (that is, m or n equals 1) might ignore bias with the CUBLASLT_EPILOGUE_RELU_BIAS and CUBLASLT_EPILOGUE_BIAS epilogues.
Removed: CUBLAS_MATMUL_STAGES_16x80 and CUBLAS_MATMUL_STAGES_64x80 from cublasLtMatmulStages_t .
cublasLt3mMode_t , CUBLASLT_MATMUL_PREF_MATH_MODE_MASK , and CUBLASLT_MATMUL_PREF_GAUSSIAN_MODE_MASK from cublasLtMatmulPreferenceAttributes_t .
CUBLASLT_MATMUL_PREF_POINTER_MODE_MASK , CUBLASLT_MATMUL_PREF_EPILOGUE_MASK , and CUBLASLT_MATMUL_PREF_SM_COUNT_TARGET from cublasLtMatmulPreferenceAttributes_t .
This mask was only applicable to CUBLASLT_MATMUL_PREF_MATH_MODE_MASK which was removed. 2.2. cuFFT Library  2.2.1.
cuFFT: Release 12.5  New Features Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in R2C and C2R FFTs for many sizes .
We recommend testing your R2C / C2R use cases with and without JIT LTO kernels and comparing the resulting performance.
You can enable JIT LTO kernels using the per-plan properties cuFFT API. 2.2.2. cuFFT: Release 12.4 Update 1  Resolved Issues A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header ( cufftXt.h ) in CUDA 12.4.
This routine has now been removed from the header. 2.2.3. cuFFT: Release 12.4  New Features Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in FFTs with 64-bit indexing.
These new routines can be leveraged to give users more control over the behavior of cuFFT.
Improved accuracy for certain single-precision (fp32) FFT cases, especially involving FFTs for larger sizes.
Known Issues A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header ( cufftXt.h ).
This routine is not supported by cuFFT, and will be removed from the header in a future release.
Resolved Issues Fixed an issue that could cause overwriting of user data when performing out-of-place real-to-complex (R2C) transforms with user-specified output strides (i.e.
Fixed inconsistent behavior between libcufftw and FFTW when both inembed and onembed are nullptr / NULL .
From now on, as in FFTW, passing nullptr / NULL as inembed/onembed parameter is equivalent to passing n, that is, the logical size for that dimension. 2.2.4. cuFFT: Release 12.3 Update 1  Known Issues Executing a real-to-complex (R2C) or complex-to-real (C2R) plan in a context different to the one used to create the plan could cause undefined behavior.
Resolved Issues Complex-to-complex (C2C) execution functions ( cufftExec and similar) now properly error-out in case of error during kernel launch, for example due to a missing CUDA context. 2.2.5. cuFFT: Release 12.3  New Features Callback kernels are more relaxed in terms of resource usage, and will use fewer registers.
Improved accuracy for double precision prime and composite FFT sizes with factors larger than 127.
Slightly improved planning times for some FFT sizes. 2.2.6. cuFFT: Release 12.2  New Features cufftSetStream can be used in multi-GPU plans with a stream from any GPU context, instead of from the primary context of the first GPU listed in cufftXtSetGPUs .
The improved performance spans hundreds of single precision and double precision cases for FFTs with contiguous data layout, across multiple GPU architectures (from Maxwell to Hopper GPUs) via PTX JIT.
Resolved Issues cuFFT no longer exhibits a race condition when threads simultaneously create and access plans with more than 1023 plans alive.
cuFFT no longer exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently. 2.2.7. cuFFT: Release 12.1 Update 1  Known Issues cuFFT exhibits a race condition when one thread calls cufftCreate (or cufftDestroy ) and another thread calls any API (except cufftCreate or cufftDestroy ), and when the total number of plans alive exceeds 1023.
cuFFT exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently on different plans. 2.2.8. cuFFT: Release 12.1  New Features Improved performance on Hopper GPUs for hundreds of FFTs of sizes ranging from 14 to 28800.
The improved performance spans over 542 cases across single and double precision for FFTs with contiguous data layout.
Known Issues Starting from CUDA 11.8, CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms.
An upcoming release will update the cuFFT callback implementation, removing this limitation.
cuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11.4.
Resolved Issues cuFFT no longer produces errors with compute-sanitizer at program exit if the CUDA context used at plan creation was destroyed prior to program exit. 2.2.9. cuFFT: Release 12.0 Update 1  Resolved Issues Scratch space requirements for multi-GPU, single-batch, 1D FFTs were reduced.
2.2.10. cuFFT: Release 12.0  New Features PTX JIT kernel compilation allowed the addition of many new accelerated cases for Maxwell, Pascal, Volta and Turing architectures.
Resolved Issues cuFFT plans had an unintentional small memory overhead (of a few kB) per plan.
cuSOLVER: Release 12.5 Update 1  Resolved Issues The potential out-of-bound accesses on bufferOnDevice by calls of cusolverDnXlarft have been resolved. 2.3.2. cuSOLVER: Release 12.5  New Features Performance improvements of cusolverDnXgesvd and cusolverDngesvd if jobu != 'N' or jobvt != 'N' .
Known Issues With CUDA Toolkit 12.4 Update 1, values ldt > k in calls of cusolverDnXlarft can result in out-of-bound memory accesses on bufferOnDevice .
As a workaround it is possible to allocate a larger device workspace buffer of size workspaceInBytesOnDevice=ALIGN_32((ldt*k + n*k)*sizeofCudaDataType(dataTypeT)) , with auto ALIGN_32 = []( int64_t val ) { return (( val + 31 ) / 32 ) * 32 ; }; and auto sizeofCudaDataType = []( cudaDataType dt ) { if ( dt == CUDA_R_32F ) return sizeof ( float ); if ( dt == CUDA_R_64F ) return sizeof ( double ); if ( dt == CUDA_C_32F ) return sizeof ( cuComplex ); if ( dt == CUDA_C_64F ) return sizeof ( cuDoubleComplex ); }; 2.3.3.
cuSOLVER: Release 12.4 Update 1  New Features The performance of cusolverDnXlarft has been improved.
The change in cusolverDnXlarft also results in a modest speedup in cusolverDnormqr , cusolverDnormtr , and cusolverDnXsyevd .
The job configuration that computes both left and right singular vectors is up to 1.5x faster.
Resolved Issues cusolverDnXtrtri_bufferSize now returns the correct workspace size in bytes.
Deprecations Using long-deprecated cusolverDnPotrf , cusolverDnPotrs , cusolverDnGeqrf , cusolverDnGetrf , cusolverDnGetrs , cusolverDnSyevd , cusolverDnSyevdx , cusolverDnGesvd , and their accompanying bufferSize functions will result in a deprecation warning.
The warning can be turned off by using the -DDISABLE_CUSOLVER_DEPRECATED flag while compiling; however, users should use cusolverDnXpotrf , cusolverDnXpotrs , cusolverDnXgeqrf , cusolverDnXgetrf , cusolverDnXgetrs , cusolverDnXsyevd , cusolverDnXsyevdx , cusolverDnXgesvd , and the corresponding bufferSize functions instead. 2.3.4. cuSOLVER: Release 12.4  New Features cusolverDnXlarft and cusolverDnXlarft_bufferSize APIs were introduced.
cusolverDnXlarft forms the triangular factor of a real block reflector, while cusolverDnXlarft_bufferSize returns its required workspace sizes in bytes.
Known Issues cusolverDnXtrtri_bufferSize` returns an incorrect required device workspace size.
As a workaround the returned size can be multiplied by the size of the data type (for example, 8 bytes if matrix A is of type double) to obtain the correct workspace size. 2.3.5. cuSOLVER: Release 12.2 Update 2  Resolved Issues Fixed an issue with cusolverDngesvd() , cusolverDnGesvd() , and cusolverDnXgesvd() , which could cause wrong results for matrices larger than 18918 if jobu or jobvt was unequal to ‘ N ’.
2.3.6. cuSOLVER: Release 12.2  New Features A new API to ensure deterministic results or allow non-deterministic results for improved performance.
Affected functions are: cusolverDngeqrf() , cusolverDnsyevd() , cusolverDnsyevdx() , cusolverDngesvdj() , cusolverDnXgeqrf() , cusolverDnXsyevd() , cusolverDnXsyevdx() , cusolverDnXgesvdr() , and cusolverDnXgesvdp() .
Known Issues Concurrent executions of cusolverDngetrf() or cusolverDnXgetrf() in different non-blocking CUDA streams on the same device might result in a deadlock. 2.4. cuSPARSE Library  2.4.1.
cuSPARSE: Release 12.5 Update 1  New Features Added support for BSR format in cusparseSpMM .
Resolved Issues cusparseSpMM() would sometimes get incorrect results when alpha=0 , num_batches>1 , batch_stride indicates that there is padding between batches.
cusparseSpMM_bufferSize() would return the wrong size when the sparse matrix is Blocked Ellpack and the dense matrices have only a single column (n=1).
cusparseCreateSlicedEll would return an error when the slice size is greater than the matrix number of rows.
Sliced-ELLPACK cusparseSpSV_analysis() failed due to insufficient resources for some matrices and some slice sizes. 2.4.2. cuSPARSE: Release 12.5  New Features Added support for mixed input types in SpMV: single precision input matrix, double precision input vector, double precision output vector.
Resolved Issues cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes. 2.4.3. cuSPARSE: Release 12.4  New Features Added the preprocessing step for sparse matrix-vector multiplication cusparseSpMV_preprocess() .
Added a new API cusparseSpSM_updateMatrix() to update the sparse matrix between the analysis and solving phase of cusparseSpSM() .
Known Issues cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes.
Resolved Issues cusparseSpVV() provided incorrect results when the sparse vector has many non-zeros. 2.4.4. cuSPARSE: Release 12.3 Update 1  New Features Added support for block sizes of 64 and 128 in cusparseSDDMM() .
Added a preprocessing step cusparseSDDMM_preprocess() for BSR cusparseSDDMM() that helps improve performance of the main computing stage. 2.4.5. cuSPARSE: Release 12.3  New Features The cusparseSpSV_bufferSize() and cusparseSpSV_analysis() routines now accept NULL pointers for the dense vector.
The cusparseSpSM_bufferSize() and cusparseSpSM_analysis() routines now accept dense matrix descriptors with NULL pointer for values.
Known Issues The cusparseSpSV_analysis() and cusparseSpSM_analysis() routines are blocking calls/not asynchronous.
Wrong results can occur for cusparseSpSV() using sliced ELLPACK format and transpose/transpose conjugate operation on matrix A.
Fixed an issue that caused cusparseSpSV_analysis() to hang sometimes in a multi-thread environment.
Fixed an issue with cusparseSpSV() and cusparseSpSV() that sometimes yielded wrong output when the output vector/matrix or input matrix contained NaN. 2.4.6. cuSPARSE: Release 12.2 Update 1  New Features The library now provides the opportunity to dump sparse matrices to files during the creation of the descriptor for debugging purposes.
Resolved Issues Removed CUSPARSE_SPMM_CSR_ALG3 fallback to avoid confusion in the algorithm selection process.
cusparseSpMV() now checks the validity of the buffer pointer only when it is strictly needed.
A compile-time warning has been added to all of them. 2.4.7. cuSPARSE: Release 12.1 Update 1  New Features Introduced Block Sparse Row (BSR) sparse matrix storage for the Generic APIs with support for SDDMM routine ( cusparseSDDMM ).
Introduced Sliced Ellpack (SELL) sparse matrix storage format for the Generic APIs with support for sparse matrix-vector multiplication ( cusparseSpMV ) and triangular solver with a single right-hand side ( cusparseSpSV ).
Added a new API call ( cusparseSpSV_updateMatrix ) to update matrix values and/or the matrix diagonal in the sparse triangular solver with a single right-hand side after the analysis step. 2.4.8. cuSPARSE: Release 12.0 Update 1  New Features cusparseSDDMM() now supports mixed precision computation.
Improved cusparseSpMM() alg2 mixed-precision performance on some matrices on NVIDIA Ampere architecture GPUs.
cusparseSpSV() and cusparseSpSM() now support in-place computation, namely the output and input vectors/matrices have the same memory address.
Resolved Issues cusparseSpSM() could produce wrong results if the leading dimension (ld) of the RHS matrix is greater than the number of columns/rows. 2.4.9. cuSPARSE: Release 12.0  New Features JIT LTO functionalities ( cusparseSpMMOp() ) switched from driver to nvJitLto library.
Starting from CUDA 12.0 the user needs to link to libnvJitLto.so , see cuSPARSE documentation .
Now the Generic APIs interface clearly declares when a descriptor and its data are modified by the cuSPARSE functions.
The first algorithm computes a strict bound on the number of intermediate product, while the second one allows partitioning the computation in chunks.
Deprecations Removed deprecated CUDA 11.x APIs, enumerators, and descriptors. 2.5. Math Library  2.5.1.
CUDA Math: Release 12.5  Known Issues As a result of ongoing testing we updated the interval bounds in which double precision lgamma() function may experience greater than the documented 4 ulp accuracy loss.
This finding is applicable to CUDA 12.5 and all previous versions. 2.5.2. CUDA Math: Release 12.4  Resolved Issues Host-specific code in cuda_fp16/bf16 headers is now free from type-punning and shall work correctly in the presence of optimizations based on strict-aliasing rules.
2.5.3. CUDA Math: Release 12.3  New Features Performance of SIMD Integer CUDA Math APIs was improved.
Resolved Issues The __hisinf() Math APIs from cuda_fp16.h and cuda_bf16.h headers were silently producing wrong results if compiled with the -std=c++20 compiler option because of an underlying nvcc compiler issue, resolved in version 12.3.
Known Issues Users of cuda_fp16.h and cuda_bf16.h headers are advised to disable host compilers strict aliasing rules based optimizations (e.g.
pass -fno-strict-aliasing to host GCC compiler) as these may interfere with the type-punning idioms used in the __half , __half2 , __nv_bfloat16 , __nv_bfloat162 types implementations and expose the user program to undefined behavior.
Note, the headers suppress GCC diagnostics through: #pragma GCC diagnostic ignored -Wstrict-aliasing .
This behavior may improve in future versions of the headers. 2.5.4. CUDA Math: Release 12.2  New Features CUDA Math APIs for __half and __nv_bfloat16 types received usability improvements, including host side support for many of the arithmetic operations and conversions.
__half and __nv_bfloat16 types have implicit conversions to/from integral types, which are now available with host compilers by default.
To opt-out user may want to define the following macros (these macros will be removed in the future CUDA release): __CUDA_FP16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ __CUDA_BF16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ Resolved Issues During ongoing testing, NVIDIA identified that due to an algorithm error the results of 64-bit floating-point division in default round-to-nearest-even mode could produce spurious overflow to infinity.
NVIDIA recommends that all developers requiring strict IEEE754 compliance update to CUDA Toolkit 12.2 or newer.
The affected algorithm was present in both offline compilation as well as just-in-time (JIT) compilation.
As JIT compilation is handled by the driver, NVIDIA recommends updating to driver version greater than or equal to R535 (R536 on Windows) when IEEE754 compliance is required and when using JIT.
Updated the observed worst case error bounds for single precision intrinsic functions __expf() , __exp10f() and double precision functions asinh() , acosh() . 2.5.5. CUDA Math: Release 12.1  New Features Performance and accuracy improvements in atanf , acosf , asinf , sinpif , cospif , powf , erff , and tgammaf .
2.5.6. CUDA Math: Release 12.0  New Features Introduced new integer/fp16/bf16 CUDA Math APIs to help expose performance benefits of new DPX instructions.
Known Issues Double precision inputs that cause the double precision division algorithm in the default ‘round to nearest even mode’ produce spurious overflow: an infinite result is delivered where DBL_MAX 0x7FEF_FFFF_FFFF_FFFF is expected.
Deprecations All previously deprecated undocumented APIs are removed from CUDA 12.0. 2.6. NVIDIA Performance Primitives (NPP)  2.6.1.
NPP: Release 12.4  New Features Enhanced large file support with size_t . 2.6.2. NPP: Release 12.0  Deprecations Deprecating non-CTX API support from next release.
Resolved Issues A performance issue with the NPP ResizeSqrPixel API is now fixed and shows improved performance. 2.7. nvJPEG Library  2.7.1.
nvJPEG: Release 12.4  New Features IDCT performance optimizations for single image CUDA decode.
Zero Copy behavior has been changed: Setting NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY flag will no longer enable NVJPEG_FLAGS_REDUCED_MEMORY_DECODE . 2.7.2. nvJPEG: Release 12.3 Update 1  New Features New APIs: nvjpegBufferPinnedResize and nvjpegBufferDeviceResize which can be used to resize pinned and device buffers before using them.
2.7.3. nvJPEG: Release 12.2  New Features Added support for JPEG Lossless decode (process 14, FO prediction).
nvJPEG is now supported on L4T. 2.7.4. nvJPEG: Release 12.0  New Features Immproved the GPU Memory optimisation for the nvJPEG codec.
Resolved Issues An issue that causes runtime failures when nvJPEGDecMultipleInstances was tested with a large number of threads is resolved.
Known Issues Backend NVJPEG_BACKEND_GPU_HYBRID - Unable to handle bistreams with extra scans lengths.
Deprecations The reuse of Huffman table in Encoder ( nvjpegEncoderParamsCopyHuffmanTables ).
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
CUDA 11.6 Features v12.5 | PDF | Archive NVIDIA CUDA Features Archive The list of CUDA features by release.
A future CUDA release will have the Nsight Visual Studio installer with VS2022 support integrated into it. 1.1.2. New instructions in public PTX  New instructions for bit mask creation—BMSK, and sign extension—SZEXT, are added to the public PTX ISA.
You can find documentation for these instructions in the PTX ISA guide: BMSK and SZEXT . 1.1.3. Unused Kernel Optimization  In CUDA 11.5, unused kernel pruning was introduced with the potential benefits of reducing binary size and improving performance through more efficient optimizations.
As mentioned in the 11.5 blog, there is an opt-out flag that can be used in case it becomes necessary for debug purposes or for other special situations.
New -arch=native option  In addition to the -arch=all and -arch=all-major options added in CUDA 11.5, NVCC introduced -arch= native in CUDA 11.5 update 1.
This -arch=native option is a convenient way for users to let NVCC determine the right target architecture to compile the CUDA device code to based on the GPU installed on the system.
This can be particularly helpful for testing when applications are run on the same system they are compiled in. 1.1.5. Generate PTX from nvlink:  Using the following command line, device linker, nvlink will produce PTX as an output in addition to CUBIN: nvcc -dlto -dlink -ptx Device linking by nvlink is the final stage in the CUDA compilation process.
Applications that have multiple source translation units have to be compiled in separate compilation mode.
LTO (introduced in CUDA 11.4) allowed nvlink to perform optimizations at device link time instead of at compile time so that separately compiled applications with several translation units can be optimized to the same level as whole program compilations with a single translation unit.
However, without the option to output PTX, applications that cared about forward compatibility of device code could not benefit from Link Time Optimization or had to constrain the device code to a single source file.
With the option for nvlink that performs LTO to generate the output in PTX, customer applications that require forward compatibility across GPU architectures can span across multiple files and can also take advantage of Link Time Optimization. 1.1.6. Bullseye support  NVCC compiled source code now works with the code coverage tool Bullseye.
Code coverage for device function is not supported through bullseye. 1.1.7. INT128 developer tool support  In 11.5, CUDA C++ support for 128 bit was added.
With the latest version of libcu++, int 128 data datype is supported by math functions. 2. Notices  2.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 2.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 2.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
CUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kits 2.1.
License Agreement for NVIDIA Software Development Kits v12.5 | PDF | Archive End User License Agreement NVIDIA Software License Agreement and CUDA Supplement to Software License Agreement.
Last updated: October 8, 2021 The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools.
If you do not agree with the terms and conditions of the license agreement, then do not download or use the software.
Preface The Software License Agreement in Chapter 1 and the Supplement in Chapter 2 contain license terms and conditions that govern the use of NVIDIA CUDA toolkit.
By accepting this agreement, you agree to comply with all the terms and conditions applicable to the product(s) included herein.
NVIDIA Driver Description This package contains the operating system driver and fundamental system software components for NVIDIA GPUs.
NVIDIA CUDA Toolkit Description The NVIDIA CUDA Toolkit provides command-line and graphical tools for building, debugging and optimizing the performance of applications accelerated by NVIDIA GPUs, runtime and math libraries, and documentation including programming guides, user manuals, and API references.
Default Install Location of CUDA Toolkit Windows platform: %ProgramFiles%\NVIDIA GPU Computing Toolkit\CUDA\v#.# Linux platform: /usr/local/cuda-#.# Mac platform: /Developer/NVIDIA/CUDA-#.# NVIDIA CUDA Samples Description CUDA Samples are now located in https: github.com/nvidia/cuda-samples , which includes instructions for obtaining, building, and running the samples.
NVIDIA Nsight Visual Studio Edition (Windows only) Description NVIDIA Nsight Development Platform, Visual Studio Edition is a development environment integrated into Microsoft Visual Studio that provides tools for debugging, profiling, analyzing and optimizing your GPU computing and graphics applications.
Default Install Location of Nsight Visual Studio Edition Windows platform: %ProgramFiles(x86)%\NVIDIA Corporation\Nsight Visual Studio Edition #.# 1.
License Agreement for NVIDIA Software Development Kits  Important Notice—Read before downloading, installing, copying or using the licensed software: This license agreement, including exhibits attached (“Agreement”) is a legal agreement between you and NVIDIA Corporation (“NVIDIA”) and governs your use of a NVIDIA software development kit (“SDK”).
Each SDK has its own set of software and materials, but here is a description of the types of items that may be included in a SDK: source code, header files, APIs, data sets and assets (examples include images, textures, models, scenes, videos, native API input/output files), binary software, sample code, libraries, utility programs, programming code and documentation.
This Agreement can be accepted only by an adult of legal age of majority in the country in which the SDK is used.
If you are entering into this Agreement on behalf of a company or other legal entity, you represent that you have the legal authority to bind the entity to this Agreement, in which case “you” will mean the entity you represent.
If you don’t have the required age or authority to accept this Agreement, or if you don’t accept all the terms and conditions of this Agreement, do not download, install or use the SDK.
You agree to use the SDK only for purposes that are permitted by (a) this Agreement, and (b) any applicable law, regulation or generally accepted practices or guidelines in the relevant jurisdictions. 1.1. License  1.1.1.
License Grant  Subject to the terms of this Agreement, NVIDIA hereby grants you a non-exclusive, non-transferable license, without the right to sublicense (except as expressly provided in this Agreement) to: Install and use the SDK, Modify and create derivative works of sample source code delivered in the SDK, and Distribute those portions of the SDK that are identified in this Agreement as distributable, as incorporated in object code format into a software application that meets the distribution requirements indicated in this Agreement. 1.1.2. Distribution Requirements  These are the distribution requirements for you to exercise the distribution grant: Your application must have material additional functionality, beyond the included portions of the SDK.
The following notice shall be included in modifications and derivative works of sample source code distributed: “This software contains source code provided by NVIDIA Corporation.” Unless a developer tool is identified in this Agreement as distributable, it is delivered for your internal use only.
The terms under which you distribute your application must be consistent with the terms of this Agreement, including (without limitation) terms relating to the license grant and license restrictions and protection of NVIDIA’s intellectual property rights.
Additionally, you agree that you will protect the privacy, security and legal rights of your application users.
You agree to notify NVIDIA in writing of any known or suspected distribution or use of the SDK not in compliance with the requirements of this Agreement, and to enforce the terms of your agreements with respect to distributed SDK. 1.1.3. Authorized Users  You may allow employees and contractors of your entity or of your subsidiary(ies) to access and use the SDK from your secure network to perform work on your behalf.
If you are an academic institution you may allow users enrolled or employed by the academic institution to access and use the SDK from your secure network.
You are responsible for the compliance with the terms of this Agreement by your authorized users.
If you become aware that your authorized users didn’t follow the terms of this Agreement, you agree to take reasonable steps to resolve the non-compliance and prevent new occurrences. 1.1.4. Pre-Release SDK  The SDK versions identified as alpha, beta, preview or otherwise as pre-release, may not be fully functional, may contain errors or design flaws, and may have reduced or different security, privacy, accessibility, availability, and reliability standards relative to commercial versions of NVIDIA software and materials.
Use of a pre-release SDK may result in unexpected results, loss of data, project delays or other unpredictable damage or loss.
You may use a pre-release SDK at your own risk, understanding that pre-release SDKs are not intended for use in production or business-critical systems.
NVIDIA may also choose to abandon development and terminate the availability of a pre-release SDK at any time without liability. 1.1.5. Updates  NVIDIA may, at its option, make available patches, workarounds or other updates to this SDK.
Unless the updates are provided with their separate governing terms, they are deemed part of the SDK licensed to you as provided in this Agreement.
You agree that the form and content of the SDK that NVIDIA provides may change without prior notice to you.
While NVIDIA generally maintains compatibility between versions, NVIDIA may in some cases make changes that introduce incompatibilities in future versions of the SDK. 1.1.6. Components Under Other Licenses  The SDK may come bundled with, or otherwise include or be distributed with, NVIDIA or third-party components with separate legal notices or terms as may be described in proprietary notices accompanying the SDK.
If and to the extent there is a conflict between the terms in this Agreement and the license terms associated with the component, the license terms associated with the components control only to the extent necessary to resolve the conflict.
Subject to the other terms of this Agreement, you may use the SDK to develop and test applications released under Open Source Initiative (OSI) approved open source software licenses. 1.1.7. Reservation of Rights  NVIDIA reserves all rights, title, and interest in and to the SDK, not expressly granted to you under this Agreement.
1.2. Limitations  The following license limitations apply to your use of the SDK: You may not reverse engineer, decompile or disassemble, or remove copyright or other proprietary notices from any portion of the SDK or copies of the SDK.
Except as expressly provided in this Agreement, you may not copy, sell, rent, sublicense, transfer, distribute, modify, or create derivative works of any portion of the SDK.
Unless you have an agreement with NVIDIA for this purpose, you may not indicate that an application created with the SDK is sponsored or endorsed by NVIDIA.
You may not bypass, disable, or circumvent any encryption, security, digital rights management or authentication mechanism in the SDK.
You may not use the SDK in any manner that would cause it to become subject to an open source software license.
As examples, licenses that require as a condition of use, modification, and/or distribution that the SDK be: Disclosed or distributed in source code form; Licensed for the purpose of making derivative works; or Redistributable at no charge.
You acknowledge that the SDK as delivered is not tested or certified by NVIDIA for use in connection with the design, construction, maintenance, and/or operation of any system where the use or failure of such system could result in a situation that threatens the safety of human life or results in catastrophic damages (each, a “Critical Application”).
Examples of Critical Applications include use in avionics, navigation, autonomous vehicle applications, ai solutions for automotive products, military, medical, life support or other life critical applications.
NVIDIA shall not be liable to you or any third party, in whole or in part, for any claims or damages arising from such uses.
You are solely responsible for ensuring that any product or service developed with the SDK as a whole includes sufficient features to comply with all applicable legal and regulatory standards and requirements.
You agree to defend, indemnify and hold harmless NVIDIA and its affiliates, and their respective employees, contractors, agents, officers and directors, from and against any and all claims, damages, obligations, losses, liabilities, costs or debt, fines, restitutions and expenses (including but not limited to attorney’s fees and costs incident to establishing the right of indemnification) arising out of or related to products or services that use the SDK in or for Critical Applications, and for use of the SDK outside of the scope of this Agreement or not in compliance with its terms.
You may not reverse engineer, decompile or disassemble any portion of the output generated using SDK elements for the purpose of translating such output artifacts to target a non-NVIDIA platform. 1.3. Ownership  NVIDIA or its licensors hold all rights, title and interest in and to the SDK and its modifications and derivative works, including their respective intellectual property rights, subject to your rights under Section 1.3.2 .
This SDK may include software and materials from NVIDIA’s licensors, and these licensors are intended third party beneficiaries that may enforce this Agreement with respect to their intellectual property rights.
You hold all rights, title and interest in and to your applications and your derivative works of the sample source code delivered in the SDK, including their respective intellectual property rights, subject to NVIDIA’s rights under Section 1.3.1 .
You may, but don’t have to, provide to NVIDIA suggestions, feature requests or other feedback regarding the SDK, including possible enhancements or modifications to the SDK.
For any feedback that you voluntarily provide, you hereby grant NVIDIA and its affiliates a perpetual, non-exclusive, worldwide, irrevocable license to use, reproduce, modify, license, sublicense (through multiple tiers of sublicensees), and distribute (through multiple tiers of distributors) it without the payment of any royalties or fees to you.
NVIDIA is constantly looking for ways to improve its products, so you may send feedback to NVIDIA through the developer portal at https: developer.nvidia.com . 1.4. No Warranties  THE SDK IS PROVIDED BY NVIDIA “AS IS” AND “WITH ALL FAULTS.” TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES EXPRESSLY DISCLAIM ALL WARRANTIES OF ANY KIND OR NATURE, WHETHER EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, NON-INFRINGEMENT, OR THE ABSENCE OF ANY DEFECTS THEREIN, WHETHER LATENT OR PATENT.
NO WARRANTY IS MADE ON THE BASIS OF TRADE USAGE, COURSE OF DEALING OR COURSE OF TRADE. 1.5. Limitation of Liability  TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES SHALL NOT BE LIABLE FOR ANY (I) SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL DAMAGES, OR (II) DAMAGES FOR (A) ANY LOST PROFITS, LOSS OF USE, LOSS OF DATA OR LOSS OF GOODWILL, OR (B) THE COSTS OF PROCURING SUBSTITUTE PRODUCTS, ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE USE OR PERFORMANCE OF THE SDK, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON BREACH OF CONTRACT, BREACH OF WARRANTY, TORT (INCLUDING NEGLIGENCE), PRODUCT LIABILITY OR ANY OTHER CAUSE OF ACTION OR THEORY OF LIABILITY.
IN NO EVENT WILL NVIDIA’S AND ITS AFFILIATES TOTAL CUMULATIVE LIABILITY UNDER OR ARISING OUT OF THIS AGREEMENT EXCEED US$10.00.
THE NATURE OF THE LIABILITY OR THE NUMBER OF CLAIMS OR SUITS SHALL NOT ENLARGE OR EXTEND THIS LIMIT.
These exclusions and limitations of liability shall apply regardless if NVIDIA or its affiliates have been advised of the possibility of such damages, and regardless of whether a remedy fails its essential purpose.
These exclusions and limitations of liability form an essential basis of the bargain between the parties, and, absent any of these exclusions or limitations of liability, the provisions of this Agreement, including, without limitation, the economic terms, would be substantially different. 1.6. Termination  This Agreement will continue to apply until terminated by either you or NVIDIA as described below.
NVIDIA may, at any time, terminate this Agreement if: (i) you fail to comply with any term of this Agreement and the non-compliance is not fixed within thirty (30) days following notice from NVIDIA (or immediately if you violate NVIDIA’s intellectual property rights); (ii) you commence or participate in any legal proceeding against NVIDIA with respect to the SDK; or (iii) NVIDIA decides to no longer provide the SDK in a country or, in NVIDIA’s sole discretion, the continued use of it is no longer commercially viable.
Upon any termination of this Agreement, you agree to promptly discontinue use of the SDK and destroy all copies in your possession or control.
Your prior distributions in accordance with this Agreement are not affected by the termination of this Agreement.
Upon written request, you will certify in writing that you have complied with your commitments under this section.
Upon any termination of this Agreement all provisions survive except for the license grant provisions. 1.7. General  If you wish to assign this Agreement or your rights and obligations, including by merger, consolidation, dissolution or operation of law, contact NVIDIA to ask for permission.
Any attempted assignment not approved by NVIDIA in writing shall be void and of no effect.
NVIDIA may assign, delegate or transfer this Agreement and its rights and obligations, and if to a non-affiliate you will be notified.
You agree to cooperate with NVIDIA and provide reasonably requested information to verify your compliance with this Agreement.
This Agreement will be governed in all respects by the laws of the United States and of the State of Delaware, without regard to the conflicts of laws principles.
The United Nations Convention on Contracts for the International Sale of Goods is specifically disclaimed.
The state or federal courts residing in Santa Clara County, California shall have exclusive jurisdiction over any dispute or claim arising out of this Agreement.
Notwithstanding this, you agree that NVIDIA shall still be allowed to apply for injunctive remedies or an equivalent type of urgent legal relief in any jurisdiction.
If any court of competent jurisdiction determines that any provision of this Agreement is illegal, invalid or unenforceable, such provision will be construed as limited to the extent necessary to be consistent with and fully enforceable under the law and the remaining provisions will remain in full force and effect.
Each party acknowledges and agrees that the other is an independent contractor in the performance of this Agreement.
The SDK has been developed entirely at private expense and is “commercial items” consisting of “commercial computer software” and “commercial computer software documentation” provided with RESTRICTED RIGHTS.
Government subcontractor is subject to the restrictions in this Agreement pursuant to DFARS 227.7202-3(a) or as set forth in subparagraphs (c)(1) and (2) of the Commercial Computer Software - Restricted Rights clause at FAR 52.227-19, as applicable.
You agree that you will not ship, transfer or export the SDK into any country, or use the SDK in any manner, prohibited by the United States Bureau of Industry and Security or economic sanctions regulations administered by the U.S.
Department of Treasury’s Office of Foreign Assets Control (OFAC), or any applicable export laws, restrictions or regulations.
By accepting this Agreement, you confirm that you are not located in a country currently embargoed by the U.S.
Any notice delivered by NVIDIA to you under this Agreement will be delivered via mail, email or fax.
You agree that any notices that NVIDIA sends you electronically will satisfy any legal communication requirements.
Please direct your legal notices or other correspondence to NVIDIA Corporation, 2788 San Tomas Expressway, Santa Clara, California 95051, United States of America, Attention: Legal Department.
This Agreement and any exhibits incorporated into this Agreement constitute the entire agreement of the parties with respect to the subject matter of this Agreement and supersede all prior negotiations or documentation exchanged between the parties relating to this SDK license.
Any additional and/or conflicting terms on documents issued by you are null, void, and invalid.
Any amendment or waiver under this Agreement shall be in writing and signed by representatives of both parties. 2. CUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kits  The terms in this supplement govern your use of the NVIDIA CUDA Toolkit SDK under the terms of your license agreement (“Agreement”) as modified by this supplement.
Capitalized terms used but not defined below have the meaning assigned to them in the Agreement.
This supplement is an exhibit to the Agreement and is incorporated as an integral part of the Agreement.
In the event of conflict between the terms in this supplement and the terms in the Agreement, the terms in this supplement govern. 2.1. License Scope  The SDK is licensed for you to develop applications only for use in systems with NVIDIA GPUs.
2.2. Distribution  The portions of the SDK that are distributable under the Agreement are listed in Attachment A.
2.3. Operating Systems  Those portions of the SDK designed exclusively for use on the Linux or FreeBSD operating systems, or other operating systems derived from the source code to these operating systems, may be copied and redistributed for use in accordance with this Agreement, provided that the object code files are not modified in any way (except for unzipping of compressed files).
2.4. Audio and Video Encoders and Decoders  You acknowledge and agree that it is your sole responsibility to obtain any additional third-party licenses required to make, have made, use, have used, sell, import, and offer for sale your products or services that include or incorporate any third-party software and content relating to audio and/or video encoders and decoders from, including but not limited to, Microsoft, Thomson, Fraunhofer IIS, Sisvel S.p.A., MPEG-LA, and Coding Technologies.
NVIDIA does not grant to you under this Agreement any necessary patent or other rights with respect to any audio and/or video encoders and decoders. 2.5. Licensing  If the distribution terms in this Agreement are not suitable for your organization, or for any questions regarding this Agreement, please contact NVIDIA at nvidia-compute-license-questions @ nvidia .
com . 2.6. Attachment A  The following CUDA Toolkit files may be distributed with applications developed by you, including certain variations of these files that have version number or architecture specific information embedded in the file name - as an example only, for release version 9.0 of the 64-bit Windows software, the file cudart64_90.dll is redistributable.
GPL v3 terms and conditions are hereby incorporated into the Agreement by this reference: http: www.gnu.org/licenses/gpl.txt Consistent with these licensing requirements, the software listed below is provided under the terms of the specified open source software licenses.
To obtain source code for software provided under licenses that require redistribution of source code, including the GNU General Public License (GPL) and GNU Lesser General Public License (LGPL), contact oss-requests @ nvidia .
This offer is valid for a period of three (3) years from the date of the distribution of this product by NVIDIA CORPORATION.
Component License CUDA-GDB GPL v3 Licensee represents and warrants that any and all third party licensing and/or royalty payment obligations in connection with Licensee’s use of the H.264 video codecs are solely the responsibility of Licensee.
Licensee’s use of the Thrust library is subject to the terms and conditions of the Apache License Version 2.0.
Apache License Version 2.0 terms and conditions are hereby incorporated into the Agreement by this reference.
http: www.apache.org/licenses/LICENSE-2.0.html In addition, Licensee acknowledges the following notice: Thrust includes source code from the Boost Iterator, Tuple, System, and Random Number libraries.
THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT.
IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
Licensee’s use of the LLVM third party component is subject to the following terms and conditions: = LLVM Release License = University of Illinois/NCSA Open Source License Copyright (c) 2003-2010 University of Illinois at Urbana-Champaign.
Developed by: LLVM Team University of Illinois at Urbana-Champaign http: llvm.org Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.
* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.
* Neither the names of the LLVM Team, University of Illinois at Urbana- Champaign, nor the names of its contributors may be used to endorse or promote products derived from this Software without specific prior written permission.
THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.
Licensee’s use of the PCRE third party component is subject to the following terms and conditions: - PCRE LICENCE - PCRE is a library of functions to support regular expressions whose syntax and semantics are as close as possible to those of the Perl 5 language.
Release 8 of PCRE is distributed under the terms of the "BSD" licence, as specified below.
The documentation for PCRE, supplied in the "doc" directory, is distributed under the same terms as the software itself.
Also included in the distribution is a set of C++ wrapper functions, and a just- in-time compiler that can be used to optimize pattern matching.
THE BASIC LIBRARY FUNCTIONS - Written by: Philip Hazel Email local part: ph10 Email domain: cam.ac.uk University of Cambridge Computing Service, Cambridge, England.
PCRE JUST-IN-TIME COMPILATION SUPPORT - Written by: Zoltan Herczeg Email local part: hzmester Emain domain: freemail.hu Copyright(c) 2010-2012 Zoltan Herczeg All rights reserved.
STACK-LESS JUST-IN-TIME COMPILER - Written by: Zoltan Herczeg Email local part: hzmester Emain domain: freemail.hu Copyright(c) 2009-2012 Zoltan Herczeg All rights reserved.
THE "BSD" LICENCE - Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
nor the names of their contributors may be used to endorse or promote products derived from this software without specific prior written permission.
THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
Some of the cuBLAS library routines were written by or derived from code written by Vasily Volkov and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2007-2009, Regents of the University of California All rights reserved.
Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
* Neither the name of the University of California, Berkeley nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
THIS SOFTWARE IS PROVIDED BY THE AUTHOR "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
Some of the cuBLAS library routines were written by or derived from code written by Davide Barbieri and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2008-2009 Davide Barbieri @ University of Rome Tor Vergata.
* The name of the author may not be used to endorse or promote products derived from this software without specific prior written permission.
Some of the cuBLAS library routines were derived from code developed by the University of Tennessee and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2010 The University of Tennessee.
* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer listed in this license in the documentation and/or other materials provided with the distribution.
* Neither the name of the copyright holders nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
Some of the cuBLAS library routines were written by or derived from code written by Jonathan Hogg and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2012, The Science and Technology Facilities Council (STFC).
* Neither the name of the STFC nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
IN NO EVENT SHALL THE STFC BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
Some of the cuBLAS library routines were written by or derived from code written by Ahmad M.
Abdelfattah, David Keyes, and Hatem Ltaief, and are subject to the Apache License, Version 2.0, as follows: -- (C) Copyright 2013 King Abdullah University of Science and Technology Authors: Ahmad Abdelfattah (ahmad.ahmad@kaust.edu.sa) David Keyes (david.keyes@kaust.edu.sa) Hatem Ltaief (hatem.ltaief@kaust.edu.sa) Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
* Neither the name of the King Abdullah University of Science and Technology nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE Some of the cuSPARSE library routines were written by or derived from code written by Li-Wen Chang and are subject to the NCSA Open Source License as follows: Copyright (c) 2012, University of Illinois.
Developed by: IMPACT Group, University of Illinois, http: impact.crhc.illinois.edu Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
* Neither the names of IMPACT Group, University of Illinois, nor the names of its contributors may be used to endorse or promote products derived from this Software without specific prior written permission.
Some of the cuRAND library routines were written by or derived from code written by Mutsuo Saito and Makoto Matsumoto and are subject to the following license: Copyright (c) 2009, 2010 Mutsuo Saito, Makoto Matsumoto and Hiroshima University.
Copyright (c) 2011 Mutsuo Saito, Makoto Matsumoto, Hiroshima University and University of Tokyo.
* Neither the name of the Hiroshima University nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions, and the following disclaimer.
* Redistributions in binary form must reproduce the above copyright notice, this list of conditions, and the following disclaimer in the documentation and/or other materials provided with the distribution.
Shaw Research nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
Some of the Math library routines were written by or derived from code developed by Norbert Juffa and are subject to the following license: Copyright (c) 2015-2017, Norbert Juffa All rights reserved.
Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1.
Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
Licensee’s use of the lz4 third party component is subject to the following terms and conditions: Copyright (C) 2011-2013, Yann Collet.
BSD 2-Clause License (http: www.opensource.org/licenses/bsd-license.php) Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
The NPP library uses code from the Boost Math Toolkit, and is subject to the following license: Boost Software License - Version 1.0 - August 17th, 2003 .
Portions of the Nsight Eclipse Edition is subject to the following license: The Eclipse Foundation makes available all content in this plug-in ("Content").
Unless otherwise indicated below, the Content is provided to you under the terms and conditions of the Eclipse Public License Version 1.0 ("EPL").
If you did not receive this Content directly from the Eclipse Foundation, the Content is being redistributed by another party ("Redistributor") and different terms and conditions may apply to your use of any object code in the Content.
Unless otherwise indicated below, the terms and conditions of the EPL still apply to any source code in the Content and such source code may be obtained at http: www.eclipse.org.
Some of the cuBLAS library routines uses code from OpenAI, which is subject to the following license: License URL https: github.com/openai/openai-gemm/blob/master/LICENSE License Text The MIT License Copyright (c) 2016 OpenAI (http: openai.com), 2016 Google Inc.
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
Licensee’s use of the Visual Studio Setup Configuration Samples is subject to the following license: The MIT License (MIT) Copyright (C) Microsoft Corporation.
Licensee’s use of linmath.h header for CPU functions for GL vector/matrix operations from lunarG is subject to the Apache License Version 2.0.
Components of the driver and compiler used for binary management, including nvFatBin, nvcc, and cuobjdump, use the Zstandard library which is subject to the following license: BSD License For Zstandard software Copyright (c) Meta Platforms, Inc.
* Neither the name Facebook, nor Meta, nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2024, NVIDIA Corporation.
Introduction v12.5 | PDF | Archive CUDA Quick Start Guide Minimal first-steps instructions to get CUDA running on a standard system.
Introduction  This guide covers the basic instructions needed to install CUDA and verify that a CUDA application can run on each supported platform.
These instructions are intended to be used on a clean installation of a supported platform.
For questions which are not answered in this document, please refer to the Windows Installation Guide and Linux Installation Guide .
The CUDA installation packages can be found on the CUDA Downloads Page . 2. Windows  When installing CUDA on Windows, you can choose between the Network Installer and the Local Installer.
For more details, refer to the Windows Installation Guide . 2.1. Network Installer  Perform the following steps to install CUDA and verify the installation.
Once the installation completes, click “next” to acknowledge the Nsight Visual Studio Edition installation summary.
Navigate to the Samples’ nbody directory in https: github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody .
Open the nbody Visual Studio solution file for the version of Visual Studio you have installed, for example, nbody_vs2019.sln .
Note Run samples by navigating to the executable’s location, otherwise it will fail to locate dependent resources. 2.2. Local Installer  Perform the following steps to install CUDA and verify the installation.
Once the installation completes, click next to acknowledge the Nsight Visual Studio Edition installation summary.
Open the nbody Visual Studio solution file for the version of Visual Studio you have installed. 2.3. Pip Wheels - Windows  NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python.
These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo.
If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules.
If these Python modules are out-of-date then the commands which follow later in this section may fail.
py -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module.
py -m pip install nvidia-pyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https: pypi.ngc.nvidia.com Procedure Install the CUDA runtime package: py -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: py -m pip install nvidia- Metapackages The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version.
nvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-nvml-dev-cu12 nvidia-cuda-nvrtc-cu12 nvidia-nvtx-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cublas-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvjpeg-cu12 These metapackages install the following packages: nvidia-nvml-dev-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-runtime-cu125 nvidia-cuda-cupti-cu125 nvidia-cublas-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-nvtx-cu125 nvidia-cuda-nvrtc-cu125 nvidia-npp-cu125 nvidia-cusparse-cu125 nvidia-cusolver-cu125 nvidia-curand-cu125 nvidia-cufft-cu125 nvidia-nvjpeg-cu125 2.4.
Installation To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda -c nvidia Uninstallation To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 3.
Linux  CUDA on Linux can be installed using an RPM, Debian, Runfile, or Conda package, depending on the platform being installed on. 3.1. Linux x86_64  For development on the x86_64 architecture.
See the Linux Installation Guide for more details. 3.1.1. Redhat / CentOS  When installing CUDA on Redhat or CentOS, you can choose between the Runfile Installer and the RPM Installer.
In the case of the RPM installers, the instructions for the Local and Network variants are the same.
For more details, refer to the Linux Installation Guide . 3.1.1.1. RPM Installer  Perform the following steps to install CUDA and verify the installation.
Install EPEL to satisfy the DKMS dependency by following the instructions at EPEL’s website .
Enable optional repos : On RHEL 8 Linux only, execute the following steps to enable optional repositories.
Disable the Nouveau drivers: Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda__linux.run --silent Create an xorg.conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https: github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https: github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . 3.1.2. Fedora  When installing CUDA on Fedora, you can choose between the Runfile Installer and the RPM Installer.
3.1.2.2. Disable the Nouveau drivers: Create a file at /usr/lib/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force Run the below command: sudo grub2-mkconfig -o /boot/grub2/grub.cfg Reboot the system: sudo reboot Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda__linux.run --silent Create an xorg.conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface.
Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https: github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https: github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . 3.1.3. SUSE Linux Enterprise Server  When installing CUDA on SUSE Linux Enterprise Server, you can choose between the Runfile Installer and the RPM Installer.
3.1.3.2. Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters.
3.1.4.1. Install the repository meta-data, refresh the Zypper cache, and install CUDA: sudo rpm --install cuda-repo--..rpm sudo rpm --erase gpg-pubkey-7fa2af80* sudo zypper refresh sudo zypper install cuda Add the user to the video group: sudo usermod -a -G video Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https: github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https: github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody .
3.1.4.2. Disable the Nouveau drivers: Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initrd: sudo /sbin/mkinitrd Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters.
The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) kernel-modules-extra-$(uname -r) Choose an installation method: local repo or network repo . 3.1.5.2. Local Repo Installation for Amazon Linux  Install local repository on file system: sudo rpm --install cuda-repo-amzn2023-X-Y-local-*.x86_64.rpm 3.1.5.3.
Network Repo Installation for Amazon Linux  Enable the network repository and clean the DN cache: sudo dnf config-manager --add-repo https: developer.download.nvidia.com/compute/cuda/repos/amzn2023/x86_64/cuda-amzn2023.repo sudo dnf clean expire-cache 3.1.5.4.
Common Installation Instructions for Amazon Linux  These instructions apply to both local and network installation for Amazon Linux.
Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms sudo dnf install cuda-toolkit Install GPUDirect Filesystem: sudo dnf install nvidia-gds Add libcuda.so symbolic link, if necessary: The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory.
For pre-existing projects which use libcuda.so , it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory.
Reboot the system: sudo reboot Perform the post-installation actions. 3.1.6. Pip Wheels - Linux  NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python.
python3 -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module.
python3 -m pip install nvidia-pyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https: pypi.ngc.nvidia.com Procedure Install the CUDA runtime package: python3 -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: python3 -m pip install nvidia- Metapackages The following metapackages will install the latest version of the named component on Linux for the indicated CUDA version.
nvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-nvml-dev-cu12 nvidia-cuda-nvrtc-cu12 nvidia-nvtx-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cublas-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvjpeg-cu12 nvidia-opencl-cu12 nvidia-nvjitlink-cu12 These metapackages install the following packages: nvidia-nvml-dev-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-runtime-cu125 nvidia-cuda-cupti-cu125 nvidia-cublas-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-nvtx-cu125 nvidia-cuda-nvrtc-cu125 nvidia-npp-cu125 nvidia-cusparse-cu125 nvidia-cusolver-cu125 nvidia-curand-cu125 nvidia-cufft-cu125 nvidia-nvjpeg-cu125 nvidia-opencl-cu125 nvidia-nvjitlink-cu125 3.1.7.
Installation To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda -c nvidia Uninstallation To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 3.1.8.
Install repository meta-data sudo dpkg -i cuda-repo-__.deb Update the CUDA public GPG key sudo apt-key del 7fa2af80 When installing using the local repo: sudo cp /var/cuda-repo-ubuntu2004-12-0-local/cuda-*-keyring.gpg /usr/share/keyrings/ When installing using the network repo: wget https: developer.download.nvidia.com/compute/cuda/repos /cuda-keyring_1.1-1_all.deb sudo dpkg -i cuda-keyring_1.1-1_all.deb Pin file to prioritize CUDA repository: wget https: developer.download.nvidia.com/compute/cuda/repos /cuda-.pin sudo mv cuda-.pin /etc/apt/preferences.d/cuda-repository-pin-600 Update the Apt repository cache and install CUDA sudo apt-get update sudo apt-get install cuda 3.1.9.
Ubuntu  When installing CUDA on Ubuntu, you can choose between the Runfile Installer and the Debian Installer.
In the case of the Debian installers, the instructions for the Local and Network variants are the same. 3.1.9.1. Debian Installer  Perform the following steps to install CUDA and verify the installation.
3.1.10. Debian  When installing CUDA on Debian 10, you can choose between the Runfile Installer and the Debian Installer.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 4.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 4.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2015-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Introduction v12.5 | PDF | Archive CUDA Installation Guide for Microsoft Windows The installation instructions for the CUDA Toolkit on Microsoft Windows systems.
Introduction  CUDA ® is a parallel computing platform and programming model invented by NVIDIA.
It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU).
CUDA was developed with several design goals in mind: Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms.
With CUDA C/C++, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation.
Serial portions of applications are run on the CPU, and parallel portions are offloaded to the GPU.
This configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources.
CUDA-capable GPUs have hundreds of cores that can collectively run thousands of computing threads.
The on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus.
32-bit compilation native and cross-compilation is removed from CUDA 12.0 and later Toolkit.
CUDA Driver will continue to support running 32-bit application binaries on GeForce GPUs until Ada.
Support for running x86 32-bit applications on x86_64 Windows is limited to use with: CUDA Driver CUDA Runtime (cudart) CUDA Math Library (math.h) 1.2.
About This Document  This document is intended for readers familiar with Microsoft Windows operating systems and the Microsoft Visual Studio environment.
You do not need previous experience with CUDA or experience with parallel computation. 2. Installing CUDA Development Tools  Basic instructions can be found in the Quick Start Guide .
The setup of CUDA development tools on a system running the appropriate version of Windows consists of a few simple steps: Verify the system has a CUDA-capable GPU.
Test that the installed software runs correctly and communicates with the hardware. 2.1. Verify You Have a CUDA-Capable GPU  You can verify that you have a CUDA-capable GPU through the Display Adapters section in the Windows Device Manager .
If you have an NVIDIA card that is listed in https: developer.nvidia.com/cuda-gpus , that GPU is CUDA-capable.
The Windows Device Manager can be opened via the following steps: Open a run window from the Start Menu Run: control /name Microsoft.DeviceManager 2.2.
Download the NVIDIA CUDA Toolkit  The NVIDIA CUDA Toolkit is available at https: developer.nvidia.com/cuda-downloads .
Choose the platform you are using and one of the following installer formats: Network Installer: A minimal installer which later downloads packages required for installation.
Full Installer: An installer which contains all the components of the CUDA Toolkit and does not require any further download.
This installer is useful for systems which lack network access and for enterprise deployment.
The CUDA Toolkit installs the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources.
Download Verification The download can be verified by comparing the MD5 checksum posted at https: developer.download.nvidia.com/compute/cuda/12.5.1/docs/sidebar/md5sum.txt with that of the downloaded file.
If either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again. 2.3. Install the CUDA Software  Before installing the toolkit, you should read the Release Notes, as they provide details on installation and software functionality.
If you have not installed a stand-alone driver, install the driver from the NVIDIA CUDA Toolkit.
Graphical Installation Install the CUDA Software by executing the CUDA installer and following the on-screen prompts.
Silent Installation The installer can be executed in silent mode by executing the package with the -s flag.
Additional parameters can be passed which will install specific subpackages instead of all packages.
Table 2 Possible Subpackage Names  Subpackage Name Subpackage Description Toolkit Subpackages (defaults to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.5) cuda_profiler_api_12.5 CUDA Profiler API.
cupti_12.5 The CUDA Profiling Tools Interface for creating profiling and tracing tools that target CUDA applications.
documentation_12.5 CUDA HTML and PDF documentation files including the CUDA C++ Programming Guide, CUDA C++ Best Practices Guide, CUDA library documentation, etc.
nvprof_12.5 Tool for collecting and viewing CUDA application profiling data from the command-line.
nvprune_12.5 Prunes host object files and libraries to only contain device code for the specified targets.
visual_studio_integration_12.5 Installs CUDA project wizard and builds customization files in VS.
For example, to install only the compiler and driver components: .exe -s nvcc_12.1 Display.Driver Use the -n option if you do not want to reboot automatically after install or uninstall, even if reboot is required.
Extracting and Inspecting the Files Manually Sometimes it may be desirable to extract or inspect the installable files directly, such as in enterprise deployment, or to browse the files before installation.
The full installation package can be extracted using a decompression tool which supports the LZMA compression method, such as 7-zip or WinZip .
Once extracted, the CUDA Toolkit files will be in the CUDAToolkit folder, and similarily for CUDA Visual Studio Integration.
Within each directory is a .dll and .nvi file that can be ignored as they are not part of the installable files.
Note Accessing the files in this manner does not set up any environment settings, such as variables or Visual Studio integration.
This is intended for enterprise-level deployment. 2.3.1. Uninstalling the CUDA Software  All subpackages can be uninstalled through the Windows Control Panel by using the Programs and Features widget.
2.4. Using Conda to Install the CUDA Software  This section describes the installation and configuration of CUDA when using the Conda installer.
The Conda packages are available at https: anaconda.org/nvidia . 2.4.1. Conda Overview  The Conda installation installs the CUDA Toolkit.
The installation steps are listed below. 2.4.2. Installation  To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda - c nvidia 2.4.3.
Uninstallation  To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 2.4.4.
Installing Previous CUDA Releases  All Conda packages released under a specific CUDA version are labeled with that release version.
To install a previous version, include that label in the install command such as: conda install cuda - c nvidia / label / cuda -11.3.0 Note Some CUDA releases do not move to new versions of all installable components.
When this is the case these components will be moved to the new label, and you may need to modify the install command to include both labels such as: conda install cuda - c nvidia / label / cuda -11.3.0 - c nvidia / label / cuda -11.3.1 This example will install all packages released as part of CUDA 11.3.1. 2.5. Use a Suitable Driver Model  On Windows 10 and later, the operating system provides two driver models under which the NVIDIA Driver may operate: The WDDM driver model is used for display devices.
The Tesla Compute Cluster (TCC) mode of the NVIDIA Driver is available for non-display devices such as NVIDIA Tesla GPUs and the GeForce GTX Titan GPUs; it uses the Windows WDM driver model.
To check which driver mode is in use and/or to switch driver modes, use the nvidia-smi tool that is included with the NVIDIA Driver installation (see nvidia-smi -h for details).
Note Keep in mind that when TCC mode is enabled for a particular GPU, that GPU cannot be used as a display device.
Note NVIDIA GeForce GPUs (excluding GeForce GTX Titan GPUs) do not support TCC mode. 2.6. Verify the Installation  Before continuing, it is important to verify that the CUDA toolkit can find and communicate correctly with the CUDA-capable hardware.
To do this, you need to compile and run some of the included sample programs. 2.6.1. Running the Compiled Examples  The version of the CUDA Toolkit can be checked by running nvcc -V in a Command Prompt window.
You can display a Command Prompt window by going to: Start > All Programs > Accessories > Command Prompt CUDA Samples are located in https: github.com/nvidia/cuda-samples .
To use the samples, clone the project, build the samples, and run them using the instructions on the Github page.
To verify a correct configuration of the hardware and software, it is highly recommended that you build and run the deviceQuery sample program.
If CUDA is installed and configured correctly, the output should look similar to Figure 1 .
Figure 1 Valid Results from deviceQuery CUDA Sample  The exact appearance and the output lines might be different on your system.
The important outcomes are that a device was found, that the device(s) match what is installed in your system, and that the test passed.
If a CUDA-capable device and the CUDA Driver are installed but deviceQuery reports that no CUDA-capable devices are present, ensure the deivce and driver are properly installed.
Running the bandwidthTest program, located in the same directory as deviceQuery above, ensures that the system and the CUDA-capable device are able to communicate correctly.
Figure 2 Valid Results from bandwidthTest CUDA Sample  The device name (second line) and the bandwidth numbers vary from system to system.
The important items are the second line, which confirms a CUDA device was found, and the second-to-last line, which confirms that all necessary tests passed.
If the tests do not pass, make sure you do have a CUDA-capable NVIDIA GPU on your system and make sure it is properly installed.
To see a graphical representation of what CUDA can do, run the particles sample at https: github.com/NVIDIA/cuda-samples/tree/master/Samples/particles 3.
Pip Wheels  NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python.
These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo.
If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules.
If these Python modules are out-of-date then the commands which follow later in this section may fail.
py -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module.
py -m pip install nvidia-pyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https: pypi.ngc.nvidia.com Procedure Install the CUDA runtime package: py -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: py -m pip install nvidia- Metapackages The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version.
nvidia-cublas-cu12 nvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-cuda-nvrtc-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvfatbin-cu12 nvidia-nvjitlink-cu12 nvidia-nvjpeg-cu12 nvidia-nvml-dev-cu12 nvidia-nvtx-cu12 nvidia-opencl-cu12 These metapackages install the following packages: nvidia-cublas-cu125 nvidia-cuda-runtime-cu125 nvidia-cuda-cupti-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-nvrtc-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-cufft-cu125 nvidia-curand-cu125 nvidia-cusolver-cu125 nvidia-cusparse-cu125 nvidia-npp-cu125 nvidia-nvfatbin-cu125 nvidia-nvjitlink-cu125 nvidia-nvjpeg-cu125 nvidia-nvml-dev-cu125 nvidia-nvtx-cu125 nvidia-opencl-cu125 4.
Compiling CUDA Programs  The project files in the CUDA Samples have been designed to provide simple, one-click builds of the programs that include all source code.
To build the Windows projects (for release or debug mode), use the provided *.sln solution files for Microsoft Visual Studio 2015 (deprecated in CUDA 11.1), 2017, 2019, or 2022.
You can use either the solution files located in each of the examples directories in https: github.com/nvidia/cuda-samples 4.1.
Compiling Sample Projects  The bandwidthTest project is a good sample project to build and run.
It is located in https: github.com/NVIDIA/cuda-samples/tree/master/Samples/1_Utilities/bandwidthTest .
If you elected to use the default installation location, the output is placed in CUDA Samples\v12.5\bin\win64\Release .
If all works correctly, the output should be similar to Figure 2 . 4.2. Sample Projects  The sample projects come in two configurations: debug and release (where release contains no debugging information) and different Visual Studio projects.
These sample projects also make use of the $CUDA_PATH environment variable to locate where the CUDA Toolkit and the associated .props files are.
The environment variable is set automatically using the Build Customization CUDA 12.5.props file, and is installed automatically as part of the CUDA Toolkit installation process.
Table 3 CUDA Visual Studio .props locations  Visual Studio CUDA 12.5 .props file Install Directory Visual Studio 2015 (deprecated) C:Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\BuildCustomizations Visual Studio 2017 \Common7\IDE\VC\VCTargets\BuildCustomizations Visual Studio 2019 C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\MSBuild\Microsoft\VC\v160\BuildCustomizations Visual Studio 2022 C:\Program Files\Microsoft Visual Studio\2022\Professional\MSBuild\Microsoft\VC\v170\BuildCustomizations You can reference this CUDA 12.5.props file when building your own CUDA applications. 4.3. Build Customizations for New Projects  When creating a new CUDA application, the Visual Studio project file must be configured to include CUDA build customizations.
To accomplish this, click File-> New | Project… NVIDIA-> CUDA->, then select a template for your CUDA Toolkit version.
For example, selecting the “CUDA 12.5 Runtime” template will configure your project for use with the CUDA 12.5 Toolkit.
The new project is technically a C++ project (.vcxproj) that is preconfigured to use NVIDIA’s Build Customizations.
To specify a custom CUDA Toolkit location, under CUDA C/C++ , select Common , and set the CUDA Toolkit Custom Dir field as desired.
Note A supported version of MSVC must be installed to use this feature. 4.4. Build Customizations for Existing Projects  When adding CUDA acceleration to existing applications, the relevant Visual Studio project files must be updated to include CUDA build customizations.
This can be done using one of the following two methods: Open the Visual Studio project, right click on the project name, and select Build Dependencies > Build Customizations… , then select the CUDA Toolkit version you would like to target.
Alternatively, you can configure your project always to build with the most recently installed version of the CUDA Toolkit.
Under CUDA C/C++ , select Common , and set the CUDA Toolkit Custom Dir field to $(CUDA_PATH) .
While Option 2 will allow your project to automatically use any new CUDA Toolkit version you may install in the future, selecting the toolkit version explicitly as in Option 1 is often better in practice, because if there are new CUDA configuration options added to the build customization rules accompanying the newer toolkit, you would not see those new options using Option 2.
If you use the $(CUDA_PATH) environment variable to target a version of the CUDA Toolkit for building, and you perform an installation or uninstallation of any version of the CUDA Toolkit, you should validate that the $(CUDA_PATH) environment variable points to the correct installation directory of the CUDA Toolkit for your purposes.
You can access the value of the $(CUDA_PATH) environment variable via the following steps: Open a run window from the Start Menu.
This can done when adding the file by right clicking the project you wish to add the file to, selecting Add New Item , selecting NVIDIA CUDA 12.5\CodeCUDA C/C++ File , and then selecting the file you wish to add.
For advanced users, if you wish to try building your project against a newer CUDA Toolkit without making changes to any of your project files, go to the Visual Studio command prompt, change the current directory to the location of your project, and execute a command such as the following: msbuild /t:Rebuild /p:CudaToolkitDir="drive:/path/to/new/toolkit/" 5.
Additional Considerations  Now that you have CUDA-capable hardware and the NVIDIA CUDA Toolkit installed, you can examine and enjoy the numerous included programs.
To begin using CUDA to accelerate the performance of your own applications, consult the CUDA C Programming Guide, located in the CUDA Toolkit documentation directory.
A number of helpful development tools are included in the CUDA Toolkit or are available for download from the NVIDIA Developer Zone to assist you as you develop your CUDA programs, such as NVIDIA ® Nsight™ Visual Studio Edition, and NVIDIA Visual Profiler.
For technical support on programming questions, consult and participate in the developer forums at https: developer.nvidia.com/cuda/ . 6. Notices  6.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 6.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 6.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2009-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Why do I see “nvcc: No such file or directory” when I try to build a CUDA application? 15.3.
Why do I see “error while loading shared libraries: : cannot open shared object file: No such file or directory” when I try to run a CUDA application that uses a CUDA library? 15.4.
Why do I see multiple “404 Not Found” errors when updating my repository meta-data on Ubuntu? 15.5.
What do I do if the display does not load, or CUDA does not work, after performing a system update? 15.9.
Introduction v12.5 | PDF | Archive NVIDIA CUDA Installation Guide for Linux The installation instructions for the CUDA Toolkit on Linux.
Introduction  CUDA ® is a parallel computing platform and programming model invented by NVIDIA ® .
It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU).
CUDA was developed with several design goals in mind: Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms.
With CUDA C/C﻿+﻿+, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation.
Serial portions of applications are run on the CPU, and parallel portions are offloaded to the GPU.
This configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources.
CUDA-capable GPUs have hundreds of cores that can collectively run thousands of computing threads.
The on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus.
This guide will show you how to install and check the correct operation of the CUDA development tools. 1.1. System Requirements  To use NVIDIA CUDA on your system, you will need the following installed: CUDA-capable GPU A supported version of Linux with a gcc compiler and toolchain CUDA Toolkit (available at https: developer.nvidia.com/cuda-downloads ) The CUDA development environment relies on tight integration with the host development environment, including the host compiler and C runtime libraries, and is therefore only supported on distribution versions that have been qualified for this CUDA Toolkit release.
Table 1 Native Linux Distribution Support in CUDA 12.5 Update 1  Distribution Kernel 1 Default GCC GLIBC x86_64 RHEL 9.y (y =10.x >=11.x >=22.x 22.x 1.4.
About This Document  This document is intended for readers familiar with the Linux environment and the compilation of C programs from the command line.
For systems that have enabled the sudo package, use the sudo prefix for all necessary commands. 2. Pre-installation Actions  Some actions must be taken before the CUDA Toolkit and Driver can be installed on Linux: Verify the system has a CUDA-capable GPU.
Note You can override the install-time prerequisite checks by running the installer with the -override flag.
Remember that the prerequisites will still be required to use the NVIDIA CUDA Toolkit. 2.1. Verify You Have a CUDA-Capable GPU  To verify that your GPU is CUDA-capable, go to your distribution’s equivalent of System Properties, or, from the command line, enter: lspci | grep -i nvidia If you do not see any settings, update the PCI hardware database that Linux maintains by entering update-pciids (generally found in /sbin ) at the command line and rerun the previous lspci command.
If your graphics card is from NVIDIA and it is listed in https: developer.nvidia.com/cuda-gpus , your GPU is CUDA-capable.
The Release Notes for the CUDA Toolkit also contain a list of supported products. 2.2. Verify You Have a Supported Version of Linux  The CUDA Development Tools are only supported on some specific distributions of Linux.
To determine which distribution and release number you’re running, type the following at the command line: uname -m && cat /etc/*release You should see output similar to the following, modified for your particular system: x86_64 Red Hat Enterprise Linux Workstation release 6.0 (Santiago) The x86_64 line indicates you are running on a 64-bit system.
The remainder gives information about your distribution. 2.3. Verify the System Has gcc Installed  The gcc compiler is required for development using the CUDA Toolkit.
It is generally installed as part of the Linux installation, and in most cases the version of gcc installed with a supported version of Linux will work correctly.
To verify the version of gcc installed on your system, type the following on the command line: gcc --version If an error message displays, you need to install the development tools from your Linux distribution or obtain a version of gcc and its accompanying toolchain from the Web. 2.4. Verify the System has the Correct Kernel Headers and Development Packages Installed  The CUDA Driver requires that the kernel headers and development packages for the running version of the kernel be installed at the time of the driver installation, as well whenever the driver is rebuilt.
For example, if your system is running kernel version 3.17.4-301, the 3.17.4-301 kernel headers and development packages must also be installed.
While the Runfile installation performs no package validation, the RPM and Deb installations of the driver will make an attempt to install the kernel header and development packages if no version of these packages is currently installed.
However, it will install the latest version of these packages, which may or may not match the version of the kernel your system is using.
Therefore, it is best to manually ensure the correct version of the kernel headers and development packages are installed prior to installing the CUDA Drivers, as well as whenever you change the kernel version.
The version of the kernel your system is running can be found by running the following command: uname -r This is the version of the kernel headers and development packages that must be installed prior to installing the CUDA Drivers.
This command will be used multiple times below to specify the version of the packages to install.
More advanced cases, such as custom kernel branches, should ensure that their kernel headers and sources match the kernel build they are running.
Note If you perform a system update which changes the version of the Linux kernel being used, make sure to rerun the commands below to ensure you have the correct kernel headers and kernel development packages installed.
Otherwise, the CUDA Driver will fail to work with the new kernel. 2.5. Install GPUDirect Storage  If you intend to use GPUDirectStorage (GDS), you must install the CUDA package and MLNX_OFED package.
GDS is supported in two different modes: GDS (default/full perf mode) and Compatibility mode.
Compatibility mode is the only mode that is supported on certain distributions due to software dependency limitations.
Full GDS support is restricted to the following Linux distros: Ubuntu 20.04, Ubuntu 22.04 RHEL 8.3, RHEL 8.4, RHEL 9.0 Starting with CUDA toolkit 12.2.2, GDS kernel driver package nvidia-gds version 12.2.2-1 (provided by nvidia-fs-dkms 2.17.5-1) and above is only supported with the NVIDIA open kernel driver.
Follow the instructions in Removing CUDA Toolkit and Driver to remove existing NVIDIA driver packages and then follow instructions in NVIDIA Open GPU Kernel Modules to install NVIDIA open kernel driver packages. 2.6. Choose an Installation Method  The CUDA Toolkit can be installed using either of two different installation mechanisms: distribution-specific packages (RPM and Deb packages), or a distribution-independent package (runfile packages).
The distribution-independent package has the advantage of working across a wider set of Linux distributions, but does not update the distribution’s native package management system.
The distribution-specific packages interface with the distribution’s native package management system.
Note For both native as well as cross development, the toolkit must be installed using the distribution-specific installer.
See the CUDA Cross-Platform Installation section for more details. 2.7. Download the NVIDIA CUDA Toolkit  The NVIDIA CUDA Toolkit is available at https: developer.nvidia.com/cuda-downloads .
The CUDA Toolkit contains the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources.
Download Verification The download can be verified by comparing the MD5 checksum posted at https: developer.download.nvidia.com/compute/cuda/12.5.1/docs/sidebar/md5sum.txt with that of the downloaded file.
If either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again.
Address Custom xorg.conf, If Applicable  The driver relies on an automatically generated xorg.conf file at /etc/X11/xorg.conf .
If a custom-built xorg.conf file is present, this functionality will be disabled and the driver may not work.
You can try removing the existing xorg.conf file, or adding the contents of /etc/X11/xorg.conf.d/00-nvidia.conf to the xorg.conf file.
The xorg.conf file will most likely need manual tweaking for systems with a non-trivial GPU configuration. 2.9. Handle Conflicting Installation Methods  Before installing CUDA, any previous installations that could conflict should be uninstalled.
This will not affect systems which have not had CUDA installed previously, or systems where the installation method has been preserved (RPM/Deb vs.
Read on for more detailed instructions. 3.1. Overview  Installation using RPM or Debian packages interfaces with your system’s package management system.
When using RPM or Debian local repo installers, the downloaded package contains a repository snapshot stored on the local filesystem in /var/.
Such a package only informs the package manager where to find the actual installation packages, but will not install them.
If the online network repository is enabled, RPM or Debian packages will be automatically downloaded at installation time using the package manager: apt-get, dnf, yum, or zypper.
Distribution-specific instructions detail how to install CUDA: RHEL 8 / Rocky Linux 8 RHEL 9 / Rocky Linux 9 KylinOS 10 Fedora SLES OpenSUSE WSL Ubuntu Debian Amazon Linux 2023 Finally, some helpful package manager capabilities are detailed.
Note Optional components such as nvidia-fs , libnvidia_nscq , and fabricmanager are not installed by default and will have to be installed separately as needed. 3.2. RHEL 8 / Rocky 8  3.2.1.
The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) If matching kernel-headers and kernel-devel packages are not available for the currently running kernel version, you may need to use the previously shipped version of these packages.
Satisfy third-party package dependency: Satisfy DKMS dependency : The NVIDIA driver RPM packages depend on other external packages, such as DKMS and libvdpau .
Any such third-party repositories must be added to the package manager repository database before installing the NVIDIA driver RPM packages, or missing dependencies will prevent the installation from proceeding.
To enable EPEL: sudo dnf install https: dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm Enable optional repos: On RHEL 8 Linux only, execute the following steps to enable optional repositories.
On x86_64 systems: subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms Remove Outdated Signing Key: sudo rpm --erase gpg-pubkey-7fa2af80* Choose an installation method: local repo or network repo . 3.2.2. Local Repo Installation for RHEL 8 / Rocky 8  Install local repository on file system: sudo rpm --install cuda-repo--X-Y-local-*..rpm 3.2.3.
Network Repo Installation for RHEL 8 / Rocky 8  Enable the network repo: sudo dnf config-manager --add-repo https: developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo where $distro/$arch should be replaced by one of the following: rhel8/cross-linux-sbsa rhel8/sbsa rhel8/x86_64 Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685 .
On a fresh installation of RHEL, the dnf package manager will prompt the user to accept new keys when installing packages the first time.
For upgrades, you must also also fetch an updated .repo entry: sudo dnf config-manager --add-repo https: developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo Clean Yum repository cache: sudo dnf clean expire-cache 3.2.4.
Common Instructions for RHEL 8 / Rocky 8  These instructions apply to both local and network installation.
Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms sudo dnf install cuda-toolkit Install GPUDirect Filesystem: sudo dnf install nvidia-gds Add libcuda.so symbolic link, if necessary The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory.
For pre-existing projects which use libcuda.so , it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory.
Reboot the system: sudo reboot Perform the post-installation actions. 3.3. RHEL 9 / Rocky 9  3.3.1.
The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) Satisfy third-party package dependency: Satisfy DKMS dependency : The NVIDIA driver RPM packages depend on other external packages, such as DKMS and libvdpau .
To enable EPEL: sudo dnf install https: dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm Enable optional repos: On RHEL 9 Linux only, execute the following steps to enable optional repositories.
On x86_64 systems: subscription-manager repos --enable=rhel-9-for-x86_64-appstream-rpms subscription-manager repos --enable=rhel-9-for-x86_64-baseos-rpms subscription-manager repos --enable=codeready-builder-for-rhel-9-x86_64-rpms Remove Outdated Signing Key: sudo rpm --erase gpg-pubkey-7fa2af80* Choose an installation method: local repo or network repo . 3.3.2. Local Repo Installation for RHEL 9 / Rocky 9  Install local repository on file system: sudo rpm --install cuda-repo--X-Y-local-*..rpm 3.3.3.
Network Repo Installation for RHEL 9 / Rocky 9  Enable the network repo: sudo dnf config-manager --add-repo https: developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo where $distro/$arch should be replaced by one of the following: rhel9/cross-linux-sbsa rhel9/sbsa rhel9/x86_64 Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685 .
For upgrades, you must also also fetch an updated .repo entry: sudo dnf config-manager --add-repo https: developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo Clean Yum repository cache: sudo dnf clean expire-cache 3.3.4.
Common Instructions for RHEL 9 / Rocky 9  These instructions apply to both local and network installation. 3.4. KylinOS 10  3.4.1.
The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) Choose an installation method: local repo or network repo . 3.4.2. Local Repo Installation for KylinOS  Install local repository on file system: sudo rpm --install cuda-repo-kylin10-X-Y-local-*..rpm 3.4.3.
Network Repo Installation for KylinOS  Enable the network repo: sudo dnf config-manager --add-repo https: developer.download.nvidia.com/compute/cuda/repos/kylin10/x86_64/cuda-$distro.repo Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685 .
Common Instructions for KylinOS 10  These instructions apply to both local and network installation. 3.5. Fedora  3.5.1.
The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) Remove Outdated Signing Key: sudo rpm --erase gpg-pubkey-7fa2af80* Choose an installation method: local repo or network repo . 3.5.2. Local Repo Installation for Fedora  Install local repository on file system: sudo rpm --install cuda-repo--X-Y-local-*.x86_64.rpm where distro is fedora37 or fedora39 , for example.
3.5.3. Network Repo Installation for Fedora  Enable the network repo: sudo dnf config-manager --add-repo https: developer.download.nvidia.com/compute/cuda/repos/$distro/x86_64/cuda-$distro.repo where $distro should be replaced by one of the following: fedora37 fedora39 Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685 .
On a fresh installation of Fedora, the dnf package manager will prompt the user to accept new keys when installing packages the first time.
For upgrades, you must also fetch an updated .repo entry: sudo dnf config-manager --add-repo https: developer.download.nvidia.com/compute/cuda/repos/$distro/x86_64/cuda-$distro.repo Clean DNF repository cache: sudo dnf clean expire-cache 3.5.4.
Common Installation Instructions for Fedora  These instructions apply to both local and network installation for Fedora.
Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms sudo dnf install cuda-toolkit Note The CUDA driver installation may fail if the RPMFusion non-free repository is enabled.
In this case, CUDA installations should temporarily disable the RPMFusion non-free repository.
sudo dnf --disablerepo="rpmfusion-nonfree*" install cuda It may be necessary to rebuild the grub configuration files, particularly if you use a non-default partition scheme.
If so, then run this below command, and reboot the system: sudo grub2-mkconfig -o /boot/grub2/grub.cfg Reboot the system: sudo reboot Add libcuda.so symbolic link, if necessary: The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory.
The kernel development packages for the currently running kernel can be installed with: sudo zypper install -y kernel--devel= To run the above command, you will need the variant and version of the currently running kernel.
Use the output of the uname command to determine the currently running kernel’s variant and version: $ uname -r 3.16.6-2-default In the above example, the variant is default and version is 3.16.6-2 .
The kernel development packages for the default kernel variant can be installed with: sudo zypper install -y kernel-default-devel=$(uname -r | sed 's/\-default ') The kernel headers and development packages for the currently running kernel can be installed with: sudo zypper install -y kernel--devel= On SLES12 SP4, install the Mesa-libgl-devel Linux packages before proceeding.
Add the user to the video group: sudo usermod -a -G video Remove Outdated Signing Key: sudo rpm --erase gpg-pubkey-7fa2af80* Choose an installation method: local repo or network repo . 3.6.2. Local Repo Installation for SLES  Install local repository on file system: sudo rpm --install cuda-repo-sles15-X-Y-local-*.x86_64.rpm 3.6.3.
Network Repo Installation for SLES  Enable the network repo: sudo zypper addrepo https: developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo where $distro/$arch should be replaced by one of the following: sles15/cross-linux-sbsa sles15/sbsa sles15/x86_64 Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685 .
On a fresh installation of SLES, the zypper package manager will prompt the user to accept new keys when installing packages the first time.
For upgrades, you must also also fetch an updated .repo entry: sudo zypper removerepo cuda-$distro-$arch sudo zypper addrepo https: developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo Refresh Zypper repository cache: sudo SUSEConnect --product PackageHub/15/ sudo zypper refresh 3.6.4.
Common Installation Instructions for SLES  These instructions apply to both local and network installation for SLES.
Install CUDA SDK: sudo zypper install cuda-toolkit Install CUDA Samples GL dependencies: Refer to CUDA Cross-Platform Samples . 3.7. OpenSUSE  3.7.1.
The kernel development packages for the default kernel variant can be installed with: sudo zypper install -y kernel-default-devel=$(uname -r | sed 's/\-default ') Add the user to the video group: sudo usermod -a -G video Remove Outdated Signing Key: sudo rpm --erase gpg-pubkey-7fa2af80* Choose an installation method: local repo or network repo . 3.7.2. Local Repo Installation for OpenSUSE  Install local repository on file system: sudo rpm --install cuda-repo-opensuse15-.x86_64.rpm 3.7.3.
Network Repo Installation for OpenSUSE  Enable the network repo: sudo zypper addrepo https: developer.download.nvidia.com/compute/cuda/repos/opensuse15/x86_64/cuda-opensuse15.repo Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685 .
On fresh installation of openSUSE, the zypper package manager will prompt the user to accept new keys when installing packages the first time.
For upgrades, you must also also fetch an updated .repo entry: sudo zypper removerepo cuda-opensuse15-x86_64 sudo zypper addrepo https: developer.download.nvidia.com/compute/cuda/repos/opensuse15/x86_64/cuda-opensuse15.repo Refresh Zypper repository cache: sudo zypper refresh 3.7.4.
Common Installation Instructions for OpenSUSE  These instructions apply to both local and network installation for OpenSUSE.
Install CUDA SDK: sudo zypper install cuda-toolkit Reboot the system: sudo reboot Perform the post-installation actions. 3.8. WSL  These instructions must be used if you are installing in a WSL environment.
Do not use the Ubuntu instructions in this case; it is important to not install the cuda-drivers packages within the WSL environment. 3.8.1. Prepare WSL  Perform the pre-installation actions.
Remove Outdated Signing Key: sudo apt-key del 7fa2af80 Choose an installation method: local repo or network repo . 3.8.2. Local Repo Installation for WSL  Install local repositiry on file system: sudo dpkg -i cuda-repo-wsl-ubuntu-X-Y-local_*_x86_64.deb Enroll ephemeral public GPG key: sudo cp /var/cuda-repo-wsl-ubuntu-X-Y-local/cuda-*-keyring.gpg /usr/share/keyrings/ 3.8.3.
Network Repo Installation for WSL  The new GPG public key for the CUDA repository (Debian-based distros) is 3bf863cc .
This must be enrolled on the system, either using the cuda-keyring package or manually; the apt-key command is deprecated and not recommended.
Common Installation Instructions for WSL  These instructions apply to both local and network installation for WSL.
Update the Apt repository cache: sudo apt-get update Install CUDA SDK: sudo apt-get install cuda-toolkit Perform the post-installation actions. 3.9. Ubuntu  3.9.1.
The kernel headers and development packages for the currently running kernel can be installed with: sudo apt-get install linux-headers-$(uname -r) Remove Outdated Signing Key: sudo apt-key del 7fa2af80 Choose an installation method: local repo or network repo . 3.9.2. Local Repo Installation for Ubuntu  Install local repository on file system: sudo dpkg -i cuda-repo-__.deb Enroll ephemeral public GPG key: sudo cp /var/cuda-repo--X-Y-local/cuda-*-keyring.gpg /usr/share/keyrings/ Add pin file to prioritize CUDA repository: wget https: developer.download.nvidia.com/compute/cuda/repos x86_64/cuda-.pin sudo mv cuda-.pin /etc/apt/preferences.d/cuda-repository-pin-600 3.9.3.
Network Repo Installation for Ubuntu  The new GPG public key for the CUDA repository is 3bf863cc .
Common Installation Instructions for Ubuntu  These instructions apply to both local and network installation for Ubuntu.
Update the Apt repository cache: sudo apt-get update Install CUDA SDK: Note These two commands must be executed separately.
sudo apt-get install cuda-toolkit To include all GDS packages: sudo apt-get install nvidia-gds Reboot the system sudo reboot Perform the Post-installation Actions 3.10.
The kernel headers and development packages for the currently running kernel can be installed with: sudo apt-get install linux-headers-$(uname -r) Enable the contrib repository: sudo add-apt-repository contrib Remove Outdated Signing Key: sudo apt-key del 7fa2af80 Choose an installation method: local repo or network repo . 3.10.2. Local Repo Installation for Debian  Install local repository on file system: sudo dpkg -i cuda-repo--X-Y-local_*_x86_64.deb Enroll ephemeral public GPG key: sudo cp /var/cuda-repo--X-Y-local/cuda-*-keyring.gpg /usr/share/keyrings/ 3.10.3.
Network Repo Installation for Debian  The new GPG public key for the CUDA repository (Debian-based distros) is 3bf863cc .
Install the new cuda-keyring package: wget https: developer.download.nvidia.com/compute/cuda/repos /cuda-keyring_1.1-1_all.deb where $distro/$arch should be replaced by one of the following: debian10/x86_64 debian11/x86_64 sudo dpkg -i cuda-keyring_1.1-1_all.deb Or if you are unable to install the cuda-keyring package, you can optionally: Enroll the new signing key manually: wget https: developer.download.nvidia.com/compute/cuda/repos x86_64/cuda-archive-keyring.gpg sudo mv cuda-archive-keyring.gpg /usr/share/keyrings/cuda-archive-keyring.gpg Enable the network repository: echo "deb [signed-by=/usr/share/keyrings/cuda-archive-keyring.gpg] https: developer.download.nvidia.com/compute/cuda/repos x86_64/ /" | sudo tee /etc/apt/sources.list.d/cuda--x86_64.list 3.10.4.
Common Installation Instructions for Debian  These instructions apply to both local and network installation for Debian.
Update the Apt repository cache: sudo apt-get update Note If you are using Debian 10, you may instead need to run: sudo apt-get --allow-releaseinfo-change update Install CUDA SDK: sudo apt-get -y install cuda Reboot the system: sudo reboot Perform the post-installation actions. 3.11. Amazon Linux 2023  3.11.1.
The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) kernel-modules-extra-$(uname -r) Choose an installation method: local repo or network repo . 3.11.2. Local Repo Installation for Amazon Linux  Install local repository on file system: sudo rpm --install cuda-repo-amzn2023-X-Y-local-*.x86_64.rpm 3.11.3.
Network Repo Installation for Amazon Linux  Enable the network repository: sudo dnf config-manager --add-repo https: developer.download.nvidia.com/compute/cuda/repos/amzn2023/x86_64/cuda-amzn2023.repo Clean DNF repository cache: sudo dnf clean expire-cache 3.11.4.
Common Installation Instructions for Amazon Linux  These instructions apply to both local and network installation for Amazon Linux.
Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms sudo dnf install cuda-toolkit Install GPUDirect Filesystem: sudo dnf install nvidia-gds Add libcuda.so symbolic link, if necessary: The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. 3.12. Additional Package Manager Capabilities  Below are some additional capabilities of the package manager that users can take advantage of.
This package will install the full set of other CUDA packages required for native development and should cover most scenarios.
On supported platforms, the cuda-cross-aarch64 and cuda-cross-sbsa packages install all the packages required for cross-platform development to arm64-Jetson and arm64-Server, respectively.
The libraries and header files of the target architecture’s display driver package are also installed to enable the cross compilation of driver applications.
Note 32-bit compilation native and cross-compilation is removed from CUDA 12.0 and later Toolkit.
CUDA Driver will continue to support running existing 32-bit applications on existing GPUs except Hopper.
The packages installed by the packages above can also be installed individually by specifying their names explicitly.
The list of available packages be can obtained with: yum --disablerepo="*" --enablerepo="cuda*" list available # RedHat dnf --disablerepo="*" --enablerepo="cuda*" list available # Fedora zypper packages -r cuda # OpenSUSE & SLES cat /var/lib/apt/lists/*cuda*Packages | grep "Package:" # Ubuntu 3.12.2.
Meta Packages  Meta packages are RPM/Deb/Conda packages which contain no (or few) files but have multiple dependencies.
They are used to install many CUDA packages when you may not know the details of the packages you want.
Table 5 Meta Packages Available for CUDA 12.4  Meta Package Purpose cuda Installs all CUDA Toolkit and Driver packages.
cuda-toolkit-12-5 Installs all CUDA Toolkit packages required to develop CUDA applications.
cuda-runtime-12-5 Installs all CUDA Toolkit packages required to run CUDA applications, as well as the Driver packages.
Will not upgrade beyond the 555 branch drivers. 3.12.3. Optional 32-bit Packages for Linux x86_64 .deb/.rpm  These packages provide 32-bit driver libraries needed for things such as Steam (popular game app store/launcher), older video games, and some compute applications.
For Debian 10 and Debian 11: sudo dpkg --add-architecture i386 sudo apt-get update sudo apt-get install libcuda1-i386 nvidia-driver-libs-i386 For Debian 12: sudo dpkg --add-architecture i386 sudo apt-get update apt install nvidia-driver-libs:i386 For Ubuntu: sudo dpkg --add-architecture i386 sudo apt-get update sudo apt-get install libnvidia-compute-:i386 libnvidia-decode-:i386 \ libnvidia-encode-:i386 libnvidia-extra-:i386 libnvidia-fbc1-:i386 \ libnvidia-gl-:i386 Where is the driver version, for example 495.
For Fedora and RHEL8+: sudo dnf install nvidia-driver-cuda-libs.i686 nvidia-driver-devel.i686 \ nvidia-driver-libs.i686 nvidia-driver-NvFBCOpenGL.i686 nvidia-driver-NVML.i686 Note There is no modularity profile support.
For openSUSE/SLES: sudo zypper install nvidia-compute-G06-32bit nvidia-gl-G06-32bit nvidia-video-G06-32bit 3.12.4.
Package Upgrades  The cuda package points to the latest stable release of the CUDA Toolkit.
When a new version is available, use the following commands to upgrade the toolkit and driver: sudo dnf install cuda-toolkit # Fedora, RHEL9, RHEL8, and KylinOS sudo zypper install cuda-toolkit # OpenSUSE and SLES sudo apt-get install cuda-toolkit # Ubuntu and Debian The cuda-cross- packages can also be upgraded in the same manner.
The cuda-drivers package points to the latest driver release available in the CUDA repository.
When a new version is available, use the following commands to upgrade the driver: sudo dnf module install nvidia-driver:latest-dkms # Fedora, RHEL9, RHEL8, and KylinOS sudo zypper install cuda-drivers nvidia-gfxG04-kmp-default # OpenSUSE and SLES sudo apt-get install cuda-drivers # Ubuntu and Debian Some desktop environments, such as GNOME or KDE, will display a notification alert when new packages are available.
To avoid any automatic upgrade, and lock down the toolkit installation to the X.Y release, install the cuda-X-Y or cuda-cross--X-Y package.
For instance, to install both the X.Y CUDA Toolkit and the X.Y+1 CUDA Toolkit, install the cuda-X.Y and cuda-X.Y+1 packages. 4. Driver Installation  This section is for users who want to install a specific driver version.
For Debian and Ubuntu: sudo apt-get install cuda-drivers- For example: sudo apt-get install cuda-drivers-535 For OpenSUSE and SLES: sudo zypper -v install cuda-drivers- For example: sudo zypper -v install cuda-drivers-550 This allows you to get the highest version in the specified branch.
For Fedora and RHEL8+: sudo dnf module install nvidia-driver:/ where profile by default is “ default ” and does not need to be specified.
Example dkms streams: 450-dkms or latest-dkms Example precompiled streams: 450 or latest Note Precompiled streams are only supported on RHEL8 x86_64 and RHEL9 x86_64.
To uninstall or change streams on Fedora and RHEL8: sudo dnf module remove --all nvidia-driver sudo dnf module reset nvidia-driver 5.
NVIDIA Open GPU Kernel Modules  The NVIDIA Linux GPU Driver contains several kernel modules: nvidia.ko nvidia-modeset.ko nvidia-uvm.ko nvidia-drm.ko nvidia-peermem.ko Starting in the 515 driver release series, two “flavors” of these kernel modules are provided: Proprietary - this is the flavor that NVIDIA has historically shipped.
With every driver release, the source code to the open kernel modules will be published on https: github.com/NVIDIA/open-gpu-kernel-modules and a tarball will be provided on https: download.nvidia.com/XFree86/ .
lspci | grep VGA Experimental support for GeForce and Quadro SKUs can be enabled with: echo "options nvidia NVreg_OpenRmEnableUnsupportedGpus=1" | sudo tee /etc/modprobe.d/nvidia-gsp.conf To install NVIDIA Open GPU Kernel Modules, follow the instructions below. 5.1. CUDA Runfile  Pass the CLI argument to the CUDA runfile to opt in to NVIDIA Open GPU Kernel Modules: sh cuda___linux.run -m=kernel-open 5.2.
Debian  Install the NVIDIA Open GPU Kernel Modules package: sudo apt-get install nvidia-kernel-open-dkms Install the rest of the NVIDIA driver packages: sudo apt-get install cuda-drivers OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package: sudo apt-get install -v nvidia-kernel-open-dkms=-1 Install the rest of the NVIDIA driver packages: sudo apt-get install -v cuda-drivers- For example: sudo apt-get install -v nvidia-kernel-open-dkms=550.90.07-1 sudo apt-get install -v cuda-drivers-550 5.3.
Fedora  Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:open-dkms OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:-open 5.4.
KylinOS 10  Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:open-dkms OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:-open 5.5.
RHEL 9 and Rocky 9  Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:open-dkms OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:-open 5.6.
RHEL 8 and Rocky 8  Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:open-dkms OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:-open 5.7.
OpenSUSE and SLES  Install the NVIDIA Open GPU Kernel Modules package: sudo zypper install nvidia-open-driver-G06-kmp-default Install the rest of the NVIDIA driver packages: sudo zypper install cuda-drivers OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package: sudo zypper -v install $(zypper search -s nvidia-open-driver-G06-kmp- | sed 's| ||g' | awk -F '|' '  {print $2"="$4}') Install the rest of the NVIDIA driver packages: sudo zypper -v install cuda-drivers- For example: sudo zypper -v install $(zypper search -s nvidia-open-driver-G06-kmp-default | sed 's| ||g' | awk -F '|' '/550/ {print $2"="$4}') sudo zypper -v install cuda-drivers-550 5.8.
Ubuntu  Install the NVIDIA Open GPU Kernel Modules package: sudo apt-get install nvidia-driver--open Install the rest of the NVIDIA driver packages: sudo apt-get install cuda-drivers- Note End-users on Ubuntu should upgrade their NVIDIA Open GPU kernel modules using the following: sudo apt-get install --verbose-versions nvidia-kernel-source-550-open cuda-drivers-550 OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package: sudo apt-get install -v nvidia-driver--open Install the rest of the NVIDIA driver packages: sudo apt-get install -v cuda-drivers- For example: sudo apt-get install -v nvidia-driver-550-open sudo apt-get install -v cuda-drivers-550 6.
Precompiled Streams  Precompiled streams offer an optional method of streamlining the installation process.
The advantages of precompiled streams: Precompiled: faster boot up after driver and/or kernel updates Pre-tested: kernel and driver combination has been validated Removes gcc dependency: no compiler installation required Removes dkms dependency: enabling EPEL repository not required Removes kernel-devel and kernel-headers dependencies: no black screen if matching packages are missing When using precompiled drivers, a plugin for the dnf package manager is enabled that cleans up stale .ko files.
To prevent system breakages, the NVIDIA dnf plugin also prevents upgrading to a kernel for which no precompiled driver yet exists.
This can delay the application of security fixes but ensures that a tested kernel and driver combination is always used.
A warning is displayed by dnf during that upgrade situation: NOTE: Skipping kernel installation since no NVIDIA driver kernel module package kmod-nvidia-${driver}-${kernel} ...
latest-dkms always updates to the highest versioned driver (non-precompiled): sudo dnf module install nvidia-driver:latest-dkms Note This is the default stream.
-dkms locks the driver updates to the specified driver branch (non-precompiled): sudo dnf module install nvidia-driver:-dkms Note Valid streams include 520-dkms , 515-dkms , 470-dkms , and 450-dkms . 6.1. Precompiled Streams Support Matrix  This table shows the supported precompiled and legacy DKMS streams for each driver.
NVIDIA Driver Precompiled Stream Legacy DKMS Stream Open DKMS Stream Highest version latest latest-dkms open-dkms Locked at 520.x 520 520-dkms 520-open Locked at 515.x 515 515-dkms 515-open Prior to switching between module streams, first reset: sudo dnf module reset nvidia-driver Note This is also required for upgrading between branch locked streams.
Modularity Profiles  Modularity profiles work with any supported modularity stream and allow for additional use cases.
List of nvidia-driver Module Profiles  Stream Profile Use Case Default /default Installs all the driver packages in a stream.
NVSwitch Fabric /fm Installs all the driver packages plus components required for bootstrapping an NVSwitch system (including the Fabric Manager and NSCQ telemetry).
For example: sudo dnf module nvidia-driver:/default sudo dnf module nvidia-driver:/ks sudo dnf module nvidia-driver:/fm sudo dnf module nvidia-driver:/src You can install multiple modularity profiles using BASH curly brace expansion, for example: sudo dnf module install nvidia-driver:latest/{default,src} See https: developer.nvidia.com/blog/streamlining-nvidia-driver-deployment-on-rhel-8-with-modularity-streams in the Developer Blog and https: developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/precompiled/ for more information. 7. Kickstart Installation  7.1.
This section describes the installation and configuration of CUDA when using the standalone installer.
The standalone installer is a “.run” file and is completely self-contained. 8.1. Runfile Overview  The Runfile installation installs the NVIDIA Driver and CUDA Toolkit via an interactive ncurses-based interface.
Distribution-specific instructions on disabling the Nouveau drivers as well as steps for verifying device node creation are also provided.
The Runfile installation does not include support for cross-platform development. 8.2. Installation  Perform the pre-installation actions .
This can usually be accomplished by adding the number “3” to the end of the system’s kernel boot parameters.
Since the NVIDIA drivers are not yet installed, the text terminals may not display correctly.
Consult your system’s bootloader documentation for information on how to make the above boot parameter changes.
The reboot is required to completely unload the Nouveau drivers and prevent the graphical interface from loading.
The CUDA driver cannot be installed while the Nouveau drivers are loaded or while the graphical interface is active.
If the Nouveau drivers are still loaded, consult your distribution’s documentation to see if further steps are needed to disable Nouveau.
Run the installer and follow the on-screen prompts: sudo sh cuda__linux.run The installer will prompt for the following: EULA Acceptance CUDA Driver installation CUDA Toolkit installation, location, and /usr/local/cuda symbolic link The default installation location for the toolkit is /usr/local/cuda-12.4 : The /usr/local/cuda symbolic link points to the location where the CUDA Toolkit was installed.
This link allows projects to use the latest CUDA Toolkit without any configuration file update.
When the current privileges are insufficient to perform an action, the installer will ask for the user’s password to attempt to install with root privileges.
Actions that cause the installer to attempt to install with root privileges are: installing the CUDA Driver installing the CUDA Toolkit to a location the user does not have permission to write to creating the /usr/local/cuda symbolic link Running the installer with sudo , as shown above, will give permission to install to directories that require root permissions.
Directories and files created while running the installer with sudo will have root ownership.
If installing the driver, the installer will also ask if the openGL libraries should be installed.
If the GPU used for display is not an NVIDIA GPU, the NVIDIA openGL libraries should not be installed.
Otherwise, the openGL libraries used by the graphics driver of the non-NVIDIA GPU will be overwritten and the GUI will not work.
If performing a silent installation, the --no-opengl-libs option should be used to prevent the openGL libraries from being installed.
If the GPU used for display is an NVIDIA GPU, the X server configuration file, /etc/X11/xorg.conf , may need to be modified.
In some cases, nvidia-xconfig can be used to automatically generate an xorg.conf file that works for the system.
For non-standard systems, such as those with more than one GPU, it is recommended to manually edit the xorg.conf file.
Note Installing Mesa may overwrite the /usr/lib/libGL.so that was previously installed by the NVIDIA driver, so a reinstallation of the NVIDIA driver might be required after installing these libraries.
Reboot the system to reload the graphical interface: sudo reboot Verify the device nodes are created properly.
Perform the post-installation actions . 8.3. Disabling Nouveau  To install the Display Driver, the Nouveau drivers must first be disabled.
The Nouveau drivers are loaded if the following command prints anything: lsmod | grep nouveau 8.3.1.
Fedora  Create a file at /usr/lib/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force Run the following command: sudo grub2-mkconfig -o /boot/grub2/grub.cfg Reboot the system. 8.3.2. RHEL / Rocky and KylinOS  Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force 8.3.3.
OpenSUSE  Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initrd: sudo /sbin/mkinitrd 8.3.4.
SLES  No actions to disable Nouveau are required as Nouveau is not installed on SLES. 8.3.5. WSL  No actions to disable Nouveau are required as Nouveau is not installed on WSL.
8.3.6. Ubuntu  Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u 8.3.7.
Debian  Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u 8.4.
Device Node Verification  Check that the device files /dev/nvidia* exist and have the correct (0666) file permissions.
These files are used by the CUDA Driver to communicate with the kernel-mode portion of the NVIDIA Driver.
Applications that use the NVIDIA driver, such as a CUDA application or the X server (if any), will normally automatically create these files if they are missing using the setuid nvidia-modprobe tool that is bundled with the NVIDIA Driver.
However, some systems disallow setuid binaries, so if these files do not exist, you can create them manually by using a startup script such as the one below: #!/bin/bash /sbin/modprobe nvidia if [ "$?" -eq 0 ]; then # Count the number of NVIDIA controllers found.
NVDEVS=`lspci | grep -i NVIDIA` N3D=`echo "$NVDEVS" | grep "3D controller" | wc -l` NVGA=`echo "$NVDEVS" | grep "VGA compatible controller" | wc -l` N=`expr $N3D + $NVGA - 1` for i in `seq 0 $N`; do mknod -m 666 /dev/nvidia$i c 195 $i done mknod -m 666 /dev/nvidiactl c 195 255 else exit 1 fi /sbin/modprobe nvidia-uvm if [ "$?" -eq 0 ]; then # Find out the major device number used by the nvidia-uvm driver D=`grep nvidia-uvm /proc/devices | awk '{print $1}'` mknod -m 666 /dev/nvidia-uvm c $D 0 else exit 1 fi 8.5.
Advanced Options  Action Options Used Explanation Silent Installation --silent Required for any silent installation.
Performs an installation with no further user-input and minimal command-line output based on the options provided below.
At least one of --driver , --uninstall , and --toolkit must be passed if running with non-root permissions.
Extraction --extract= Extracts to the the following: the driver runfile, the raw files of the toolkit to .
This is especially useful when one wants to install the driver using one or more of the command-line options provided by the driver installer which are not exposed in this installer.
Overriding Installation Checks --override Ignores compiler, third-party library, and toolkit detection checks which would prevent the CUDA Toolkit from installing.
No OpenGL Libraries --no-opengl-libs Prevents the driver installation from installing NVIDIA’s GL libraries.
Overriding Kernel Source --kernel-source-path= Tells the driver installation to use as the kernel source directory when building the NVIDIA kernel module.
Running nvidia-xconfig --run-nvidia-xconfig Tells the driver installation to run nvidia-xconfig to update the system X configuration file so that the NVIDIA X driver is used.
This option should only be used to work around failures to build or install the nvidia-drm kernel module on systems that do not need the provided features.
Custom Temporary Directory Selection --tmpdir= Performs any temporary actions within instead of /tmp .
Useful in cases where /tmp cannot be used (doesn’t exist, is full, is mounted with ‘noexec’, etc.).
Show Installer Options --help Prints the list of command-line options to stdout. 8.6. Uninstallation  To uninstall the CUDA Toolkit, run the uninstallation script provided in the bin directory of the toolkit.
By default, it is located in /usr/local/cuda-12.4/bin : sudo /usr/local/cuda-12.4/bin/cuda-uninstaller To uninstall the NVIDIA Driver, run nvidia-uninstall : sudo /usr/bin/nvidia-uninstall To enable the Nouveau drivers, remove the blacklist file created in the Disabling Nouveau section, and regenerate the kernel initramfs/initrd again as described in that section. 9. Conda Installation  This section describes the installation and configuration of CUDA when using the Conda installer.
The Conda packages are available at https: anaconda.org/nvidia . 9.1. Conda Overview  The Conda installation installs the CUDA Toolkit.
9.2. Installing CUDA Using Conda  To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda -c nvidia 9.3.
Uninstalling CUDA Using Conda  To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 9.4.
Installing Previous CUDA Releases  All Conda packages released under a specific CUDA version are labeled with that release version.
To install a previous version, include that label in the install command such as: conda install cuda -c nvidia/label/cuda-11.3.0 9.5.
Upgrading from cudatoolkit Package  If you had previously installed CUDA using the cudatoolkit package and want to maintain a similar install footprint, you can limit your installation to the following packages: cuda-libraries-dev cuda-nvcc cuda-nvtx cuda-cupti Note Some extra files, such as headers, will be included in this installation which were not included in the cudatoolkit package.
If you need to reduce your installation further, replace cuda-libraries-dev with the specific libraries you need. 10. Pip Wheels  NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python.
These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo.
If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules.
If these Python modules are out-of-date then the commands which follow later in this section may fail.
python3 -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module.
python3 -m pip install nvidia-pyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https: pypi.org/simple Procedure Install the CUDA runtime package: python3 -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: python3 -m pip install nvidia- Metapackages The following metapackages will install the latest version of the named component on Linux for the indicated CUDA version.
nvidia-cuda-runtime-cu12 nvidia-cuda-cccl-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-cuda-opencl-cu12 nvidia-cuda-nvrtc-cu12 nvidia-cublas-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvfatbin-cu12 nvidia-nvjitlink-cu12 nvidia-nvjpeg-cu12 nvidia-nvml-dev-cu12 nvidia-nvtx-cu12 These metapackages install the following packages: nvidia-cuda-runtime-cu125 nvidia-cuda-cccl-cu125 nvidia-cuda-cupti-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-opencl-cu125 nvidia-cublas-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-cuda-nvrtc-cu125 nvidia-cufft-cu125 nvidia-curand-cu125 nvidia-cusolver-cu125 nvidia-cusparse-cu125 nvidia-npp-cu125 nvidia-nvfatbin-cu125 nvidia-nvjitlink-cu125 nvidia-nvjpeg-cu125 nvidia-nvml-dev-cu125 nvidia-nvtx-cu125 11.
Tarball and Zip Archive Deliverables  In an effort to meet the needs of a growing customer base requiring alternative installer packaging formats, as well as a means of input into community CI/CD systems, tarball and zip archives are available for each component.
These tarball and zip archives, known as binary archives, are provided at https: developer.download.nvidia.com/compute/cuda/redist/ .
These component .tar.xz and .zip binary archives do not replace existing packages such as .deb, .rpm, runfile, conda, etc.
For each release, a JSON manifest is provided such as redistrib_11.4.2.json , which corresponds to the CUDA 11.4.2 release label (CUDA 11.4 update 2) which includes the release date, the name of each component, license name, relative URL for each platform and checksums.
Package maintainers are advised to check the provided LICENSE for each component prior to redistribution.
Instructions for developers using CMake and Bazel build systems are provided in the next sections. 11.1. Parsing Redistrib JSON  The following example of a JSON manifest contains keys for each component: name, license, version, and a platform array which includes relative_path, sha256, md5, and size (bytes) for each archive.
Importing Tarballs into CMake  The recommended module for importing these tarballs into the CMake build system is via FindCUDAToolkit (3.17 and newer).
The path to the extraction location can be specified with the CUDAToolkit_ROOT environmental variable.
For example CMakeLists.txt file and commands, see cmake/2_ExternalProject/ . 11.3. Importing Tarballs into Bazel  The recommended method of importing these tarballs into the Bazel build system is using http_archive and pkg_tar .
For an example, see bazel/1_pkg_tar/ . 12. CUDA Cross-Platform Environment  Cross development for arm64-sbsa is supported on Ubuntu 20.04, Ubuntu 22.04, RHEL 8, RHEL 9, and SLES 15.
Cross development for arm64-Jetson is only supported on Ubuntu 20.04 We recommend selecting a host development environment that matches the supported cross-target environment.
This selection helps prevent possible host/target incompatibilities, such as GCC or GLIBC version mismatches. 12.1. CUDA Cross-Platform Installation  Some of the following steps may have already been performed as part of the native Ubuntu installation .
To install the native CUDA Toolkit on the target system, refer to the native Ubuntu installation section.
Install repository meta-data package with: sudo dpkg -i cuda-repo-cross-_all.deb where indicates the operating system, architecture, and/or the version of the package.
Update the Apt repository cache: sudo apt-get update Install the appropriate cross-platform CUDA Toolkit: For aarch64: sudo apt-get install cuda-cross-aarch64 For QNX: sudo apt-get install cuda-cross-qnx Perform the post-installation actions. 12.2. CUDA Cross-Platform Samples  CUDA Samples are now located in https: github.com/nvidia/cuda-samples , which includes instructions for obtaining, building, and running the samples.
These actions are split into mandatory, recommended, and optional sections. 13.1. Mandatory Actions  Some actions must be taken after the installation before the CUDA Toolkit and Driver can be used.
13.1.1. Environment Setup  The PATH variable needs to include export PATH=/usr/local/cuda-12.4/bin${PATH:+:${PATH}} .
Nsight Compute has moved to /opt/nvidia/nsight-compute/ only in rpm/deb installation method.
To add this path to the PATH variable: export PATH=/usr/local/cuda-12.4/bin${PATH:+:${PATH}} In addition, when using the runfile installation method, the LD_LIBRARY_PATH variable needs to contain /usr/local/cuda-12.4/lib64 on a 64-bit system, or /usr/local/cuda-12.4/lib on a 32-bit system To change the environment variables for 64-bit operating systems: export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} To change the environment variables for 32-bit operating systems: export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Note that the above paths change when using a custom install path with the runfile installation method. 13.2. Recommended Actions  Other actions are recommended to verify the integrity of the installation.
13.2.1. Install Persistence Daemon  NVIDIA is providing a user-space daemon on Linux to support persistence of driver state across CUDA job runs.
The daemon approach provides a more elegant and robust solution to this problem than persistence mode.
The NVIDIA Persistence Daemon can be started as the root user by running: /usr/bin/nvidia-persistenced --verbose This command should be run on boot.
Consult your Linux distribution’s init documentation for details on how to automate this. 13.2.2. Install Writable Samples  CUDA Samples are now located in https: github.com/nvidia/cuda-samples , which includes instructions for obtaining, building, and running the samples.
13.2.3. Verify the Installation  Before continuing, it is important to verify that the CUDA toolkit can find and communicate correctly with the CUDA-capable hardware.
To do this, you need to compile and run some of the sample programs, located in https: github.com/nvidia/cuda-samples .
Note Ensure the PATH and, if using the runfile installation method, LD_LIBRARY_PATH variables are set correctly . 13.2.3.1. Verify the Driver Version  If you installed the driver, verify that the correct version of it is loaded.
If you did not install the driver, or are using an operating system where the driver is not loaded via a kernel module, such as L4T, skip this step.
When the driver is loaded, the driver version can be found by executing the command cat /proc/driver/nvidia/version Note that this command will not work on an iGPU/dGPU system. 13.2.3.2. Running the Binaries  After compilation, find and run deviceQuery from https: github.com/nvidia/cuda-samples .
If the CUDA software is installed and configured correctly, the output for deviceQuery should look similar to that shown in Figure 1 .
Valid Results from deviceQuery CUDA Sample  The exact appearance and the output lines might be different on your system.
The important outcomes are that a device was found (the first highlighted line), that the device matches the one on your system (the second highlighted line), and that the test passed (the final highlighted line).
If a CUDA-capable device and the CUDA Driver are installed but deviceQuery reports that no CUDA-capable devices are present, this likely means that the /dev/nvidia* files are missing or have the wrong permissions.
On systems where SELinux is enabled, you might need to temporarily disable this security feature to run deviceQuery .
Running the bandwidthTest program ensures that the system and the CUDA-capable device are able to communicate correctly.
Valid Results from bandwidthTest CUDA Sample  Note that the measurements for your CUDA-capable device description will vary from system to system.
The important point is that you obtain measurements, and that the second-to-last line (in Figure 2 ) confirms that all necessary tests passed.
Should the tests not pass, make sure you have a CUDA-capable NVIDIA GPU on your system and make sure it is properly installed.
If you run into difficulties with the link step (such as libraries not being found), consult the Linux Release Notes found in https: github.com/nvidia/cuda-samples . 13.2.4. Install Nsight Eclipse Plugins  To install Nsight Eclipse plugins, an installation script is provided: /usr/local/cuda-12.4/bin/nsight_ee_plugins_manage.sh install Refer to Nsight Eclipse Plugins Installation Guide for more details.
13.2.5. Local Repo Removal  Removal of the local repo installer is recommended after installation of CUDA SDK .
Ubuntu and Debian sudo apt-get remove --purge "cuda-repo--X-Y-local*" Fedora sudo dnf remove "cuda-repo--X-Y-local*" RHEL 9 / Rocky Linux 9 and RHEL 8 / Rocky Linux 8 sudo dnf remove "cuda-repo--X-Y-local*" openSUSE 15 and SLES 15 sudo zypper remove "cuda-repo--X-Y-local*" Removal of the local repo installer is recommended after installation of NVIDA driver .
Ubuntu and Debian sudo apt-get remove --purge "nvidia-driver-local-repo-*" Fedora sudo dnf remove "nvidia-driver-local-repo-*" RHEL 9 / Rocky Linux 9 and RHEL 8 / Rocky Linux 8 sudo dnf remove "nvidia-driver-local-repo-*" openSUSE 15 and SLES 15 sudo zypper remove "nvidia-driver-local-repo-*" 13.3.
Optional Actions  Other options are not necessary to use the CUDA Toolkit, but are available to provide additional features. 13.3.1. Install Third-party Libraries  Some CUDA samples use third-party libraries which may not be installed by default on your system.
Install the Source Code for cuda-gdb  The cuda-gdb source must be explicitly selected for installation with the runfile installation method.
During the installation, in the component selection page, expand the component “CUDA Tools 12.4” and select cuda-gdb-src for installation.
To obtain a copy of the source code for cuda-gdb using the RPM and Debian installation methods, the cuda-gdb-src package must be installed.
The source code is installed as a tarball in the /usr/local/cuda-12.4/extras directory. 13.3.3. Select the Active Version of CUDA  For applications that rely on the symlinks /usr/local/cuda and /usr/local/cuda-MAJOR , you may wish to change to a different installed version of CUDA using the provided alternatives.
To show the active version of CUDA and all available versions: update-alternatives --display cuda To show the active minor version of a given major CUDA release: update-alternatives --display cuda-12 To update the active version of CUDA: sudo update-alternatives --config cuda 14.
Advanced Setup  Below is information on some advanced setup scenarios which are not covered in the basic instructions above.
Table 8 Advanced Setup Scenarios when Installing CUDA  Scenario Instructions Install CUDA using the Package Manager installation method without installing the NVIDIA GL libraries.
Fedora Install CUDA using the following command: sudo dnf install cuda-toolkit-12-4 \ nvidia-driver-cuda akmod-nvidia Follow the instructions here to ensure that Nouveau is disabled.
If performing an upgrade over a previous installation, the NVIDIA kernel module may need to be rebuilt by following the instructions here .
OpenSUSE/SLES On some system configurations the NVIDIA GL libraries may need to be locked before installation using: sudo zypper addlock nvidia-glG04 Install CUDA using the following command: sudo zypper install --no-recommends cuda-toolkit-12-4 \ nvidia-computeG04 \ nvidia-gfxG04-kmp-default Follow the instructions here to ensure that Nouveau is disabled.
Instead, the driver packages integrate with the Bumblebee framework to provide a solution for users who wish to control what applications the NVIDIA drivers are used for.
Upgrade from a RPM/Deb driver installation which includes the diagnostic driver packages to a driver installation which does not include the diagnostic driver packages.
RHEL/CentOS Remove diagnostic packages using the following command: sudo yum remove cuda-drivers-diagnostic \ xorg-x11-drv-nvidia-diagnostic Follow the instructions here to continue installation as normal.
Fedora Remove diagnostic packages using the following command: sudo dnf remove cuda-drivers-diagnostic \ xorg-x11-drv-nvidia-diagnostic Follow the instructions here to continue installation as normal.
OpenSUSE/SLES Remove diagnostic packages using the following command: sudo zypper remove cuda-drivers-diagnostic \ nvidia-diagnosticG04 Follow the instructions here to continue installation as normal.
Ubuntu Remove diagnostic packages using the following command: sudo apt-get purge cuda-drivers-diagnostic \ nvidia-384-diagnostic Follow the instructions here to continue installation as normal.
The Device entry should resemble the following: Section "Device" Identifier "Device0" Driver "driver_name" VendorName "vendor_name" BusID "bus_id" EndSection The details will you will need to add differ on a case-by-case basis.
For example, if you have two NVIDIA GPUs and you want the first GPU to be used for display, you would replace “ driver_name ” with “ nvidia ”, “ vendor_name ” with “ NVIDIA Corporation ” and “ bus_id ” with the Bus ID of the GPU.
RPM The RPM packages don’t support custom install locations through the package managers (Yum and Zypper), but it is possible to install the RPM packages to a custom location using rpm’s --relocate parameter: sudo rpm --install --relocate /usr/local/cuda-12.4=/new/toolkit package.rpm You will need to install the packages in the correct dependency order; this task is normally taken care of by the package managers.
For example, if package “foo” has a dependency on package “bar”, you should install package “bar” first, and package “foo” second.
You can check the dependencies of a RPM package as follows: rpm -qRp package.rpm Note that the driver packages cannot be relocated.
It is however possible to extract the contents of the Deb packages and move the files to the desired install location.
Runfile The Runfile can be extracted into the standalone Toolkit and Driver Runfiles by using the --extract parameter.
The Toolkit standalone Runfiles can be further extracted by running: ./runfile.run --tar mxvf The Driver Runfile can be extracted by running: ./runfile.run -x RPM The RPM packages can be extracted by running: rpm2cpio package.rpm | cpio -idmv Deb The Deb packages can be extracted by running: dpkg-deb -x package.deb output_dir Modify Ubuntu’s apt package manager to query specific architectures for specific repositories.
This is useful when a foreign architecture has been added, causing “404 Not Found” errors to appear when the repository meta-data is updated.
Each repository you wish to restrict to specific architectures must have its sources.list entry modified.
This is done by modifying the /etc/apt/sources.list file and any files containing repositories you wish to restrict under the /etc/apt/sources.list.d/ directory.
Normally, it is sufficient to modify only the entries in /etc/apt/sources.list An architecture-restricted repository entry looks like: deb [arch=,] For example, if you wanted to restrict a repository to only the amd64 and i386 architectures, it would look like: deb [arch=amd64,i386] It is not necessary to restrict the deb-src repositories, as these repositories don’t provide architecture-specific packages.
For example: nvidia: Unknown symbol drm_open (err 0) Check to see if there are any optionally installable modules that might provide these symbols which are not currently installed.
For the example of the drm_open symbol, check to see if there are any packages which provide drm_open and are not already installed.
For instance, on Ubuntu 14.04, the linux-image-extra package provides the DRM kernel module (which provides drm_open ).
This package is optional even though the kernel headers reflect the availability of DRM regardless of whether this package is installed or not.
This can occur on systems with limited storage in the TMP directory (usually /tmp ), or on systems which use a tmpfs in memory to handle temporary storage.
In this case, the --tmpdir command-line option should be used to instruct the runfile to use a directory with sufficient space to extract into.
Wayland is disabled during installation of the Fedora driver RPM due to compatability issues.
To re-enable Wayland, comment out this line in /etc/gdm/custom.conf : WaylandEnable=false In case of the error: E: Failed to fetch file:/var/cuda-repo File not found Debian and Ubuntu This can occur when installing CUDA after uninstalling a different version.
Use the following command before installation: sudo rm -v /var/lib/apt/lists/*cuda* /var/lib/apt/lists/*nvidia* Verbose installation on Debian and Ubuntu Use the --verbose-versions flag, for example: sudo apt-get install --verbose-versions cuda 15.
 The Runfile installation asks where you wish to install the Toolkit during an interactive install.
If installing using a non-interactive install, you can use the --toolkitpath parameter to change the install location: ./runfile.run --silent \ --toolkit --toolkitpath=/my/new/toolkit The RPM and Deb packages cannot be installed to a custom install location directly using the package managers.
See the “Install CUDA to a specific directory using the Package Manager installation method” scenario in the Advanced Setup section for more information.
Ensure that your PATH includes the bin directory where you installed the Toolkit, usually /usr/local/cuda-12.4/bin .
Ensure that your LD_LIBRARY_PATH includes the lib and/or lib64 directory where you installed the Toolkit, usually /usr/local/cuda-12.4/lib{,64} : export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} 15.4.
 These errors occur after adding a foreign architecture because apt is attempting to query for each architecture within each repository listed in the system’s sources.list file.
Repositories that do not host packages for the newly added architecture will present this error.
Please see the Advanced Setup section for details on how to modify your sources.list file to prevent these errors.
 To make sure X doesn’t use a certain GPU for display, you need to specify which other GPU to use for display.
For more information, please refer to the “Use a specific GPU for rendering the display” scenario in the Advanced Setup section.
 After installing CUDA, set the driver value for the intel device in /etc/X11/xorg.conf to ‘ modesetting ’ as shown below: Section "Device" Identifier "intel" Driver "modesetting" ...
EndSection To prevent Ubuntu from reverting the change in xorg.conf, edit /etc/default/grub to add “ nogpumanager ” to GRUB_CMDLINE_LINUX_DEFAULT.
In many cases, a new Linux kernel will be installed without properly updating the required Linux kernel headers and development packages.
To ensure the CUDA driver continues to work when performing a system update, rerun the commands in the Kernel Headers and Development Packages section.
Additionally, on Fedora, the Akmods framework will sometimes fail to correctly rebuild the NVIDIA kernel module packages when a new Linux kernel is installed.
When this happens, it is usually sufficient to invoke Akmods manually and regenerate the module mapping files by running the following commands in a virtual console, and then rebooting: sudo akmods --force sudo depmod You can reach a virtual console by hitting ctrl+alt+f2 at the same time.
 To install a CUDA driver at a version earlier than 367 using a network repo, the required packages will need to be explicitly installed at the desired version.
For example, to install 352.99, instead of installing the cuda-drivers metapackage at version 352.99, you will need to install all required packages of cuda-drivers at version 352.99.
 Depending on your system configuration, you may not be able to install old versions of CUDA using the cuda metapackage.
In order to install a specific version of CUDA, you may need to specify all of the packages that would normally be installed by the cuda metapackage at the version you want to install.
If you are using yum to install certain packages at an older version, the dependencies may not resolve as expected.
In this case you may need to pass “ --setopt=obsoletes=0 ” to yum to allow an install of packages which are obsoleted at a later version than you are trying to install.
 This dependency comes from the SUSE repositories and shouldn’t affect the use of the NVIDIA driver or the CUDA Toolkit.
To disable this dependency, you can lock that package with the following command: sudo zypper al Mesa-dri-nouveau 15.12.
Run the following commands: sudo apt-get install glx-diversions --reinstall sudo apt-get remove nvidia-alternative Then re-run the commands from Removing CUDA Toolkit and Driver .
Additional Considerations  Now that you have CUDA-capable hardware and the NVIDIA CUDA Toolkit installed, you can examine and enjoy the numerous included programs.
To begin using CUDA to accelerate the performance of your own applications, consult the CUDA C++ Programming Guide, located in /usr/local/cuda-12.4/doc .
A number of helpful development tools are included in the CUDA Toolkit to assist you as you develop your CUDA programs, such as NVIDIA ® Nsight™ Eclipse Edition, NVIDIA Visual Profiler, CUDA-GDB, and CUDA-MEMCHECK.
For technical support on programming questions, consult and participate in the developer forums at https: forums.developer.nvidia.com/c/accelerated-computing/cuda/206 . 17. Switching between Driver Module Flavors  Use the following steps to switch between the NVIDIA driver legacy and open module flavors on your system.
Note If switching to open module, experimental support for GeForce and Quadro SKUs can be enabled with: echo "options nvidia NVreg_OpenRmEnableUnsupportedGpus=1" | sudo tee /etc/modprobe.d/nvidia-gsp.conf Note Replace XXX with the NVIDIA driver branch number such as 550.
Fedora, RHEL 9 / Rocky Linux 9, RHEL 8 / Rocky Linux 8 To switch between legacy and open: uninstall, then reinstall.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 19.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 19.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated. 20. Copyright  © 2009-2024 NVIDIA Corporation & affiliates.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2009-2024, NVIDIA Corporation & affiliates.
Introduction v12.5 | PDF | Archive CUDA C++ Programming Guide The programming guide to the CUDA model and interface.
Changes from Version 12.4 Added section Asynchronous Data Copies using Tensor Memory Access (TMA) .
Added Unified Memory Programming guide supporting Grace Hopper with Address Translation Service (ATS) and Heterogeneous Memory Management (HMM ) on x86.
The Benefits of Using GPUs  The Graphics Processing Unit (GPU) 1 provides much higher instruction throughput and memory bandwidth than the CPU within a similar price and power envelope.
Many applications leverage these higher capabilities to run faster on the GPU than on the CPU (see GPU Applications ).
Other computing devices, like FPGAs, are also very energy efficient, but offer much less programming flexibility than GPUs.
This difference in capabilities between the GPU and the CPU exists because they are designed with different goals in mind.
While the CPU is designed to excel at executing a sequence of operations, called a thread , as fast as possible and can execute a few tens of these threads in parallel, the GPU is designed to excel at executing thousands of them in parallel (amortizing the slower single-thread performance to achieve greater throughput).
The GPU is specialized for highly parallel computations and therefore designed such that more transistors are devoted to data processing rather than data caching and flow control.
The schematic Figure 1 shows an example distribution of chip resources for a CPU versus a GPU.
Figure 1 The GPU Devotes More Transistors to Data Processing  Devoting more transistors to data processing, for example, floating-point computations, is beneficial for highly parallel computations; the GPU can hide memory access latencies with computation, instead of relying on large data caches and complex flow control to avoid long memory access latencies, both of which are expensive in terms of transistors.
In general, an application has a mix of parallel parts and sequential parts, so systems are designed with a mix of GPUs and CPUs in order to maximize overall performance.
Applications with a high degree of parallelism can exploit this massively parallel nature of the GPU to achieve higher performance than on the CPU. 1.2. CUDA®: A General-Purpose Parallel Computing Platform and Programming Model  In November 2006, NVIDIA ® introduced CUDA ® , a general purpose parallel computing platform and programming model that leverages the parallel compute engine in NVIDIA GPUs to solve many complex computational problems in a more efficient way than on a CPU.
CUDA comes with a software environment that allows developers to use C++ as a high-level programming language.
As illustrated by Figure 2 , other languages, application programming interfaces, or directives-based approaches are supported, such as FORTRAN, DirectCompute, OpenACC.
CUDA is designed to support various languages and application programming interfaces.  1.3. A Scalable Programming Model  The advent of multicore CPUs and manycore GPUs means that mainstream processor chips are now parallel systems.
The challenge is to develop application software that transparently scales its parallelism to leverage the increasing number of processor cores, much as 3D graphics applications transparently scale their parallelism to manycore GPUs with widely varying numbers of cores.
The CUDA parallel programming model is designed to overcome this challenge while maintaining a low learning curve for programmers familiar with standard programming languages such as C.
At its core are three key abstractions — a hierarchy of thread groups, shared memories, and barrier synchronization — that are simply exposed to the programmer as a minimal set of language extensions.
These abstractions provide fine-grained data parallelism and thread parallelism, nested within coarse-grained data parallelism and task parallelism.
They guide the programmer to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each sub-problem into finer pieces that can be solved cooperatively in parallel by all threads within the block.
This decomposition preserves language expressivity by allowing threads to cooperate when solving each sub-problem, and at the same time enables automatic scalability.
Indeed, each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors as illustrated by Figure 3 , and only the runtime system needs to know the physical multiprocessor count.
This scalable programming model allows the GPU architecture to span a wide market range by simply scaling the number of multiprocessors and memory partitions: from the high-performance enthusiast GeForce GPUs and professional Quadro and Tesla computing products to a variety of inexpensive, mainstream GeForce GPUs (see CUDA-Enabled GPUs for a list of all CUDA-enabled GPUs).
Figure 3 Automatic Scalability  Note A GPU is built around an array of Streaming Multiprocessors (SMs) (see Hardware Implementation for more details).
A multithreaded program is partitioned into blocks of threads that execute independently from each other, so that a GPU with more multiprocessors will automatically execute the program in less time than a GPU with fewer multiprocessors. 1.4. Document Structure  This document is organized into the following sections: Introduction is a general introduction to CUDA.
Cooperative Groups describes synchronization primitives for various groups of CUDA threads.
Stream Ordered Memory Allocator describes how applications can order memory allocation and deallocation.
Compute Capabilities gives the technical specifications of various devices, as well as more architectural details.
1 The graphics qualifier comes from the fact that when the GPU was originally created, two decades ago, it was designed as a specialized processor to accelerate graphics rendering.
Driven by the insatiable market demand for real-time, high-definition, 3D graphics, it has evolved into a general processor used for many more workloads than just graphics rendering. 2. Programming Model  This chapter introduces the main concepts behind the CUDA programming model by outlining how they are exposed in C++.
Full code for the vector addition example used in this chapter and the next can be found in the vectorAdd CUDA sample . 2.1. Kernels  CUDA C++ extends C++ by allowing the programmer to define C++ functions, called kernels , that, when called, are executed N times in parallel by N different CUDA threads , as opposed to only once like regular C++ functions.
A kernel is defined using the __global__ declaration specifier and the number of CUDA threads that execute that kernel for a given kernel call is specified using a new >> execution configuration syntax (see C++ Language Extensions ).
Each thread that executes the kernel is given a unique thread ID that is accessible within the kernel through built-in variables.
As an illustration, the following sample code, using the built-in variable threadIdx , adds two vectors A and B of size N and stores the result into vector C :   Kernel definition __global__ void VecAdd ( float * A , float * B , float * C ) { int i = threadIdx .
} Here, each of the N threads that execute VecAdd() performs one pair-wise addition. 2.2. Thread Hierarchy  For convenience, threadIdx is a 3-component vector, so that threads can be identified using a one-dimensional, two-dimensional, or three-dimensional thread index , forming a one-dimensional, two-dimensional, or three-dimensional block of threads, called a thread block .
This provides a natural way to invoke computation across the elements in a domain such as a vector, matrix, or volume.
The index of a thread and its thread ID relate to each other in a straightforward way: For a one-dimensional block, they are the same; for a two-dimensional block of size (Dx, Dy) , the thread ID of a thread of index (x, y) is (x + y Dx) ; for a three-dimensional block of size (Dx, Dy, Dz) , the thread ID of a thread of index (x, y, z) is (x + y Dx + z Dx Dy) .
As an example, the following code adds two matrices A and B of size NxN and stores the result into matrix C :   Kernel definition __global__ void MatAdd ( float A [ N ][ N ], float B [ N ][ N ], float C [ N ][ N ]) { int i = threadIdx .
Kernel invocation with one block of N * N * 1 threads int numBlocks = 1 ; dim3 threadsPerBlock ( N , N ); MatAdd >> ( A , B , C ); ...
} There is a limit to the number of threads per block, since all threads of a block are expected to reside on the same streaming multiprocessor core and must share the limited memory resources of that core.
However, a kernel can be executed by multiple equally-shaped thread blocks, so that the total number of threads is equal to the number of threads per block times the number of blocks.
Blocks are organized into a one-dimensional, two-dimensional, or three-dimensional grid of thread blocks as illustrated by Figure 4 .
The number of thread blocks in a grid is usually dictated by the size of the data being processed, which typically exceeds the number of processors in the system.
Figure 4 Grid of Thread Blocks  The number of threads per block and the number of blocks per grid specified in the >> syntax can be of type int or dim3 .
Each block within the grid can be identified by a one-dimensional, two-dimensional, or three-dimensional unique index accessible within the kernel through the built-in blockIdx variable.
The dimension of the thread block is accessible within the kernel through the built-in blockDim variable.
Extending the previous MatAdd() example to handle multiple blocks, the code becomes as follows.
Kernel definition __global__ void MatAdd ( float A [ N ][ N ], float B [ N ][ N ], float C [ N ][ N ]) { int i = blockIdx .
} A thread block size of 16x16 (256 threads), although arbitrary in this case, is a common choice.
For simplicity, this example assumes that the number of threads per grid in each dimension is evenly divisible by the number of threads per block in that dimension, although that need not be the case.
Thread blocks are required to execute independently: It must be possible to execute them in any order, in parallel or in series.
This independence requirement allows thread blocks to be scheduled in any order across any number of cores as illustrated by Figure 3 , enabling programmers to write code that scales with the number of cores.
Threads within a block can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses.
More precisely, one can specify synchronization points in the kernel by calling the __syncthreads() intrinsic function; __syncthreads() acts as a barrier at which all threads in the block must wait before any is allowed to proceed.
In addition to __syncthreads() , the Cooperative Groups API provides a rich set of thread-synchronization primitives.
For efficient cooperation, the shared memory is expected to be a low-latency memory near each processor core (much like an L1 cache) and __syncthreads() is expected to be lightweight. 2.2.1. Thread Block Clusters  With the introduction of NVIDIA Compute Capability 9.0 , the CUDA programming model introduces an optional level of hierarchy called Thread Block Clusters that are made up of thread blocks.
Similar to how threads in a thread block are guaranteed to be co-scheduled on a streaming multiprocessor, thread blocks in a cluster are also guaranteed to be co-scheduled on a GPU Processing Cluster (GPC) in the GPU.
Similar to thread blocks, clusters are also organized into a one-dimension, two-dimension, or three-dimension as illustrated by Figure 5 .
The number of thread blocks in a cluster can be user-defined, and a maximum of 8 thread blocks in a cluster is supported as a portable cluster size in CUDA.
Note that on GPU hardware or MIG configurations which are too small to support 8 multiprocessors the maximum cluster size will be reduced accordingly.
Identification of these smaller configurations, as well as of larger configurations supporting a thread block cluster size beyond 8, is architecture-specific and can be queried using the cudaOccupancyMaxPotentialClusterSize API.
Figure 5 Grid of Thread Block Clusters  Note In a kernel launched using cluster support, the gridDim variable still denotes the size in terms of number of thread blocks, for compatibility purposes.
A thread block cluster can be enabled in a kernel either using a compiler time kernel attribute using __cluster_dims__(X,Y,Z) or using the CUDA kernel launch API cudaLaunchKernelEx .
The cluster size using kernel attribute is fixed at compile time and then the kernel can be launched using the classical >> .
If a kernel uses compile-time cluster size, the cluster size cannot be modified when launching the kernel.
Kernel definition   Compile time cluster size 2 in X-dimension and 1 in Y and Z dimension __global__ void __cluster_dims__ ( 2 , 1 , 1 ) cluster_kernel ( float * input , float * output ) { } int main () { float * input , * output ;   Kernel invocation with compile time cluster size dim3 threadsPerBlock ( 16 , 16 ); dim3 numBlocks ( N / threadsPerBlock .
y );   The grid dimension is not affected by cluster launch, and is still enumerated   using number of blocks.
cluster_kernel >> ( input , output ); } A thread block cluster size can also be set at runtime and the kernel can be launched using the CUDA kernel launch API cudaLaunchKernelEx .
Kernel definition   No compile time attribute attached to the kernel __global__ void cluster_kernel ( float * input , float * output ) { } int main () { float * input , * output ; dim3 threadsPerBlock ( 16 , 16 ); dim3 numBlocks ( N / threadsPerBlock .
y );   Kernel invocation with runtime cluster size { cudaLaunchConfig_t config = { 0 };   The grid dimension is not affected by cluster launch, and is still enumerated   using number of blocks.
numAttrs = 1 ; cudaLaunchKernelEx ( & config , cluster_kernel , input , output ); } } In GPUs with compute capability 9.0, all the thread blocks in the cluster are guaranteed to be co-scheduled on a single GPU Processing Cluster (GPC) and allow thread blocks in the cluster to perform hardware-supported synchronization using the Cluster Group API cluster.sync() .
Cluster group also provides member functions to query cluster group size in terms of number of threads or number of blocks using num_threads() and num_blocks() API respectively.
The rank of a thread or block in the cluster group can be queried using dim_threads() and dim_blocks() API respectively.
Thread blocks in a cluster have the ability to read, write, and perform atomics to any address in the distributed shared memory.
Distributed Shared Memory gives an example of performing histograms in distributed shared memory. 2.3. Memory Hierarchy  CUDA threads may access data from multiple memory spaces during their execution as illustrated by Figure 6 .
Each thread block has shared memory visible to all threads of the block and with the same lifetime as the block.
Thread blocks in a thread block cluster can perform read, write, and atomics operations on each other’s shared memory.
There are also two additional read-only memory spaces accessible by all threads: the constant and texture memory spaces.
The global, constant, and texture memory spaces are optimized for different memory usages (see Device Memory Accesses ).
Texture memory also offers different addressing modes, as well as data filtering, for some specific data formats (see Texture and Surface Memory ).
The global, constant, and texture memory spaces are persistent across kernel launches by the same application.
Heterogeneous Programming  As illustrated by Figure 7 , the CUDA programming model assumes that the CUDA threads execute on a physically separate device that operates as a coprocessor to the host running the C++ program.
This is the case, for example, when the kernels execute on a GPU and the rest of the C++ program executes on a CPU.
The CUDA programming model also assumes that both the host and the device maintain their own separate memory spaces in DRAM, referred to as host memory and device memory , respectively.
Therefore, a program manages the global, constant, and texture memory spaces visible to kernels through calls to the CUDA runtime (described in Programming Interface ).
This includes device memory allocation and deallocation as well as data transfer between host and device memory.
Managed memory is accessible from all CPUs and GPUs in the system as a single, coherent memory image with a common address space.
This capability enables oversubscription of device memory and can greatly simplify the task of porting applications by eliminating the need to explicitly mirror data on host and device.
Figure 7 Heterogeneous Programming  Note Serial code executes on the host while parallel code executes on the device. 2.5. Asynchronous SIMT Programming Model  In the CUDA programming model a thread is the lowest level of abstraction for doing a computation or a memory operation.
Starting with devices based on the NVIDIA Ampere GPU architecture, the CUDA programming model provides acceleration to memory operations via the asynchronous programming model.
The asynchronous programming model defines the behavior of asynchronous operations with respect to CUDA threads.
The asynchronous programming model defines the behavior of Asynchronous Barrier for synchronization between CUDA threads.
The model also explains and defines how cuda::memcpy_async can be used to move data asynchronously from global memory while computing in the GPU. 2.5.1. Asynchronous Operations  An asynchronous operation is defined as an operation that is initiated by a CUDA thread and is executed asynchronously as-if by another thread.
In a well formed program one or more CUDA threads synchronize with the asynchronous operation.
The CUDA thread that initiated the asynchronous operation is not required to be among the synchronizing threads.
Such an asynchronous thread (an as-if thread) is always associated with the CUDA thread that initiated the asynchronous operation.
An asynchronous operation uses a synchronization object to synchronize the completion of the operation.
Such a synchronization object can be explicitly managed by a user (e.g., cuda::memcpy_async ) or implicitly managed within a library (e.g., cooperative_groups::memcpy_async ).
These objects are explained in detail in Asynchronous Barrier and Asynchronous Data Copies using cuda::pipeline .
A scope defines the set of threads that may use the synchronization object to synchronize with the asynchronous operation.
The following table defines the thread scopes available in CUDA C++ and the threads that can be synchronized with each.
Thread Scope Description cuda::thread_scope::thread_scope_thread Only the CUDA thread which initiated asynchronous operations synchronizes.
cuda::thread_scope::thread_scope_block All or any CUDA threads within the same thread block as the initiating thread synchronizes.
cuda::thread_scope::thread_scope_device All or any CUDA threads in the same GPU device as the initiating thread synchronizes.
cuda::thread_scope::thread_scope_system All or any CUDA or CPU threads in the same system as the initiating thread synchronizes.
These thread scopes are implemented as extensions to standard C++ in the CUDA Standard C++ library. 2.6. Compute Capability  The compute capability of a device is represented by a version number, also sometimes called its “SM version”.
This version number identifies the features supported by the GPU hardware and is used by applications at runtime to determine which hardware features and/or instructions are available on the present GPU.
The compute capability comprises a major revision number X and a minor revision number Y and is denoted by X.Y .
The major revision number is 9 for devices based on the NVIDIA Hopper GPU architecture, 8 for devices based on the NVIDIA Ampere GPU architecture, 7 for devices based on the Volta architecture, 6 for devices based on the Pascal architecture, 5 for devices based on the Maxwell architecture, and 3 for devices based on the Kepler architecture.
The minor revision number corresponds to an incremental improvement to the core architecture, possibly including new features.
Turing is the architecture for devices of compute capability 7.5, and is an incremental update based on the Volta architecture.
Note The compute capability version of a particular GPU should not be confused with the CUDA version (for example, CUDA 7.5, CUDA 8, CUDA 9), which is the version of the CUDA software platform .
The CUDA platform is used by application developers to create applications that run on many generations of GPU architectures, including future GPU architectures yet to be invented.
While new versions of the CUDA platform often add native support for a new GPU architecture by supporting the compute capability version of that architecture, new versions of the CUDA platform typically also include software features that are independent of hardware generation.
The Tesla and Fermi architectures are no longer supported starting with CUDA 7.0 and CUDA 9.0, respectively. 3. Programming Interface  CUDA C++ provides a simple path for users familiar with the C++ programming language to easily write programs for execution by the device.
They allow programmers to define a kernel as a C++ function and use some new syntax to specify the grid and block dimension each time the function is called.
Any source file that contains some of these extensions must be compiled with nvcc as outlined in Compilation with NVCC .
It provides C and C++ functions that execute on the host to allocate and deallocate device memory, transfer data between host memory and device memory, manage systems with multiple devices, etc.
The runtime is built on top of a lower-level C API, the CUDA driver API, which is also accessible by the application.
The driver API provides an additional level of control by exposing lower-level concepts such as CUDA contexts - the analogue of host processes for the device - and CUDA modules - the analogue of dynamically loaded libraries for the device.
Most applications do not use the driver API as they do not need this additional level of control and when using the runtime, context and module management are implicit, resulting in more concise code.
As the runtime is interoperable with the driver API, most applications that need some driver API features can default to use the runtime API and only use the driver API where needed.
The driver API is introduced in Driver API and fully described in the reference manual. 3.1. Compilation with NVCC  Kernels can be written using the CUDA instruction set architecture, called PTX , which is described in the PTX reference manual.
It is however usually more effective to use a high-level programming language such as C++.
In both cases, kernels must be compiled into binary code by nvcc to execute on the device.
nvcc is a compiler driver that simplifies the process of compiling C++ or PTX code: It provides simple and familiar command line options and executes them by invoking the collection of tools that implement the different compilation stages.
A complete description can be found in the nvcc user manual. 3.1.1. Compilation Workflow  3.1.1.1.
Offline Compilation  Source files compiled with nvcc can include a mix of host code (i.e., code that executes on the host) and device code (i.e., code that executes on the device).
nvcc ’s basic workflow consists in separating device code from host code and then: compiling the device code into an assembly form ( PTX code) and/or binary form ( cubin object), and modifying the host code by replacing the >> syntax introduced in Kernels (and described in more details in Execution Configuration ) by the necessary CUDA runtime function calls to load and launch each compiled kernel from the PTX code and/or cubin object.
The modified host code is output either as C++ code that is left to be compiled using another tool or as object code directly by letting nvcc invoke the host compiler during the last compilation stage.
Applications can then: Either link to the compiled host code (this is the most common case), Or ignore the modified host code (if any) and use the CUDA driver API (see Driver API ) to load and execute the PTX code or cubin object. 3.1.1.2. Just-in-Time Compilation  Any PTX code loaded by an application at runtime is compiled further to binary code by the device driver.
Just-in-time compilation increases application load time, but allows the application to benefit from any new compiler improvements coming with each new device driver.
It is also the only way for applications to run on devices that did not exist at the time the application was compiled, as detailed in Application Compatibility .
When the device driver just-in-time compiles some PTX code for some application, it automatically caches a copy of the generated binary code in order to avoid repeating the compilation in subsequent invocations of the application.
The cache - referred to as compute cache - is automatically invalidated when the device driver is upgraded, so that applications can benefit from the improvements in the new just-in-time compiler built into the device driver.
Environment variables are available to control just-in-time compilation as described in CUDA Environment Variables As an alternative to using nvcc to compile CUDA C++ device code, NVRTC can be used to compile CUDA C++ device code to PTX at runtime.
NVRTC is a runtime compilation library for CUDA C++; more information can be found in the NVRTC User guide. 3.1.2. Binary Compatibility  Binary code is architecture-specific.
A cubin object is generated using the compiler option -code that specifies the targeted architecture: For example, compiling with -code=sm_80 produces binary code for devices of compute capability 8.0.
Binary compatibility is guaranteed from one minor revision to the next one, but not from one minor revision to the previous one or across major revisions.
In other words, a cubin object generated for compute capability X.y will only execute on devices of compute capability X.z where z≥y .
Also, the binary compatibility between desktop and Tegra is not supported. 3.1.3. PTX Compatibility  Some PTX instructions are only supported on devices of higher compute capabilities.
For example, Warp Shuffle Functions are only supported on devices of compute capability 5.0 and above.
The -arch compiler option specifies the compute capability that is assumed when compiling C++ to PTX code.
So, code that contains warp shuffle, for example, must be compiled with -arch=compute_50 (or higher).
PTX code produced for some specific compute capability can always be compiled to binary code of greater or equal compute capability.
Note that a binary compiled from an earlier PTX version may not make use of some hardware features.
For example, a binary targeting devices of compute capability 7.0 (Volta) compiled from PTX generated for compute capability 6.0 (Pascal) will not make use of Tensor Core instructions, since these were not available on Pascal.
As a result, the final binary may perform worse than would be possible if the binary were generated using the latest version of PTX.
PTX code compiled to target architecture conditional features only run on the exact same physical architecture and nowhere else.
Example code compiled with sm_90a or compute_90a only runs on devices with compute capability 9.0 and is not backward or forward compatible. 3.1.4. Application Compatibility  To execute code on devices of specific compute capability, an application must load binary or PTX code that is compatible with this compute capability as described in Binary Compatibility and PTX Compatibility .
In particular, to be able to execute code on future architectures with higher compute capability (for which no binary code can be generated yet), an application must load PTX code that will be just-in-time compiled for these devices (see Just-in-Time Compilation ).
Which PTX and binary code gets embedded in a CUDA C++ application is controlled by the -arch and -code compiler options or the -gencode compiler option as detailed in the nvcc user manual.
For example, nvcc x.cu -gencode arch=compute_50,code=sm_50 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_70,code=\"compute_70,sm_70\" embeds binary code compatible with compute capability 5.0 and 6.0 (first and second -gencode options) and PTX and binary code compatible with compute capability 7.0 (third -gencode option).
Host code is generated to automatically select at runtime the most appropriate code to load and execute, which, in the above example, will be: 5.0 binary code for devices with compute capability 5.0 and 5.2, 6.0 binary code for devices with compute capability 6.0 and 6.1, 7.0 binary code for devices with compute capability 7.0 and 7.5, PTX code which is compiled to binary code at runtime for devices with compute capability 8.0 and 8.6.
x.cu can have an optimized code path that uses warp reduction operations, for example, which are only supported in devices of compute capability 8.0 and higher.
The __CUDA_ARCH__ macro can be used to differentiate various code paths based on compute capability.
If x.cu is compiled for architecture conditional features example with sm_90a or compute_90a , the code can only run on devices with compute capability 9.0.
Applications using the driver API must compile code to separate files and explicitly load and execute the most appropriate file at runtime.
The Volta architecture introduces Independent Thread Scheduling which changes the way threads are scheduled on the GPU.
For code relying on specific behavior of SIMT scheduling in previous architectures, Independent Thread Scheduling may alter the set of participating threads, leading to incorrect results.
To aid migration while implementing the corrective actions detailed in Independent Thread Scheduling , Volta developers can opt-in to Pascal’s thread scheduling with the compiler option combination -arch=compute_60 -code=sm_70 .
The nvcc user manual lists various shorthands for the -arch , -code , and -gencode compiler options.
For example, -arch=sm_70 is a shorthand for -arch=compute_70 -code=compute_70,sm_70 (which is the same as -gencode arch=compute_70,code=\"compute_70,sm_70\" ). 3.1.5. C++ Compatibility  The front end of the compiler processes CUDA source files according to C++ syntax rules.
However, only a subset of C++ is fully supported for the device code as described in C++ Language Support . 3.1.6. 64-Bit Compatibility  The 64-bit version of nvcc compiles device code in 64-bit mode (i.e., pointers are 64-bit).
Device code compiled in 64-bit mode is only supported with host code compiled in 64-bit mode. 3.2. CUDA Runtime  The runtime is implemented in the cudart library, which is linked to the application, either statically via cudart.lib or libcudart.a , or dynamically via cudart.dll or libcudart.so .
Applications that require cudart.dll and/or cudart.so for dynamic linking typically include them as part of the application installation package.
It is only safe to pass the address of CUDA runtime symbols between components that link to the same instance of the CUDA runtime.
As mentioned in Heterogeneous Programming , the CUDA programming model assumes a system composed of a host and a device, each with their own separate memory.
Shared Memory illustrates the use of shared memory, introduced in Thread Hierarchy , to maximize performance.
Page-Locked Host Memory introduces page-locked host memory that is required to overlap kernel execution with data transfers between host and device memory.
Asynchronous Concurrent Execution describes the concepts and API used to enable asynchronous concurrent execution at various levels in the system.
Multi-Device System shows how the programming model extends to a system with multiple devices attached to the same host.
Texture and Surface Memory presents the texture and surface memory spaces that provide another way to access device memory; they also expose a subset of the GPU texturing hardware.
Graphics Interoperability introduces the various functions the runtime provides to interoperate with the two main graphics APIs, OpenGL and Direct3D. 3.2.1. Initialization  As of CUDA 12.0, the cudaInitDevice() and cudaSetDevice() calls initialize the runtime and the primary context associated with the specified device.
Absent these calls, the runtime will implicitly use device 0 and self-initialize as needed to process other runtime API requests.
One needs to keep this in mind when timing runtime function calls and when interpreting the error code from the first call into the runtime.
Before 12.0, cudaSetDevice() would not initialize the runtime and applications would often use the no-op runtime call cudaFree(0) to isolate the runtime initialization from other api activity (both for the sake of timing and error handling).
The runtime creates a CUDA context for each device in the system (see Context for more details on CUDA contexts).
This context is the primary context for this device and is initialized at the first runtime function which requires an active context on this device.
As part of this context creation, the device code is just-in-time compiled if necessary (see Just-in-Time Compilation ) and loaded into device memory.
If needed, for example, for driver API interoperability, the primary context of a device can be accessed from the driver API as described in Interoperability between Runtime and Driver APIs .
When a host thread calls cudaDeviceReset() , this destroys the primary context of the device the host thread currently operates on (i.e., the current device as defined in Device Selection ).
The next runtime function call made by any host thread that has this device as current will create a new primary context for this device.
Note The CUDA interfaces use global state that is initialized during host program initiation and destroyed during host program termination.
The CUDA runtime and driver cannot detect if this state is invalid, so using any of these interfaces (implicitly or explicitly) during program initiation or termination after main) will result in undefined behavior.
As of CUDA 12.0, cudaSetDevice() will now explicitly initialize the runtime after changing the current device for the host thread.
Previous versions of CUDA delayed runtime initialization on the new device until the first runtime call was made after cudaSetDevice() .
This change means that it is now very important to check the return value of cudaSetDevice() for initialization errors.
The runtime functions from the error handling and version management sections of the reference manual do not initialize the runtime. 3.2.2. Device Memory  As mentioned in Heterogeneous Programming , the CUDA programming model assumes a system composed of a host and a device, each with their own separate memory.
Kernels operate out of device memory, so the runtime provides functions to allocate, deallocate, and copy device memory, as well as transfer data between host memory and device memory.
Linear memory is allocated in a single unified address space, which means that separately allocated entities can reference one another via pointers, for example, in a binary tree or linked list.
The size of the address space depends on the host system (CPU) and the compute capability of the used GPU: Table 1 Linear Memory Address Space  x86_64 (AMD64) POWER (ppc64le) ARM64 up to compute capability 5.3 (Maxwell) 40bit 40bit 40bit compute capability 6.0 (Pascal) or newer up to 47bit up to 49bit up to 48bit Note On devices of compute capability 5.3 (Maxwell) and earlier, the CUDA driver creates an uncommitted 40bit virtual address reservation to ensure that memory allocations (pointers) fall into the supported range.
This reservation appears as reserved virtual memory, but does not occupy any physical memory until the program actually allocates memory.
Linear memory is typically allocated using cudaMalloc() and freed using cudaFree() and data transfer between host memory and device memory are typically done using cudaMemcpy() .
In the vector addition code sample of Kernels , the vectors need to be copied from host memory to device memory:   Device code __global__ void VecAdd ( float * A , float * B , float * C , int N ) { int i = blockDim .
x ; if ( i >> ( d_A , d_B , d_C , N );   Copy result from device memory to host memory   h_C contains the result in host memory cudaMemcpy ( h_C , d_C , size , cudaMemcpyDeviceToHost );   Free device memory cudaFree ( d_A ); cudaFree ( d_B ); cudaFree ( d_C );   Free host memory ...
These functions are recommended for allocations of 2D or 3D arrays as it makes sure that the allocation is appropriately padded to meet the alignment requirements described in Device Memory Accesses , therefore ensuring best performance when accessing the row addresses or performing copies between 2D arrays and other regions of device memory (using the cudaMemcpy2D() and cudaMemcpy3D() functions).
The following code sample allocates a width x height 2D array of floating-point values and shows how to loop over the array elements in device code:   Host code int width = 64 , height = 64 ; float * devPtr ; size_t pitch ; cudaMallocPitch ( & devPtr , & pitch , width * sizeof ( float ), height ); MyKernel >> ( devPtr , pitch , width , height );   Device code __global__ void MyKernel ( float * devPtr , size_t pitch , int width , int height ) { for ( int r = 0 ; r >> ( devPitchedPtr , width , height , depth );   Device code __global__ void MyKernel ( cudaPitchedPtr devPitchedPtr , int width , int height , int depth ) { char * devPtr = devPitchedPtr .
pitch ; size_t slicePitch = pitch * height ; for ( int z = 0 ; z ( ptr );   Global Memory data pointer stream_attribute .
hitProp = cudaAccessPropertyPersisting ;   Type of access property on cache hit stream_attribute .
Set the attributes to a CUDA stream of type cudaStream_t cudaStreamSetAttribute ( stream , cudaStreamAttributeAccessPolicyWindow , & stream_attribute ); When a kernel subsequently executes in CUDA stream , memory accesses within the global memory extent [ptr..ptr+num_bytes) are more likely to persist in the L2 cache than accesses to other global memory locations.
L2 persistence can also be set for a CUDA Graph Kernel Node as shown in the example below: CUDA GraphKernelNode Example cudaKernelNodeAttrValue node_attribute ;   Kernel level attributes data structure node_attribute .
hitProp = cudaAccessPropertyPersisting ;   Type of access property on cache hit node_attribute .
Set the attributes to a CUDA Graph Kernel node of type cudaGraphNode_t cudaGraphKernelNodeSetAttribute ( node , cudaKernelNodeAttributeAccessPolicyWindow , & node_attribute ); The hitRatio parameter can be used to specify the fraction of accesses that receive the hitProp property.
In both of the examples above, 60% of the memory accesses in the global memory region [ptr..ptr+num_bytes) have the persisting property and 40% of the memory accesses have the streaming property.
Which specific memory accesses are classified as persisting (the hitProp ) is random with a probability of approximately hitRatio ; the probability distribution depends upon the hardware architecture and the memory extent.
For example, if the L2 set-aside cache size is 16KB and the num_bytes in the accessPolicyWindow is 32KB: With a hitRatio of 0.5, the hardware will select, at random, 16KB of the 32KB window to be designated as persisting and cached in the set-aside L2 cache area.
With a hitRatio of 1.0, the hardware will attempt to cache the whole 32KB window in the set-aside L2 cache area.
Since the set-aside area is smaller than the window, cache lines will be evicted to keep the most recently used 16KB of the 32KB data in the set-aside portion of the L2 cache.
The hitRatio can therefore be used to avoid thrashing of cache lines and overall reduce the amount of data moved into and out of the L2 cache.
A hitRatio value below 1.0 can be used to manually control the amount of data different accessPolicyWindow s from concurrent CUDA streams can cache in L2.
For example, let the L2 set-aside cache size be 16KB; two concurrent kernels in two different CUDA streams, each with a 16KB accessPolicyWindow , and both with hitRatio value 1.0, might evict each others’ cache lines when competing for the shared L2 resource.
However, if both accessPolicyWindows have a hitRatio value of 0.5, they will be less likely to evict their own or each others’ persisting cache lines. 3.2.3.3. L2 Access Properties  Three types of access properties are defined for different global memory data accesses: cudaAccessPropertyStreaming : Memory accesses that occur with the streaming property are less likely to persist in the L2 cache because these accesses are preferentially evicted.
cudaAccessPropertyPersisting : Memory accesses that occur with the persisting property are more likely to persist in the L2 cache because these accesses are preferentially retained in the set-aside portion of L2 cache.
cudaAccessPropertyNormal : This access property forcibly resets previously applied persisting access property to a normal status.
Memory accesses with the persisting property from previous CUDA kernels may be retained in L2 cache long after their intended use.
This persistence-after-use reduces the amount of L2 cache available to subsequent kernels that do not use the persisting property.
Resetting an access property window with the cudaAccessPropertyNormal property removes the persisting (preferential retention) status of the prior access, as if the prior access had been without an access property. 3.2.3.4. L2 Persistence Example  The following example shows how to set-aside L2 cache for persistent accesses, use the set-aside L2 cache in CUDA kernels via CUDA Stream and then reset the L2 cache.
cudaStream_t stream ; cudaStreamCreate ( & stream );   Create CUDA stream cudaDeviceProp prop ;   CUDA device properties variable cudaGetDeviceProperties ( & prop , device_id );   Query GPU properties size_t size = min ( int ( prop .
persistingL2CacheMaxSize ); cudaDeviceSetLimit ( cudaLimitPersistingL2CacheSize , size );   set-aside 3/4 of L2 cache for persisting accesses or the max allowed size_t window_size = min ( prop .
accessPolicyMaxWindowSize , num_bytes );   Select minimum of user defined num_bytes and max window size.
cudaStreamAttrValue stream_attribute ;   Stream level attributes data structure stream_attribute .
missProp = cudaAccessPropertyStreaming ;   Type of access property on cache miss cudaStreamSetAttribute ( stream , cudaStreamAttributeAccessPolicyWindow , & stream_attribute );   Set the attributes to a CUDA Stream for ( int i = 0 ; i >> ( data1 );   This data1 is used by a kernel multiple times }   [data1 + num_bytes) benefits from L2 persistence cuda_kernelB >> ( data1 );   A different kernel in the same stream can also benefit   from the persistence of data1 stream_attribute .
num_bytes = 0 ;   Setting the window size to 0 disable it cudaStreamSetAttribute ( stream , cudaStreamAttributeAccessPolicyWindow , & stream_attribute );   Overwrite the access policy attribute to a CUDA Stream cudaCtxResetPersistingL2Cache ();   Remove any persistent lines in L2 cuda_kernelC >> ( data2 );   data2 can now benefit from full L2 in normal mode 3.2.3.5.
Reset L2 Access to Normal  A persisting L2 cache line from a previous CUDA kernel may persist in L2 long after it has been used.
Hence, a reset to normal for L2 cache is important for streaming or normal memory accesses to utilize the L2 cache with normal priority.
Reset a previous persisting memory region with the access property, cudaAccessPropertyNormal .
Reset all persisting L2 cache lines to normal by calling cudaCtxResetPersistingL2Cache() .
Reliance on automatic reset is strongly discouraged because of the undetermined length of time required for automatic reset to occur. 3.2.3.6. Manage Utilization of L2 set-aside cache  Multiple CUDA kernels executing concurrently in different CUDA streams may have a different access policy window assigned to their streams.
However, the L2 set-aside cache portion is shared among all these concurrent CUDA kernels.
As a result, the net utilization of this set-aside cache portion is the sum of all the concurrent kernels’ individual use.
The benefits of designating memory accesses as persisting diminish as the volume of persisting accesses exceeds the set-aside L2 cache capacity.
To manage utilization of the set-aside L2 cache portion, an application must consider the following: Size of L2 set-aside cache.
When and how L2 reset is required to allow normal or streaming accesses to utilize the previously set-aside L2 cache with equal priority. 3.2.3.7. Query L2 cache Properties  Properties related to L2 cache are a part of cudaDeviceProp struct and can be queried using CUDA runtime API cudaGetDeviceProperties CUDA Device Properties include: l2CacheSize : The amount of available L2 cache on the GPU.
persistingL2CacheMaxSize : The maximum amount of L2 cache that can be set-aside for persisting memory accesses.
accessPolicyMaxWindowSize : The maximum size of the access policy window. 3.2.3.8. Control L2 Cache Set-Aside Size for Persisting Memory Access  The L2 set-aside cache size for persisting memory accesses is queried using CUDA runtime API cudaDeviceGetLimit and set using CUDA runtime API cudaDeviceSetLimit as a cudaLimit .
Shared Memory  As detailed in Variable Memory Space Specifiers shared memory is allocated using the __shared__ memory space specifier.
Shared memory is expected to be much faster than global memory as mentioned in Thread Hierarchy and detailed in Shared Memory .
It can be used as scratchpad memory (or software managed cache) to minimize global memory accesses from a CUDA block as illustrated by the following matrix multiplication example.
The following code sample is a straightforward implementation of matrix multiplication that does not take advantage of shared memory.
Each thread reads one row of A and one column of B and computes the corresponding element of C as illustrated in Figure 8 .
Matrices are stored in row-major order:   M(row, col) = *(M.elements + row * M.width + col) typedef struct { int width ; int height ; float * elements ; } Matrix ;   Thread block size #define BLOCK_SIZE 16   Forward declaration of the matrix multiplication kernel __global__ void MatMulKernel ( const Matrix , const Matrix , Matrix );   Matrix multiplication - Host code   Matrix dimensions are assumed to be multiples of BLOCK_SIZE void MatMul ( const Matrix A , const Matrix B , Matrix C ) {   Load A and B to device memory Matrix d_A ; d_A .
elements , size , cudaMemcpyHostToDevice );   Allocate C in device memory Matrix d_C ; d_C .
elements , size );   Invoke kernel dim3 dimBlock ( BLOCK_SIZE , BLOCK_SIZE ); dim3 dimGrid ( B .
elements ); }   Matrix multiplication kernel called by MatMul() __global__ void MatMulKernel ( Matrix A , Matrix B , Matrix C ) {   Each thread computes one element of C   by accumulating results into Cvalue float Cvalue = 0 ; int row = blockIdx .
x ; for ( int e = 0 ; e >> ( d_A , d_B , d_C );   Read C from device memory cudaMemcpy ( C .
elements ); }   Matrix multiplication kernel called by MatMul() __global__ void MatMulKernel ( Matrix A , Matrix B , Matrix C ) {   Block row and column int blockRow = blockIdx .
x ;   Each thread block computes one sub-matrix Csub of C Matrix Csub = GetSubMatrix ( C , blockRow , blockCol );   Each thread computes one element of Csub   by accumulating results into Cvalue float Cvalue = 0 ;   Thread row and column within Csub int row = threadIdx .
x ;   Loop over all the sub-matrices of A and B that are   required to compute Csub   Multiply each pair of sub-matrices together   and accumulate the results for ( int m = 0 ; m   Distributed Shared memory histogram kernel __global__ void clusterHist_kernel ( int * bins , const int nbins , const int bins_per_block , const int * __restrict__ input , size_t array_size ) { extern __shared__ int smem []; namespace cg = cooperative_groups ; int tid = cg :: this_grid ().
cg :: cluster_group cluster = cg :: this_cluster (); unsigned int clusterBlockRank = cluster .
x ; i = nbins ) binid = nbins - 1 ;  Find destination block rank and offset for computing  distributed shared memory histogram int dst_block_rank = ( int )( binid / bins_per_block ); int dst_offset = binid % bins_per_block ;  Pointer to target block shared memory int * dst_smem = cluster .
map_shared_rank ( smem , dst_block_rank );  Perform atomic update of the histogram bin atomicAdd ( dst_smem + dst_offset , 1 ); }   cluster synchronization is required to ensure all distributed shared   memory operations are completed and no thread block exits while   other thread blocks are still accessing distributed shared memory cluster .
sync ();   Perform global memory histogram, using the local distributed memory histogram int * lbins = bins + cluster .
x ; i a ( 0 ); __managed__ cuda :: atomic b ( 0 ); Thread 1 (SM) x = 1 ; a = 1 ; Thread 2 (SM) while ( a != 1 ) ; assert ( x == 1 ); b = 1 ; Thread 3 (CPU) while ( b != 1 ) ; assert ( x == 1 ); Consider the example above.
The CUDA memory consistency model guarantees that the asserted condition will be true, so the write to x from thread 1 must be visible to thread 3, before the write to b from thread 2.
The memory ordering provided by the release and acquire of a is only sufficient to make x visible to thread 2, not thread 3, as it is a device-scope operation.
The system-scope ordering provided by release and acquire of b , therefore, needs to ensure not only writes issued from thread 2 itself are visible to thread 3, but also writes from other threads that are visible to thread 2.
As the GPU cannot know at the time of execution which writes have been guaranteed at the source level to be visible and which are visible only by chance timing, it must cast a conservatively wide net for in-flight memory operations.
This sometimes leads to interference: because the GPU is waiting on memory operations it is not required to at the source level, the fence/flush may take longer than necessary.
Note that fences may occur explicitly as intrinsics or atomics in code, like in the example, or implicitly to implement synchronizes-with relationships at task boundaries.
A common example is when a kernel is performing computation in local GPU memory, and a parallel kernel (e.g.
Upon completion, the local kernel will implicitly flush its writes to satisfy any synchronizes-with relationships to downstream work.
This may unnecessarily wait, fully or partially, on slower nvlink or PCIe writes from the communication kernel. 3.2.7.2. Isolating Traffic with Domains  Beginning with Hopper architecture GPUs and CUDA 12.0, the memory synchronization domains feature provides a way to alleviate such interference.
In exchange for explicit assistance from code, the GPU can reduce the net cast by a fence operation.
Writes and fences are tagged with the ID, and a fence will only order writes matching the fence’s domain.
In the concurrent compute vs communication example, the communication kernels can be placed in a different domain.
When using domains, code must abide by the rule that ordering or synchronization between distinct domains on the same GPU requires system-scope fencing .
This is necessary for cumulativity as one kernel’s writes will not be encompassed by a fence issued from a kernel in another domain.
In essence, cumulativity is satisfied by ensuring that cross-domain traffic is flushed to the system scope ahead of time.
However, because kernels will default to domain 0 as described below, backward compatibility is maintained. 3.2.7.3. Using Domains in CUDA  Domains are accessible via the new launch attributes cudaLaunchAttributeMemSyncDomain and cudaLaunchAttributeMemSyncDomainMap .
The former selects between logical domains cudaLaunchMemSyncDomainDefault and cudaLaunchMemSyncDomainRemote , and the latter provides a mapping from logical to physical domains.
The remote domain is intended for kernels performing remote memory access in order to isolate their memory traffic from local kernels.
Note, however, the selection of a particular domain does not affect what memory access a kernel may legally perform.
To facilitate portable code, domains functionality can be used on all devices and CUDA will report a count of 1 prior to Hopper.
An individual kernel launch at a low level in the stack, such as from NCCL, can select a semantic logical domain without concern for the surrounding application architecture.
The default value for the logical domain if it is not set is the default domain, and the default mapping is to map the default domain to 0 and the remote domain to 1 (on GPUs with more than 1 domain).
Specific libraries may tag launches with the remote domain in CUDA 12.0 and later; for example, NCCL 2.16 will do so.
Together, this provides a beneficial use pattern for common applications out of the box, with no code changes needed in other components, frameworks, or at application level.
An alternative use pattern, for example in an application using nvshmem or with no clear separation of kernel types, could be to partition parallel streams.
Example of launching a kernel with the remote logical domain cudaLaunchAttribute domainAttr ; domainAttr .
val = cudaLaunchMemSyncDomainRemote ; cudaLaunchConfig_t config ;   Fill out other config fields config .
numAttrs = 1 ; cudaLaunchKernelEx ( & config , myKernel , kernelArg1 , kernelArg2 ...);   Example of setting a mapping for a stream   (This mapping is the default for streams starting on Hopper if not   explicitly set, and provided for illustration) cudaLaunchAttributeValue mapAttr ; mapAttr .
remote = 1 ; cudaStreamSetAttribute ( stream , cudaLaunchAttributeMemSyncDomainMap , & mapAttr );   Example of mapping different streams to different physical domains, ignoring   logical domain settings cudaLaunchAttributeValue mapAttr ; mapAttr .
remote = 0 ; cudaStreamSetAttribute ( streamA , cudaLaunchAttributeMemSyncDomainMap , & mapAttr ); mapAttr .
remote = 1 ; cudaStreamSetAttribute ( streamB , cudaLaunchAttributeMemSyncDomainMap , & mapAttr ); As with other launch attributes, these are exposed uniformly on CUDA streams, individual launches using cudaLaunchKernelEx , and kernel nodes in CUDA graphs.
A typical use would set the mapping at stream level and the logical domain at launch level (or bracketing a section of stream use) as described above.
Graphs take both attributes from the node itself, essentially an indirect way of specifying a physical domain.
Domain-related attributes set on the stream a graph is launched into are not used in execution of the graph. 3.2.8. Asynchronous Concurrent Execution  CUDA exposes the following operations as independent tasks that can operate concurrently with one another: Computation on the host; Computation on the device; Memory transfers from the host to the device; Memory transfers from the device to the host; Memory transfers within the memory of a given device; Memory transfers among devices.
The level of concurrency achieved between these operations will depend on the feature set and compute capability of the device as described below. 3.2.8.1. Concurrent Execution between Host and Device  Concurrent host execution is facilitated through asynchronous library functions that return control to the host thread before the device completes the requested task.
Using asynchronous calls, many device operations can be queued up together to be executed by the CUDA driver when appropriate device resources are available.
This relieves the host thread of much of the responsibility to manage the device, leaving it free for other tasks.
The following device operations are asynchronous with respect to the host: Kernel launches; Memory copies within a single device’s memory; Memory copies from host to device of a memory block of 64 KB or less; Memory copies performed by functions that are suffixed with Async ; Memory set function calls.
Programmers can globally disable asynchronicity of kernel launches for all CUDA applications running on a system by setting the CUDA_LAUNCH_BLOCKING environment variable to 1.
This feature is provided for debugging purposes only and should not be used as a way to make production software run reliably.
Kernel launches are synchronous if hardware counters are collected via a profiler (Nsight, Visual Profiler) unless concurrent kernel profiling is enabled.
Async memory copies might also be synchronous if they involve host memory that is not page-locked. 3.2.8.2. Concurrent Kernel Execution  Some devices of compute capability 2.x and higher can execute multiple kernels concurrently.
Applications may query this capability by checking the concurrentKernels device property (see Device Enumeration ), which is equal to 1 for devices that support it.
The maximum number of kernel launches that a device can execute concurrently depends on its compute capability and is listed in Table 21 .
A kernel from one CUDA context cannot execute concurrently with a kernel from another CUDA context.
If a user wants to run kernels from multiple process simultaneously on the SM, one must enable MPS.
Kernels that use many textures or a large amount of local memory are less likely to execute concurrently with other kernels. 3.2.8.3. Overlap of Data Transfer and Kernel Execution  Some devices can perform an asynchronous memory copy to or from the GPU concurrently with kernel execution.
Applications may query this capability by checking the asyncEngineCount device property (see Device Enumeration ), which is greater than zero for devices that support it.
It is also possible to perform an intra-device copy simultaneously with kernel execution (on devices that support the concurrentKernels device property) and/or with copies to or from the device (for devices that support the asyncEngineCount property).
Intra-device copies are initiated using the standard memory copy functions with destination and source addresses residing on the same device. 3.2.8.4. Concurrent Data Transfers  Some devices of compute capability 2.x and higher can overlap copies to and from the device.
Applications may query this capability by checking the asyncEngineCount device property (see Device Enumeration ), which is equal to 2 for devices that support it.
In order to be overlapped, any host memory involved in the transfers must be page-locked. 3.2.8.5. Streams  Applications manage the concurrent operations described above through streams .
A stream is a sequence of commands (possibly issued by different host threads) that execute in order.
Different streams, on the other hand, may execute their commands out of order with respect to one another or concurrently; this behavior is not guaranteed and should therefore not be relied upon for correctness (for example, inter-kernel communication is undefined).
The commands issued on a stream may execute when all the dependencies of the command are met.
The dependencies could be previously launched commands on same stream or dependencies from other streams.
The successful completion of synchronize call guarantees that all the commands launched are completed. 3.2.8.5.1. Creation and Destruction of Streams  A stream is defined by creating a stream object and specifying it as the stream parameter to a sequence of kernel launches and host device memory copies.
The following code sample creates two streams and allocates an array hostPtr of float in page-locked memory.
cudaStream_t stream [ 2 ]; for ( int i = 0 ; i >> ( outputDevPtr + i * size , inputDevPtr + i * size , size ); cudaMemcpyAsync ( hostPtr + i * size , outputDevPtr + i * size , size , cudaMemcpyDeviceToHost , stream [ i ]); } Each stream copies its portion of input array hostPtr to array inputDevPtr in device memory, processes inputDevPtr on the device by calling MyKernel() , and copies the result outputDevPtr back to the same portion of hostPtr .
Overlapping Behavior describes how the streams overlap in this example depending on the capability of the device.
for ( int i = 0 ; i device memory copies that do not specify any stream parameter, or equivalently that set the stream parameter to zero, are issued to the default stream.
For code that is compiled using the --default-stream per-thread compilation flag (or that defines the CUDA_API_PER_THREAD_DEFAULT_STREAM macro before including CUDA headers ( cuda.h and cuda_runtime.h )), the default stream is a regular stream and each host thread has its own default stream.
Note #define CUDA_API_PER_THREAD_DEFAULT_STREAM 1 cannot be used to enable this behavior when the code is compiled by nvcc as nvcc implicitly includes cuda_runtime.h at the top of the translation unit.
In this case the --default-stream per-thread compilation flag needs to be used or the CUDA_API_PER_THREAD_DEFAULT_STREAM macro needs to be defined with the -DCUDA_API_PER_THREAD_DEFAULT_STREAM=1 compiler flag.
For code that is compiled using the --default-stream legacy compilation flag, the default stream is a special stream called the NULL stream and each device has a single NULL stream used for all host threads.
The NULL stream is special as it causes implicit synchronization as described in Implicit Synchronization .
For code that is compiled without specifying a --default-stream compilation flag, --default-stream legacy is assumed as the default. 3.2.8.5.3. Explicit Synchronization  There are various ways to explicitly synchronize streams with each other.
cudaDeviceSynchronize() waits until all preceding commands in all streams of all host threads have completed.
cudaStreamSynchronize() takes a stream as a parameter and waits until all preceding commands in the given stream have completed.
It can be used to synchronize the host with a specific stream, allowing other streams to continue executing on the device.
cudaStreamWaitEvent() takes a stream and an event as parameters (see Events for a description of events)and makes all the commands added to the given stream after the call to cudaStreamWaitEvent() delay their execution until the given event has completed.
cudaStreamQuery() provides applications with a way to know if all preceding commands in a stream have completed. 3.2.8.5.4. Implicit Synchronization  Two commands from different streams cannot run concurrently if any one of the following operations is issued in-between them by the host thread: a page-locked host memory allocation, a device memory allocation, a device memory set, a memory copy between two addresses to the same device memory, any CUDA command to the NULL stream, a switch between the L1/shared memory configurations described in Compute Capability 7.x .
Operations that require a dependency check include any other commands within the same stream as the launch being checked and any call to cudaStreamQuery() on that stream.
Therefore, applications should follow these guidelines to improve their potential for concurrent kernel execution: All independent operations should be issued before dependent operations, Synchronization of any kind should be delayed as long as possible. 3.2.8.5.5. Overlapping Behavior  The amount of execution overlap between two streams depends on the order in which the commands are issued to each stream and whether or not the device supports overlap of data transfer and kernel execution (see Overlap of Data Transfer and Kernel Execution ), concurrent kernel execution (see Concurrent Kernel Execution ), and/or concurrent data transfers (see Concurrent Data Transfers ).
For example, on devices that do not support concurrent data transfers, the two streams of the code sample of Creation and Destruction do not overlap at all because the memory copy from host to device is issued to stream[1] after the memory copy from device to host is issued to stream[0], so it can only start once the memory copy from device to host issued to stream[0] has completed.
If the code is rewritten the following way (and assuming the device supports overlap of data transfer and kernel execution) for ( int i = 0 ; i >> ( outputDevPtr + i * size , inputDevPtr + i * size , size ); for ( int i = 0 ; i >> ( devPtrOut [ i ], devPtrIn [ i ], size ); cudaMemcpyAsync ( hostPtr [ i ], devPtrOut [ i ], size , cudaMemcpyDeviceToHost , stream [ i ]); cudaLaunchHostFunc ( stream [ i ], MyCallback , ( void * ) i ); } The commands that are issued in a stream after a host function do not start executing before the function has completed.
A host function enqueued into a stream must not make CUDA API calls (directly or indirectly), as it might end up waiting on itself if it makes such a call leading to a deadlock. 3.2.8.5.7. Stream Priorities  The relative priorities of streams can be specified at creation using cudaStreamCreateWithPriority() .
The range of allowable priorities, ordered as [ highest priority, lowest priority ] can be obtained using the cudaDeviceGetStreamPriorityRange() function.
At runtime, pending work in higher-priority streams takes preference over pending work in low-priority streams.
The following code sample obtains the allowable range of priorities for the current device, and creates streams with the highest and lowest available priorities.
get the range of stream priorities for this device int priority_high , priority_low ; cudaDeviceGetStreamPriorityRange ( & priority_low , & priority_high );   create streams with highest and lowest available priorities cudaStream_t st_high , st_low ; cudaStreamCreateWithPriority ( & st_high , cudaStreamNonBlocking , priority_high ); cudaStreamCreateWithPriority ( & st_low , cudaStreamNonBlocking , priority_low ); 3.2.8.6.
Programmatic Dependent Launch and Synchronization  The Programmatic Dependent Launch mechanism allows for a dependent secondary kernel to launch before the primary kernel it depends on in the same CUDA stream has finished executing.
Available starting with devices of compute capability 9.0, this technique can provide performance benefits when the secondary kernel can complete significant work that does not depend on the results of the primary kernel. 3.2.8.6.1. Background  A CUDA application utilizes the GPU by launching and executing multiple kernels on it.
Figure 10 GPU activity timeline  Here, secondary_kernel is launched after primary_kernel finishes its execution.
Serialized execution is usually necessary because secondary_kernel depends on result data produced by primary_kernel .
If secondary_kernel has no dependency on primary_kernel , both of them can be launched concurrently by using CUDA streams .
Even if secondary_kernel is dependent on primary_kernel , there is some potential for concurrent execution.
For example, almost all the kernels have some sort of preamble section during which tasks such as zeroing buffers or loading constant values are performed.
Figure 11 Preamble section of secondary_kernel  Figure 11 demonstrates the portion of secondary_kernel that could be executed concurrently without impacting the application.
Note that concurrent launch also allows us to hide the launch latency of secondary_kernel behind the execution of primary_kernel .
Figure 12 Concurrent execution of primary_kernel and secondary_kernel  The concurrent launch and execution of secondary_kernel shown in Figure 12 is achievable using Programmatic Dependent Launch .
Programmatic Dependent Launch introduces changes to the CUDA kernel launch APIs as explained in following section.
These APIs require at least compute capability 9.0 to provide overlapping execution. 3.2.8.6.2. API Description  In Programmatic Dependent Launch, a primary and a secondary kernel are launched in the same CUDA stream.
The primary kernel should execute cudaTriggerProgrammaticLaunchCompletion with all thread blocks when it’s ready for the secondary kernel to launch.
__global__ void primary_kernel () {   Initial work that should finish before starting secondary kernel   Trigger the secondary kernel cudaTriggerProgrammaticLaunchCompletion ();   Work that can coincide with the secondary kernel } __global__ void secondary_kernel () {   Independent work   Will block until all primary kernels the secondary kernel is dependent on have completed and flushed results to global memory cudaGridDependencySynchronize ();   Dependent work } cudaLaunchAttribute attribute [ 1 ]; attribute [ 0 ].
numAttrs = 1 ; primary_kernel >> (); cudaLaunchKernelEx ( & configSecondary , secondary_kernel ); When the secondary kernel is launched using the cudaLaunchAttributeProgrammaticStreamSerialization attribute, the CUDA driver is safe to launch the secondary kernel early and not wait on the completion and memory flush of the primary before launching the secondary.
The CUDA driver can launch the secondary kernel when all primary thread blocks have launched and executed cudaTriggerProgrammaticLaunchCompletion .
If the primary kernel doesn’t execute the trigger, it implicitly occurs after all thread blocks in the primary kernel exit.
In either case, the secondary thread blocks might launch before data written by the primary kernel is visible.
As such, when the secondary kernel is configured with Programmatic Dependent Launch , it must always use cudaGridDependencySynchronize or other means to verify that the result data from the primary is available.
Please note that these methods provide the opportunity for the primary and secondary kernels to execute concurrently, however this behavior is opportunistic and not guaranteed to lead to concurrent kernel execution.
Reliance on concurrent execution in this manner is unsafe and can lead to deadlock. 3.2.8.6.3. Use in CUDA Graphs  Programmatic Dependent Launch can be used in CUDA Graphs via stream capture or directly via edge data .
To program this feature in a CUDA Graph with edge data, use a cudaGraphDependencyType value of cudaGraphDependencyTypeProgrammatic on an edge connecting two kernel nodes.
This edge type makes the upstream kernel visible to a cudaGridDependencySynchronize() in the downstream kernel.
This type must be used with an outgoing port of either cudaGraphKernelNodePortLaunchCompletion or cudaGraphKernelNodePortProgrammatic .
The resulting graph equivalents for stream capture are as follows: Stream code (abbreviated) Resulting graph edge cudaLaunchAttribute attribute ; attribute .
from_port = cudaGraphKernelNodePortProgrammatic ; cudaLaunchAttribute attribute ; attribute .
A graph is a series of operations, such as kernel launches, connected by dependencies, which is defined separately from its execution.
Separating out the definition of a graph from its execution enables a number of optimizations: first, CPU launch costs are reduced compared to streams, because much of the setup is done in advance; second, presenting the whole workflow to CUDA enables optimizations which might not be possible with the piecewise work submission mechanism of streams.
To see the optimizations possible with graphs, consider what happens in a stream: when you place a kernel into a stream, the host driver performs a sequence of operations in preparation for the execution of the kernel on the GPU.
These operations, necessary for setting up and launching the kernel, are an overhead cost which must be paid for each kernel that is issued.
For a GPU kernel with a short execution time, this overhead cost can be a significant fraction of the overall end-to-end execution time.
Work submission using graphs is separated into three distinct stages: definition, instantiation, and execution.
During the definition phase, a program creates a description of the operations in the graph along with the dependencies between them.
Instantiation takes a snapshot of the graph template, validates it, and performs much of the setup and initialization of work with the aim of minimizing what needs to be done at launch.
It may be launched any number of times without repeating the instantiation. 3.2.8.7.1. Graph Structure  An operation forms a node in a graph.
An operation may be scheduled at any time once the nodes on which it depends are complete.
Scheduling is left up to the CUDA system. 3.2.8.7.1.1. Node Types  A graph node can be one of: kernel CPU function call memory copy memset empty node waiting on an event recording an event signalling an external semaphore waiting on an external semaphore conditional node child graph: To execute a separate nested graph, as shown in the following figure.
Edge data modifies a dependency specified by an edge and consists of three parts: an outgoing port, an incoming port, and a type.
Port values are specific to node type and direction, and edge types may be restricted to specific node types.
Outgoing port 0 waits on an entire task, incoming port 0 blocks an entire task, and edge type 0 is associated with a full dependency with memory synchronizing behavior.
Edge data is optionally specified in various graph APIs via a parallel array to the associated nodes.
If it is omitted as an output (query) parameter, the API accepts this if the edge data being ignored is all zero-initialized, and returns cudaErrorLossyQuery if the call would discard information.
Edge data is also available in some stream capture APIs: cudaStreamBeginCaptureToGraph() , cudaStreamGetCaptureInfo() , and cudaStreamUpdateCaptureDependencies() .
The data is associated with a dangling edge (half edge) which will either be connected to a future captured node or discarded at termination of stream capture.
These edges are ignored when considering if a stream capture has been fully rejoined to the origin stream, and cannot be discarded at the end of capture.
Currently, no node types define additional incoming ports, and only kernel nodes define additional outgoing ports.
There is one non-default dependency type, cudaGraphDependencyTypeProgrammatic , which enables Programmatic Dependent Launch between two kernel nodes. 3.2.8.7.2. Creating a Graph Using Graph APIs  Graphs can be created via two mechanisms: explicit API and stream capture.
Figure 14 Creating a Graph Using Graph APIs Example    Create the graph - it starts out empty cudaGraphCreate ( & graph , 0 );   For the purpose of this example, we'll create   the nodes separately from the dependencies to   demonstrate that it can be done in two stages.
cudaGraphAddKernelNode ( & a , graph , NULL , 0 , & nodeParams ); cudaGraphAddKernelNode ( & b , graph , NULL , 0 , & nodeParams ); cudaGraphAddKernelNode ( & c , graph , NULL , 0 , & nodeParams ); cudaGraphAddKernelNode ( & d , graph , NULL , 0 , & nodeParams );   Now set up dependencies on each node cudaGraphAddDependencies ( graph , & a , & b , 1 );   A->B cudaGraphAddDependencies ( graph , & a , & c , 1 );   A->C cudaGraphAddDependencies ( graph , & b , & d , 1 );   B->D cudaGraphAddDependencies ( graph , & c , & d , 1 );   C->D 3.2.8.7.3.
Creating a Graph Using Stream Capture  Stream capture provides a mechanism to create a graph from existing stream-based APIs.
A section of code which launches work into streams, including existing code, can be bracketed with calls to cudaStreamBeginCapture() and cudaStreamEndCapture() .
cudaGraph_t graph ; cudaStreamBeginCapture ( stream ); kernel_A >> (...); kernel_B >> (...); libraryCall ( stream ); kernel_C >> (...); cudaStreamEndCapture ( stream , & graph ); A call to cudaStreamBeginCapture() places a stream in capture mode.
When a stream is being captured, work launched into the stream is not enqueued for execution.
This graph is then returned by calling cudaStreamEndCapture() , which also ends capture mode for the stream.
A graph which is actively being constructed by stream capture is referred to as a capture graph.
Stream capture can be used on any CUDA stream except cudaStreamLegacy (the “NULL stream”).
If a program is using the legacy stream, it may be possible to redefine stream 0 to be the per-thread stream with no functional change.
Instead of capturing to an internal graph, work is captured to a graph provided by the user. 3.2.8.7.3.1. Cross-stream Dependencies and Events  Stream capture can handle cross-stream dependencies expressed with cudaEventRecord() and cudaStreamWaitEvent() , provided the event being waited upon was recorded into the same capture graph.
When an event is recorded in a stream that is in capture mode, it results in a captured event.
When a captured event is waited on by a stream, it places the stream in capture mode if it is not already, and the next item in the stream will have additional dependencies on the nodes in the captured event.
When cross-stream dependencies are present in stream capture, cudaStreamEndCapture() must still be called in the same stream where cudaStreamBeginCapture() was called; this is the origin stream .
Any other streams which are being captured to the same capture graph, due to event-based dependencies, must also be joined back to the origin stream.
All streams being captured to the same capture graph are taken out of capture mode upon cudaStreamEndCapture() .
Failure to rejoin to the origin stream will result in failure of the overall capture operation.
stream1 is the origin stream cudaStreamBeginCapture ( stream1 ); kernel_A >> (...);   Fork into stream2 cudaEventRecord ( event1 , stream1 ); cudaStreamWaitEvent ( stream2 , event1 ); kernel_B >> (...); kernel_C >> (...);   Join stream2 back to origin stream (stream1) cudaEventRecord ( event2 , stream2 ); cudaStreamWaitEvent ( stream1 , event2 ); kernel_D >> (...);   End capture in the origin stream cudaStreamEndCapture ( stream1 , & graph );   stream1 and stream2 no longer in capture mode Graph returned by the above code is shown in Figure 14 .
Note When a stream is taken out of capture mode, the next non-captured item in the stream (if any) will still have a dependency on the most recent prior non-captured item, despite intermediate items having been removed. 3.2.8.7.3.2. Prohibited and Unhandled Operations  It is invalid to synchronize or query the execution status of a stream which is being captured or a captured event, because they do not represent items scheduled for execution.
It is also invalid to query the execution status of or synchronize a broader handle which encompasses an active stream capture, such as a device or context handle when any associated stream is in capture mode.
When any stream in the same context is being captured, and it was not created with cudaStreamNonBlocking , any attempted use of the legacy stream is invalid.
This is because the legacy stream handle at all times encompasses these other streams; enqueueing to the legacy stream would create a dependency on the streams being captured, and querying it or synchronizing it would query or synchronize the streams being captured.
Synchronous APIs, such as cudaMemcpy() , enqueue work to the legacy stream and synchronize it before returning.
Note As a general rule, when a dependency relation would connect something that is captured with something that was not captured and instead enqueued for execution, CUDA prefers to return an error rather than ignore the dependency.
An exception is made for placing a stream into or out of capture mode; this severs a dependency relation between items added to the stream immediately before and after the mode transition.
It is invalid to merge two separate capture graphs by waiting on a captured event from a stream which is being captured and is associated with a different capture graph than the event.
It is invalid to wait on a non-captured event from a stream which is being captured without specifying the cudaEventWaitExternal flag.
A small number of APIs that enqueue asynchronous operations into streams are not currently supported in graphs and will return an error if called with a stream which is being captured, such as cudaStreamAttachMemAsync() . 3.2.8.7.3.3. Invalidation  When an invalid operation is attempted during stream capture, any associated capture graphs are invalidated .
When a capture graph is invalidated, further use of any streams which are being captured or captured events associated with the graph is invalid and will return an error, until stream capture is ended with cudaStreamEndCapture() .
This call will take the associated streams out of capture mode, but will also return an error value and a NULL graph. 3.2.8.7.4. CUDA User Objects  CUDA User Objects can be used to help manage the lifetime of resources used by asynchronous work in CUDA.
Consider for example an event-based pool or a synchronous-create, asynchronous-destroy scheme.
Library API with pool allocation void libraryWork ( cudaStream_t stream ) { auto & resource = pool .
recordReadyEvent ( stream ); }   Library API with asynchronous resource deletion void libraryWork ( cudaStream_t stream ) { Resource * resource = new Resource (...); launchWork ( stream , resource ); cudaStreamAddCallback ( stream , []( cudaStream_t , cudaError_t , void * resource ) { delete static_cast ( resource ); }, resource , 0 );   Error handling considerations not shown } These schemes are difficult with CUDA graphs because of the non-fixed pointer or handle for the resource which requires indirection or graph update, and the synchronous CPU code needed each time the work is submitted.
They also do not work with stream capture if these considerations are hidden from the caller of the library, and because of use of disallowed APIs during capture.
A CUDA user object associates a user-specified destructor callback with an internal refcount, similar to C++ shared_ptr .
Note that for user-owned references, unlike C++ smart pointers, there is no object representing the reference; users must track user-owned references manually.
A typical use case would be to immediately move the sole user-owned reference to a CUDA graph after the user object is created.
When a reference is associated to a CUDA graph, CUDA will manage the graph operations automatically.
A cloned cudaGraph_t retains a copy of every reference owned by the source cudaGraph_t , with the same multiplicity.
An instantiated cudaGraphExec_t retains a copy of every reference in the source cudaGraph_t .
When a cudaGraphExec_t is destroyed without being synchronized, the references are retained until the execution is completed.
If the destructor callback had signaled a synchronization object, it would   be safe to wait on it at this point.
References owned by graphs in child graph nodes are associated to the child graphs, not the parents.
If an executable graph or child graph is updated with cudaGraphExecUpdate or cudaGraphExecChildGraphNodeSetParams , the references in the new source graph are cloned and replace the references in the target graph.
In either case, if previous launches are not synchronized, any references which would be released are held until the launches have finished executing.
In addition, it is not legal to call CUDA APIs from the destructor, similar to the restriction on cudaLaunchHostFunc .
It is legal to signal another thread to perform an API call, if the dependency is one way and the thread doing the call cannot block forward progress of CUDA work.
User objects are created with cudaUserObjectCreate , which is a good starting point to browse related APIs. 3.2.8.7.5. Updating Instantiated Graphs  Work submission using graphs is separated into three distinct stages: definition, instantiation, and execution.
In situations where the workflow is not changing, the overhead of definition and instantiation can be amortized over many executions, and graphs provide a clear advantage over streams.
A graph is a snapshot of a workflow, including kernels, parameters, and dependencies, in order to replay it as rapidly and efficiently as possible.
In situations where the workflow changes the graph becomes out of date and must be modified.
Major changes to graph structure such as topology or types of nodes will require re-instantiation of the source graph because various topology-related optimization techniques must be re-applied.
The cost of repeated instantiation can reduce the overall performance benefit from graph execution, but it is common for only node parameters, such as kernel parameters and cudaMemcpy addresses, to change while graph topology remains the same.
For this case, CUDA provides a lightweight mechanism known as “Graph Update,” which allows certain node parameters to be modified in-place without having to rebuild the entire graph.
Updates will take effect the next time the graph is launched, so they will not impact previous graph launches, even if they are running at the time of the update.
A graph may be updated and relaunched repeatedly, so multiple updates/launches can be queued on a stream.
CUDA provides two mechanisms for updating instantiated graph parameters, whole graph update and individual node update.
Whole graph update allows the user to supply a topologically identical cudaGraph_t object whose nodes contain updated parameters.
Individual node update allows the user to explicitly update the parameters of individual nodes.
Using an updated cudaGraph_t is more convenient when a large number of nodes are being updated, or when the graph topology is unknown to the caller (i.e., The graph resulted from stream capture of a library call).
Using individual node update is preferred when the number of changes is small and the user has the handles to the nodes requiring updates.
Individual node update skips the topology checks and comparisons for unchanged nodes, so it can be more efficient in many cases.
CUDA also provides a mechanism for enabling and disabling individual nodes without affecting their current parameters.
The following sections explain each approach in more detail. 3.2.8.7.5.1. Graph Update Limitations  Kernel nodes: The owning context of the function cannot change.
A node whose function originally did not use CUDA dynamic parallelism cannot be updated to a function which uses CUDA dynamic parallelism.
cudaMemset and cudaMemcpy nodes: The CUDA device(s) to which the operand(s) was allocated/mapped cannot change.
The source/destination memory must be allocated from the same context as the original source/destination memory.
Additional memcpy node restrictions: Changing either the source or destination memory type (i.e., cudaPitchedPtr , cudaArray_t , etc.
External semaphore wait nodes and record nodes: Changing the number of semaphores is not supported.
Conditional nodes: The order of handle creation and assignment must match between the graphs.
Changing parameters of nodes within the conditional body graph is subject to the rules above.
There are no restrictions on updates to host nodes, event record nodes, or event wait nodes. 3.2.8.7.5.2. Whole Graph Update  cudaGraphExecUpdate() allows an instantiated graph (the “original graph”) to be updated with the parameters from a topologically identical graph (the “updating” graph).
The topology of the updating graph must be identical to the original graph used to instantiate the cudaGraphExec_t .
More explicitly, following the following rules will cause cudaGraphExecUpdate() to pair the nodes in the original graph and the updating graph deterministically: For any capturing stream, the API calls operating on that stream must be made in the same order, including event wait and other api calls not directly corresponding to node creation.
The API calls which directly manipulate a given graph node’s incoming edges (including captured stream APIs, node add APIs, and edge addition / removal APIs) must be made in the same order.
Moreover, when dependencies are specified in arrays to these APIs, the order in which the dependencies are specified inside those arrays must match.
Sink nodes are nodes without dependent nodes / outgoing edges in the final graph at the time of the cudaGraphExecUpdate() invocation.
The following operations affect sink node ordering (if present) and must (as a combined set) be made in the same order: Node add APIs resulting in a sink node.
cudaStreamUpdateCaptureDependencies() , if it removes a sink node from a capturing stream’s dependency set.
The following example shows how the API could be used to update an instantiated graph: cudaGraphExec_t graphExec = NULL ; for ( int i = 0 ; i >> ( gExec2 ); cudaStreamEndCapture ( stream , & g1 ); cudaGraphInstantiate ( & gExec1 , g1 );   Launch the host graph, which will in turn launch the device graph.
cudaGraphLaunch ( gExec1 , stream ); } A graph can have up to 120 total fire-and-forget graphs during the course of its execution.
This total resets between launches of the same parent graph. 3.2.8.7.7.2.1.2. Graph Execution Environments  In order to fully understand the device-side synchronization model, it is first necessary to understand the concept of an execution environment.
When a graph is launched from the device, it is launched into its own execution environment.
The execution environment of a given graph encapsulates all work in the graph as well as all generated fire and forget work.
The graph can be considered complete when it has completed execution and when all generated child work is complete.
The below diagram shows the environment encapsulation that would be generated by the fire-and-forget sample code in the previous section.
Figure 16 Fire and forget launch, with execution environments  These environments are also hierarchical, so a graph environment can include multiple levels of child-environments from fire and forget launches.
Figure 17 Nested fire and forget environments  When a graph is launched from the host, there exists a stream environment that parents the execution environment of the launched graph.
downstream dependent work may now run) when the overall stream environment is marked as complete.
Tail Launch  Unlike on the host, it is not possible to synchronize with device graphs from the GPU via traditional methods such as cudaDeviceSynchronize() or cudaStreamSynchronize() .
Rather, in order to enable serial work dependencies, a different launch mode - tail launch - is offered, to provide similar functionality.
A tail launch executes when a graph’s environment is considered complete - ie, when the graph and all its children are complete.
When a graph completes, the environment of the next graph in the tail launch list will replace the completed environment as a child of the parent environment.
Figure 19 A simple tail launch  The above execution flow can be generated by the code below: __global__ void launchTailGraph ( cudaGraphExec_t graph ) { cudaGraphLaunch ( graph , cudaStreamGraphTailLaunch ); } void graphSetup () { cudaGraphExec_t gExec1 , gExec2 ; cudaGraph_t g1 , g2 ;   Create, instantiate, and upload the device graph.
create_graph ( & g2 ); cudaGraphInstantiate ( & gExec2 , g2 , cudaGraphInstantiateFlagDeviceLaunch ); cudaGraphUpload ( gExec2 , stream );   Create and instantiate the launching graph.
cudaStreamBeginCapture ( stream , cudaStreamCaptureModeGlobal ); launchTailGraph >> ( gExec2 ); cudaStreamEndCapture ( stream , & g1 ); cudaGraphInstantiate ( & gExec1 , g1 );   Launch the host graph, which will in turn launch the device graph.
cudaGraphLaunch ( gExec1 , stream ); } Tail launches enqueued by a given graph will execute one at a time, in order of when they were enqueued.
Figure 20 Tail launch ordering  Tail launches enqueued by a tail graph will execute before tail launches enqueued by previous graphs in the tail launch list.
Figure 21 Tail launch ordering when enqueued from multiple graphs  A graph can have up to 255 pending tail launches. 3.2.8.7.7.2.1.3.1. Tail Self-launch  It is possible for a device graph to enqueue itself for a tail launch, although a given graph can only have one self-launch enqueued at a time.
In order to query the currently running device graph so that it can be relaunched, a new device-side function is added: cudaGraphExec_t cudaGetCurrentGraphExec (); This function returns the handle of the currently running graph if it is a device graph.
If the currently executing kernel is not a node within a device graph, this function will return NULL.
Below is sample code showing usage of this function for a relaunch loop: __device__ int relaunchCount = 0 ; __global__ void relaunchSelf () { int relaunchMax = 100 ; if ( threadIdx .
x == 0 ) { if ( relaunchCount >> ( gExec2 ); cudaStreamEndCapture ( stream , & g1 ); cudaGraphInstantiate ( & gExec1 , g1 );   Launch the host graph, which will in turn launch the device graph.
cudaGraphLaunch ( gExec1 , stream ); } Since sibling launches are not launched into the launching graph’s execution environment, they will not gate tail launches enqueued by the launching graph. 3.2.8.7.8. Conditional Graph Nodes  Conditional nodes allow conditional execution and looping of a graph contained within the conditional node.
This allows dynamic and iterative workflows to be represented completely within a graph and frees up the host CPU to perform other work in parallel.
Evaluation of the condition value is performed on the device when the dependencies of the conditional node have been met.
Conditional nodes can be one of the following types: Conditional IF nodes execute their body graph once if the condition value is non-zero when the node is executed.
Conditional WHILE nodes execute their body graph if the condition value is non-zero when the node is executed and will continue to execute their body graph until the condition value is zero.
A condition value is accessed by a conditional handle , which must be created before the node.
A default value, applied on each graph launch, can also be specified when the handle is created.
When the conditional node is created, an empty graph is created and the handle is returned to the user so that the graph can be populated.
This conditional body graph can be populated using either the graph APIs or cudaStreamBeginCaptureToGraph() .
Conditional nodes can be nested. 3.2.8.7.8.1. Conditional Handles  A condition value is represented by cudaGraphConditionalHandle and is created by cudaGraphConditionalHandleCreate() .
If cudaGraphCondAssignDefault is specified when the handle is created, the condition value will be initialized to the specified default before every graph launch.
If this flag is not provided, it is up to the user to initialize the condition value in a kernel upstream of the conditional node which tests it.
If the condition value is not initialized by one of these methods, its value is undefined.
The default value and flags associated with a handle will be updated during whole graph update . 3.2.8.7.8.2. Condtional Node Body Graph Requirements  General requirements: The graph’s nodes must all reside on a single device.
The graph can only contain kernel nodes, empty nodes, memcpy nodes, memset nodes, child graph nodes, and conditional nodes.
Memcpy/Memset nodes: Only copies/memsets involving device memory and/or pinned device-mapped host memory are permitted.
Note that the copy operation will be performed from the device on which the graph resides, even if it is targeting memory on another device. 3.2.8.7.8.3. Conditional IF Nodes  The body graph of an IF node will be executed once if the condition is non-zero when the node is executed.
The following diagram depicts a 3 node graph where the middle node, B, is a conditional node: Figure 23 Conditional IF Node  The following code illustrates the creation of a graph containing an IF conditional node.
} void graphSetup () { cudaGraph_t graph ; cudaGraphExec_t graphExec ; cudaGraphNode_t node ; void * kernelArgs [ 1 ]; int value = 1 ; cudaGraphCreate ( & graph , 0 ); cudaGraphConditionalHandle handle ; cudaGraphConditionalHandleCreate ( & handle , graph );   Use a kernel upstream of the conditional to set the handle value cudaGraphNodeParams params = { cudaGraphNodeTypeKernel }; params .
kernelParams = kernelArgs ; kernelArgs [ 0 ] = & handle ; cudaGraphAddNode ( & node , graph , NULL , 0 , & params ); cudaGraphNodeParams cParams = { cudaGraphNodeTypeConditional }; cParams .
size = 1 ; cudaGraphAddNode ( & node , graph , & node , 1 , & cParams ); cudaGraph_t bodyGraph = cParams .
cudaGraphAddNode ( & node , bodyGraph , NULL , 0 , & params ); cudaGraphInstantiate ( & graphExec , graph , NULL , NULL , 0 ); cudaGraphLaunch ( graphExec , 0 ); cudaDeviceSynchronize (); cudaGraphExecDestroy ( graphExec ); cudaGraphDestroy ( graph ); } 3.2.8.7.8.4.
Conditional WHILE Nodes  The body graph of a WHILE node will be executed until the condition is non-zero.
The condition will be evaluated when the node is executed and after completion of the body graph.
The following diagram depicts a 3 node graph where the middle node, B, is a conditional node: Figure 24 Conditional WHILE Node  The following code illustrates the creation of a graph containing a WHILE conditional node.
The handle is created using cudaGraphCondAssignDefault to avoid the need for an upstream kernel.
__global__ void loopKernel ( cudaGraphConditionalHandle handle ) { static int count = 10 ; cudaGraphSetConditional ( handle , -- count ? 1 : 0 ); } void graphSetup () { cudaGraph_t graph ; cudaGraphExec_t graphExec ; cudaGraphNode_t node ; void * kernelArgs [ 1 ]; cuGraphCreate ( & graph , 0 ); cudaGraphConditionalHandle handle ; cudaGraphConditionalHandleCreate ( & handle , graph , 1 , cudaGraphCondAssignDefault ); cudaGraphNodeParams cParams = { cudaGraphNodeTypeConditional }; cParams .
size = 1 ; cudaGraphAddNode ( & node , graph , NULL , 0 , & cParams ); cudaGraph_t bodyGraph = cParams .
kernelParams = kernelArgs ; kernelArgs [ 0 ] = & handle ; cudaGraphAddNode ( & node , bodyGraph , NULL , 0 , & params ); cudaGraphInstantiate ( & graphExec , graph , NULL , NULL , 0 ); cudaGraphLaunch ( graphExec , 0 ); cudaDeviceSynchronize (); cudaGraphExecDestroy ( graphExec ); cudaGraphDestroy ( graph ); } 3.2.8.8.
Events  The runtime also provides a way to closely monitor the device’s progress, as well as perform accurate timing, by letting the application asynchronously record events at any point in the program, and query when these events are completed.
An event has completed when all tasks - or optionally, all commands in a given stream - preceding the event have completed.
Events in stream zero are completed after all preceding tasks and commands in all streams are completed. 3.2.8.8.1. Creation and Destruction of Events  The following code sample creates two events: cudaEvent_t start , stop ; cudaEventCreate ( & start ); cudaEventCreate ( & stop ); They are destroyed this way: cudaEventDestroy ( start ); cudaEventDestroy ( stop ); 3.2.8.8.2.
Elapsed Time  The events created in Creation and Destruction can be used to time the code sample of Creation and Destruction the following way: cudaEventRecord ( start , 0 ); for ( int i = 0 ; i >> ( outputDev + i * size , inputDev + i * size , size ); cudaMemcpyAsync ( outputHost + i * size , outputDev + i * size , size , cudaMemcpyDeviceToHost , stream [ i ]); } cudaEventRecord ( stop , 0 ); cudaEventSynchronize ( stop ); float elapsedTime ; cudaEventElapsedTime ( & elapsedTime , start , stop ); 3.2.8.9.
Synchronous Calls  When a synchronous function is called, control is not returned to the host thread before the device has completed the requested task.
Whether the host thread will then yield, block, or spin can be specified by calling cudaSetDeviceFlags() with some specific flags (see reference manual for details) before any other CUDA call is performed by the host thread. 3.2.9. Multi-Device System  3.2.9.1.
The following code sample shows how to enumerate these devices, query their properties, and determine the number of CUDA-enabled devices.
int deviceCount ; cudaGetDeviceCount ( & deviceCount ); int device ; for ( device = 0 ; device >> ( p0 );   Launch kernel on device 0 cudaSetDevice ( 1 );   Set device 1 as current float * p1 ; cudaMalloc ( & p1 , size );   Allocate memory on device 1 MyKernel >> ( p1 );   Launch kernel on device 1 3.2.9.3.
Stream and Event Behavior  A kernel launch will fail if it is issued to a stream that is not associated to the current device as illustrated in the following code sample.
cudaSetDevice ( 0 );   Set device 0 as current cudaStream_t s0 ; cudaStreamCreate ( & s0 );   Create stream s0 on device 0 MyKernel >> ();   Launch kernel on device 0 in s0 cudaSetDevice ( 1 );   Set device 1 as current cudaStream_t s1 ; cudaStreamCreate ( & s1 );   Create stream s1 on device 1 MyKernel >> ();   Launch kernel on device 1 in s1   This kernel launch will fail: MyKernel >> ();   Launch kernel on device 1 in s0 A memory copy will succeed even if it is issued to a stream that is not associated to the current device.
cudaEventRecord() will fail if the input event and input stream are associated to different devices.
cudaEventElapsedTime() will fail if the two input events are associated to different devices.
cudaEventSynchronize() and cudaEventQuery() will succeed even if the input event is associated to a device that is different from the current device.
cudaStreamWaitEvent() will succeed even if the input stream and input event are associated to different devices.
cudaStreamWaitEvent() can therefore be used to synchronize multiple devices with each other.
Each device has its own default stream (see Default Stream ), so commands issued to the default stream of a device may execute out of order or concurrently with respect to commands issued to the default stream of any other device. 3.2.9.4. Peer-to-Peer Memory Access  Depending on the system properties, specifically the PCIe and/or NVLINK topology, devices are able to address each other’s memory (i.e., a kernel executing on one device can dereference a pointer to the memory of the other device).
This peer-to-peer memory access feature is supported between two devices if cudaDeviceCanAccessPeer() returns true for these two devices.
Peer-to-peer memory access is only supported in 64-bit applications and must be enabled between two devices by calling cudaDeviceEnablePeerAccess() as illustrated in the following code sample.
On non-NVSwitch enabled systems, each device can support a system-wide maximum of eight peer connections.
A unified address space is used for both devices (see Unified Virtual Address Space ), so the same pointer can be used to address memory from both devices as shown in the code sample below.
cudaSetDevice ( 0 );   Set device 0 as current float * p0 ; size_t size = 1024 * sizeof ( float ); cudaMalloc ( & p0 , size );   Allocate memory on device 0 MyKernel >> ( p0 );   Launch kernel on device 0 cudaSetDevice ( 1 );   Set device 1 as current cudaDeviceEnablePeerAccess ( 0 , 0 );   Enable peer-to-peer access   with device 0   Launch kernel on device 1   This kernel launch can access memory on device 0 at address p0 MyKernel >> ( p0 ); 3.2.9.4.1.
IOMMU on Linux  On Linux only, CUDA and the display driver does not support IOMMU-enabled bare-metal PCIe peer to peer memory copy.
As a consequence, users on Linux, when running on a native bare metal system, should disable the IOMMU.
The IOMMU should be enabled and the VFIO driver be used as a PCIe pass through for virtual machines.
See also Allocating DMA Buffers on 64-bit Platforms . 3.2.9.5. Peer-to-Peer Memory Copy  Memory copies can be performed between the memories of two different devices.
When a unified address space is used for both devices (see Unified Virtual Address Space ), this is done using the regular memory copy functions mentioned in Device Memory .
Otherwise, this is done using cudaMemcpyPeer() , cudaMemcpyPeerAsync() , cudaMemcpy3DPeer() , or cudaMemcpy3DPeerAsync() as illustrated in the following code sample.
cudaSetDevice ( 0 );   Set device 0 as current float * p0 ; size_t size = 1024 * sizeof ( float ); cudaMalloc ( & p0 , size );   Allocate memory on device 0 cudaSetDevice ( 1 );   Set device 1 as current float * p1 ; cudaMalloc ( & p1 , size );   Allocate memory on device 1 cudaSetDevice ( 0 );   Set device 0 as current MyKernel >> ( p0 );   Launch kernel on device 0 cudaSetDevice ( 1 );   Set device 1 as current cudaMemcpyPeer ( p1 , 1 , p0 , 0 , size );   Copy p0 to p1 MyKernel >> ( p1 );   Launch kernel on device 1 A copy (in the implicit NULL stream) between the memories of two different devices: does not start until all commands previously issued to either device have completed and runs to completion before any commands (see Asynchronous Concurrent Execution ) issued after the copy to either device can start.
Consistent with the normal behavior of streams, an asynchronous copy between the memories of two devices may overlap with copies or kernels in another stream.
Note that if peer-to-peer access is enabled between two devices via cudaDeviceEnablePeerAccess() as described in Peer-to-Peer Memory Access , peer-to-peer memory copy between these two devices no longer needs to be staged through the host and is therefore faster. 3.2.10. Unified Virtual Address Space  When the application is run as a 64-bit process, a single address space is used for the host and all the devices of compute capability 2.0 and higher.
All host memory allocations made via CUDA API calls and all device memory allocations on supported devices are within this virtual address range.
As a consequence: The location of any memory on the host allocated through CUDA, or on any of the devices which use the unified address space, can be determined from the value of the pointer using cudaPointerGetAttributes() .
When copying to or from the memory of any device which uses the unified address space, the cudaMemcpyKind parameter of cudaMemcpy*() can be set to cudaMemcpyDefault to determine locations from the pointers.
This also works for host pointers not allocated through CUDA, as long as the current device uses unified addressing.
Allocations via cudaHostAlloc() are automatically portable (see Portable Memory ) across all the devices for which the unified address space is used, and pointers returned by cudaHostAlloc() can be used directly from within kernels running on these devices (i.e., there is no need to obtain a device pointer via cudaHostGetDevicePointer() as described in Mapped Memory .
Applications may query if the unified address space is used for a particular device by checking that the unifiedAddressing device property (see Device Enumeration ) is equal to 1. 3.2.11. Interprocess Communication  Any device memory pointer or event handle created by a host thread can be directly referenced by any other thread within the same process.
It is not valid outside this process however, and therefore cannot be directly referenced by threads belonging to a different process.
To share device memory pointers and events across processes, an application must use the Inter Process Communication API, which is described in detail in the reference manual.
The IPC API is only supported for 64-bit processes on Linux and for devices of compute capability 2.0 and higher.
Using this API, an application can get the IPC handle for a given device memory pointer using cudaIpcGetMemHandle() , pass it to another process using standard IPC mechanisms (for example, interprocess shared memory or files), and use cudaIpcOpenMemHandle() to retrieve a device pointer from the IPC handle that is a valid pointer within this other process.
Note that allocations made by cudaMalloc() may be sub-allocated from a larger block of memory for performance reasons.
In such case, CUDA IPC APIs will share the entire underlying memory block which may cause other sub-allocations to be shared, which can potentially lead to information disclosure between processes.
To prevent this behavior, it is recommended to only share allocations with a 2MiB aligned size.
An example of using the IPC API is where a single primary process generates a batch of input data, making the data available to multiple secondary processes without requiring regeneration or copying.
Applications using CUDA IPC to communicate with each other should be compiled, linked, and run with the same CUDA driver and runtime.
Note Since CUDA 11.5, only events-sharing IPC APIs are supported on L4T and embedded Linux Tegra devices with compute capability 7.x and higher.
The memory-sharing IPC APIs are still not supported on Tegra platforms. 3.2.12. Error Checking  All runtime functions return an error code, but for an asynchronous function (see Asynchronous Concurrent Execution ), this error code cannot possibly report any of the asynchronous errors that could occur on the device since the function returns before the device has completed the task; the error code only reports errors that occur on the host prior to executing the task, typically related to parameter validation; if an asynchronous error occurs, it will be reported by some subsequent unrelated runtime function call.
The only way to check for asynchronous errors just after some asynchronous function call is therefore to synchronize just after the call by calling cudaDeviceSynchronize() (or by using any other synchronization mechanisms described in Asynchronous Concurrent Execution ) and checking the error code returned by cudaDeviceSynchronize() .
The runtime maintains an error variable for each host thread that is initialized to cudaSuccess and is overwritten by the error code every time an error occurs (be it a parameter validation error or an asynchronous error).
Kernel launches do not return any error code, so cudaPeekAtLastError() or cudaGetLastError() must be called just after the kernel launch to retrieve any pre-launch errors.
To ensure that any error returned by cudaPeekAtLastError() or cudaGetLastError() does not originate from calls prior to the kernel launch, one has to make sure that the runtime error variable is set to cudaSuccess just before the kernel launch, for example, by calling cudaGetLastError() just before the kernel launch.
Kernel launches are asynchronous, so to check for asynchronous errors, the application must synchronize in-between the kernel launch and the call to cudaPeekAtLastError() or cudaGetLastError() .
Note that cudaErrorNotReady that may be returned by cudaStreamQuery() and cudaEventQuery() is not considered an error and is therefore not reported by cudaPeekAtLastError() or cudaGetLastError() . 3.2.13. Call Stack  On devices of compute capability 2.x and higher, the size of the call stack can be queried using cudaDeviceGetLimit() and set using cudaDeviceSetLimit() .
When the call stack overflows, the kernel call fails with a stack overflow error if the application is run via a CUDA debugger (CUDA-GDB, Nsight) or an unspecified launch error, otherwise.
When the compiler cannot determine the stack size, it issues a warning saying Stack size cannot be statically determined.
Once this warning is issued, user will need to set stack size manually if default stack size is not sufficient. 3.2.14. Texture and Surface Memory  CUDA supports a subset of the texturing hardware that the GPU uses for graphics to access texture and surface memory.
Reading data from texture or surface memory instead of global memory can have several performance benefits as described in Device Memory Accesses . 3.2.14.1. Texture Memory  Texture memory is read from kernels using the device functions described in Texture Functions .
The process of reading a texture calling one of these functions is called a texture fetch .
Each texture fetch specifies a parameter called a texture object for the texture object API.
The texture object specifies: The texture , which is the piece of texture memory that is fetched.
Texture objects are created at runtime and the texture is specified when creating the texture object as described in Texture Object API .
Its dimensionality that specifies whether the texture is addressed as a one dimensional array using one texture coordinate, a two-dimensional array using two texture coordinates, or a three-dimensional array using three texture coordinates.
Table 21 lists the maximum texture width, height, and depth depending on the compute capability of the device.
The type of a texel, which is restricted to the basic integer and single-precision floating-point types and any of the 1-, 2-, and 4-component vector types defined in Built-in Vector Types that are derived from the basic integer and single-precision floating-point types.
The read mode , which is equal to cudaReadModeNormalizedFloat or cudaReadModeElementType .
If it is cudaReadModeNormalizedFloat and the type of the texel is a 16-bit or 8-bit integer type, the value returned by the texture fetch is actually returned as floating-point type and the full range of the integer type is mapped to [0.0, 1.0] for unsigned integer type and [-1.0, 1.0] for signed integer type; for example, an unsigned 8-bit texture element with the value 0xff reads as 1.
By default, textures are referenced (by the functions of Texture Functions ) using floating-point coordinates in the range [0, N-1] where N is the size of the texture in the dimension corresponding to the coordinate.
For example, a texture that is 64x32 in size will be referenced with coordinates in the range [0, 63] and [0, 31] for the x and y dimensions, respectively.
Normalized texture coordinates cause the coordinates to be specified in the range [0.0, 1.0-1/N] instead of [0, N-1], so the same 64x32 texture would be addressed by normalized coordinates in the range [0, 1-1/N] in both the x and y dimensions.
Normalized texture coordinates are a natural fit to some applications’ requirements, if it is preferable for the texture coordinates to be independent of the texture size.
It is valid to call the device functions of Section B.8 with coordinates that are out of range.
The default addressing mode is to clamp the coordinates to the valid range: [0, N) for non-normalized coordinates and [0.0, 1.0) for normalized coordinates.
If the border mode is specified instead, texture fetches with out-of-range texture coordinates return zero.
When using the wrap mode, each coordinate x is converted to frac(x)=x - floor(x) where floor(x) is the largest integer not greater than x .
When using the mirror mode, each coordinate x is converted to frac(x) if floor(x) is even and 1-frac(x) if floor(x) is odd.
The addressing mode is specified as an array of size three whose first, second, and third elements specify the addressing mode for the first, second, and third texture coordinates, respectively; the addressing mode are cudaAddressModeBorder , cudaAddressModeClamp , cudaAddressModeWrap , and cudaAddressModeMirror ; cudaAddressModeWrap and cudaAddressModeMirror are only supported for normalized texture coordinates The filtering mode which specifies how the value returned when fetching the texture is computed based on the input texture coordinates.
Linear texture filtering may be done only for textures that are configured to return floating-point data.
When enabled, the texels surrounding a texture fetch location are read and the return value of the texture fetch is interpolated based on where the texture coordinates fell between the texels.
Simple linear interpolation is performed for one-dimensional textures, bilinear interpolation for two-dimensional textures, and trilinear interpolation for three-dimensional textures.
If it is cudaFilterModePoint , the returned value is the texel whose texture coordinates are the closest to the input texture coordinates.
If it is cudaFilterModeLinear , the returned value is the linear interpolation of the two (for a one-dimensional texture), four (for a two dimensional texture), or eight (for a three dimensional texture) texels whose texture coordinates are the closest to the input texture coordinates.
Cubemap Textures and Cubemap Layered Textures describe a special type of texture, the cubemap texture.
Simple transformation kernel __global__ void transformKernel ( float * output , cudaTextureObject_t texObj , int width , int height , float theta ) {   Calculate normalized texture coordinates unsigned int x = blockIdx .
16-Bit Floating-Point Textures  The 16-bit floating-point or half format supported by CUDA arrays is the same as the IEEE 754-2008 binary2 format.
CUDA C++ does not support a matching data type, but provides intrinsic functions to convert to and from the 32-bit floating-point format via the unsigned short type: __float2half_rn(float) and __half2float(unsigned short) .
16-bit floating-point components are promoted to 32 bit float during texture fetching before any filtering is performed.
A channel description for the 16-bit floating-point format can be created by calling one of the cudaCreateChannelDescHalf*() functions. 3.2.14.1.3. Layered Textures  A one-dimensional or two-dimensional layered texture (also known as texture array in Direct3D and array texture in OpenGL) is a texture made up of a sequence of layers, all of which are regular textures of same dimensionality, size, and data type.
A one-dimensional layered texture is addressed using an integer index and a floating-point texture coordinate; the index denotes a layer within the sequence and the coordinate addresses a texel within that layer.
A two-dimensional layered texture is addressed using an integer index and two floating-point texture coordinates; the index denotes a layer within the sequence and the coordinates address a texel within that layer.
A layered texture can only be a CUDA array by calling cudaMalloc3DArray() with the cudaArrayLayered flag (and a height of zero for one-dimensional layered texture).
Layered textures are fetched using the device functions described in tex1DLayered() and tex2DLayered() .
Layered textures are only supported on devices of compute capability 2.0 and higher. 3.2.14.1.4. Cubemap Textures  A cubemap texture is a special type of two-dimensional layered texture that has six layers representing the faces of a cube: The width of a layer is equal to its height.
The cubemap is addressed using three texture coordinates x , y , and z that are interpreted as a direction vector emanating from the center of the cube and pointing to one face of the cube and a texel within the layer corresponding to that face.
More specifically, the face is selected by the coordinate with largest magnitude m and the corresponding layer is addressed using coordinates (s/m+1)/2 and (t/m+1)/2 where s and t are defined in Table 3 .
Table 3 Cubemap Fetch  face m s t |x| > |y| and |x| > |z| x ≥ 0 0 x -z -y x |x| and |y| > |z| y ≥ 0 2 y x z y |x| and |z| > |y| z ≥ 0 4 z x -y z >> ( inputSurfObj , outputSurfObj , width , height );   Copy data from device back to host cudaMemcpy2DFromArray ( h_data , spitch , cuOutputArray , 0 , 0 , 4 * width * sizeof ( unsigned char ), height , cudaMemcpyDeviceToHost );   Destroy surface objects cudaDestroySurfaceObject ( inputSurfObj ); cudaDestroySurfaceObject ( outputSurfObj );   Free device memory cudaFreeArray ( cuInputArray ); cudaFreeArray ( cuOutputArray );   Free host memory free ( h_data ); return 0 ; } 3.2.14.2.2.
Cubemap Surfaces  Cubemap surfaces are accessed using surfCubemapread() and surfCubemapwrite() ( surfCubemapread and surfCubemapwrite ) as a two-dimensional layered surface, i.e., using an integer index denoting a face and two floating-point texture coordinates addressing a texel within the layer corresponding to this face.
Faces are ordered as indicated in Table 3 . 3.2.14.2.3. Cubemap Layered Surfaces  Cubemap layered surfaces are accessed using surfCubemapLayeredread() and surfCubemapLayeredwrite() ( surfCubemapLayeredread() and surfCubemapLayeredwrite() ) as a two-dimensional layered surface, i.e., using an integer index denoting a face of one of the cubemaps and two floating-point texture coordinates addressing a texel within the layer corresponding to this face.
Faces are ordered as indicated in Table 3 , so index ((2 * 6) + 3), for example, accesses the fourth face of the third cubemap. 3.2.14.3. CUDA Arrays  CUDA arrays are opaque memory layouts optimized for texture fetching.
They are one dimensional, two dimensional, or three-dimensional and composed of elements, each of which has 1, 2 or 4 components that may be signed or unsigned 8-, 16-, or 32-bit integers, 16-bit floats, or 32-bit floats.
CUDA arrays are only accessible by kernels through texture fetching as described in Texture Memory or surface reading and writing as described in Surface Memory . 3.2.14.4. Read/Write Coherency  The texture and surface memory is cached (see Device Memory Accesses ) and within the same kernel call, the cache is not kept coherent with respect to global memory writes and surface memory writes, so any texture fetch or surface read to an address that has been written to via a global write or a surface write in the same kernel call returns undefined data.
In other words, a thread can safely read some texture or surface memory location only if this memory location has been updated by a previous kernel call or memory copy, but not if it has been previously updated by the same thread or another thread from the same kernel call. 3.2.15. Graphics Interoperability  Some resources from OpenGL and Direct3D may be mapped into the address space of CUDA, either to enable CUDA to read data written by OpenGL or Direct3D, or to enable CUDA to write data for consumption by OpenGL or Direct3D.
A resource must be registered to CUDA before it can be mapped using the functions mentioned in OpenGL Interoperability and Direct3D Interoperability .
These functions return a pointer to a CUDA graphics resource of type struct cudaGraphicsResource .
Registering a resource is potentially high-overhead and therefore typically called only once per resource.
Each CUDA context which intends to use the resource is required to register it separately.
Once a resource is registered to CUDA, it can be mapped and unmapped as many times as necessary using cudaGraphicsMapResources() and cudaGraphicsUnmapResources() .
cudaGraphicsResourceSetMapFlags() can be called to specify usage hints (write-only, read-only) that the CUDA driver can use to optimize resource management.
A mapped resource can be read from or written to by kernels using the device memory address returned by cudaGraphicsResourceGetMappedPointer() for buffers and cudaGraphicsSubResourceGetMappedArray() for CUDA arrays.
Accessing a resource through OpenGL, Direct3D, or another CUDA context while it is mapped produces undefined results.
OpenGL Interoperability and Direct3D Interoperability give specifics for each graphics API and some code samples.
SLI Interoperability gives specifics for when the system is in SLI mode. 3.2.15.1. OpenGL Interoperability  The OpenGL resources that may be mapped into the address space of CUDA are OpenGL buffer, texture, and renderbuffer objects.
In CUDA, it appears as a device pointer and can therefore be read and written by kernels or via cudaMemcpy() calls.
They can also write to it via the surface write functions if the resource has been registered with the cudaGraphicsRegisterFlagsSurfaceLoadStore flag.
cudaGraphicsGLRegisterImage() supports all texture formats with 1, 2, or 4 components and an internal type of float (for example, GL_RGBA_FLOAT32 ), normalized integer (for example, GL_RGBA8, GL_INTENSITY16 ), and unnormalized integer (for example, GL_RGBA8UI ) (please note that since unnormalized integer formats require OpenGL 3.0, they can only be written by shaders, not the fixed function pipeline).
The OpenGL context whose resources are being shared has to be current to the host thread making any OpenGL interoperability API calls.
Please note: When an OpenGL texture is made bindless (say for example by requesting an image or texture handle using the glGetTextureHandle */ glGetImageHandle * APIs) it cannot be registered with CUDA.
The application needs to register the texture for interop before requesting an image or texture handle.
The following code sample uses a kernel to dynamically modify a 2D width x height grid of vertices stored in a vertex buffer object: GLuint positionsVBO ; struct cudaGraphicsResource * positionsVBO_CUDA ; int main () {   Initialize OpenGL and GLUT for device 0   and make the OpenGL context current ...
glutDisplayFunc ( display );   Explicitly set device 0 cudaSetDevice ( 0 );   Create buffer object and register it with CUDA glGenBuffers ( 1 , & positionsVBO ); glBindBuffer ( GL_ARRAY_BUFFER , positionsVBO ); unsigned int size = width * height * 4 * sizeof ( float ); glBufferData ( GL_ARRAY_BUFFER , size , 0 , GL_DYNAMIC_DRAW ); glBindBuffer ( GL_ARRAY_BUFFER , 0 ); cudaGraphicsGLRegisterBuffer ( & positionsVBO_CUDA , positionsVBO , cudaGraphicsMapFlagsWriteDiscard );   Launch rendering loop glutMainLoop (); ...
} void display () {   Map buffer object for writing from CUDA float4 * positions ; cudaGraphicsMapResources ( 1 , & positionsVBO_CUDA , 0 ); size_t num_bytes ; cudaGraphicsResourceGetMappedPointer (( void ** ) & positions , & num_bytes , positionsVBO_CUDA ));   Execute kernel dim3 dimBlock ( 16 , 16 , 1 ); dim3 dimGrid ( width / dimBlock .
y , 1 ); createVertices >> ( positions , time , width , height );   Unmap buffer object cudaGraphicsUnmapResources ( 1 , & positionsVBO_CUDA , 0 );   Render from buffer object glClear ( GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT ); glBindBuffer ( GL_ARRAY_BUFFER , positionsVBO ); glVertexPointer ( 4 , GL_FLOAT , 0 , 0 ); glEnableClientState ( GL_VERTEX_ARRAY ); glDrawArrays ( GL_POINTS , 0 , width * height ); glDisableClientState ( GL_VERTEX_ARRAY );   Swap buffers glutSwapBuffers (); glutPostRedisplay (); } void deleteVBO () { cudaGraphicsUnregisterResource ( positionsVBO_CUDA ); glDeleteBuffers ( 1 , & positionsVBO ); } __global__ void createVertices ( float4 * positions , float time , unsigned int width , unsigned int height ) { unsigned int x = blockIdx .
y ;   Calculate uv coordinates float u = x / ( float ) width ; float v = y / ( float ) height ; u = u * 2.0f - 1.0f ; v = v * 2.0f - 1.0f ;   calculate simple sine wave pattern float freq = 4.0f ; float w = sinf ( u * freq + time ) * cosf ( v * freq + time ) * 0.5f ;   Write positions positions [ y * width + x ] = make_float4 ( u , w , v , 1.0f ); } On Windows and for Quadro GPUs, cudaWGLGetDevice() can be used to retrieve the CUDA device associated to the handle returned by wglEnumGpusNV() .
Quadro GPUs offer higher performance OpenGL interoperability than GeForce and Tesla GPUs in a multi-GPU configuration where OpenGL rendering is performed on the Quadro GPU and CUDA computations are performed on other GPUs in the system. 3.2.15.2. Direct3D Interoperability  Direct3D interoperability is supported for Direct3D 9Ex, Direct3D 10, and Direct3D 11.
A CUDA context may interoperate only with Direct3D devices that fulfill the following criteria: Direct3D 9Ex devices must be created with DeviceType set to D3DDEVTYPE_HAL and BehaviorFlags with the D3DCREATE_HARDWARE_VERTEXPROCESSING flag; Direct3D 10 and Direct3D 11 devices must be created with DriverType set to D3D_DRIVER_TYPE_HARDWARE .
The Direct3D resources that may be mapped into the address space of CUDA are Direct3D buffers, textures, and surfaces.
These resources are registered using cudaGraphicsD3D9RegisterResource() , cudaGraphicsD3D10RegisterResource() , and cudaGraphicsD3D11RegisterResource() .
The following code sample uses a kernel to dynamically modify a 2D width x height grid of vertices stored in a vertex buffer object. 3.2.15.2.1. Direct3D 9 Version  IDirect3D9 * D3D ; IDirect3DDevice9 * device ; struct CUSTOMVERTEX { FLOAT x , y , z ; DWORD color ; }; IDirect3DVertexBuffer9 * positionsVB ; struct cudaGraphicsResource * positionsVB_CUDA ; int main () { int dev ;   Initialize Direct3D D3D = Direct3DCreate9Ex ( D3D_SDK_VERSION );   Get a CUDA-enabled adapter unsigned int adapter = 0 ; for (; adapter GetAdapterCount (); adapter ++ ) { D3DADAPTER_IDENTIFIER9 adapterId ; g_pD3D -> GetAdapterIdentifier ( adapter , 0 , & adapterId ); if ( cudaD3D9GetDevice ( & dev , adapterId .
D3D -> CreateDeviceEx ( adapter , D3DDEVTYPE_HAL , hWnd , D3DCREATE_HARDWARE_VERTEXPROCESSING , & params , NULL , & device );   Use the same device cudaSetDevice ( dev );   Create vertex buffer and register it with CUDA unsigned int size = width * height * sizeof ( CUSTOMVERTEX ); device -> CreateVertexBuffer ( size , 0 , D3DFVF_CUSTOMVERTEX , D3DPOOL_DEFAULT , & positionsVB , 0 ); cudaGraphicsD3D9RegisterResource ( & positionsVB_CUDA , positionsVB , cudaGraphicsRegisterFlagsNone ); cudaGraphicsResourceSetMapFlags ( positionsVB_CUDA , cudaGraphicsMapFlagsWriteDiscard );   Launch rendering loop while (...) { ...
Render (); ... } ... } void Render () {   Map vertex buffer for writing from CUDA float4 * positions ; cudaGraphicsMapResources ( 1 , & positionsVB_CUDA , 0 ); size_t num_bytes ; cudaGraphicsResourceGetMappedPointer (( void ** ) & positions , & num_bytes , positionsVB_CUDA ));   Execute kernel dim3 dimBlock ( 16 , 16 , 1 ); dim3 dimGrid ( width / dimBlock .
y , 1 ); createVertices >> ( positions , time , width , height );   Unmap vertex buffer cudaGraphicsUnmapResources ( 1 , & positionsVB_CUDA , 0 );   Draw and present ...
} void releaseVB () { cudaGraphicsUnregisterResource ( positionsVB_CUDA ); positionsVB -> Release (); } __global__ void createVertices ( float4 * positions , float time , unsigned int width , unsigned int height ) { unsigned int x = blockIdx .
y ;   Calculate uv coordinates float u = x / ( float ) width ; float v = y / ( float ) height ; u = u * 2.0f - 1.0f ; v = v * 2.0f - 1.0f ;   Calculate simple sine wave pattern float freq = 4.0f ; float w = sinf ( u * freq + time ) * cosf ( v * freq + time ) * 0.5f ;   Write positions positions [ y * width + x ] = make_float4 ( u , w , v , __int_as_float ( 0xff00ff00 )); } 3.2.15.2.2.
Direct3D 10 Version  ID3D10Device * device ; struct CUSTOMVERTEX { FLOAT x , y , z ; DWORD color ; }; ID3D10Buffer * positionsVB ; struct cudaGraphicsResource * positionsVB_CUDA ; int main () { int dev ;   Get a CUDA-enabled adapter IDXGIFactory * factory ; CreateDXGIFactory ( __uuidof ( IDXGIFactory ), ( void ** ) & factory ); IDXGIAdapter * adapter = 0 ; for ( unsigned int i = 0 ; ! adapter ; ++ i ) { if ( FAILED ( factory -> EnumAdapters ( i , & adapter )) break ; if ( cudaD3D10GetDevice ( & dev , adapter ) == cudaSuccess ) break ; adapter -> Release (); } factory -> Release ();   Create swap chain and device ...
D3D10CreateDeviceAndSwapChain ( adapter , D3D10_DRIVER_TYPE_HARDWARE , 0 , D3D10_CREATE_DEVICE_DEBUG , D3D10_SDK_VERSION , & swapChainDesc , & swapChain , & device ); adapter -> Release ();   Use the same device cudaSetDevice ( dev );   Create vertex buffer and register it with CUDA unsigned int size = width * height * sizeof ( CUSTOMVERTEX ); D3D10_BUFFER_DESC bufferDesc ; bufferDesc .
MiscFlags = 0 ; device -> CreateBuffer ( & bufferDesc , 0 , & positionsVB ); cudaGraphicsD3D10RegisterResource ( & positionsVB_CUDA , positionsVB , cudaGraphicsRegisterFlagsNone ); cudaGraphicsResourceSetMapFlags ( positionsVB_CUDA , cudaGraphicsMapFlagsWriteDiscard );   Launch rendering loop while (...) { ...
y ;   Calculate uv coordinates float u = x / ( float ) width ; float v = y / ( float ) height ; u = u * 2.0f - 1.0f ; v = v * 2.0f - 1.0f ;   Calculate simple sine wave pattern float freq = 4.0f ; float w = sinf ( u * freq + time ) * cosf ( v * freq + time ) * 0.5f ;   Write positions positions [ y * width + x ] = make_float4 ( u , w , v , __int_as_float ( 0xff00ff00 )); } 3.2.15.2.3.
Direct3D 11 Version  ID3D11Device * device ; struct CUSTOMVERTEX { FLOAT x , y , z ; DWORD color ; }; ID3D11Buffer * positionsVB ; struct cudaGraphicsResource * positionsVB_CUDA ; int main () { int dev ;   Get a CUDA-enabled adapter IDXGIFactory * factory ; CreateDXGIFactory ( __uuidof ( IDXGIFactory ), ( void ** ) & factory ); IDXGIAdapter * adapter = 0 ; for ( unsigned int i = 0 ; ! adapter ; ++ i ) { if ( FAILED ( factory -> EnumAdapters ( i , & adapter )) break ; if ( cudaD3D11GetDevice ( & dev , adapter ) == cudaSuccess ) break ; adapter -> Release (); } factory -> Release ();   Create swap chain and device ...
sFnPtr_D3D11CreateDeviceAndSwapChain ( adapter , D3D11_DRIVER_TYPE_HARDWARE , 0 , D3D11_CREATE_DEVICE_DEBUG , featureLevels , 3 , D3D11_SDK_VERSION , & swapChainDesc , & swapChain , & device , & featureLevel , & deviceContext ); adapter -> Release ();   Use the same device cudaSetDevice ( dev );   Create vertex buffer and register it with CUDA unsigned int size = width * height * sizeof ( CUSTOMVERTEX ); D3D11_BUFFER_DESC bufferDesc ; bufferDesc .
MiscFlags = 0 ; device -> CreateBuffer ( & bufferDesc , 0 , & positionsVB ); cudaGraphicsD3D11RegisterResource ( & positionsVB_CUDA , positionsVB , cudaGraphicsRegisterFlagsNone ); cudaGraphicsResourceSetMapFlags ( positionsVB_CUDA , cudaGraphicsMapFlagsWriteDiscard );   Launch rendering loop while (...) { ...
y ;   Calculate uv coordinates float u = x / ( float ) width ; float v = y / ( float ) height ; u = u * 2.0f - 1.0f ; v = v * 2.0f - 1.0f ;   Calculate simple sine wave pattern float freq = 4.0f ; float w = sinf ( u * freq + time ) * cosf ( v * freq + time ) * 0.5f ;   Write positions positions [ y * width + x ] = make_float4 ( u , w , v , __int_as_float ( 0xff00ff00 )); } 3.2.15.3.
SLI Interoperability  In a system with multiple GPUs, all CUDA-enabled GPUs are accessible via the CUDA driver and runtime as separate devices.
There are however special considerations as described below when the system is in SLI mode.
First, an allocation in one CUDA device on one GPU will consume memory on other GPUs that are part of the SLI configuration of the Direct3D or OpenGL device.
Second, applications should create multiple CUDA contexts, one for each GPU in the SLI configuration.
While this is not a strict requirement, it avoids unnecessary data transfers between devices.
The application can use the cudaD3D[9|10|11]GetDevices() for Direct3D and cudaGLGetDevices() for OpenGL set of calls to identify the CUDA device handle(s) for the device(s) that are performing the rendering in the current and next frame.
Given this information the application will typically choose the appropriate device and map Direct3D or OpenGL resources to the CUDA device returned by cudaD3D[9|10|11]GetDevices() or cudaGLGetDevices() when the deviceList parameter is set to cudaD3D[9|10|11]DeviceListCurrentFrame or cudaGLDeviceListCurrentFrame .
Please note that resource returned from cudaGraphicsD9D[9|10|11]RegisterResource and cudaGraphicsGLRegister[Buffer|Image] must be only used on device the registration happened.
Therefore on SLI configurations when data for different frames is computed on different CUDA devices it is necessary to register the resources for each separately.
See Direct3D Interoperability and OpenGL Interoperability for details on how the CUDA runtime interoperate with Direct3D and OpenGL, respectively. 3.2.16. External Resource Interoperability  External resource interoperability allows CUDA to import certain resources that are explicitly exported by other APIs.
These objects are typically exported by other APIs using handles native to the Operating System, like file descriptors on Linux or NT handles on Windows.
They could also be exported using other unified interfaces such as the NVIDIA Software Communication Interface.
There are two types of resources that can be imported: memory objects and synchronization objects.
An imported memory object can be accessed from within kernels using device pointers mapped onto the memory object via cudaExternalMemoryGetMappedBuffer() or CUDA mipmapped arrays mapped via cudaExternalMemoryGetMappedMipmappedArray() .
Depending on the type of memory object, it may be possible for more than one mapping to be setup on a single memory object.
Therefore, any device pointers mapped onto that object must be explicitly freed using cudaFree() and any CUDA mipmapped arrays mapped onto that object must be explicitly freed using cudaFreeMipmappedArray() .
An imported synchronization object can then be signaled using cudaSignalExternalSemaphoresAsync() and waited on using cudaWaitExternalSemaphoresAsync() .
Also, depending on the type of the imported synchronization object, there may be additional constraints imposed on how they can be signaled and waited on, as described in subsequent sections.
All outstanding signals and waits must have completed before the semaphore object is destroyed. 3.2.16.1. Vulkan Interoperability  3.2.16.1.1.
Matching device UUIDs  When importing memory and synchronization objects exported by Vulkan, they must be imported and mapped on the same device as they were created on.
The CUDA device that corresponds to the Vulkan physical device on which the objects were created can be determined by comparing the UUID of a CUDA device with that of the Vulkan physical device, as shown in the following code sample.
Note that the Vulkan physical device should not be part of a device group that contains more than one Vulkan physical device.
The device group as returned by vkEnumeratePhysicalDeviceGroups that contains the given Vulkan physical device must have a physical device count of 1.
int getCudaDeviceForVulkanPhysicalDevice ( VkPhysicalDevice vkPhysicalDevice ) { VkPhysicalDeviceIDProperties vkPhysicalDeviceIDProperties = {}; vkPhysicalDeviceIDProperties .
pNext = NULL ; VkPhysicalDeviceProperties2 vkPhysicalDeviceProperties2 = {}; vkPhysicalDeviceProperties2 .
pNext = & vkPhysicalDeviceIDProperties ; vkGetPhysicalDeviceProperties2 ( vkPhysicalDevice , & vkPhysicalDeviceProperties2 ); int cudaDeviceCount ; cudaGetDeviceCount ( & cudaDeviceCount ); for ( int cudaDevice = 0 ; cudaDevice GetAdapterLuid (); int cudaDeviceCount ; cudaGetDeviceCount ( & cudaDeviceCount ); for ( int cudaDevice = 0 ; cudaDevice QueryInterface ( __uuidof ( IDXGIDevice ), ( void ** ) & dxgiDevice ); IDXGIAdapter * dxgiAdapter ; dxgiDevice -> GetAdapter ( & dxgiAdapter ); DXGI_ADAPTER_DESC dxgiAdapterDesc ; dxgiAdapter -> GetDesc ( & dxgiAdapterDesc ); LUID d3d11Luid = dxgiAdapterDesc .
AdapterLuid ; int cudaDeviceCount ; cudaGetDeviceCount ( & cudaDeviceCount ); for ( int cudaDevice = 0 ; cudaDevice ( bufattrs [ 0 ].
value ));   Note cache and compression are per GPU attributes, so read values for specific gpu by comparing UUID   Read cacheability granted by NvSciBuf int numGpus = bufattrs [ 1 ].
len / sizeof ( NvSciBufAttrValGpuCache ); NvSciBufAttrValGpuCache [] cacheVal = ( NvSciBufAttrValGpuCache * ) bufattrs [ 1 ].
value ; bool ret_cacheVal ; for ( int i = 0 ; i >> ( array , arrayCount ); cudaDeviceSynchronize ();   If interested, the occupancy can be calculated with   cudaOccupancyMaxActiveBlocksPerMultiprocessor return 0 ; } The following code sample shows how to use the cluster occupancy API to find the max number of active clusters of a given size.
Cluster size of 8 is forward compatible starting compute capability 9.0, except on GPU hardware or MIG configurations which are too small to support 8 multiprocessors in which case the maximum cluster size will be reduced.
But it is recommended that the users query the maximum cluster size before launching a cluster kernel.
dynamicSmemBytes = dynamic_shared_memory_size ; cudaLaunchAttribute attribute [ 1 ]; attribute [ 0 ].
numAttrs = 1 ; int max_cluster_size = 0 ; cudaOccupancyMaxPotentialClusterSize ( & max_cluster_size , ( void * ) kernel , & config ); int max_active_clusters = 0 ; cudaOccupancyMaxActiveClusters ( & max_active_clusters , ( void * ) kernel , & config ); std :: cout /include/cuda_occupancy.h for any use cases that cannot depend on the CUDA software stack.
The Nsight Compute version of the occupancy calculator is particularly useful as a learning tool that visualizes the impact of changes to the parameters that affect occupancy (block size, registers per thread, and shared memory per thread). 5.3. Maximize Memory Throughput  The first step in maximizing overall memory throughput for the application is to minimize data transfers with low bandwidth.
That means minimizing data transfers between the host and the device, as detailed in Data Transfer between Host and Device , since these have much lower bandwidth than data transfers between global memory and the device.
That also means minimizing data transfers between global memory and the device by maximizing use of on-chip memory: shared memory and caches (i.e., L1 cache and L2 cache available on devices of compute capability 2.x and higher, texture cache and constant cache available on all devices).
Shared memory is equivalent to a user-managed cache: The application explicitly allocates and accesses it.
As illustrated in CUDA Runtime , a typical programming pattern is to stage data coming from device memory into shared memory; in other words, to have each thread of a block: Load data from device memory to shared memory, Synchronize with all the other threads of the block so that each thread can safely read shared memory locations that were populated by different threads, Process the data in shared memory, Synchronize again if necessary to make sure that shared memory has been updated with the results, Write the results back to device memory.
For some applications (for example, for which global memory access patterns are data-dependent), a traditional hardware-managed cache is more appropriate to exploit data locality.
As mentioned in Compute Capability 7.x , Compute Capability 8.x and Compute Capability 9.0 , for devices of compute capability 7.x, 8.x and 9.0, the same on-chip memory is used for both L1 and shared memory, and how much of it is dedicated to L1 versus shared memory is configurable for each kernel call.
The throughput of memory accesses by a kernel can vary by an order of magnitude depending on access pattern for each type of memory.
The next step in maximizing memory throughput is therefore to organize memory accesses as optimally as possible based on the optimal memory access patterns described in Device Memory Accesses .
This optimization is especially important for global memory accesses as global memory bandwidth is low compared to available on-chip bandwidths and arithmetic instruction throughput, so non-optimal global memory accesses generally have a high impact on performance. 5.3.1. Data Transfer between Host and Device  Applications should strive to minimize data transfer between the host and the device.
One way to accomplish this is to move more code from the host to the device, even if that means running kernels that do not expose enough parallelism to execute on the device with full efficiency.
Intermediate data structures may be created in device memory, operated on by the device, and destroyed without ever being mapped by the host or copied to host memory.
Also, because of the overhead associated with each transfer, batching many small transfers into a single large transfer always performs better than making each transfer separately.
On systems with a front-side bus, higher performance for data transfers between host and device is achieved by using page-locked host memory as described in Page-Locked Host Memory .
In addition, when using mapped page-locked memory ( Mapped Memory ), there is no need to allocate any device memory and explicitly copy data between device and host memory.
For maximum performance, these memory accesses must be coalesced as with accesses to global memory (see Device Memory Accesses ).
Assuming that they are and that the mapped memory is read or written only once, using mapped page-locked memory instead of explicit copies between device and host memory can be a win for performance.
On integrated systems where device memory and host memory are physically the same, any copy between host and device memory is superfluous and mapped page-locked memory should be used instead.
Applications may query a device is integrated by checking that the integrated device property (see Device Enumeration ) is equal to 1. 5.3.2. Device Memory Accesses  An instruction that accesses addressable memory (i.e., global, local, shared, constant, or texture memory) might need to be re-issued multiple times depending on the distribution of the memory addresses across the threads within the warp.
How the distribution affects the instruction throughput this way is specific to each type of memory and described in the following sections.
For example, for global memory, as a general rule, the more scattered the addresses are, the more reduced the throughput is.
Global Memory Global memory resides in device memory and device memory is accessed via 32-, 64-, or 128-byte memory transactions.
These memory transactions must be naturally aligned: Only the 32-, 64-, or 128-byte segments of device memory that are aligned to their size (i.e., whose first address is a multiple of their size) can be read or written by memory transactions.
When a warp executes an instruction that accesses global memory, it coalesces the memory accesses of the threads within the warp into one or more of these memory transactions depending on the size of the word accessed by each thread and the distribution of the memory addresses across the threads.
In general, the more transactions are necessary, the more unused words are transferred in addition to the words accessed by the threads, reducing the instruction throughput accordingly.
For example, if a 32-byte memory transaction is generated for each thread’s 4-byte access, throughput is divided by 8.
How many transactions are necessary and how much throughput is ultimately affected varies with the compute capability of the device.
Compute Capability 5.x , Compute Capability 6.x , Compute Capability 7.x , Compute Capability 8.x and Compute Capability 9.0 give more details on how global memory accesses are handled for various compute capabilities.
To maximize global memory throughput, it is therefore important to maximize coalescing by: Following the most optimal access patterns based on Compute Capability 5.x , Compute Capability 6.x , Compute Capability 7.x , Compute Capability 8.x and Compute Capability 9.0 Using data types that meet the size and alignment requirement detailed in the section Size and Alignment Requirement below, Padding data in some cases, for example, when accessing a two-dimensional array as described in the section Two-Dimensional Arrays below.
Size and Alignment Requirement Global memory instructions support reading or writing words of size equal to 1, 2, 4, 8, or 16 bytes.
Any access (via a variable or a pointer) to data residing in global memory compiles to a single global memory instruction if and only if the size of the data type is 1, 2, 4, 8, or 16 bytes and the data is naturally aligned (i.e., its address is a multiple of that size).
If this size and alignment requirement is not fulfilled, the access compiles to multiple instructions with interleaved access patterns that prevent these instructions from fully coalescing.
It is therefore recommended to use types that meet this requirement for data that resides in global memory.
For structures, the size and alignment requirements can be enforced by the compiler using the alignment specifiers __align__(8) or __align__(16) , such as struct __align__ ( 8 ) { float x ; float y ; }; or struct __align__ ( 16 ) { float x ; float y ; float z ; }; Any address of a variable residing in global memory or returned by one of the memory allocation routines from the driver or runtime API is always aligned to at least 256 bytes.
Reading non-naturally aligned 8-byte or 16-byte words produces incorrect results (off by a few words), so special care must be taken to maintain alignment of the starting address of any value or array of values of these types.
A typical case where this might be easily overlooked is when using some custom global memory allocation scheme, whereby the allocations of multiple arrays (with multiple calls to cudaMalloc() or cuMemAlloc() ) is replaced by the allocation of a single large block of memory partitioned into multiple arrays, in which case the starting address of each array is offset from the block’s starting address.
Two-Dimensional Arrays A common global memory access pattern is when each thread of index (tx,ty) uses the following address to access one element of a 2D array of width width , located at address BaseAddress of type type* (where type meets the requirement described in Maximize Utilization ): BaseAddress + width * ty + tx For these accesses to be fully coalesced, both the width of the thread block and the width of the array must be a multiple of the warp size.
In particular, this means that an array whose width is not a multiple of this size will be accessed much more efficiently if it is actually allocated with a width rounded up to the closest multiple of this size and its rows padded accordingly.
The cudaMallocPitch() and cuMemAllocPitch() functions and associated memory copy functions described in the reference manual enable programmers to write non-hardware-dependent code to allocate arrays that conform to these constraints.
Local Memory Local memory accesses only occur for some automatic variables as mentioned in Variable Memory Space Specifiers .
Automatic variables that the compiler is likely to place in local memory are: Arrays for which it cannot determine that they are indexed with constant quantities, Large structures or arrays that would consume too much register space, Any variable if the kernel uses more registers than available (this is also known as register spilling ).
Inspection of the PTX assembly code (obtained by compiling with the -ptx or -keep option) will tell if a variable has been placed in local memory during the first compilation phases as it will be declared using the .local mnemonic and accessed using the ld.local and st.local mnemonics.
Even if it has not, subsequent compilation phases might still decide otherwise though if they find it consumes too much register space for the targeted architecture: Inspection of the cubin object using cuobjdump will tell if this is the case.
Also, the compiler reports total local memory usage per kernel ( lmem ) when compiling with the --ptxas-options=-v option.
Note that some mathematical functions have implementation paths that might access local memory.
The local memory space resides in device memory, so local memory accesses have the same high latency and low bandwidth as global memory accesses and are subject to the same requirements for memory coalescing as described in Device Memory Accesses .
Local memory is however organized such that consecutive 32-bit words are accessed by consecutive thread IDs.
Accesses are therefore fully coalesced as long as all threads in a warp access the same relative address (for example, same index in an array variable, same member in a structure variable).
On devices of compute capability 5.x onwards, local memory accesses are always cached in L2 in the same way as global memory accesses (see Compute Capability 5.x and Compute Capability 6.x ).
Shared Memory Because it is on-chip, shared memory has much higher bandwidth and much lower latency than local or global memory.
To achieve high bandwidth, shared memory is divided into equally-sized memory modules, called banks, which can be accessed simultaneously.
Any memory read or write request made of n addresses that fall in n distinct memory banks can therefore be serviced simultaneously, yielding an overall bandwidth that is n times as high as the bandwidth of a single module.
However, if two addresses of a memory request fall in the same memory bank, there is a bank conflict and the access has to be serialized.
The hardware splits a memory request with bank conflicts into as many separate conflict-free requests as necessary, decreasing throughput by a factor equal to the number of separate memory requests.
If the number of separate memory requests is n , the initial memory request is said to cause n -way bank conflicts.
To get maximum performance, it is therefore important to understand how memory addresses map to memory banks in order to schedule the memory requests so as to minimize bank conflicts.
This is described in Compute Capability 5.x , Compute Capability 6.x , Compute Capability 7.x , Compute Capability 8.x , and Compute Capability 9.0 for devices of compute capability 5.x, 6.x, 7.x, 8.x, and 9.0 respectively.
Constant Memory The constant memory space resides in device memory and is cached in the constant cache.
A request is then split into as many separate requests as there are different memory addresses in the initial request, decreasing throughput by a factor equal to the number of separate requests.
The resulting requests are then serviced at the throughput of the constant cache in case of a cache hit, or at the throughput of device memory otherwise.
Texture and Surface Memory The texture and surface memory spaces reside in device memory and are cached in texture cache, so a texture fetch or surface read costs one memory read from device memory only on a cache miss, otherwise it just costs one read from texture cache.
The texture cache is optimized for 2D spatial locality, so threads of the same warp that read texture or surface addresses that are close together in 2D will achieve best performance.
Also, it is designed for streaming fetches with a constant latency; a cache hit reduces DRAM bandwidth demand but not fetch latency.
In this section, throughputs are given in number of operations per clock cycle per multiprocessor.
For a warp size of 32, one instruction corresponds to 32 operations, so if N is the number of operations per clock cycle, the instruction throughput is N/32 instructions per clock cycle.
They must be multiplied by the number of multiprocessors in the device to get throughput for the whole device. 5.4.1. Arithmetic Instructions  The following table gives the throughputs of the arithmetic instructions that are natively supported in hardware for devices of various compute capabilities.
(Number of Results per Clock Cycle per Multiprocessor)  Compute Capability 5.0, 5.2 5.3 6.0 6.1 6.2 7.x 8.0 8.6 8.9 9.0 16-bit floating-point add, multiply, multiply-add N/A 256 128 2 256 128 256 3 128 256 32-bit floating-point add, multiply, multiply-add 128 64 128 64 128 64-bit floating-point add, multiply, multiply-add 4 32 4 32 5 32 2 64 32-bit floating-point reciprocal, reciprocal square root, base-2 logarithm ( __log2f ), base 2 exponential ( exp2f ), sine ( __sinf ), cosine ( __cosf ) 32 16 32 16 32-bit integer add, extended-precision add, subtract, extended-precision subtract 128 64 128 64 32-bit integer multiply, multiply-add, extended-precision multiply-add Multiple instruct.
32-bit integer shift 64 32 64 compare, minimum, maximum 64 32 64 32-bit integer bit reverse 64 32 64 16 Bit field extract/insert 64 32 64 Multiple Instruct.
64 32-bit bitwise AND, OR, XOR 128 64 128 64 count of leading zeros, most significant non-sign bit 32 16 32 16 population count 32 16 32 16 warp shuffle 32 32 8 32 warp reduce Multiple instruct.
16 warp vote 64 sum of absolute difference 64 32 64 SIMD video instructions vabsdiff2 Multiple instruct.
Type conversions from 8-bit and 16-bit integer to 32-bit integer types 32 16 32 64 Type conversions from and to 64-bit types 4 16 4 16 10 16 2 2 16 All other type conversions 32 16 32 16 16-bit DPX Multiple instruct.
The implementation may be different for devices of different compute capabilities, and the number of native instructions after compilation may fluctuate with every compiler version.
The implementation of some functions are readily available on the CUDA header files ( math_functions.h , device_functions.h , …).
In general, code compiled with -ftz=true (denormalized numbers are flushed to zero) tends to have higher performance than code compiled with -ftz=false .
Similarly, code compiled with -prec-div=false (less precise division) tends to have higher performance code than code compiled with -prec-div=true , and code compiled with -prec-sqrt=false (less precise square root) tends to have higher performance than code compiled with -prec-sqrt=true .
Single-Precision Floating-Point Division __fdividef(x, y) (see Intrinsic Functions ) provides faster single-precision floating-point division than the division operator.
Single-Precision Floating-Point Reciprocal Square Root To preserve IEEE-754 semantics the compiler can optimize 1.0/sqrtf() into rsqrtf() only when both reciprocal and square root are approximate, (i.e., with -prec-div=false and -prec-sqrt=false ).
Single-Precision Floating-Point Square Root Single-precision floating-point square root is implemented as a reciprocal square root followed by a reciprocal instead of a reciprocal square root followed by a multiplication so that it gives correct results for 0 and infinity.
Sine and Cosine sinf(x) , cosf(x) , tanf(x) , sincosf(x) , and corresponding double-precision instructions are much more expensive and even more so if the argument x is large in magnitude.
More precisely, the argument reduction code (see Mathematical Functions for implementation) comprises two code paths referred to as the fast path and the slow path, respectively.
The fast path is used for arguments sufficiently small in magnitude and essentially consists of a few multiply-add operations.
The slow path is used for arguments large in magnitude and consists of lengthy computations required to achieve correct results over the entire argument range.
At present, the argument reduction code for the trigonometric functions selects the fast path for arguments whose magnitude is less than 105615.0f for the single-precision functions, and less than 2147483648.0 for the double-precision functions.
As the slow path requires more registers than the fast path, an attempt has been made to reduce register pressure in the slow path by storing some intermediate variables in local memory, which may affect performance because of local memory high latency and bandwidth (see Device Memory Accesses ).
At present, 28 bytes of local memory are used by single-precision functions, and 44 bytes are used by double-precision functions.
Due to the lengthy computations and use of local memory in the slow path, the throughput of these trigonometric functions is lower by one order of magnitude when the slow path reduction is required as opposed to the fast path reduction.
Integer Arithmetic Integer division and modulo operation are costly as they compile to up to 20 instructions.
They can be replaced with bitwise operations in some cases: If n is a power of 2, ( i/n ) is equivalent to (i>>log2(n)) and (i%n) is equivalent to ( i&(n-1) ); the compiler will perform these conversions if n is literal.
__brev and __popc map to a single instruction and __brevll and __popcll to a few instructions.
Half Precision Arithmetic In order to achieve good performance for 16-bit precision floating-point add, multiply or multiply-add, it is recommended that the half2 datatype is used for half precision and __nv_bfloat162 be used for __nv_bfloat16 precision.
Vector intrinsics (for example, __hadd2 , __hsub2 , __hmul2 , __hfma2 ) can then be used to do two operations in a single instruction.
Using half2 or __nv_bfloat162 in place of two calls using half or __nv_bfloat16 may also help performance of other intrinsics, such as warp shuffles.
The intrinsic __halves2half2 is provided to convert two half precision values to the half2 datatype.
The intrinsic __halves2bfloat162 is provided to convert two __nv_bfloat precision values to the __nv_bfloat162 datatype.
Type Conversion Sometimes, the compiler must insert conversion instructions, introducing additional execution cycles.
This is the case for: Functions operating on variables of type char or short whose operands generally need to be converted to int , Double-precision floating-point constants (i.e., those constants defined without any type suffix) used as input to single-precision floating-point computations (as mandated by C/C++ standards).
This last case can be avoided by using single-precision floating-point constants, defined with an f suffix such as 3.141592653589793f , 1.0f , 0.5f . 5.4.2. Control Flow Instructions  Any flow control instruction ( if , switch , do , for , while ) can significantly impact the effective instruction throughput by causing threads of the same warp to diverge (i.e., to follow different execution paths).
If this happens, the different executions paths have to be serialized, increasing the total number of instructions executed for this warp.
To obtain best performance in cases where the control flow depends on the thread ID, the controlling condition should be written so as to minimize the number of divergent warps.
This is possible because the distribution of the warps across the block is deterministic as mentioned in SIMT Architecture .
A trivial example is when the controlling condition only depends on ( threadIdx / warpSize ) where warpSize is the warp size.
In this case, no warp diverges since the controlling condition is perfectly aligned with the warps.
Sometimes, the compiler may unroll loops or it may optimize out short if or switch blocks by using branch predication instead, as detailed below.
The programmer can also control loop unrolling using the #pragma unroll directive (see #pragma unroll ).
When using branch predication none of the instructions whose execution depends on the controlling condition gets skipped.
Instead, each of them is associated with a per-thread condition code or predicate that is set to true or false based on the controlling condition and although each of these instructions gets scheduled for execution, only the instructions with a true predicate are actually executed.
Instructions with a false predicate do not write results, and also do not evaluate addresses or read operands. 5.4.3. Synchronization Instruction  Throughput for __syncthreads() is 32 operations per clock cycle for devices of compute capability 6.0, 16 operations per clock cycle for devices of compute capability 7.x as well as 8.x and 64 operations per clock cycle for devices of compute capability 5.x, 6.1 and 6.2.
Note that __syncthreads() can impact performance by forcing the multiprocessor to idle as detailed in Device Memory Accesses . 5.5. Minimize Memory Thrashing  Applications that constantly allocate and free memory too often may find that the allocation calls tend to get slower over time up to a limit.
This is typically expected due to the nature of releasing memory back to the operating system for its own use.
For best performance in this regard, we recommend the following: Try to size your allocation to the problem at hand.
Don’t try to allocate all available memory with cudaMalloc / cudaMallocHost / cuMemCreate , as this forces memory to be resident immediately and prevents other applications from being able to use that memory.
This can put more pressure on operating system schedulers, or just prevent other applications using the same GPU from running entirely.
Try to allocate memory in appropriately sized allocations early in the application and allocations only when the application does not have any use for it.
Reduce the number of cudaMalloc + cudaFree calls in the application, especially in performance-critical regions.
If an application cannot allocate enough device memory, consider falling back on other memory types such as cudaMallocHost or cudaMallocManaged , which may not be as performant, but will enable the application to make progress.
For platforms that support the feature, cudaMallocManaged allows for oversubscription, and with the correct cudaMemAdvise policies enabled, will allow the application to retain most if not all the performance of cudaMalloc .
cudaMallocManaged also won’t force an allocation to be resident until it is needed or prefetched, reducing the overall pressure on the operating system schedulers and better enabling multi-tenet use cases.
3 128 for __nv_bfloat16 4 8 for GeForce GPUs, except for Titan GPUs 5 2 for compute capability 7.5 GPUs 6 32 for extended-precision 7 32 for GeForce GPUs, except for Titan GPUs 8 16 for compute capabilities 7.5 GPUs 9 8 for GeForce GPUs, except for Titan GPUs 10 2 for compute capabilities 7.5 GPUs 6.
CUDA-Enabled GPUs  https: developer.nvidia.com/cuda-gpus lists all CUDA-enabled devices with their compute capability.
The compute capability, number of multiprocessors, clock frequency, total amount of device memory, and other properties can be queried using the runtime (see reference manual). 7. C++ Language Extensions  7.1.
Function Execution Space Specifiers  Function execution space specifiers denote whether a function executes on the host or on the device and whether it is callable from the host or from the device. 7.1.1. __global__  The __global__ execution space specifier declares a function as being a kernel.
Such a function is: Executed on the device, Callable from the host, Callable from the device for devices of compute capability 5.0 or higher (see CUDA Dynamic Parallelism for more details).
Any call to a __global__ function must specify its execution configuration as described in Execution Configuration .
A call to a __global__ function is asynchronous, meaning it returns before the device has completed its execution. 7.1.2. __device__  The __device__ execution space specifier declares a function that is: Executed on the device, Callable from the device only.
The __global__ and __device__ execution space specifiers cannot be used together. 7.1.3. __host__  The __host__ execution space specifier declares a function that is: Executed on the host, Callable from the host only.
It is equivalent to declare a function with only the __host__ execution space specifier or to declare it without any of the __host__ , __device__ , or __global__ execution space specifier; in either case the function is compiled for the host only.
The __device__ and __host__ execution space specifiers can be used together however, in which case the function is compiled for both the host and the device.
The __CUDA_ARCH__ macro introduced in Application Compatibility can be used to differentiate code paths between host and device: __host__ __device__ func () { #if __CUDA_ARCH__ >= 800   Device code path for compute capability 8.x #elif __CUDA_ARCH__ >= 700   Device code path for compute capability 7.x #elif __CUDA_ARCH__ >= 600   Device code path for compute capability 6.x #elif __CUDA_ARCH__ >= 500   Device code path for compute capability 5.x #elif !defined(__CUDA_ARCH__)   Host code path #endif } 7.1.4.
Undefined behavior  A ‘cross-execution space’ call has undefined behavior when: __CUDA_ARCH__ is defined, a call from within a __global__ , __device__ or __host__ __device__ function to a __host__ function.
__CUDA_ARCH__ is undefined, a call from within a __host__ function to a __device__ function. 9 7.1.5. __noinline__ and __forceinline__  The compiler inlines any __device__ function when deemed appropriate.
The __noinline__ function qualifier can be used as a hint for the compiler not to inline the function if possible.
The __forceinline__ function qualifier can be used to force the compiler to inline the function.
The __noinline__ and __forceinline__ function qualifiers cannot be used together, and neither function qualifier can be applied to an inline function. 7.1.6. __inline_hint__  The __inline_hint__ qualifier enables more aggressive inlining in the compiler.
Neither the __noinline__ nor the __forceinline__ function qualifier can be used with the __inline_hint__ function qualifier. 7.2. Variable Memory Space Specifiers  Variable memory space specifiers denote the memory location on the device of a variable.
An automatic variable declared in device code without any of the __device__ , __shared__ and __constant__ memory space specifiers described in this section generally resides in a register.
However in some cases the compiler might choose to place it in local memory, which can have adverse performance consequences as detailed in Device Memory Accesses . 7.2.1. __device__  The __device__ memory space specifier declares a variable that resides on the device.
At most one of the other memory space specifiers defined in the next three sections may be used together with __device__ to further denote which memory space the variable belongs to.
If none of them is present, the variable: Resides in global memory space, Has the lifetime of the CUDA context in which it is created, Has a distinct object per device, Is accessible from all the threads within the grid and from the host through the runtime library (cudaGetSymbolAddress() / cudaGetSymbolSize() / cudaMemcpyToSymbol() / cudaMemcpyFromSymbol() ). 7.2.2. __constant__  The __constant__ memory space specifier, optionally used together with __device__ , declares a variable that: Resides in constant memory space, Has the lifetime of the CUDA context in which it is created, Has a distinct object per device, Is accessible from all the threads within the grid and from the host through the runtime library ( cudaGetSymbolAddress() / cudaGetSymbolSize() / cudaMemcpyToSymbol() / cudaMemcpyFromSymbol() ).
The behavior of modifying a constant from the host while there is a concurrent grid that access that constant at any point of this grid’s lifetime is undefined. 7.2.3. __shared__  The __shared__ memory space specifier, optionally used together with __device__ , declares a variable that: Resides in the shared memory space of a thread block, Has the lifetime of the block, Has a distinct object per block, Is only accessible from all the threads within the block, Does not have a constant address.
When declaring a variable in shared memory as an external array such as extern __shared__ float shared []; the size of the array is determined at launch time (see Execution Configuration ).
All variables declared in this fashion, start at the same address in memory, so that the layout of the variables in the array must be explicitly managed through offsets.
For example, if one wants the equivalent of short array0 [ 128 ]; float array1 [ 64 ]; int array2 [ 256 ]; in dynamically allocated shared memory, one could declare and initialize the arrays the following way: extern __shared__ float array []; __device__ void func ()   __device__ or __global__ function { short * array0 = ( short * ) array ; float * array1 = ( float * ) & array0 [ 128 ]; int * array2 = ( int * ) & array1 [ 64 ]; } Note that pointers need to be aligned to the type they point to, so the following code, for example, does not work since array1 is not aligned to 4 bytes.
extern __shared__ float array []; __device__ void func ()   __device__ or __global__ function { short * array0 = ( short * ) array ; float * array1 = ( float * ) & array0 [ 127 ]; } Alignment requirements for the built-in vector types are listed in Table 5 . 7.2.4. __grid_constant__  The __grid_constant__ annotation for compute architectures greater or equal to 7.0 annotates a const -qualified __global__ function parameter of non-reference type that: Has the lifetime of the grid, Is private to the grid, i.e., the object is not accessible to host threads and threads from other grids, including sub-grids, Has a distinct object per grid, i.e., all threads in the grid see the same address, Is read-only, i.e., modifying a __grid_constant__ object or any of its sub-objects is undefined behavior , including mutable members.
Requirements: Kernel parameters annotated with __grid_constant__ must have const -qualified non-reference types.
A function template specialization must match the primary template declaration with respect to any __grid_constant__ parameters.
A function template instantiation directive must match the primary template declaration with respect to any __grid_constant__ parameters.
If the address of a __global__ function parameter is taken, the compiler will ordinarily make a copy of the kernel parameter in thread local memory and use the address of the copy, to partially support C++ semantics, which allow each thread to modify its own local copy of function parameters.
Annotating a __global__ function parameter with __grid_constant__ ensures that the compiler will not create a copy of the kernel parameter in thread local memory, but will instead use the generic address of the parameter itself.
__device__ void unknown_function ( S const & ); __global__ void kernel ( const __grid_constant__ S s ) { s .
x ;   Undefined Behavior: tried to modify read-only memory   Compiler will _not_ create a per-thread thread local copy of "s": unknown_function ( s ); } 7.2.5.
__managed__  The __managed__ memory space specifier, optionally used together with __device__ , declares a variable that: Can be referenced from both device and host code, for example, its address can be taken or it can be read or written directly from a device or host function.
See __managed__ Memory Space Specifier for more details. 7.2.6. __restrict__  nvcc supports restricted pointers via the __restrict__ keyword.
Restricted pointers were introduced in C99 to alleviate the aliasing problem that exists in C-type languages, and which inhibits all kind of optimization from code re-ordering to common sub-expression elimination.
Here is an example subject to the aliasing issue, where use of restricted pointer can help the compiler to reduce the number of instructions: void foo ( const float * a , const float * b , float * c ) { c [ 0 ] = a [ 0 ] * b [ 0 ]; c [ 1 ] = a [ 0 ] * b [ 0 ]; c [ 2 ] = a [ 0 ] * b [ 0 ] * a [ 1 ]; c [ 3 ] = a [ 0 ] * a [ 1 ]; c [ 4 ] = a [ 0 ] * b [ 0 ]; c [ 5 ] = b [ 0 ]; ...
} In C-type languages, the pointers a , b , and c may be aliased, so any write through c could modify elements of a or b .
This means that to guarantee functional correctness, the compiler cannot load a[0] and b[0] into registers, multiply them, and store the result to both c[0] and c[1] , because the results would differ from the abstract execution model if, say, a[0] is really the same location as c[0] .
Likewise, the compiler cannot just reorder the computation of c[4] into the proximity of the computation of c[0] and c[1] because the preceding write to c[3] could change the inputs to the computation of c[4] .
By making a , b , and c restricted pointers, the programmer asserts to the compiler that the pointers are in fact not aliased, which in this case means writes through c would never overwrite elements of a or b .
This changes the function prototype as follows: void foo ( const float * __restrict__ a , const float * __restrict__ b , float * __restrict__ c ); Note that all pointer arguments need to be made restricted for the compiler optimizer to derive any benefit.
With the __restrict__ keywords added, the compiler can now reorder and do common sub-expression elimination at will, while retaining functionality identical with the abstract execution model: void foo ( const float * __restrict__ a , const float * __restrict__ b , float * __restrict__ c ) { float t0 = a [ 0 ]; float t1 = b [ 0 ]; float t2 = t0 * t1 ; float t3 = a [ 1 ]; c [ 0 ] = t2 ; c [ 1 ] = t2 ; c [ 4 ] = t2 ; c [ 2 ] = t2 * t3 ; c [ 3 ] = t0 * t3 ; c [ 5 ] = t1 ; ...
} The effects here are a reduced number of memory accesses and reduced number of computations.
This is balanced by an increase in register pressure due to “cached” loads and common sub-expressions.
Since register pressure is a critical issue in many CUDA codes, use of restricted pointers can have negative performance impact on CUDA code, due to reduced occupancy. 7.3. Built-in Vector Types  7.3.1.
char, short, int, long, longlong, float, double  These are vector types derived from the basic integer and floating-point types.
They are structures and the 1st, 2nd, 3rd, and 4th components are accessible through the fields x , y , z , and w , respectively.
They all come with a constructor function of the form make_ ; for example, int2 make_int2 ( int x , int y ); which creates a vector of type int2 with value (x, y) .
Table 5 Alignment Requirements  Type Alignment char1, uchar1 1 char2, uchar2 2 char3, uchar3 1 char4, uchar4 4 short1, ushort1 2 short2, ushort2 4 short3, ushort3 2 short4, ushort4 8 int1, uint1 4 int2, uint2 8 int3, uint3 4 int4, uint4 16 long1, ulong1 4 if sizeof(long) is equal to sizeof(int) 8, otherwise long2, ulong2 8 if sizeof(long) is equal to sizeof(int), 16, otherwise long3, ulong3 4 if sizeof(long) is equal to sizeof(int), 8, otherwise long4, ulong4 16 longlong1, ulonglong1 8 longlong2, ulonglong2 16 longlong3, ulonglong3 8 longlong4, ulonglong4 16 float1 4 float2 8 float3 4 float4 16 double1 8 double2 16 double3 8 double4 16 7.3.2.
dim3  This type is an integer vector type based on uint3 that is used to specify dimensions.
When defining a variable of type dim3 , any component left unspecified is initialized to 1. 7.4. Built-in Variables  Built-in variables specify the grid and block dimensions and the block and thread indices.
They are only valid within functions that are executed on the device. 7.4.1. gridDim  This variable is of type dim3 (see dim3 ) and contains the dimensions of the grid.
7.4.2. blockIdx  This variable is of type uint3 (see char, short, int, long, longlong, float, double ) and contains the block index within the grid.
7.4.3. blockDim  This variable is of type dim3 (see dim3 ) and contains the dimensions of the block.
7.4.4. threadIdx  This variable is of type uint3 (see char, short, int, long, longlong, float, double ) and contains the thread index within the block.
7.4.5. warpSize  This variable is of type int and contains the warp size in threads (see SIMT Architecture for the definition of a warp).
7.5. Memory Fence Functions  The CUDA programming model assumes a device with a weakly-ordered memory model, that is the order in which a CUDA thread writes data to shared memory, global memory, page-locked host memory, or the memory of a peer device is not necessarily the order in which the data is observed being written by another CUDA or host thread.
It is undefined behavior for two threads to read from or write to the same memory location without synchronization.
In the following example, thread 1 executes writeXY() , while thread 2 executes readXY() .
__device__ int X = 1 , Y = 2 ; __device__ void writeXY () { X = 10 ; Y = 20 ; } __device__ void readXY () { int B = Y ; int A = X ; } The two threads read and write from the same memory locations X and Y simultaneously.
Memory fence functions can be used to enforce a sequentially-consistent ordering on memory accesses.
The memory fence functions differ in the scope in which the orderings are enforced but they are independent of the accessed memory space (shared memory, global memory, page-locked host memory, and the memory of a peer device).
void __threadfence_block (); is equivalent to cuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope_block) and ensures that: All writes to all memory made by the calling thread before the call to __threadfence_block() are observed by all threads in the block of the calling thread as occurring before all writes to all memory made by the calling thread after the call to __threadfence_block() ; All reads from all memory made by the calling thread before the call to __threadfence_block() are ordered before all reads from all memory made by the calling thread after the call to __threadfence_block() .
void __threadfence (); is equivalent to cuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope_device) and ensures that no writes to all memory made by the calling thread after the call to __threadfence() are observed by any thread in the device as occurring before any write to all memory made by the calling thread before the call to __threadfence() .
void __threadfence_system (); is equivalent to cuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope_system) and ensures that all writes to all memory made by the calling thread before the call to __threadfence_system() are observed by all threads in the device, host threads, and all threads in peer devices as occurring before all writes to all memory made by the calling thread after the call to __threadfence_system() .
In the previous code sample, we can insert fences in the codes as follows: __device__ int X = 1 , Y = 2 ; __device__ void writeXY () { X = 10 ; __threadfence (); Y = 20 ; } __device__ void readXY () { int B = Y ; __threadfence (); int A = X ; } For this code, the following outcomes can be observed: A equal to 1 and B equal to 2, A equal to 10 and B equal to 2, A equal to 10 and B equal to 20.
The fourth outcome is not possible, because the first write must be visible before the second write.
If thread 1 and 2 do not belong to the same block, __threadfence() must be used if they are CUDA threads from the same device and __threadfence_system() must be used if they are CUDA threads from two different devices.
A common use case is when threads consume some data produced by other threads as illustrated by the following code sample of a kernel that computes the sum of an array of N numbers in one call.
When all blocks are done, the last block done reads each of these partial sums from global memory and sums them to obtain the final result.
In order to determine which block is finished last, each block atomically increments a counter to signal that it is done with computing and storing its partial sum (see Atomic Functions about atomic functions).
If no fence is placed between storing the partial sum and incrementing the counter, the counter might increment before the partial sum is stored and therefore, might reach gridDim.x-1 and let the last block start reading partial sums before they have been actually updated in memory.
Memory fence functions only affect the ordering of memory operations by a thread; they do not, by themselves, ensure that these memory operations are visible to other threads (like __syncthreads() does for threads within a block (see Synchronization Functions )).
In the code sample below, the visibility of memory operations on the result variable is ensured by declaring it as volatile (see Volatile Qualifier ).
__device__ unsigned int count = 0 ; __shared__ bool isLastBlockDone ; __global__ void sum ( const float * array , unsigned int N , volatile float * result ) {   Each block sums a subset of the input array.
The compiler will use   a store operation that bypasses the L1 cache   since the "result" variable is declared as   volatile.
This ensures that the threads of   the last block will read the correct partial   sums computed by all other blocks.
x ] = partialSum ;   Thread 0 makes sure that the incrementing   of the "count" variable is only performed after   the partial sum has been written to global memory.
x - 1 )); }   Synchronize to make sure that each thread reads   the correct value of isLastBlockDone.
__syncthreads (); if ( isLastBlockDone ) {   The last block sums the partial sums   stored in result[0 ..
x == 0 ) {   Thread 0 of last block stores the total sum   to global memory and resets the count   variable, so that the next kernel call   works properly.
Synchronization Functions  void __syncthreads (); waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to __syncthreads() are visible to all threads in the block.
__syncthreads() is used to coordinate communication between the threads of the same block.
When some threads within a block access the same addresses in shared or global memory, there are potential read-after-write, write-after-read, or write-after-write hazards for some of these memory accesses.
__syncthreads() is allowed in conditional code but only if the conditional evaluates identically across the entire thread block, otherwise the code execution is likely to hang or produce unintended side effects.
Devices of compute capability 2.x and higher support three variations of __syncthreads() described below.
int __syncthreads_count ( int predicate ); is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns the number of threads for which predicate evaluates to non-zero.
int __syncthreads_and ( int predicate ); is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for all of them.
int __syncthreads_or ( int predicate ); is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for any of them.
void __syncwarp ( unsigned mask = 0xffffffff ); will cause the executing thread to wait until all warp lanes named in mask have executed a __syncwarp() (with the same mask) before resuming execution.
Each calling thread must have its own bit set in the mask and all non-exited threads named in mask must execute a corresponding __syncwarp() with the same mask, or the result is undefined.
Executing __syncwarp() guarantees memory ordering among threads participating in the barrier.
Thus, threads within a warp that wish to communicate via memory can store to memory, execute __syncwarp() , and then safely read values stored by other threads in the warp.
Note For .target sm_6x or below, all threads in mask must execute the same __syncwarp() in convergence, and the union of all values in mask must be equal to the active mask.
Otherwise, the behavior is undefined. 7.7. Mathematical Functions  The reference manual lists all C/C++ standard library mathematical functions that are supported in device code and all intrinsic functions that are only supported in device code.
Mathematical Functions provides accuracy information for some of these functions when relevant. 7.8. Texture Functions  Texture objects are described in Texture Object API Texture fetching is described in Texture Fetching .
tex1Dfetch()  template T tex1Dfetch ( cudaTextureObject_t texObj , int x ); fetches from the region of linear memory specified by the one-dimensional texture object texObj using integer texture coordinate x .
tex1Dfetch() only works with non-normalized coordinates, so only the border and clamp addressing modes are supported.
For integer types, it may optionally promote the integer to single-precision floating point. 7.8.1.2. tex1D()  template T tex1D ( cudaTextureObject_t texObj , float x ); fetches from the CUDA array specified by the one-dimensional texture object texObj using texture coordinate x .
7.8.1.3. tex1DLod()  template T tex1DLod ( cudaTextureObject_t texObj , float x , float level ); fetches from the CUDA array specified by the one-dimensional texture object texObj using texture coordinate x at the level-of-detail level .
7.8.1.4. tex1DGrad()  template T tex1DGrad ( cudaTextureObject_t texObj , float x , float dx , float dy ); fetches from the CUDA array specified by the one-dimensional texture object texObj using texture coordinate x .
The level-of-detail is derived from the X-gradient dx and Y-gradient dy . 7.8.1.5. tex2D()  template T tex2D ( cudaTextureObject_t texObj , float x , float y ); fetches from the CUDA array or the region of linear memory specified by the two-dimensional texture object texObj using texture coordinate (x,y) .
7.8.1.6. tex2D() for sparse CUDA arrays  template T tex2D ( cudaTextureObject_t texObj , float x , float y , bool * isResident ); fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) .
If not, the values fetched will be zeros. 7.8.1.7. tex2Dgather()  template T tex2Dgather ( cudaTextureObject_t texObj , float x , float y , int comp = 0 ); fetches from the CUDA array specified by the 2D texture object texObj using texture coordinates x and y and the comp parameter as described in Texture Gather .
7.8.1.8. tex2Dgather() for sparse CUDA arrays  template T tex2Dgather ( cudaTextureObject_t texObj , float x , float y , bool * isResident , int comp = 0 ); fetches from the CUDA array specified by the 2D texture object texObj using texture coordinates x and y and the comp parameter as described in Texture Gather .
7.8.1.9. tex2DGrad()  template T tex2DGrad ( cudaTextureObject_t texObj , float x , float y , float2 dx , float2 dy ); fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) .
The level-of-detail is derived from the dx and dy gradients. 7.8.1.10. tex2DGrad() for sparse CUDA arrays  template T tex2DGrad ( cudaTextureObject_t texObj , float x , float y , float2 dx , float2 dy , bool * isResident ); fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) .
7.8.1.11. tex2DLod()  template tex2DLod ( cudaTextureObject_t texObj , float x , float y , float level ); fetches from the CUDA array or the region of linear memory specified by the two-dimensional texture object texObj using texture coordinate (x,y) at level-of-detail level .
7.8.1.12. tex2DLod() for sparse CUDA arrays  template tex2DLod ( cudaTextureObject_t texObj , float x , float y , float level , bool * isResident ); fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) at level-of-detail level .
7.8.1.13. tex3D()  template T tex3D ( cudaTextureObject_t texObj , float x , float y , float z ); fetches from the CUDA array specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) .
7.8.1.14. tex3D() for sparse CUDA arrays  template T tex3D ( cudaTextureObject_t texObj , float x , float y , float z , bool * isResident ); fetches from the CUDA array specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) .
7.8.1.15. tex3DLod()  template T tex3DLod ( cudaTextureObject_t texObj , float x , float y , float z , float level ); fetches from the CUDA array or the region of linear memory specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) at level-of-detail level .
7.8.1.16. tex3DLod() for sparse CUDA arrays  template T tex3DLod ( cudaTextureObject_t texObj , float x , float y , float z , float level , bool * isResident ); fetches from the CUDA array or the region of linear memory specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) at level-of-detail level .
7.8.1.17. tex3DGrad()  template T tex3DGrad ( cudaTextureObject_t texObj , float x , float y , float z , float4 dx , float4 dy ); fetches from the CUDA array specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) at a level-of-detail derived from the X and Y gradients dx and dy .
7.8.1.18. tex3DGrad() for sparse CUDA arrays  template T tex3DGrad ( cudaTextureObject_t texObj , float x , float y , float z , float4 dx , float4 dy , bool * isResident ); fetches from the CUDA array specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) at a level-of-detail derived from the X and Y gradients dx and dy .
7.8.1.19. tex1DLayered()  template T tex1DLayered ( cudaTextureObject_t texObj , float x , int layer ); fetches from the CUDA array specified by the one-dimensional texture object texObj using texture coordinate x and index layer , as described in Layered Textures 7.8.1.20.
tex1DLayeredLod()  template T tex1DLayeredLod ( cudaTextureObject_t texObj , float x , int layer , float level ); fetches from the CUDA array specified by the one-dimensional layered texture at layer layer using texture coordinate x and level-of-detail level . 7.8.1.21. tex1DLayeredGrad()  template T tex1DLayeredGrad ( cudaTextureObject_t texObj , float x , int layer , float dx , float dy ); fetches from the CUDA array specified by the one-dimensional layered texture at layer layer using texture coordinate x and a level-of-detail derived from the dx and dy gradients.
7.8.1.22. tex2DLayered()  template T tex2DLayered ( cudaTextureObject_t texObj , float x , float y , int layer ); fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) and index layer , as described in Layered Textures .
7.8.1.23. tex2DLayered() for sparse CUDA arrays  template T tex2DLayered ( cudaTextureObject_t texObj , float x , float y , int layer , bool * isResident ); fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) and index layer , as described in Layered Textures .
7.8.1.24. tex2DLayeredLod()  template T tex2DLayeredLod ( cudaTextureObject_t texObj , float x , float y , int layer , float level ); fetches from the CUDA array specified by the two-dimensional layered texture at layer layer using texture coordinate (x,y) .
7.8.1.25. tex2DLayeredLod() for sparse CUDA arrays  template T tex2DLayeredLod ( cudaTextureObject_t texObj , float x , float y , int layer , float level , bool * isResident ); fetches from the CUDA array specified by the two-dimensional layered texture at layer layer using texture coordinate (x,y) .
7.8.1.26. tex2DLayeredGrad()  template T tex2DLayeredGrad ( cudaTextureObject_t texObj , float x , float y , int layer , float2 dx , float2 dy ); fetches from the CUDA array specified by the two-dimensional layered texture at layer layer using texture coordinate (x,y) and a level-of-detail derived from the dx and dy gradients.
7.8.1.27. tex2DLayeredGrad() for sparse CUDA arrays  template T tex2DLayeredGrad ( cudaTextureObject_t texObj , float x , float y , int layer , float2 dx , float2 dy , bool * isResident ); fetches from the CUDA array specified by the two-dimensional layered texture at layer layer using texture coordinate (x,y) and a level-of-detail derived from the dx and dy gradients.
7.8.1.28. texCubemap()  template T texCubemap ( cudaTextureObject_t texObj , float x , float y , float z ); fetches the CUDA array specified by the cubemap texture object texObj using texture coordinate (x,y,z) , as described in Cubemap Textures .
7.8.1.29. texCubemapGrad()  template T texCubemapGrad ( cudaTextureObject_t texObj , float x , float , y , float z , float4 dx , float4 dy ); fetches from the CUDA array specified by the cubemap texture object texObj using texture coordinate (x,y,z) as described in Cubemap Textures .
The level-of-detail used is derived from the dx and dy gradients. 7.8.1.30. texCubemapLod()  template T texCubemapLod ( cudaTextureObject_t texObj , float x , float , y , float z , float level ); fetches from the CUDA array specified by the cubemap texture object texObj using texture coordinate (x,y,z) as described in Cubemap Textures .
The level-of-detail used is given by level . 7.8.1.31. texCubemapLayered()  template T texCubemapLayered ( cudaTextureObject_t texObj , float x , float y , float z , int layer ); fetches from the CUDA array specified by the cubemap layered texture object texObj using texture coordinates (x,y,z) , and index layer , as described in Cubemap Layered Textures .
7.8.1.32. texCubemapLayeredGrad()  template T texCubemapLayeredGrad ( cudaTextureObject_t texObj , float x , float y , float z , int layer , float4 dx , float4 dy ); fetches from the CUDA array specified by the cubemap layered texture object texObj using texture coordinate (x,y,z) and index layer , as described in Cubemap Layered Textures , at level-of-detail derived from the dx and dy gradients.
7.8.1.33. texCubemapLayeredLod()  template T texCubemapLayeredLod ( cudaTextureObject_t texObj , float x , float y , float z , int layer , float level ); fetches from the CUDA array specified by the cubemap layered texture object texObj using texture coordinate (x,y,z) and index layer , as described in Cubemap Layered Textures , at level-of-detail level level .
7.9. Surface Functions  Surface functions are only supported by devices of compute capability 2.0 and higher.
Surface objects are described in described in Surface Object API In the sections below, boundaryMode specifies the boundary mode, that is how out-of-range surface coordinates are handled; it is equal to either cudaBoundaryModeClamp , in which case out-of-range coordinates are clamped to the valid range, or cudaBoundaryModeZero , in which case out-of-range reads return zero and out-of-range writes are ignored, or cudaBoundaryModeTrap , in which case out-of-range accesses cause the kernel execution to fail. 7.9.1. Surface Object API  7.9.1.1.
surf1Dread()  template T surf1Dread ( cudaSurfaceObject_t surfObj , int x , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the one-dimensional surface object surfObj using byte coordinate x. 7.9.1.2. surf1Dwrite  template void surf1Dwrite ( T data , cudaSurfaceObject_t surfObj , int x , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the one-dimensional surface object surfObj at byte coordinate x.
7.9.1.3. surf2Dread()  template T surf2Dread ( cudaSurfaceObject_t surfObj , int x , int y , boundaryMode = cudaBoundaryModeTrap ); template void surf2Dread ( T * data , cudaSurfaceObject_t surfObj , int x , int y , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the two-dimensional surface object surfObj using byte coordinates x and y.
7.9.1.4. surf2Dwrite()  template void surf2Dwrite ( T data , cudaSurfaceObject_t surfObj , int x , int y , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the two-dimensional surface object surfObj at byte coordinate x and y.
7.9.1.5. surf3Dread()  template T surf3Dread ( cudaSurfaceObject_t surfObj , int x , int y , int z , boundaryMode = cudaBoundaryModeTrap ); template void surf3Dread ( T * data , cudaSurfaceObject_t surfObj , int x , int y , int z , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the three-dimensional surface object surfObj using byte coordinates x, y, and z.
7.9.1.6. surf3Dwrite()  template void surf3Dwrite ( T data , cudaSurfaceObject_t surfObj , int x , int y , int z , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the three-dimensional object surfObj at byte coordinate x, y, and z.
7.9.1.7. surf1DLayeredread()  template T surf1DLayeredread ( cudaSurfaceObject_t surfObj , int x , int layer , boundaryMode = cudaBoundaryModeTrap ); template void surf1DLayeredread ( T data , cudaSurfaceObject_t surfObj , int x , int layer , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the one-dimensional layered surface object surfObj using byte coordinate x and index layer .
7.9.1.8. surf1DLayeredwrite()  template void surf1DLayeredwrite ( T data , cudaSurfaceObject_t surfObj , int x , int layer , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the two-dimensional layered surface object surfObj at byte coordinate x and index layer .
7.9.1.9. surf2DLayeredread()  template T surf2DLayeredread ( cudaSurfaceObject_t surfObj , int x , int y , int layer , boundaryMode = cudaBoundaryModeTrap ); template void surf2DLayeredread ( T data , cudaSurfaceObject_t surfObj , int x , int y , int layer , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the two-dimensional layered surface object surfObj using byte coordinate x and y, and index layer .
7.9.1.10. surf2DLayeredwrite()  template void surf2DLayeredwrite ( T data , cudaSurfaceObject_t surfObj , int x , int y , int layer , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the one-dimensional layered surface object surfObj at byte coordinate x and y, and index layer .
7.9.1.11. surfCubemapread()  template T surfCubemapread ( cudaSurfaceObject_t surfObj , int x , int y , int face , boundaryMode = cudaBoundaryModeTrap ); template void surfCubemapread ( T data , cudaSurfaceObject_t surfObj , int x , int y , int face , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the cubemap surface object surfObj using byte coordinate x and y, and face index face.
7.9.1.12. surfCubemapwrite()  template void surfCubemapwrite ( T data , cudaSurfaceObject_t surfObj , int x , int y , int face , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the cubemap object surfObj at byte coordinate x and y, and face index face.
7.9.1.13. surfCubemapLayeredread()  template T surfCubemapLayeredread ( cudaSurfaceObject_t surfObj , int x , int y , int layerFace , boundaryMode = cudaBoundaryModeTrap ); template void surfCubemapLayeredread ( T data , cudaSurfaceObject_t surfObj , int x , int y , int layerFace , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the cubemap layered surface object surfObj using byte coordinate x and y, and index layerFace.
7.9.1.14. surfCubemapLayeredwrite()  template void surfCubemapLayeredwrite ( T data , cudaSurfaceObject_t surfObj , int x , int y , int layerFace , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the cubemap layered object surfObj at byte coordinate x and y , and index layerFace .
7.10. Read-Only Data Cache Load Function  The read-only data cache load function is only supported by devices of compute capability 5.0 and higher.
T __ldg ( const T * address ); returns the data of type T located at address address , where T is char , signed char , short , int , long , long long unsigned char , unsigned short , unsigned int , unsigned long , unsigned long long , char2 , char4 , short2 , short4 , int2 , int4 , longlong2 uchar2 , uchar4 , ushort2 , ushort4 , uint2 , uint4 , ulonglong2 float , float2 , float4 , double , or double2 .
Similarly, with the cuda_bf16.h header included, T can also be __nv_bfloat16 or __nv_bfloat162 .
The operation is cached in the read-only data cache (see Global Memory ). 7.11. Load Functions Using Cache Hints  These load functions are only supported by devices of compute capability 5.0 and higher.
T __ldcg ( const T * address ); T __ldca ( const T * address ); T __ldcs ( const T * address ); T __ldlu ( const T * address ); T __ldcv ( const T * address ); returns the data of type T located at address address , where T is char , signed char , short , int , long , long long unsigned char , unsigned short , unsigned int , unsigned long , unsigned long long , char2 , char4 , short2 , short4 , int2 , int4 , longlong2 uchar2 , uchar4 , ushort2 , ushort4 , uint2 , uint4 , ulonglong2 float , float2 , float4 , double , or double2 .
Store Functions Using Cache Hints  These store functions are only supported by devices of compute capability 5.0 and higher.
void __stwb ( T * address , T value ); void __stcg ( T * address , T value ); void __stcs ( T * address , T value ); void __stwt ( T * address , T value ); stores the value argument of type T to the location at address address , where T is char , signed char , short , int , long , long long unsigned char , unsigned short , unsigned int , unsigned long , unsigned long long , char2 , char4 , short2 , short4 , int2 , int4 , longlong2 uchar2 , uchar4 , ushort2 , ushort4 , uint2 , uint4 , ulonglong2 float , float2 , float4 , double , or double2 .
Time Function  clock_t clock (); long long int clock64 (); when executed in device code, returns the value of a per-multiprocessor counter that is incremented every clock cycle.
Sampling this counter at the beginning and at the end of a kernel, taking the difference of the two samples, and recording the result per thread provides a measure for each thread of the number of clock cycles taken by the device to completely execute the thread, but not of the number of clock cycles the device actually spent executing thread instructions.
The former number is greater than the latter since threads are time sliced. 7.14. Atomic Functions  An atomic function performs a read-modify-write atomic operation on one 32-bit, 64-bit, or 128-bit word residing in global or shared memory.
In the case of float2 or float4 , the read-modify-write operation is performed on each element of the vector residing in global memory.
For example, atomicAdd() reads a word at some address in global or shared memory, adds a number to it, and writes the result back to the same address.
The atomic functions described in this section have ordering cuda::memory_order_relaxed and are only atomic at a particular scope : Atomic APIs with _system suffix (example: atomicAdd_system ) are atomic at scope cuda::thread_scope_system if they meet particular conditions .
Atomic APIs without a suffix (example: atomicAdd ) are atomic at scope cuda::thread_scope_device .
Atomic APIs with _block suffix (example: atomicAdd_block ) are atomic at scope cuda::thread_scope_block .
In the following example both the CPU and the GPU atomically update an integer value at address addr : __global__ void mykernel ( int * addr ) { atomicAdd_system ( addr , 10 );   only available on devices with compute capability 6.x } void foo () { int * addr ; cudaMallocManaged ( & addr , 4 ); * addr = 0 ; mykernel >> ( addr ); __sync_fetch_and_add ( addr , 10 );   CPU atomic operation } Note that any atomic operation can be implemented based on atomicCAS() (Compare And Swap).
For example, atomicAdd() for double-precision floating-point numbers is not available on devices with compute capability lower than 6.0 but it can be implemented as follows: #if __CUDA_ARCH__ T atomicExch ( T * address , T val ); reads the 128-bit word old located at the address address in global or shared memory and stores val back to memory at the same address.
The type T must meet the following requirements: sizeof ( T ) == 16 alignof ( T ) >= 16 std :: is_trivially_copyable :: value == true   for C++03 and older std :: is_default_constructible :: value == true So, T must be 128-bit and properly aligned, be trivially copyable, and on C++03 or older, it must also be default constructible.
The 128-bit atomicExch() is only supported by devices of compute capability 9.x and higher. 7.14.1.4. atomicMin()  int atomicMin ( int * address , int val ); unsigned int atomicMin ( unsigned int * address , unsigned int val ); unsigned long long int atomicMin ( unsigned long long int * address , unsigned long long int val ); long long int atomicMin ( long long int * address , long long int val ); reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes the minimum of old and val , and stores the result back to memory at the same address.
The 64-bit version of atomicMin() is only supported by devices of compute capability 5.0 and higher. 7.14.1.5. atomicMax()  int atomicMax ( int * address , int val ); unsigned int atomicMax ( unsigned int * address , unsigned int val ); unsigned long long int atomicMax ( unsigned long long int * address , unsigned long long int val ); long long int atomicMax ( long long int * address , long long int val ); reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes the maximum of old and val , and stores the result back to memory at the same address.
The 64-bit version of atomicMax() is only supported by devices of compute capability 5.0 and higher. 7.14.1.6. atomicInc()  unsigned int atomicInc ( unsigned int * address , unsigned int val ); reads the 32-bit word old located at the address address in global or shared memory, computes ((old >= val) ? 0 : (old+1)) , and stores the result back to memory at the same address.
7.14.1.7. atomicDec()  unsigned int atomicDec ( unsigned int * address , unsigned int val ); reads the 32-bit word old located at the address address in global or shared memory, computes (((old == 0) || (old > val)) ? val : (old-1) ), and stores the result back to memory at the same address.
7.14.1.8. atomicCAS()  int atomicCAS ( int * address , int compare , int val ); unsigned int atomicCAS ( unsigned int * address , unsigned int compare , unsigned int val ); unsigned long long int atomicCAS ( unsigned long long int * address , unsigned long long int compare , unsigned long long int val ); unsigned short int atomicCAS ( unsigned short int * address , unsigned short int compare , unsigned short int val ); reads the 16-bit, 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old == compare ? val : old) , and stores the result back to memory at the same address.
template T atomicCAS ( T * address , T compare , T val ); reads the 128-bit word old located at the address address in global or shared memory, computes (old == compare ? The 128-bit atomicCAS() is only supported by devices of compute capability 9.x and higher. 7.14.2. Bitwise Functions  7.14.2.1.
atomicAnd()  int atomicAnd ( int * address , int val ); unsigned int atomicAnd ( unsigned int * address , unsigned int val ); unsigned long long int atomicAnd ( unsigned long long int * address , unsigned long long int val ); reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old & val ), and stores the result back to memory at the same address.
The 64-bit version of atomicAnd() is only supported by devices of compute capability 5.0 and higher. 7.14.2.2. atomicOr()  int atomicOr ( int * address , int val ); unsigned int atomicOr ( unsigned int * address , unsigned int val ); unsigned long long int atomicOr ( unsigned long long int * address , unsigned long long int val ); reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old | val) , and stores the result back to memory at the same address.
The 64-bit version of atomicOr() is only supported by devices of compute capability 5.0 and higher. 7.14.2.3. atomicXor()  int atomicXor ( int * address , int val ); unsigned int atomicXor ( unsigned int * address , unsigned int val ); unsigned long long int atomicXor ( unsigned long long int * address , unsigned long long int val ); reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old ^ val) , and stores the result back to memory at the same address.
The 64-bit version of atomicXor() is only supported by devices of compute capability 5.0 and higher. 7.15. Address Space Predicate Functions  The functions described in this section have unspecified behavior if the argument is a null pointer.
7.15.1. __isGlobal()  __device__ unsigned int __isGlobal ( const void * ptr ); Returns 1 if ptr contains the generic address of an object in global memory space, otherwise returns 0.
7.15.2. __isShared()  __device__ unsigned int __isShared ( const void * ptr ); Returns 1 if ptr contains the generic address of an object in shared memory space, otherwise returns 0.
7.15.3. __isConstant()  __device__ unsigned int __isConstant ( const void * ptr ); Returns 1 if ptr contains the generic address of an object in constant memory space, otherwise returns 0.
7.15.4. __isGridConstant()  __device__ unsigned int __isGridConstant ( const void * ptr ); Returns 1 if ptr contains the generic address of a kernel parameter annotated with __grid_constant__ , otherwise returns 0.
Only supported for compute architectures greater than or equal to 7.x or later. 7.15.5. __isLocal()  __device__ unsigned int __isLocal ( const void * ptr ); Returns 1 if ptr contains the generic address of an object in local memory space, otherwise returns 0.
__cvta_generic_to_global()  __device__ size_t __cvta_generic_to_global ( const void * ptr ); Returns the result of executing the PTX cvta.to.global instruction on the generic address denoted by ptr . 7.16.2. __cvta_generic_to_shared()  __device__ size_t __cvta_generic_to_shared ( const void * ptr ); Returns the result of executing the PTX cvta.to.shared instruction on the generic address denoted by ptr .
7.16.3. __cvta_generic_to_constant()  __device__ size_t __cvta_generic_to_constant ( const void * ptr ); Returns the result of executing the PTX cvta.to.const instruction on the generic address denoted by ptr .
7.16.4. __cvta_generic_to_local()  __device__ size_t __cvta_generic_to_local ( const void * ptr ); Returns the result of executing the PTX cvta.to.local instruction on the generic address denoted by ptr .
7.16.5. __cvta_global_to_generic()  __device__ void * __cvta_global_to_generic ( size_t rawbits ); Returns the generic pointer obtained by executing the PTX cvta.global instruction on the value provided by rawbits .
7.16.6. __cvta_shared_to_generic()  __device__ void * __cvta_shared_to_generic ( size_t rawbits ); Returns the generic pointer obtained by executing the PTX cvta.shared instruction on the value provided by rawbits .
7.16.7. __cvta_constant_to_generic()  __device__ void * __cvta_constant_to_generic ( size_t rawbits ); Returns the generic pointer obtained by executing the PTX cvta.const instruction on the value provided by rawbits .
7.16.8. __cvta_local_to_generic()  __device__ void * __cvta_local_to_generic ( size_t rawbits ); Returns the generic pointer obtained by executing the PTX cvta.local instruction on the value provided by rawbits .
Description  The alloca() function allocates size bytes of memory in the stack frame of the caller.
The returned value is a pointer to allocated memory, the beginning of the memory is 16 bytes aligned when the function is invoked from device code.
Using alloca() may cause the stack to overflow, user needs to adjust stack size accordingly.
It is supported with compute capability 5.2 or higher. 7.17.3. Example  __device__ void foo ( unsigned int num ) { int4 * ptr = ( int4 * ) alloca ( num * sizeof ( int4 ));   use of ptr ...
} 7.18. Compiler Optimization Hint Functions  The functions described in this section can be used to provide additional information to the compiler optimizer.
7.18.1. __builtin_assume_aligned()  void * __builtin_assume_aligned ( const void * exp , size_t align ) Allows the compiler to assume that the argument pointer is aligned to at least align bytes, and returns the argument pointer.
Example: void * res = __builtin_assume_aligned ( ptr , 32 );   compiler can assume 'res' is   at least 32-byte aligned Three parameter version: void * __builtin_assume_aligned ( const void * exp , size_t align , offset ) Allows the compiler to assume that (char *)exp - offset is aligned to at least align bytes, and returns the argument pointer.
Example: void * res = __builtin_assume_aligned ( ptr , 32 , 8 );   compiler can assume   '(char *)res - 8' is   at least 32-byte aligned. 7.18.2. __builtin_assume()  void __builtin_assume ( bool exp ) Allows the compiler to assume that the Boolean argument is true.
Example: __device__ int get ( int * ptr , int idx ) { __builtin_assume ( idx __global__ void bcast ( int arg ) { int laneId = threadIdx .
x & 0x1f ; int value ; if ( laneId == 0 )   Note unused variable for value = arg ;   all threads except lane 0 value = __shfl_sync ( 0xffffffff , value , 0 );   Synchronize all threads in warp, and get "value" from lane 0 if ( value != arg ) printf ( "Thread %d failed.
Inclusive plus-scan across sub-partitions of 8 threads  #include __global__ void scan4 () { int laneId = threadIdx .
x & 0x1f ;   Seed sample starting value (inverse of lane ID) int value = 31 - laneId ;   Loop to accumulate scan within my partition.
Scan requires log2(n) == 3 steps for 8 threads   It works by an accumulated sum up the warp   by 1, 2, 4, 8 etc.
for ( int i = 1 ; i = i ) value += n ; } printf ( "Thread %d final value = %d   " , threadIdx .
x , value ); } int main () { scan4 >> (); cudaDeviceSynchronize (); return 0 ; } 7.22.3.3.
Reduction across a warp  #include __global__ void warpReduce () { int laneId = threadIdx .
x & 0x1f ;   Seed starting value as inverse lane ID int value = 31 - laneId ;   Use XOR mode to perform butterfly reduction for ( int i = 16 ; i >= 1 ; i /= 2 ) value += __shfl_xor_sync ( 0xffffffff , value , i , 32 );   "value" now contains the sum across all threads printf ( "Thread %d final value = %d   " , threadIdx .
x , value ); } int main () { warpReduce >> (); cudaDeviceSynchronize (); return 0 ; } 7.23.
Description  __nanosleep(ns) suspends the thread for a sleep duration of approximately ns nanoseconds.
It is supported with compute capability 7.0 or higher. 7.23.3. Example  The following code implements a mutex with exponential back-off.
__device__ void mutex_lock ( unsigned int * mutex ) { unsigned int ns = 8 ; while ( atomicCAS ( mutex , 0 , 1 ) == 1 ) { __nanosleep ( ns ); if ( ns class fragment ; void load_matrix_sync ( fragment & a , const T * mptr , unsigned ldm ); void load_matrix_sync ( fragment & a , const T * mptr , unsigned ldm , layout_t layout ); void store_matrix_sync ( T * mptr , const fragment & a , unsigned ldm , layout_t layout ); void fill_fragment ( fragment & a , const T & v ); void mma_sync ( fragment & d , const fragment & a , const fragment & b , const fragment & c , bool satf = false ); fragment An overloaded class containing a section of a matrix distributed across all threads in the warp.
The mapping of matrix elements into fragment internal storage is unspecified and subject to change in future architectures.
The first template parameter specifies how the fragment will participate in the matrix operation.
Acceptable values for Use are: matrix_a when the fragment is used as the first multiplicand, A , matrix_b when the fragment is used as the second multiplicand, B , or accumulator when the fragment is used as the source or destination accumulators ( C or D , respectively).
The m , n and k sizes describe the shape of the warp-wide matrix tiles participating in the multiply-accumulate operation.
For matrix_a the tile takes dimension m x k ; for matrix_b the dimension is k x n , and accumulator tiles are m x n .
The data type, T , may be double , float , __half , __nv_bfloat16 , char , or unsigned char for multiplicands and double , float , int , or __half for accumulators.
As documented in Element Types and Matrix Sizes , limited combinations of accumulator and multiplicand types are supported.
row_major or col_major indicate that elements within a matrix row or column are contiguous in memory, respectively.
A row or column layout is specified only when the accumulator is loaded or stored as described below.
load_matrix_sync Waits until all warp lanes have arrived at load_matrix_sync and then loads the matrix fragment a from memory.
mptr must be a 256-bit aligned pointer pointing to the first element of the matrix in memory.
ldm describes the stride in elements between consecutive rows (for row major layout) or columns (for column major layout) and must be a multiple of 8 for __half element type or multiple of 4 for float element type.
If the fragment is an accumulator , the layout argument must be specified as either mem_row_major or mem_col_major .
For matrix_a and matrix_b fragments, the layout is inferred from the fragment’s layout parameter.
The values of mptr , ldm , layout and all template parameters for a must be the same for all threads in the warp.
store_matrix_sync Waits until all warp lanes have arrived at store_matrix_sync and then stores the matrix fragment a to memory.
The layout of the output matrix must be specified as either mem_row_major or mem_col_major .
Because the mapping of matrix elements to each fragment is unspecified, this function is ordinarily called by all threads in the warp with a common value for v .
mma_sync Waits until all warp lanes have arrived at mma_sync, and then performs the warp-synchronous matrix multiply-accumulate operation D=A*B+C .
The value of satf and template parameters for each matrix fragment must be the same for all threads in the warp.
If satf (saturate to finite value) mode is true , the following additional numerical properties apply for the destination accumulator: If an element result is +Infinity, the corresponding accumulator will contain +MAX_NORM If an element result is -Infinity, the corresponding accumulator will contain -MAX_NORM If an element result is NaN, the corresponding accumulator will contain +0 Because the map of matrix elements into each thread’s fragment is unspecified, individual matrix elements must be accessed from memory (shared or global) after calling store_matrix_sync .
In the special case where all threads in the warp will apply an element-wise operation uniformly to all fragment elements, direct element access can be implemented using the following fragment class members.
enum fragment :: num_elements ; T fragment :: x [ num_elements ]; As an example, the following code scales an accumulator matrix tile by half.
wmma :: fragment frag ; float alpha = 0.5f ;   Same value for all threads in warp /*...*/ for ( int t = 0 ; t =10 bits).
In order to use this floating point format with WMMA operations, the input matrices must be manually converted to tf32 precision.
While the input and output arguments to the intrinsic are of float type, the output will be tf32 numerically.
This new precision is intended to be used with Tensor Cores only, and if mixed with other float type operations, the precision and range of the result will be undefined.
Once an input matrix ( matrix_a or matrix_b ) is converted to tf32 precision, the combination of a fragment with precision::tf32 precision, and a data type of float to load_matrix_sync will take advantage of this new capability.
The elements of the fragment are represented as float , hence the mapping from element_type to storage_element_type is: precision :: tf32 -> float 7.24.3.
Double Precision  Tensor Cores support double-precision floating point operations on devices with compute capability 8.0 and higher.
The mma_sync operation will be performed with the .rn (rounds to nearest even) rounding modifier. 7.24.4. Sub-byte Operations  Sub-byte WMMA operations provide a way to access the low-precision capabilities of Tensor Cores.
the data structures and APIs for them are subject to change and may not be compatible with future releases.
This functionality is available via the nvcuda::wmma::experimental namespace: namespace experimental { namespace precision { struct u4 ;   4-bit unsigned struct s4 ;   4-bit signed struct b1 ;   1-bit } enum bmmaBitOp { bmmaBitOpXOR = 1 ,   compute_75 minimum bmmaBitOpAND = 2   compute_80 minimum }; enum bmmaAccumulateOp { bmmaAccumulateOpPOPC = 1 }; } For 4 bit precision, the APIs available remain the same, but you must specify experimental::precision::u4 or experimental::precision::s4 as the fragment data type.
Since the elements of the fragment are packed together, num_storage_elements will be smaller than num_elements for that fragment.
The num_elements variable for a sub-byte fragment, hence returns the number of elements of sub-byte type element_type .
This is true for single bit precision as well, in which case, the mapping from element_type to storage_element_type is as follows: experimental :: precision :: u4 -> unsigned ( 8 elements in 1 storage element ) experimental :: precision :: s4 -> int ( 8 elements in 1 storage element ) experimental :: precision :: b1 -> unsigned ( 32 elements in 1 storage element ) T -> T  all other types The allowed layouts for sub-byte fragments is always row_major for matrix_a and col_major for matrix_b .
For sub-byte operations the value of ldm in load_matrix_sync should be a multiple of 32 for element type experimental::precision::u4 and experimental::precision::s4 or a multiple of 128 for element type experimental::precision::b1 (i.e., multiple of 16 bytes in both cases).
Note Support for the following variants for MMA instructions is deprecated and will be removed in sm_90: experimental::precision::u4 experimental::precision::s4 experimental::precision::b1 with bmmaBitOp set to bmmaBitOpXOR bmma_sync Waits until all warp lanes have executed bmma_sync, and then performs the warp-synchronous bit matrix multiply-accumulate operation D = (A op B) + C , where op consists of a logical operation bmmaBitOp followed by the accumulation defined by bmmaAccumulateOp .
The available operations are: bmmaBitOpXOR , a 128-bit XOR of a row in matrix_a with the 128-bit column of matrix_b bmmaBitOpAND , a 128-bit AND of a row in matrix_a with the 128-bit column of matrix_b , available on devices with compute capability 8.0 and higher.
The accumulate op is always bmmaAccumulateOpPOPC which counts the number of set bits. 7.24.5. Restrictions  The special format required by tensor cores may be different for each major and minor device architecture.
This is further complicated by threads holding only a fragment (opaque architecture-specific ABI data structure) of the overall matrix, with the developer not allowed to make assumptions on how the individual parameters are mapped to the registers participating in the matrix multiply-accumulate.
Since fragments are architecture-specific, it is unsafe to pass them from function A to function B if the functions have been compiled for different link-compatible architectures and linked together into the same device executable.
In this case, the size and layout of the fragment will be specific to one architecture and using WMMA APIs in the other will lead to incorrect results or potentially, corruption.
An example of two link-compatible architectures, where the layout of the fragment differs, is sm_70 and sm_75.
cu : void bar ( wmma :: fragment * mat_a ) {   operate on mat_a }   sm_70 fragment layout $ > nvcc - dc - arch = compute_70 - code = sm_70 fragA .
o This undefined behavior might also be undetectable at compilation time and by tools at runtime, so extra care is needed to make sure the layout of the fragments is consistent.
This linking hazard is most likely to appear when linking with a legacy library that is both built for a different link-compatible architecture and expecting to be passed a WMMA fragment.
Note that in the case of weak linkages (for example, a CUDA C++ inline function), the linker may choose any available function definition which may result in implicit passes between compilation units.
To avoid these sorts of problems, the matrix should always be stored out to memory for transit through external interfaces (e.g.
wmma::store_matrix_sync(dst, …); ) and then it can be safely passed to bar() as a pointer type [e.g.
Note that since sm_70 can run on sm_75, the above example sm_75 code can be changed to sm_70 and correctly work on sm_75.
However, it is recommended to have sm_75 native code in your application when linking with other sm_75 separately compiled binaries. 7.24.6. Element Types and Matrix Sizes  Tensor Cores support a variety of element types and matrix sizes.
Example  The following code implements a 16x16x16 matrix multiplication in a single warp.
#include using namespace nvcuda ; __global__ void wmma_ker ( half * a , half * b , float * c ) {   Declare the fragments wmma :: fragment a_frag ; wmma :: fragment b_frag ; wmma :: fragment c_frag ;   Initialize the output to zero wmma :: fill_fragment ( c_frag , 0.0f );   Load the inputs wmma :: load_matrix_sync ( a_frag , a , 16 ); wmma :: load_matrix_sync ( b_frag , b , 16 );   Perform the matrix multiplication wmma :: mma_sync ( c_frag , a_frag , b_frag , c_frag );   Store the output wmma :: store_matrix_sync ( c , c_frag , 16 , wmma :: mem_row_major ); } 7.25.
Asynchronous Barrier  The NVIDIA C++ standard library introduces a GPU implementation of std::barrier .
Along with the implementation of std::barrier the library provides extensions that allow users to specify the scope of barrier objects.
Devices of compute capability 8.0 or higher provide hardware acceleration for barrier operations and integration of these barriers with the memcpy_async feature.
On devices with compute capability below 8.0 but starting 7.0, these barriers are available without hardware acceleration.
nvcuda::experimental::awbarrier is deprecated in favor of cuda::barrier . 7.26.1. Simple Synchronization Pattern  Without the arrive/wait barrier, synchronization is achieved using __syncthreads() (to synchronize all threads in a block) or group.sync() when using Cooperative Groups .
#include __global__ void simple_sync ( int iteration_count ) { auto block = cooperative_groups :: this_thread_block (); for ( int i = 0 ; i #include __device__ void compute ( float * data , int curr_iteration ); __global__ void split_arrive_wait ( int iteration_count , float * data ) { using barrier = cuda :: barrier ; __shared__ barrier bar ; auto block = cooperative_groups :: this_thread_block (); if ( block .
sync (); for ( int curr_iter = 0 ; curr_iter #include __global__ void init_barrier () { __shared__ cuda :: barrier bar ; auto block = cooperative_groups :: this_thread_block (); if ( block .
sync (); } Before any thread can participate in cuda::barrier , the barrier must be initialized using init() with an expected arrival count , block.size() in this example.
This poses a bootstrapping challenge in that threads must synchronize before participating in the cuda::barrier , but threads are creating a cuda::barrier in order to synchronize.
In this example, threads that will participate are part of a cooperative group and use block.sync() to bootstrap initialization.
In this example a whole thread block is participating in initialization, hence __syncthreads() could also be used.
The second parameter of init() is the expected arrival count , i.e., the number of times bar.arrive() will be called by participating threads before a participating thread is unblocked from its call to bar.wait(std::move(token)) .
In the prior example the cuda::barrier is initialized with the number of threads in the thread block i.e., cooperative_groups::this_thread_block().size() , and all threads within the thread block participate in the barrier.
A cuda::barrier is flexible in specifying how threads participate (split arrive/wait) and which threads participate.
In contrast this_thread_block.sync() from cooperative groups or __syncthreads() is applicable to whole-thread-block and __syncwarp(mask) is a specified subset of a warp.
If the intention of the user is to synchronize a full thread block or a full warp we recommend using __syncthreads() and __syncwarp(mask) respectively for performance reasons. 7.26.4. A Barrier’s Phase: Arrival, Countdown, Completion, and Reset  A cuda::barrier counts down from the expected arrival count to zero as participating threads call bar.arrive() .
When the last call to bar.arrive() causes the countdown to reach zero, the countdown is automatically and atomically reset.
The reset assigns the countdown to the expected arrival count, and moves the cuda::barrier to the next phase.
A token object of class cuda::barrier::arrival_token , as returned from token=bar.arrive() , is associated with the current phase of the barrier.
A call to bar.wait(std::move(token)) blocks the calling thread while the cuda::barrier is in the current phase, i.e., while the phase associated with the token matches the phase of the cuda::barrier .
If the phase is advanced (because the countdown reaches zero) before the call to bar.wait(std::move(token)) then the thread does not block; if the phase is advanced while the thread is blocked in bar.wait(std::move(token)) , the thread is unblocked.
It is essential to know when a reset could or could not occur, especially in non-trivial arrive/wait synchronization patterns.
A thread’s calls to token=bar.arrive() and bar.wait(std::move(token)) must be sequenced such that token=bar.arrive() occurs during the cuda::barrier ’s current phase, and bar.wait(std::move(token)) occurs during the same or next phase.
After barrier initialization, if a thread’s call to bar.arrive() causes the countdown to reach zero then a call to bar.wait(std::move(token)) must happen before the barrier can be reused for a subsequent call to bar.arrive() .
bar.wait() must only be called using a token object of the current phase or the immediately preceding phase.
For simple arrive/wait synchronization patterns, compliance with these usage rules is straightforward. 7.26.5. Spatial Partitioning (also known as Warp Specialization)  A thread block can be spatially partitioned such that warps are specialized to perform independent computations.
Spatial partitioning is used in a producer or consumer pattern, where one subset of threads produces data that is concurrently consumed by the other (disjoint) subset of threads.
A producer/consumer spatial partitioning pattern requires two one sided synchronizations to manage a data buffer between the producer and consumer.
Producer Consumer wait for buffer to be ready to be filled signal buffer is ready to be filled produce data and fill the buffer signal buffer is filled wait for buffer to be filled consume data in filled buffer Producer threads wait for consumer threads to signal that the buffer is ready to be filled; however, consumer threads do not wait for this signal.
Consumer threads wait for producer threads to signal that the buffer is filled; however, producer threads do not wait for this signal.
For full producer/consumer concurrency this pattern has (at least) double buffering where each buffer requires two cuda::barrier s.
#include #include using barrier = cuda :: barrier ; __device__ void producer ( barrier ready [], barrier filled [], float * buffer , float * in , int N , int buffer_len ) { for ( int i = 0 ; i #include __device__ bool condition_check (); __global__ void early_exit_kernel ( int N ) { using barrier = cuda :: barrier ; __shared__ barrier bar ; auto block = cooperative_groups :: this_thread_block (); if ( block .
sync (); for ( int i = 0 ; i is executed once per phase, after the last thread arrives and before any thread is unblocked from the wait .
Memory operations performed by the threads that arrived at the barrier during the phase are visible to the thread executing the CompletionFunction , and all memory operations performed within the CompletionFunction are visible to all threads waiting at the barrier once they are unblocked from the wait .
#include #include #include namespace cg = cooperative_groups ; __device__ int divergent_compute ( int * , int ); __device__ int independent_computation ( int * , int ); __global__ void psum ( int * data , int n , int * acc ) { auto block = cg :: this_thread_block (); constexpr int BlockSize = 128 ; __shared__ int smem [ BlockSize ]; assert ( BlockSize == block .
size ()); assert ( n % 128 == 0 ); auto completion_fn = [ & ] { int sum = 0 ; for ( int i = 0 ; i ; __shared__ std :: aligned_storage bar_storage ;   Initialize barrier: barrier_t * bar = ( barrier_t * ) & bar_storage ; if ( block .
size (), completion_fn };   equivalent to: init(bar, block.size(), completion_fn); } block .
sync ();   Main loop for ( int i = 0 ; i arrive ();   We can do independent computation here bar -> wait ( std :: move ( t ));   shared-memory is safe to re-use in the next iteration   since all threads are done with it, including the one   that did the reduction } } 7.26.8.
Memory Barrier Primitives Interface  Memory barrier primitives are C-like interfaces to cuda::barrier functionality.
These primitives are available through including the header. 7.26.8.1. Data Types  typedef /* implementation defined */ __mbarrier_t ; typedef /* implementation defined */ __mbarrier_token_t ; 7.26.8.2.
Memory Barrier Primitives API  uint32_t __mbarrier_maximum_count (); void __mbarrier_init ( __mbarrier_t * bar , uint32_t expected_count ); bar must be a pointer to __shared__ memory.
expected_count __device__ void compute ( int * global_out , int const * shared_in ) {   Computes using all values of current batch from shared memory.
} __global__ void without_memcpy_async ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); assert ( size == batch_sz * grid .
size ());   Exposition: input size fits batch_sz * grid_size extern __shared__ int shared [];   block.size() * sizeof(int) bytes size_t local_idx = block .
thread_rank (); for ( size_t batch = 0 ; batch #include __device__ void compute ( int * global_out , int const * shared_in ); __global__ void with_memcpy_async ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); assert ( size == batch_sz * grid .
size ());   Exposition: input size fits batch_sz * grid_size extern __shared__ int shared [];   block.size() * sizeof(int) bytes for ( size_t batch = 0 ; batch #include __device__ void compute ( int * global_out , int const * shared_in ); __global__ void with_barrier ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); assert ( size == batch_sz * grid .
size ());   Assume input size fits batch_sz * grid_size extern __shared__ int shared [];   block.size() * sizeof(int) bytes   Create a synchronization object (C++20 barrier) __shared__ cuda :: barrier barrier ; if ( block .
sync (); for ( size_t batch = 0 ; batch (size_t size) Shape can be used to supply a proof that both pointers passed to memcpy_async are aligned to an Align alignment boundary and that size is a multiple of Align , by passing it as an argument where the memcpy_async APIs expect a Shape : cuda :: memcpy_async ( group , dst , src , cuda :: aligned_size_t ( N * block .
size ()), pipeline ); If the proof is incorrect, the behavior is undefined. 7.27.6.2. Trivially copyable  On devices with compute capability 8.0, the cp.async family of instructions allows copying data from global to shared memory asynchronously.
If the pointer types passed to memcpy_async do not point to TriviallyCopyable types, the copy constructor of each output element needs to be invoked, and these instructions cannot be used to accelerate memcpy_async . 7.27.6.3. Warp Entanglement - Commit  The sequence of memcpy_async batches is shared across the warp.
The commit operation is coalesced such that the sequence is incremented once for all converged threads that invoke the commit operation.
If the warp is fully converged, the sequence is incremented by one; if the warp is fully diverged, the sequence is incremented by 32.
PB = {BP0, BP1, BP2, …, BPL} Let TB be a thread’s perceived sequence of batches, as if the sequence were only incremented by this thread’s invocation of the commit operation.
TB = {BT0, BT1, BT2, …, BTL} The pipeline::producer_commit() return value is from the thread’s perceived batch sequence.
An index in a thread’s perceived sequence always aligns to an equal or larger index in the actual warp-shared sequence.
The sequences are equal only when all commit operations are invoked from converged threads.
BTn ≡ BPm where n () or pipeline::consumer_wait() to wait for batches in the perceived sequence TB to complete.
Note that pipeline::consumer_wait() is equivalent to pipeline_consumer_wait_prior() , where N = PL .
The pipeline_consumer_wait_prior() function waits for batches in the actual sequence at least up to and including PL-N .
Since TL #include __device__ void compute ( int * global_out , int const * shared_in ); __global__ void with_single_stage ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); assert ( size == batch_sz * grid .
size ());   Assume input size fits batch_sz * grid_size constexpr size_t stages_count = 1 ;   Pipeline with one stage   One batch must fit in shared memory: extern __shared__ int shared [];   block.size() * sizeof(int) bytes   Allocate shared storage for a single stage cuda::pipeline: __shared__ cuda :: pipeline_shared_state shared_state ; auto pipeline = cuda :: make_pipeline ( block , & shared_state );   Each thread processes `batch_sz` elements.
Compute offset of the batch `batch` of this thread block in global memory: auto block_batch = [ & ]( size_t batch ) -> int { return block .
size () * batch ; }; for ( size_t batch = 0 ; batch #include __device__ void compute ( int * global_out , int const * shared_in ); __global__ void with_staging ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); assert ( size == batch_sz * grid .
size ());   Assume input size fits batch_sz * grid_size constexpr size_t stages_count = 2 ;   Pipeline with two stages   Two batches must fit in shared memory: extern __shared__ int shared [];   stages_count * block.size() * sizeof(int) bytes size_t shared_offset [ stages_count ] = { 0 , block .
size () };   Offsets to each batch   Allocate shared storage for a two-stage cuda::pipeline: __shared__ cuda :: pipeline_shared_state shared_state ; auto pipeline = cuda :: make_pipeline ( block , & shared_state );   Each thread processes `batch_sz` elements.
size () * batch ; };   Initialize first pipeline stage by submitting a `memcpy_async` to fetch a whole batch for the block: if ( batch_sz == 0 ) return ; pipeline .
producer_acquire (); cuda :: memcpy_async ( block , shared + shared_offset [ 0 ], global_in + block_batch ( 0 ), sizeof ( int ) * block .
producer_commit ();   Pipelined copy/compute: for ( size_t batch = 1 ; batch encapsulates the finite resources that allow a pipeline to process up to count concurrent stages.
If all resources are in use, pipeline.producer_acquire() blocks producer threads until the resources of the next pipeline stage are released by consumer threads.
This example can be written in a more concise manner by merging the prolog and epilog of the loop with the loop itself as follows: template __global__ void with_staging_unified ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); assert ( size == batch_sz * grid .
size ());   Assume input size fits batch_sz * grid_size extern __shared__ int shared [];   stages_count * block.size() * sizeof(int) bytes size_t shared_offset [ stages_count ]; for ( int s = 0 ; s shared_state ; auto pipeline = cuda :: make_pipeline ( block , & shared_state ); auto block_batch = [ & ]( size_t batch ) -> int { return block .
size () * batch ; };   compute_batch: next batch to process   fetch_batch: next batch to fetch from global memory for ( size_t compute_batch = 0 , fetch_batch = 0 ; compute_batch primitive used above is very flexible, and supports two features that our examples above are not using: any arbitrary subset of threads in the block can participate in the pipeline , and from the threads that participate, any subsets can be producers, consumers, or both.
In the following example, threads with an “even” thread rank are producers, while other threads are consumers: __device__ void compute ( int * global_out , int shared_in ); template __global__ void with_specialized_staging_unified ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block ();   In this example, threads with "even" thread rank are producers, while threads with "odd" thread rank are consumers: const cuda :: pipeline_role thread_role = block .
thread_rank () % 2 == 0 ? cuda :: pipeline_role :: producer : cuda :: pipeline_role :: consumer ;   Each thread block only has half of its threads as producers: auto producer_threads = block .
size () / 2 ;   Map adjacent even and odd threads to the same id: const int thread_idx = block .
thread_rank () / 2 ; auto elements_per_batch = size / batch_sz ; auto elements_per_batch_per_block = elements_per_batch / grid .
x ; extern __shared__ int shared [];   stages_count * elements_per_batch_per_block * sizeof(int) bytes size_t shared_offset [ stages_count ]; for ( int s = 0 ; s shared_state ; cuda :: pipeline pipeline = cuda :: make_pipeline ( block , & shared_state , thread_role );   Each thread block processes `batch_sz` batches.
Compute offset of the batch `batch` of this thread block in global memory: auto block_batch = [ & ]( size_t batch ) -> int { return elements_per_batch * batch + elements_per_batch_per_block * blockIdx .
x ; }; for ( size_t compute_batch = 0 , fetch_batch = 0 ; compute_batch by using a pipeline combined with __syncthreads() : template __global__ void with_staging_scope_thread ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); auto thread = cooperative_groups :: this_thread (); assert ( size == batch_sz * grid .
size ());   Assume input size fits batch_sz * grid_size extern __shared__ int shared [];   stages_count * block.size() * sizeof(int) bytes size_t shared_offset [ stages_count ]; for ( int s = 0 ; s pipeline = cuda :: make_pipeline (); auto block_batch = [ & ]( size_t batch ) -> int { return block .
For a C-like interface, when compiling without ISO C++ 2011 compatibility, see Pipeline Primitives Interface . 7.28.4. Pipeline Primitives Interface  Pipeline primitives are a C-like interface for memcpy_async functionality.
When compiling without ISO C++ 2011 compatibility, include the header. 7.28.4.1. memcpy_async Primitive  void __pipeline_memcpy_async ( void * __restrict__ dst_shared , const void * __restrict__ src_global , size_t size_and_align , size_t zfill = 0 ); Request that the following operation be submitted for asynchronous evaluation: size_t i = 0 ; for (; i #include using barrier = cuda :: barrier ; namespace ptx = cuda :: ptx ; static constexpr size_t buf_len = 1024 ; __global__ void add_one_kernel ( int * data , size_t offset ) {   Shared memory buffer.
a) Initialize shared memory barrier with the number of threads participating in the barrier.
#pragma nv_diag_suppress static_var_with_dynamic_init __shared__ barrier bar ; if ( threadIdx .
cuda::memcpy_async arrives on the barrier and communicates   how many bytes are expected to come in (the transaction count) cuda :: memcpy_async ( smem_data , data + offset , cuda :: aligned_size_t ( sizeof ( smem_data )), bar ); }   3b.
Shared memory barriers are described in more detail in Asynchronous Data Copies using cuda::barrier .
To make the initialized barrier visible to subsequent bulk-asynchronous copies, the fence.proxy.async.shared::cta instruction is used.
This instruction ensures that subsequent bulk-asynchronous copy operations operate on the initialized barrier.
The bulk-asynchronous copy instruction directs the hardware to copy a large chunk of data into shared memory, and to update the transaction count of the shared memory barrier after completing the read.
In general, issuing as few bulk copies with as big a size as possible results in the best performance.
Because the copy can be performed asynchronously by the hardware, it is not necessary to split the copy into smaller chunks.
The thread that initiates the bulk-asynchronous copy operation arrives at the barrier using mbarrier.expect_tx .
This tells the barrier that the thread has arrived and also how many bytes (tx / transactions) are expected to arrive.
If multiple threads update the transaction count, the expected transaction will be the sum of the updates.
Once the barrier has flipped, the bytes are safe to read from shared memory, both by the threads as well as by subsequent bulk-asynchronous copies.
It can either return true, indicating that the wait is over, or return false, which may mean that the wait timed out.
To make the writes visible to subsequent bulk-asynchronous copies, the fence.proxy.async.shared::cta instruction is used.
This orders the writes to shared memory before subsequent reads from bulk-asynchronous copy operations, which read through the async proxy.
So each thread first orders the writes to objects in shared memory in the async proxy via the fence.proxy.async.shared::cta , and these operations by all threads are ordered before the async operation performed in thread 0 using __syncthreads() .
Afterwards, the thread can wait for all operations in this group to have completed reading from shared memory (as in the code above) or to have completed writing to global memory, making the writes visible to the initiating thread.
Note that the bulk-asynchronous and non-bulk asynchronous copy instructions have different async-groups: there exist both cp.async.wait_group and cp.async.bulk.wait_group instructions.
The bulk-asynchronous instructions have specific alignment requirements on their source and destination addresses.
Table 7 Alignment requirements for one-dimensional bulk-asynchronous operations in Compute Capability 9.0.
Shared memory barrier address Must be 8 byte aligned (this is guaranteed by cuda::barrier ).
Size of transfer Must be a multiple of 16 bytes. 7.29.2. Using TMA to transfer multi-dimensional arrays  The primary difference between the one-dimensional and multi-dimensional case is that a tensor map must be created on the host and passed to the CUDA kernel.
This section describes how to create a tensor map using the CUDA driver API, how to pass it to device, and how to use it on device.
This API can be accessed by linking to the driver directly ( -lcuda ) or by using the cudaGetDriverEntryPoint API.
Among them are the base pointer to an array in global memory, the size of the array (in number of elements), the stride from one row to the next (in bytes), the size of the shared memory buffer (in number of elements).
The code below creates a tensor map to describe a two-dimensional row-major array of size GMEM_HEIGHT x GMEM_WIDTH .
constexpr uint32_t rank = 2 ; uint64_t size [ rank ] = { GMEM_WIDTH , GMEM_HEIGHT };   The stride is the number of bytes to traverse from the first element of one row to the next.
uint64_t stride [ rank - 1 ] = { GMEM_WIDTH * sizeof ( int )};   The box_size is the size of the shared memory buffer that is used as the   destination of a TMA transfer.
uint32_t box_size [ rank ] = { SMEM_WIDTH , SMEM_HEIGHT };   The distance between elements in units of sizeof(element).
A stride of 2   can be used to load only the real component of a complex-valued tensor, for instance.
uint32_t elem_stride [ rank ] = { 1 , 1 };   Get a function pointer to the cuTensorMapEncodeTiled driver API.
auto cuTensorMapEncodeTiled = get_cuTensorMapEncodeTiled ();   Create the tensor descriptor.
CUresult res = cuTensorMapEncodeTiled ( & tensor_map ,   CUtensorMap *tensorMap, CUtensorMapDataType :: CU_TENSOR_MAP_DATA_TYPE_INT32 , rank ,   cuuint32_t tensorRank, tensor_ptr ,   void *globalAddress, size ,   const cuuint64_t *globalDim, stride ,   const cuuint64_t *globalStrides, box_size ,   const cuuint32_t *boxDim, elem_stride ,   const cuuint32_t *elementStrides,   Interleave patterns can be used to accelerate loading of values that   are less than 4 bytes long.
CUtensorMapInterleave :: CU_TENSOR_MAP_INTERLEAVE_NONE ,   Swizzling can be used to avoid shared memory bank conflicts.
CUtensorMapSwizzle :: CU_TENSOR_MAP_SWIZZLE_NONE ,   L2 Promotion can be used to widen the effect of a cache-policy to a wider   set of L2 cache lines.
CUtensorMapL2promotion :: CU_TENSOR_MAP_L2_PROMOTION_NONE ,   Any element that is outside of bounds will be set to zero by the TMA transfer.
This can be achieved by using constant memory or by passing the tensor map as a const __grid_constant__ parameter to a kernel.
When passing the tensor map as a parameter, some versions of the GCC C++ compiler issue the warning “the ABI for passing parameters with 64-byte alignment has changed in GCC 4.6”.
__global__ void kernel ( const __grid_constant__ CUtensorMap tensor_map ) {   Use tensor_map here.
] kernel >> ( map ); } As an alternative to the __grid_constant__ kernel parameter, a global constant variable can be used.
__constant__ CUtensorMap global_tensor_map ; __global__ void kernel () {   Use global_tensor_map here.
] cudaMemcpyToSymbol ( global_tensor_map , & local_tensor_map , sizeof ( CUtensorMap )); kernel >> (); } The following example copies the tensor map to global device memory.
Using a pointer to a tensor map in global device memory is undefined behavior and will lead to silent and difficult to track down bugs.
__device__ CUtensorMap global_tensor_map ; __global__ void kernel ( CUtensorMap * tensor_map ) {   Do *not* use tensor_map here.
Using a global memory pointer is   undefined behavior and can fail silently and unreliably.
] cudaMemcpy ( global_tensor_map , & local_tensor_map , sizeof ( CUtensorMap )); kernel >> ( global_tensor_map ); } Use .
#include   CUtensormap #include using barrier = cuda :: barrier ; namespace cde = cuda :: device :: experimental ; __global__ void kernel ( const __grid_constant__ CUtensorMap tensor_map , int x , int y ) {   The destination shared memory buffer of a bulk tensor operation should be   128 byte aligned.
__shared__ alignas ( 128 ) int smem_buffer [ SMEM_HEIGHT ][ SMEM_WIDTH ];   Initialize shared memory barrier with the number of threads participating in the barrier.
cde :: fence_proxy_async_shared_cta (); }   Syncthreads so initialized barrier is visible to all threads.
cde :: cp_async_bulk_tensor_2d_global_to_shared ( & smem_buffer , & tensor_map , x , y , bar );   Arrive on the barrier and tell how many bytes are expected to come in.
token = cuda :: device :: barrier_arrive_tx ( bar , 1 , sizeof ( smem_buffer )); } else {   Other threads just arrive.
cde :: fence_proxy_async_shared_cta (); __syncthreads ();   After syncthreads, writes by all threads are visible to TMA engine.
x == 0 ) { cde :: cp_async_bulk_tensor_2d_shared_to_global ( & tensor_map , x , y , & smem_buffer );   Wait for TMA transfer to have finished reading shared memory.
cde :: cp_async_bulk_commit_group ();   Wait for the group to have completed reading from shared memory.
If   further computations were to take place in the kernel, this allows the   memory location of the shared memory barrier to be reused.
When part of the tile that is being read from global to shared memory is out of bounds, the shared memory that corresponds to the out of bounds area is zero-filled.
When writing from shared to global memory, parts of the tile may be out of bounds, but the top left corner cannot have any negative indices.
Due to alignment requirements, a 4 x 3 row-major matrix of integers must have strides of 4 and 16 bytes as well.
Each row is padded with 4 extra bytes to ensure that the start of the next row is aligned to 16 bytes.
For more information regarding alignment, refer to Table Alignment requirements for multi-dimensional bulk tensor asynchronous copy operations in Compute Capability 9.0. . Table 8 Alignment requirements for multi-dimensional bulk tensor asynchronous copy operations in Compute Capability 9.0.
Shared memory address Must be 128 byte aligned. 7.29.2.1. Multi-dimensional TMA PTX wrappers  Below, the PTX instructions are ordered by their use in the example code above.
The cp.async.bulk.tensor instructions initiate a bulk tensor asynchronous copy between global and shared memory.
The wrappers below read from global to shared memory and write from shared to global memory.
Profiler Counter Function  Each multiprocessor has a set of sixteen hardware counters that an application can increment with a single instruction by calling the __prof_trigger() function.
void __prof_trigger ( int counter ); increments by one per warp the per-multiprocessor hardware counter of index counter .
The value of counters 0, 1, …, 7 can be obtained via nvprof by nvprof --events prof_trigger_0x where x is 0, 1, …, 7.
All counters are reset before each kernel launch (note that when collecting counters, kernel launches are synchronous as mentioned in Concurrent Execution between Host and Device ). 7.31. Assertion  Assertion is only supported by devices of compute capability 2.x and higher.
void assert ( int expression ); stops the kernel execution if expression is equal to zero.
If the program is run within a debugger, this triggers a breakpoint and the debugger can be used to inspect the current state of the device.
Otherwise, each thread for which expression is equal to zero prints a message to stderr after synchronization with the host via cudaDeviceSynchronize() , cudaStreamSynchronize() , or cudaEventSynchronize() .
The format of this message is as follows: ::: block: [blockId.x,blockId.x,blockIdx.z], thread: [threadIdx.x,threadIdx.y,threadIdx.z] Assertion `` failed.
Any subsequent host-side synchronization calls made for the same device will return cudaErrorAssert .
No more commands can be sent to this device until cudaDeviceReset() is called to reinitialize the device.
For example, the following program from source file test.cu #include __global__ void testAssert ( void ) { int is_one = 1 ; int should_be_one = 0 ;   This will have no effect assert ( is_one );   This will halt kernel execution assert ( should_be_one ); } int main ( int argc , char * argv []) { testAssert >> (); cudaDeviceSynchronize (); return 0 ; } will output: test.cu:19: void testAssert(): block: [0,0,0], thread: [0,0,0] Assertion `should_be_one` failed.
They can affect performance and it is therefore recommended to disable them in production code.
They can be disabled at compile time by defining the NDEBUG preprocessor macro before including assert.h .
Note that expression should not be an expression with side effects (something like (++i > 0) , for example), otherwise disabling the assertion will affect the functionality of the code. 7.32. Trap function  A trap operation can be initiated by calling the __trap() function from any device thread.
void __trap (); The execution of the kernel is aborted and an interrupt is raised in the host program. 7.33. Breakpoint Function  Execution of a kernel function can be suspended by calling the __brkpt() function from any device thread.
Formatted Output  Formatted output is only supported by devices of compute capability 2.x and higher.
int printf ( const char * format [, arg , ...]); prints formatted output from a kernel to a host-side output stream.
The in-kernel printf() function behaves in a similar way to the standard C-library printf() function, and the user is referred to the host system’s manual pages for a complete description of printf() behavior.
In essence, the string passed in as format is output to a stream on the host, with substitutions made from the argument list wherever a format specifier is encountered.
The printf() command is executed as any other device-side function: per-thread, and in the context of the calling thread.
From a multi-threaded kernel, this means that a straightforward call to printf() will be executed by every thread, using that thread’s data as specified.
Multiple versions of the output string will then appear at the host stream, once for each thread which encountered the printf() .
It is up to the programmer to limit the output to a single thread if only a single output string is desired (see Examples for an illustrative example).
Unlike the C-standard printf() , which returns the number of characters printed, CUDA’s printf() returns the number of arguments parsed.
If an internal error occurs, -2 is returned. 7.34.1. Format Specifiers  As for standard printf() , format specifiers take the form: %[flags][width][.precision][size]type The following fields are supported (see widely-available documentation for a complete description of all behaviors): Flags: '#' ' ' '0' '+' '-' Width: '*' '0-9' Precision: '0-9' Size: 'h' 'l' 'll' Type: "%cdiouxXpeEfgGaAs" Note that CUDA’s printf() will accept any combination of flag, width, precision, size and type, whether or not overall they form a valid format specifier.
In other words, “ %hd ” will be accepted and printf will expect a double-precision variable in the corresponding location in the argument list. 7.34.2. Limitations  Final formatting of the printf() output takes place on the host system.
This means that the format string must be understood by the host-system’s compiler and C library.
Every effort has been made to ensure that the format specifiers supported by CUDA’s printf function form a universal subset from the most common host compilers, but exact behavior will be host-OS-dependent.
As described in Format Specifiers , printf() will accept all combinations of valid flags and types.
This is because it cannot determine what will and will not be valid on the host system where the final output is formatted.
The effect of this is that output may be undefined if the program emits a format string which contains invalid combinations.
Owing to the differing size of the long type on 64-bit Windows platforms (four bytes on 64-bit Windows platforms, eight bytes on other 64-bit platforms), a kernel which is compiled on a non-Windows 64-bit machine but then run on a win64 machine will see corrupted output for all format strings which include “ %ld ”.
It is recommended that the compilation platform matches the execution platform to ensure safety.
The output buffer for printf() is set to a fixed size before kernel launch (see Associated Host-Side API ).
It is circular and if more output is produced during kernel execution than can fit in the buffer, older output is overwritten.
It is flushed only when one of these actions is performed: Kernel launch via >> or cuLaunchKernel() (at the start of the launch, and if the CUDA_LAUNCH_BLOCKING environment variable is set to 1, at the end of the launch as well), Synchronization via cudaDeviceSynchronize() , cuCtxSynchronize() , cudaStreamSynchronize() , cuStreamSynchronize() , cudaEventSynchronize() , or cuEventSynchronize() , Memory copies via any blocking version of cudaMemcpy*() or cuMemcpy*() , Module loading/unloading via cuModuleLoad() or cuModuleUnload() , Context destruction via cudaDeviceReset() or cuCtxDestroy() .
Prior to executing a stream callback added by cudaStreamAddCallback or cuStreamAddCallback .
The user must call cudaDeviceReset() or cuCtxDestroy() explicitly, as shown in the examples below.
Internally printf() uses a shared data structure and so it is possible that calling printf() might change the order of execution of threads.
In particular, a thread which calls printf() might take a longer execution path than one which does not call printf() , and that path length is dependent upon the parameters of the printf() .
Note, however, that CUDA makes no guarantees of thread execution order except at explicit __syncthreads() barriers, so it is impossible to tell whether execution order has been modified by printf() or by other scheduling behavior in the hardware. 7.34.3. Associated Host-Side API  The following API functions get and set the size of the buffer used to transfer the printf() arguments and internal metadata to the host (default is 1 megabyte): cudaDeviceGetLimit(size_t* size,cudaLimitPrintfFifoSize) cudaDeviceSetLimit(cudaLimitPrintfFifoSize, size_t size) 7.34.4.
Examples  The following code sample: #include __global__ void helloCUDA ( float f ) { printf ( "Hello thread %d, f=%f   " , threadIdx .
x , f ); } int main () { helloCUDA >> ( 1.2345f ); cudaDeviceSynchronize (); return 0 ; } will output: Hello thread 2, f=1.2345 Hello thread 1, f=1.2345 Hello thread 4, f=1.2345 Hello thread 0, f=1.2345 Hello thread 3, f=1.2345 Notice how each thread encounters the printf() command, so there are as many lines of output as there were threads launched in the grid.
As expected, global values (i.e., float f ) are common between all threads, and local values (i.e., threadIdx.x ) are distinct per-thread.
The following code sample: #include __global__ void helloCUDA ( float f ) { if ( threadIdx .
x , f ) ; } int main () { helloCUDA >> ( 1.2345f ); cudaDeviceSynchronize (); return 0 ; } will output: Hello thread 0, f=1.2345 Self-evidently, the if() statement limits which threads will call printf , so that only a single line of output is seen. 7.35. Dynamic Global Memory Allocation and Operations  Dynamic global memory allocation and operations are only supported by devices of compute capability 2.x and higher.
__host__ __device__ void * malloc ( size_t size ); __device__ void * __nv_aligned_device_malloc ( size_t size , size_t align ); __host__ __device__ void free ( void * ptr ); allocate and free memory dynamically from a fixed-size heap in global memory.
__host__ __device__ void * memcpy ( void * dest , const void * src , size_t size ); copy size bytes from the memory location pointed by src to the memory location pointed by dest .
__host__ __device__ void * memset ( void * ptr , int value , size_t size ); set size bytes of memory block pointed by ptr to value (interpreted as an unsigned char).
The CUDA in-kernel malloc() function allocates at least size bytes from the device heap and returns a pointer to the allocated memory or NULL if insufficient memory exists to fulfill the request.
The CUDA in-kernel __nv_aligned_device_malloc() function allocates at least size bytes from the device heap and returns a pointer to the allocated memory or NULL if insufficient memory exists to fulfill the requested size or alignment.
The CUDA in-kernel free() function deallocates the memory pointed to by ptr , which must have been returned by a previous call to malloc() or __nv_aligned_device_malloc() .
The memory allocated by a given CUDA thread via malloc() or __nv_aligned_device_malloc() remains allocated for the lifetime of the CUDA context, or until it is explicitly released by a call to free() .
Any CUDA thread may free memory allocated by another thread, but care should be taken to ensure that the same pointer is not freed more than once. 7.35.1. Heap Memory Allocation  The device memory heap has a fixed size that must be specified before any program using malloc() , __nv_aligned_device_malloc() or free() is loaded into the context.
A default heap of eight megabytes is allocated if any program uses malloc() or __nv_aligned_device_malloc() without explicitly specifying the heap size.
The following API functions get and set the heap size: cudaDeviceGetLimit(size_t* size, cudaLimitMallocHeapSize) cudaDeviceSetLimit(cudaLimitMallocHeapSize, size_t size) The heap size granted will be at least size bytes.
The actual memory allocation for the heap occurs when a module is loaded into the context, either explicitly via the CUDA driver API (see Module ), or implicitly via the CUDA runtime API (see CUDA Runtime ).
If the memory allocation fails, the module load will generate a CUDA_ERROR_SHARED_OBJECT_INIT_FAILED error.
Heap size cannot be changed once a module load has occurred and it does not resize dynamically according to need.
Memory reserved for the device heap is in addition to memory allocated through host-side CUDA API calls such as cudaMalloc() . 7.35.2. Interoperability with Host Memory API  Memory allocated via device malloc() or __nv_aligned_device_malloc() cannot be freed using the runtime (i.e., by calling any of the free memory functions from Device Memory ).
Similarly, memory allocated via the runtime (i.e., by calling any of the memory allocation functions from Device Memory ) cannot be freed via free() .
In addition, memory allocated by a call to malloc() or __nv_aligned_device_malloc() in device code cannot be used in any runtime or driver API calls (i.e.
Per Thread Allocation  The following code sample: #include #include __global__ void mallocTest () { size_t size = 123 ; char * ptr = ( char * ) malloc ( size ); memset ( ptr , 0 , size ); printf ( "Thread %d got pointer: %p   " , threadIdx .
cudaDeviceSetLimit ( cudaLimitMallocHeapSize , 128 * 1024 * 1024 ); mallocTest >> (); cudaDeviceSynchronize (); return 0 ; } will output: Thread 0 got pointer : 00057020 Thread 1 got pointer : 000570 8 c Thread 2 got pointer : 000570f 8 Thread 3 got pointer : 00057164 Thread 4 got pointer : 000571 d0 Notice how each thread encounters the malloc() and memset() commands and so receives and initializes its own allocation.
Per Thread Block Allocation  #include __global__ void mallocTest () { __shared__ int * data ;   The first thread in the block does the allocation and then   shares the pointer with all other threads through shared memory,   so that access can easily be coalesced.
x * 64 ; data = ( int * ) malloc ( size ); } __syncthreads ();   Check for failure if ( data == NULL ) return ;   Threads index into the memory, ensuring coalescence int * ptr = data ; for ( int i = 0 ; i >> (); cudaDeviceSynchronize (); return 0 ; } 7.35.3.3.
Allocation Persisting Between Kernel Launches  #include #include #define NUM_BLOCKS 20 __device__ int * dataptr [ NUM_BLOCKS ];   Per-block pointer __global__ void allocmem () {   Only the first thread in the block does the allocation   since we want only one allocation per block.
x ] = 0 ; }   Simple example: store thread ID into each element __global__ void usemem () { int * ptr = dataptr [ blockIdx .
x ; }   Print the content of the buffer before freeing it __global__ void freemem () { int * ptr = dataptr [ blockIdx .
x ]);   Only free from one thread! x == 0 ) free ( ptr ); } int main () { cudaDeviceSetLimit ( cudaLimitMallocHeapSize , 128 * 1024 * 1024 );   Allocate memory allocmem >> ();   Use memory usemem >> (); usemem >> (); usemem >> ();   Free memory freemem >> (); cudaDeviceSynchronize (); return 0 ; } 7.36.
Execution Configuration  Any call to a __global__ function must specify the execution configuration for that call.
The execution configuration defines the dimension of the grid and blocks that will be used to execute the function on the device, as well as the associated stream (see CUDA Runtime for a description of streams).
As an example, a function declared as __global__ void Func ( float * parameter ); must be called like this: Func >> ( parameter ); The arguments to the execution configuration are evaluated before the actual function arguments.
The function call will fail if Dg or Db are greater than the maximum sizes allowed for the device as specified in Compute Capabilities , or if Ns is greater than the maximum amount of shared memory available on the device, minus the amount of shared memory required for static allocation.
Compute capability 9.0 and above allows users to specify compile time thread block cluster dimensions, so that the kernel can use the cluster hierarchy in CUDA.
The example below shows compile time cluster size of 2 in X dimension and 1 in Y and Z dimension.
__global__ void __cluster_dims__ ( 2 , 1 , 1 ) Func ( float * parameter ); Thread block cluster dimensions can also be specified at runtime and kernel with the cluster can be launched using cudaLaunchKernelEx API.
The API takes a configuration arugument of type cudaLaunchConfig_t , kernel function pointer and kernel arguments.
__global__ void Func ( float * parameter );   Kernel invocation with runtime cluster size { cudaLaunchConfig_t config = { 0 };   The grid dimension is not affected by cluster launch, and is still enumerated   using number of blocks.
numAttrs = 1 ; float * parameter ; cudaLaunchKernelEx ( & config , Func , parameter ); } 7.37.
Launch Bounds  As discussed in detail in Multiprocessor Level , the fewer registers a kernel uses, the more threads and thread blocks are likely to reside on a multiprocessor, which can improve performance.
Therefore, the compiler uses heuristics to minimize register usage while keeping register spilling (see Device Memory Accesses ) and instruction count to a minimum.
An application can optionally aid these heuristics by providing additional information to the compiler in the form of launch bounds that are specified using the __launch_bounds__() qualifier in the definition of a __global__ function: __global__ void __launch_bounds__ ( maxThreadsPerBlock , minBlocksPerMultiprocessor , maxBlocksPerCluster ) MyKernel (...) { ...
} maxThreadsPerBlock specifies the maximum number of threads per block with which the application will ever launch MyKernel() ; it compiles to the .maxntid PTX directive.
minBlocksPerMultiprocessor is optional and specifies the desired minimum number of resident blocks per multiprocessor; it compiles to the .minnctapersm PTX directive.
maxBlocksPerCluster is optional and specifies the desired maximum number thread blocks per cluster with which the application will ever launch MyKernel() ; it compiles to the .maxclusterrank PTX directive.
If launch bounds are specified, the compiler first derives from them the upper limit L on the number of registers the kernel should use to ensure that minBlocksPerMultiprocessor blocks (or a single block if minBlocksPerMultiprocessor is not specified) of maxThreadsPerBlock threads can reside on the multiprocessor (see Hardware Multithreading for the relationship between the number of registers used by a kernel and the number of registers allocated per block).
A kernel will fail to launch if it is executed with more threads per block than its launch bound maxThreadsPerBlock .
A kernel will fail to launch if it is executed with more thread blocks per cluster than its launch bound maxBlocksPerCluster .
Per thread resources required by a CUDA kernel might limit the maximum block size in an unwanted way.
In order to maintain forward compatibility to future hardware and toolkits and to ensure that at least one thread block can run on an SM, developers should include the single argument __launch_bounds__(maxThreadsPerBlock) which specifies the largest block size that the kernel will be launched with.
Providing the two argument version of __launch_bounds__(maxThreadsPerBlock,minBlocksPerMultiprocessor) can improve performance in some cases.
The right value for minBlocksPerMultiprocessor should be determined using a detailed per kernel analysis.
Optimal launch bounds for a given kernel will usually differ across major architecture revisions.
The sample code below shows how this is typically handled in device code using the __CUDA_ARCH__ macro introduced in Application Compatibility #define THREADS_PER_BLOCK 256 #if __CUDA_ARCH__ >= 200 #define MY_KERNEL_MAX_THREADS (2 * THREADS_PER_BLOCK) #define MY_KERNEL_MIN_BLOCKS 3 #else #define MY_KERNEL_MAX_THREADS THREADS_PER_BLOCK #define MY_KERNEL_MIN_BLOCKS 2 #endif   Device code __global__ void __launch_bounds__ ( MY_KERNEL_MAX_THREADS , MY_KERNEL_MIN_BLOCKS ) MyKernel (...) { ...
} In the common case where MyKernel is invoked with the maximum number of threads per block (specified as the first parameter of __launch_bounds__() ), it is tempting to use MY_KERNEL_MAX_THREADS as the number of threads per block in the execution configuration:   Host code MyKernel >> (...); This will not work however since __CUDA_ARCH__ is undefined in host code as mentioned in Application Compatibility , so MyKernel will launch with 256 threads per block even when __CUDA_ARCH__ is greater or equal to 200.
Instead the number of threads per block should be determined: Either at compile time using a macro that does not depend on __CUDA_ARCH__ , for example   Host code MyKernel >> (...); Or at runtime based on the compute capability   Host code cudaGetDeviceProperties ( & deviceProp , device ); int threadsPerBlock = ( deviceProp .
major >= 2 ? 2 * THREADS_PER_BLOCK : THREADS_PER_BLOCK ); MyKernel >> (...); Register usage is reported by the --ptxas-options=-v compiler option.
The number of resident blocks can be derived from the occupancy reported by the CUDA profiler (see Device Memory Accesses for a definition of occupancy).
The __launch_bounds__() and __maxnreg__() qualifiers cannot be applied to the same kernel.
Register usage can also be controlled for all __global__ functions in a file using the maxrregcount compiler option.
The value of maxrregcount is ignored for functions with launch bounds. 7.38. Maximum Number of Registers per Thread  To provide a mechanism for low-level performance tuning, CUDA C++ provides the __maxnreg()__ function qualifier to pass performance tuning information to the backend optimizing compiler.
The __maxnreg__() qualifier specifies the maximum number of registers to be allocated to a single thread in a thread block.
In the definition of a __global__ function: __global__ void __maxnreg__ ( maxNumberRegistersPerThread ) MyKernel (...) { ...
} maxNumberRegistersPerThread specifies the maximum number of registers to be allocated to a single thread in a thread block of the kernel MyKernel() ; it compiles to the .maxnreg PTX directive.
The value of maxrregcount is ignored for functions with the __maxnreg__ qualifier. 7.39. #pragma unroll  By default, the compiler unrolls small loops with a known trip count.
The pragma will be ignored if the ICE evaluates to a non-positive integer or to an integer greater than the maximum value representable by the int data type.
Examples: struct S1_t { static const int value = 4 ; }; template __device__ void foo ( int * p1 , int * p2 ) {   no argument specified, loop will be completely unrolled #pragma unroll for ( int i = 0 ; i ( p1 , p2 ); } 7.40.
SIMD Video Instructions  PTX ISA version 3.0 includes SIMD (Single Instruction, Multiple Data) video instructions which operate on pairs of 16-bit values and quads of 8-bit values.
The SIMD video instructions are: vadd2, vadd4 vsub2, vsub4 vavrg2, vavrg4 vabsdiff2, vabsdiff4 vmin2, vmin4 vmax2, vmax4 vset2, vset4 PTX instructions, such as the SIMD video instructions, can be included in CUDA programs by way of the assembler, asm() , statement.
The basic syntax of an asm() statement is: asm ( "template-string" : "constraint" ( output ) : "constraint" ( input ) ")); An example of using the vabsdiff4 PTX instruction is: asm ( "vabsdiff4.u32.u32.u32.add" " %0, %1, %2, %3;" : "=r" ( result ) : "r" ( A ), "r" ( B ), "r" ( C )); This uses the vabsdiff4 instruction to compute an integer quad byte SIMD sum of absolute differences.
The absolute difference value is computed for each byte of the unsigned integers A and B in SIMD fashion.
Refer to the document “Using Inline PTX Assembly in CUDA” for details on using the assembly statement in your code.
Refer to the PTX ISA documentation (“Parallel Thread Execution ISA Version 3.0” for example) for details on the PTX instructions for the version of PTX that you are using. 7.41. Diagnostic Pragmas  The following pragmas may be used to control the error severity used when a given diagnostic message is issued.
#pragma nv_diag_suppress #pragma nv_diag_warning #pragma nv_diag_error #pragma nv_diag_default #pragma nv_diag_once Uses of these pragmas have the following form: #pragma nv_diag_xxx error_number, error_number ...
Any diagnostic may be overridden to be an error, but only warnings may have their severity suppressed or be restored to a warning after being promoted to an error.
The nv_diag_default pragma is used to return the severity of a diagnostic to the one that was in effect before any pragmas were issued (i.e., the normal severity of the message as modified by any command-line options).
The following example suppresses the "declared but never referenced" warning on the declaration of foo : #pragma nv_diag_suppress 177 void foo () { int i = 0 ; } #pragma nv_diag_default 177 void bar () { int i = 0 ; } The following pragmas may be used to save and restore the current diagnostic pragma state: #pragma nv_diagnostic push #pragma nv_diagnostic pop Examples: #pragma nv_diagnostic push #pragma nv_diag_suppress 177 void foo () { int i = 0 ; } #pragma nv_diagnostic pop void bar () { int i = 0 ; } Note that the pragmas only affect the nvcc CUDA frontend compiler; they have no effect on the host compiler.
Removal Notice: The support of diagnostic pragmas without nv_ prefix are removed from CUDA 12.0, if the pragmas are inside the device code, warning unrecognized #pragma in device code will be emitted, otherwise they will be passed to the host compiler.
11 When the enclosing __host__ function is a template, nvcc may currently fail to issue a diagnostic message in some cases; this behavior may change in the future.
12 The intent is to prevent the host compiler from encountering the call to the function if the host compiler does not support it.
13 ( 1 , 2 ) See the C++ Standard for definition of integral constant expression. 8. Cooperative Groups  8.1.
Introduction  Cooperative Groups is an extension to the CUDA programming model, introduced in CUDA 9, for organizing groups of communicating threads.
Cooperative Groups allows developers to express the granularity at which threads are communicating, helping them to express richer, more efficient parallel decompositions.
Historically, the CUDA programming model has provided a single, simple construct for synchronizing cooperating threads: a barrier across all threads of a thread block, as implemented with the __syncthreads() intrinsic function.
However, programmers would like to define and synchronize groups of threads at other granularities to enable greater performance, design flexibility, and software reuse in the form of “collective” group-wide function interfaces.
In an effort to express broader patterns of parallel interaction, many performance-oriented programmers have resorted to writing their own ad hoc and unsafe primitives for synchronizing threads within a single warp, or across sets of thread blocks running on a single GPU.
Whilst the performance improvements achieved have often been valuable, this has resulted in an ever-growing collection of brittle code that is expensive to write, tune, and maintain over time and across GPU generations.
Cooperative Groups addresses this by providing a safe and future-proof mechanism to enable performant code. 8.2. What’s New in Cooperative Groups  8.2.1.
CUDA 12.2  barrier_arrive and barrier_wait member functions were added for grid_group and thread_block .
Description of the API is available here . 8.2.2. CUDA 12.1  invoke_one and invoke_one_broadcast APIs were added.
8.2.3. CUDA 12.0  The following experimental APIs are now moved to the main namespace: asynchronous reduce and scan update added in CUDA 11.7 thread_block_tile larger than 32 added in CUDA 11.1 It is no longer required to provide memory using the block_tile_memory object in order to create these large tiles on Compute Capability 8.0 or higher.
8.3. Programming Model Concept  The Cooperative Groups programming model describes synchronization patterns both within and across CUDA thread blocks.
It provides both the means for applications to define their own groups of threads, and the interfaces to synchronize them.
It also provides new launch APIs that enforce certain restrictions and therefore can guarantee the synchronization will work.
These primitives enable new patterns of cooperative parallelism within CUDA, including producer-consumer parallelism, opportunistic parallelism, and global synchronization across the entire Grid.
The Cooperative Groups programming model consists of the following elements: Data types for representing groups of cooperating threads; Operations to obtain implicit groups defined by the CUDA launch API (e.g., thread blocks); Collectives for partitioning existing groups into new groups; Collective Algorithms for data movement and manipulation (e.g.
memcpy_async, reduce, scan); An operation to synchronize all threads within the group; Operations to inspect the group properties; Collectives that expose low-level, group-specific and often HW accelerated, operations.
The main concept in Cooperative Groups is that of objects naming the set of threads that are part of it.
This expression of groups as first-class program objects improves software composition, since collective functions can receive an explicit object representing the group of participating threads.
This object also makes programmer intent explicit, which eliminates unsound architectural assumptions that result in brittle code, undesirable restrictions upon compiler optimizations, and better compatibility with new GPU generations.
To write efficient code, its best to use specialized groups (going generic loses a lot of compile time optimizations), and pass these group objects by reference to functions that intend to use these threads in some cooperative fashion.
Previously, there were hidden constraints on the implementation when writing this code: __device__ int sum ( int * x , int n ) {   ...
Entire thread block must call sum sum ( x , n ); } All threads in the thread block must arrive at the __syncthreads() barrier, however, this constraint is hidden from the developer who might want to use sum(…) .
With Cooperative Groups, a better way of writing this would be: __device__ int sum ( const thread_block & g , int * x , int n ) {   ...
Entire thread block must call sum thread_block tb = this_thread_block (); sum ( tb , x , n );   ... } 8.4. Group Types  8.4.1.
Regardless of how your kernel is written, it always has a set number of threads, blocks and block dimensions, a single grid and grid dimensions.
In addition, if the multi-device cooperative launch API is used, it can have multiple grids (single grid per device).
These groups provide a starting point for decomposition into finer grained groups which are typically HW accelerated and are more specialized for the problem the developer is solving.
Creating a handle for an implicit group is a collective operation—all threads in the group must participate.
If the group was created in a conditional branch that not all threads reach, this can lead to deadlocks or data corruption.
For this reason, it is recommended that you create a handle for the implicit group upfront (as early as possible, before any branching has occurred) and use that handle throughout the kernel.
Group handles must be initialized at declaration time (there is no default constructor) for the same reason and copy-constructing them is discouraged. 8.4.1.1. Thread Block Group  Any CUDA programmer is already familiar with a certain group of threads: the thread block.
The Cooperative Groups extension introduces a new datatype, thread_block , to explicitly represent this concept within the kernel.
class thread_block; Constructed via: thread_block g = this_thread_block (); Public Member Functions: static void sync() : Synchronize the threads named in the group, equivalent to g.barrier_wait(g.barrier_arrive()) thread_block::arrival_token barrier_arrive() : Arrive on the thread_block barrier, returns a token that needs to be passed into barrier_wait() .
More details here void barrier_wait(thread_block::arrival_token&& t) : Wait on the thread_block barrier, takes arrival token returned from barrier_arrive() as a rvalue reference.
thread_rank () == 0 ) {   load from global into shared for all threads to work with x = ( * globalInput ); }   After loading data into shared memory, you want to synchronize   if all threads in your thread block need to see it g .
sync ();   equivalent to __syncthreads(); } Note: that all threads in the group must participate in collective operations, or the behavior is undefined.
Related: The thread_block datatype is derived from the more generic thread_group datatype, which can be used to represent a wider class of groups. 8.4.1.2. Cluster Group  This group object represents all the threads launched in a single cluster.
class cluster_group; Constructed via: cluster_group g = this_cluster (); Public Member Functions: static void sync() : Synchronize the threads named in the group, equivalent to g.barrier_wait(g.barrier_arrive()) static cluster_group::arrival_token barrier_arrive() : Arrive on the cluster barrier, returns a token that needs to be passed into barrier_wait() .
More details here static void barrier_wait(cluster_group::arrival_token&& t) : Wait on the cluster barrier, takes arrival token returned from barrier_arrive() as a rvalue reference.
APIs other than sync() are available at all times, but to be able to synchronize across the grid, you need to use the cooperative launch API.
class grid_group; Constructed via: grid_group g = this_grid (); Public Member Functions: bool is_valid() const : Returns whether the grid_group can synchronize void sync() const : Synchronize the threads named in the group, equivalent to g.barrier_wait(g.barrier_arrive()) grid_group::arrival_token barrier_arrive() : Arrive on the grid barrier, returns a token that needs to be passed into barrier_wait() .
More details here void barrier_wait(grid_group::arrival_token&& t) : Wait on the grid barrier, takes arrival token returned from barrier_arrive() as a rvalue reference.
Multi Grid Group  This group object represents all the threads launched across all devices of a multi-device cooperative launch.
Unlike the grid.group, all the APIs require that you have used the appropriate launch API.
Thread Block Tile  A templated version of a tiled group, where a template parameter is used to specify the size of the tile - with this known at compile time there is the potential for more optimal execution.
template class thread_block_tile ; Constructed via: template _CG_QUALIFIER thread_block_tile tiled_partition ( const ParentT & g ) Size must be a power of 2 and less than or equal to 1024.
Notes section describes extra steps needed to create tiles of size larger than 32 on hardware with Compute Capability 7.5 or lower.
It is automatically inferred, but a value of void will store this information in the group handle rather than in the type.
Public Member Functions: void sync() const : Synchronize the threads named in the group unsigned long long num_threads() const : Total number of threads in the group unsigned long long thread_rank() const : Rank of the calling thread within [0, num_threads) unsigned long long meta_group_size() const : Returns the number of groups created when the parent group was partitioned.
unsigned long long meta_group_rank() const : Linear rank of the group within the set of tiles partitioned from a parent group (bounded by meta_group_size) T shfl(T var, unsigned int src_rank) const : Refer to Warp Shuffle Functions , Note: For sizes larger than 32 all threads in the group have to specify the same src_rank, otherwise the behavior is undefined.
T shfl_up(T var, int delta) const : Refer to Warp Shuffle Functions , available only for sizes lower or equal to 32.
T shfl_down(T var, int delta) const : Refer to Warp Shuffle Functions , available only for sizes lower or equal to 32.
T shfl_xor(T var, int delta) const : Refer to Warp Shuffle Functions , available only for sizes lower or equal to 32.
T any(int predicate) const : Refer to Warp Vote Functions T all(int predicate) const : Refer to Warp Vote Functions T ballot(int predicate) const : Refer to Warp Vote Functions , available only for sizes lower or equal to 32.
unsigned int match_any(T val) const : Refer to Warp Match Functions , available only for sizes lower or equal to 32.
unsigned int match_all(T val, int &pred) const : Refer to Warp Match Functions , available only for sizes lower or equal to 32.
Legacy member functions (aliases): unsigned long long size() const : Total number of threads in the group (alias of num_threads() ) Notes: thread_block_tile templated data structure is being used here, the size of the group is passed to the tiled_partition call as a template parameter rather than an argument.
shfl, shfl_up, shfl_down, and shfl_xor functions accept objects of any type when compiled with C++11 or later.
This means it’s possible to shuffle non-integral types as long as they satisfy the below constraints: Qualifies as trivially copyable i.e., is_trivially_copyable::value == true sizeof(T) struct block_tile_memory ; MaxBlockSize Specifies the maximal number of threads in the current thread block.
This parameter can be used to minimize the shared memory usage of block_tile_memory in kernels launched only with smaller thread counts.
This block_tile_memory needs be then passed into cooperative_groups::this_thread_block , allowing the resulting thread_block to be partitioned into tiles of sizes larger than 32.
Overload of this_thread_block accepting block_tile_memory argument is a collective operation and has to be called with all threads in the thread_block .
block_tile_memory can be used on hardware with Compute Capability 8.0 or higher in order to be able to write one source targeting multiple different Compute Capabilities.
It should consume no memory when instantiated in shared memory in cases where its not required.
Examples:  / The following code will create two sets of tiled groups, of size 32 and 4 respectively:  / The latter has the provenance encoded in the type, while the first stores it in the handle thread_block block = this_thread_block (); thread_block_tile tile32 = tiled_partition ( block ); thread_block_tile tile4 = tiled_partition ( block );  / The following code will create tiles of size 128 on all Compute Capabilities.
__global__ void kernel (...) {   reserve shared memory for thread_block_tile usage,   specify that block size will be at most 256 threads.
__shared__ block_tile_memory shared ; thread_block thb = this_thread_block ( shared );   Create tiles with 128 threads.
auto tile = tiled_partition ( thb );   ... } 8.4.2.1.1. Warp-Synchronous Code Pattern  Developers might have had warp-synchronous codes that they previously made implicit assumptions about the warp size and would code around that number.
__global__ void cooperative_kernel (...) {   obtain default "current thread block" group thread_block my_block = this_thread_block ();   subdivide into 32-thread, tiled subgroups   Tiled subgroups evenly partition a parent group into   adjacent sets of threads - in this case each one warp in size auto my_tile = tiled_partition ( my_block );   This operation will be performed by only the   first 32-thread tile of each block if ( my_tile .
Single thread group  Group representing the current thread can be obtained from this_thread function: thread_block_tile this_thread (); The following memcpy_async API uses a thread_group , to copy an int element from source to destination: #include #include cooperative_groups :: memcpy_async ( cooperative_groups :: this_thread (), dest , src , sizeof ( int )); More detailed examples of using this_thread to perform asynchronous copies can be found in the Single-Stage Asynchronous Data Copies using cuda::pipeline and Multi-Stage Asynchronous Data Copies using cuda::pipeline sections. 8.4.2.2. Coalesced Groups  In CUDA’s SIMT architecture, at the hardware level the multiprocessor executes threads in groups of 32 called warps.
If there exists a data-dependent conditional branch in the application code such that threads within a warp diverge, then the warp serially executes each branch disabling threads not on that path.
Cooperative Groups has functionality to discover, and create, a group containing all coalesced threads.
It returns the set of active threads at that point in time, and makes no guarantee about which threads are returned (as long as they are active) or that they will stay coalesced throughout execution (they will be brought back together for the execution of a collective but can diverge again afterwards).
class coalesced_group; Constructed via: coalesced_group active = coalesced_threads (); Public Member Functions: void sync() const : Synchronize the threads named in the group unsigned long long num_threads() const : Total number of threads in the group unsigned long long thread_rank() const : Rank of the calling thread within [0, num_threads) unsigned long long meta_group_size() const : Returns the number of groups created when the parent group was partitioned.
unsigned long long meta_group_rank() const : Linear rank of the group within the set of tiles partitioned from a parent group (bounded by meta_group_size).
T shfl(T var, unsigned int src_rank) const : Refer to Warp Shuffle Functions T shfl_up(T var, int delta) const : Refer to Warp Shuffle Functions T shfl_down(T var, int delta) const : Refer to Warp Shuffle Functions T any(int predicate) const : Refer to Warp Vote Functions T all(int predicate) const : Refer to Warp Vote Functions T ballot(int predicate) const : Refer to Warp Vote Functions unsigned int match_any(T val) const : Refer to Warp Match Functions unsigned int match_all(T val, int &pred) const : Refer to Warp Match Functions Legacy member functions (aliases): unsigned long long size() const : Total number of threads in the group (alias of num_threads() ) Notes: shfl, shfl_up, and shfl_down functions accept objects of any type when compiled with C++11 or later.
This means it’s possible to shuffle non-integral types as long as they satisfy the below constraints: Qualifies as trivially copyable i.e.
is_trivially_copyable::value == true sizeof(T) thread_block_tile tiled_partition ( const ParentT & g ); thread_group tiled_partition ( const thread_group & parent , unsigned int tilesz ); The tiled_partition method is a collective operation that partitions the parent group into a one-dimensional, row-major, tiling of subgroups.
A total of ((size(parent)/tilesz) subgroups will be created, therefore the parent group size must be evenly divisible by the Size .
The implementation may cause the calling thread to wait until all the members of the parent group have invoked the operation before resuming execution.
Functionality is limited to native hardware sizes, 1/2/4/8/16/32 and the cg::size(parent) must be greater than the Size parameter.
The templated version of tiled_partition supports 64/128/256/512 sizes as well, but some additional steps are required on Compute Capability 7.5 or lower, refer to Thread Block Tile for details.
Codegen Requirements: Compute Capability 5.0 minimum, C++11 for sizes larger than 32 Example:  / The following code will create a 32-thread tile thread_block block = this_thread_block (); thread_block_tile tile32 = tiled_partition ( block ); We can partition each of these groups into even smaller groups, each of size 4 threads: auto tile4 = tiled_partition ( tile32 );   or using a general group   thread_group tile4 = tiled_partition(tile32, 4); If, for instance, if we were to then include the following line of code: if ( tile4 .
thread_rank () == 0 ) printf ( "Hello from tile4 rank 0   " ); then the statement would be printed by every fourth thread in the block: the threads of rank 0 in each tile4 group, which correspond to those threads with ranks 0,4,8,12,etc.
in the block group. 8.5.2. labeled_partition  template coalesced_group labeled_partition ( const coalesced_group & g , Label label ); template coalesced_group labeled_partition ( const thread_block_tile & g , Label label ); The labeled_partition method is a collective operation that partitions the parent group into one-dimensional subgroups within which the threads are coalesced.
The implementation will evaluate a condition label and assign threads that have the same value for label into the same group.
binary_partition  coalesced_group binary_partition ( const coalesced_group & g , bool pred ); template coalesced_group binary_partition ( const thread_block_tile & g , bool pred ); The binary_partition() method is a collective operation that partitions the parent group into one-dimensional subgroups within which the threads are coalesced.
The implementation will evaluate a predicate and assign threads that have the same value into the same group.
Codegen Requirements: Compute Capability 7.0 minimum, C++11 Example:  / This example divides a 32-sized tile into a group with odd  / numbers and a group with even numbers _global__ void oddEven ( int * inputArr ) { auto block = cg :: this_thread_block (); auto tile32 = cg :: tiled_partition ( block );   inputArr contains random integers int elem = inputArr [ block .
thread_rank ()];   after this, tile32 is split into 2 groups,   a subtile where elem&1 is true and one where its false auto subtile = cg :: binary_partition ( tile32 , ( elem & 1 )); } 8.6.
Group Collectives  Cooperative Groups library provides a set of collective operations that can be performed by a group of threads.
These operations require participation of all threads in the specified group in order to complete the operation.
All threads in the group need to pass the same values for corresponding arguments to each collective call, unless different values are explicitly allowed in the argument description.
barrier_arrive and barrier_wait  T :: arrival_token T::barrier_arrive (); void T::barrier_wait ( T :: arrival_token && ); barrier_arrive and barrier_wait member functions provide a synchronization API similar to cuda::barrier (read more) .
Cooperative Groups automatically initializes the group barrier, but arrive and wait operations have an additional restriction resulting from collective nature of those operations: All threads in the group must arrive and wait at the barrier once per phase.
When barrier_arrive is called with a group, result of calling any collective operation or another barrier arrival with that group is undefined until completion of the barrier phase is observed with barrier_wait call.
Threads blocked on barrier_wait might be released from the synchronization before other threads call barrier_wait , but only after all threads in the group called barrier_arrive .
Group type T can be any of the implicit groups .This allows threads to do independent work after they arrive and before they wait for the synchronization to resolve, allowing to hide some of the synchronization latency.
barrier_arrive returns an arrival_token object that must be passed into the corresponding barrier_wait .
Example of barrier_arrive and barrier_wait used to synchronize initization of shared memory across the cluster: #include using namespace cooperative_groups ; void __device__ init_shared_data ( const thread_block & block , int * data ); void __device__ local_processing ( const thread_block & block ); void __device__ process_shared_data ( const thread_block & block , int * data ); __global__ void cluster_kernel () { extern __shared__ int array []; auto cluster = this_cluster (); auto block = this_thread_block ();   Use this thread block to initialize some shared state init_shared_data ( block , & array [ 0 ]); auto token = cluster .
barrier_arrive ();   Let other blocks know this block is running and data was initialized   Do some local processing to hide the synchronization latency local_processing ( block );   Map data in shared memory from the next block in the cluster int * dsmem = cluster .
num_blocks ());   Make sure all other blocks in the cluster are running and initialized shared data before accessing dsmem cluster .
barrier_wait ( std :: move ( token ));   Consume data in distributed shared memory process_shared_data ( block , dsmem ); cluster .
sync  static void T::sync (); template void sync ( T & group ); sync synchronizes the threads named in the group.
Group type T can be any of the existing group types, as all of them support synchronization.
Its available as a member function in every group type or as a free function taking a group as parameter.
If the group is a grid_group or a multi_grid_group the kernel must have been launched using the appropriate cooperative launch APIs.
memcpy_async  memcpy_async is a group-wide collective memcpy that utilizes hardware accelerated support for non-blocking memory transactions from global to shared memory.
Given a set of threads named in the group, memcpy_async will move specified amount of bytes or elements of the input type through a single pipeline stage.
Additionally for achieving best performance when using the memcpy_async API, an alignment of 16 bytes for both shared memory and global memory is required.
It is important to note that while this is a memcpy in the general case, it is only asynchronous if the source is global memory and the destination is shared memory and both can be addressed with 16, 8, or 4 byte alignments.
Asynchronously copied data should only be read following a call to wait or wait_prior which signals that the corresponding stage has completed moving data to shared memory.
Having to wait on all outstanding requests can lose some flexibility (but gain simplicity).
In order to efficiently overlap data transfer and execution, its important to be able to kick off an N+1 memcpy_async request while waiting on and operating on request N .
To do so, use memcpy_async and wait on it using the collective stage-based wait_prior API.
Usage 1 template void memcpy_async ( const TyGroup & group , TyElem * __restrict__ _dst , const TyElem * __restrict__ _src , const TyShape & shape ); Performs a copy of ``shape`` bytes .
Usage 2 template void memcpy_async ( const TyGroup & group , TyElem * __restrict__ dst , const TyDstLayout & dstLayout , const TyElem * __restrict__ src , const TySrcLayout & srcLayout ); Performs a copy of ``min(dstLayout, srcLayout)`` elements .
Errata The memcpy_async API introduced in CUDA 11.1 with both src and dst input layouts, expects the layout to be provided in elements rather than bytes.
If cuda::aligned_size_t type is used as the layout, the number of elements specified times sizeof(TyElem) must be a multiple of N and it is recommended to use std::byte or char as the element type.
If specified shape or layout of the copy is of type cuda::aligned_size_t , alignment will be guaranteed to be at least min(16, N) .
In that case both dst and src pointers need to be aligned to N bytes and the number of bytes copied needs to be a multiple of N.
Codegen Requirements: Compute Capability 5.0 minimum, Compute Capability 8.0 for asynchronicity, C++11 cooperative_groups/memcpy_async.h header needs to be included.
Example:  / This example streams elementsPerThreadBlock worth of data from global memory  / into a limited sized shared memory (elementsInShared) block to operate on.
#include #include namespace cg = cooperative_groups ; __global__ void kernel ( int * global_data ) { cg :: thread_block tb = cg :: this_thread_block (); const size_t elementsPerThreadBlock = 16 * 1024 ; const size_t elementsInShared = 128 ; __shared__ int local_smem [ elementsInShared ]; size_t copy_count ; size_t index = 0 ; while ( index void wait ( TyGroup & group ); template void wait_prior ( TyGroup & group ); wait and wait_prior collectives allow to wait for memcpy_async copies to complete.
wait_prior allows that the latest NumStages are still not done and waits for all the previous requests.
So with N total copies requested, it waits until the first N-NumStages are done and the last NumStages might still be in progress.
Codegen Requirements: Compute Capability 5.0 minimum, Compute Capability 8.0 for asynchronicity, C++11 cooperative_groups/memcpy_async.h header needs to be included.
Example:  / This example streams elementsPerThreadBlock worth of data from global memory  / into a limited sized shared memory (elementsInShared) block to operate on in  / multiple (two) stages.
#include #include namespace cg = cooperative_groups ; __global__ void kernel ( int * global_data ) { cg :: thread_block tb = cg :: this_thread_block (); const size_t elementsPerThreadBlock = 16 * 1024 + 64 ; const size_t elementsInShared = 128 ; __align__ ( 16 ) __shared__ int local_smem [ 2 ][ elementsInShared ]; int stage = 0 ;   First kick off an extra request size_t copy_count = elementsInShared ; size_t index = copy_count ; cg :: memcpy_async ( tb , local_smem [ stage ], elementsInShared , global_data , elementsPerThreadBlock - index ); while ( index ( tb );   Its now available and we can work with local_smem[stage] here   (...)     Calculate the amount fo data that was actually copied, for the next iteration.
copy_count = min ( elementsInShared , elementsPerThreadBlock - index ); index += copy_count ;   A cg::sync(tb) might be needed here depending on whether   the work done with local_smem[stage] can release threads to race ahead or not   Wrap to the next stage stage ^= 1 ; } cg :: wait ( tb );   The last local_smem[stage] can be handled here } 8.6.3.
reduce  template auto reduce ( const TyGroup & group , TyArg && val , TyOp && op ) -> decltype ( op ( val , val )); reduce performs a reduction operation on the data provided by each thread named in the group passed in.
This takes advantage of hardware acceleration (on compute 80 and higher devices) for the arithmetic add, min, or max operations and the logical AND, OR, or XOR, as well as providing a software fallback on older generation hardware.
val : Any type that satisfies the below requirements: Qualifies as trivially copyable i.e.
Reduce also supports lambdas and other function objects that can be invoked using operator() Asynchronous reduce template void reduce_update_async ( const TyGroup & group , TyAtomic & atomic , TyArg && val , TyOp && op ); template void reduce_store_async ( const TyGroup & group , TyAtomic & atomic , TyArg && val , TyOp && op ); template void reduce_store_async ( const TyGroup & group , TyArg * ptr , TyArg && val , TyOp && op ); *_async variants of the API are asynchronously calculating the result to either store to or update a specified destination by one of the participating threads, instead of returning it by each thread.
To observe the effect of these asynchronous calls, calling group of threads or a larger group containing them need to be synchronized.
In case of the atomic store or update variant, atomic argument can be either of cuda::atomic or cuda::atomic_ref available in CUDA C++ Standard Library .
This variant of the API is available only on platforms and devices, where these types are supported by the CUDA C++ Standard Library.
Result of the reduction is used to atomically update the atomic according to the specified op , eg.
Scope of the atomic must include all the threads in the group and if multiple groups are using the same atomic concurrently, scope must include all threads in all groups using it.
In case of the pointer store variant, result of the reduction will be weakly stored into the dst pointer.
Codegen Requirements: Compute Capability 5.0 minimum, Compute Capability 8.0 for HW acceleration, C++11.
Example of approximate standard deviation for integer vector: #include #include namespace cg = cooperative_groups ;  / Calculate approximate standard deviation of integers in vec __device__ int std_dev ( const cg :: thread_block_tile & tile , int * vec , int length ) { int thread_sum = 0 ;   calculate average first for ( int i = tile .
thread_rank (); i allows cg::reduce() to know it can use hardware acceleration for addition int avg = cg :: reduce ( tile , thread_sum , cg :: plus ()) / length ; int thread_diffs_sum = 0 ; for ( int i = tile .
thread_rank (); i ( cg :: reduce ( tile , thread_diffs_sum , cg :: plus ())) / length ; return static_cast ( sqrtf ( diff_sum )); } Example of block wide reduction: #include #include namespace cg = cooperative_groups ;  / The following example accepts input in *A and outputs a result into *sum  / It spreads the data equally within the block __device__ void block_reduce ( const int * A , int count , cuda :: atomic & total_sum ) { auto block = cg :: this_thread_block (); auto tile = cg :: tiled_partition ( block ); int thread_sum = 0 ;   Stride loop over all values, each thread accumulates its part of the array.
thread_rank (); i allows cg::reduce() to know it can use hardware acceleration for addition cg :: reduce_update_async ( tile , total_sum , thread_sum , cg :: plus ());   synchronize the block, to ensure all async reductions are ready block .
Reduce Operators  Below are the prototypes of function objects for some of the basic operations that can be done with reduce namespace cooperative_groups { template struct cg :: plus ; template struct cg :: less ; template struct cg :: greater ; template struct cg :: bit_and ; template struct cg :: bit_xor ; template struct cg :: bit_or ; } Reduce is limited to the information available to the implementation at compile time.
Thus in order to make use of intrinsics introduced in CC 8.0, the cg:: namespace exposes several functional objects that mirror the hardware.
These objects appear similar to those presented in the C++ STL, with the exception of less/greater .
The reason for any difference from the STL is that these function objects are designed to actually mirror the operation of the hardware intrinsics.
Functional description: cg::plus: Accepts two values and returns the sum of both using operator+.
cg::less: Accepts two values and returns the lesser using operator is specialized within cg::reduce and calls __reduce_add_sync(...) on CC 8.0+ cg :: reduce ( tile , ( int ) val , cg :: plus ());   cg::plus fails to match with an accelerator and instead performs a standard shuffle based reduction cg :: reduce ( tile , ( float ) val , cg :: plus ());   While individual components of a vector are supported, reduce will not use hardware intrinsics for the following   It will also be necessary to define a corresponding operator for vector and any custom types that may be used int4 vec = {...}; cg :: reduce ( tile , vec , cg :: plus ())   Finally lambdas and other function objects cannot be inspected for dispatch   and will instead perform shuffle based reductions using the provided function object.
cg :: reduce ( tile , ( int ) val , []( int l , int r ) -> int { return l + r ;}); } 8.6.3.3.
inclusive_scan and exclusive_scan  template auto inclusive_scan ( const TyGroup & group , TyVal && val , TyFn && op ) -> decltype ( op ( val , val )); template TyVal inclusive_scan ( const TyGroup & group , TyVal && val ); template auto exclusive_scan ( const TyGroup & group , TyVal && val , TyFn && op ) -> decltype ( op ( val , val )); template TyVal exclusive_scan ( const TyGroup & group , TyVal && val ); inclusive_scan and exclusive_scan performs a scan operation on the data provided by each thread named in the group passed in.
Result for each thread is a reduction of data from threads with lower thread_rank than that thread in case of exclusive_scan .
inclusive_scan and exclusive_scan also supports lambdas and other function objects that can be invoked using operator() .
Scan update template auto inclusive_scan_update ( const TyGroup & group , TyAtomic & atomic , TyVal && val , TyFn && op ) -> decltype ( op ( val , val )); template TyVal inclusive_scan_update ( const TyGroup & group , TyAtomic & atomic , TyVal && val ); template auto exclusive_scan_update ( const TyGroup & group , TyAtomic & atomic , TyVal && val , TyFn && op ) -> decltype ( op ( val , val )); template TyVal exclusive_scan_update ( const TyGroup & group , TyAtomic & atomic , TyVal && val ); *_scan_update collectives take an additional argument atomic that can be either of cuda::atomic or cuda::atomic_ref available in CUDA C++ Standard Library .
These variants of the API are available only on platforms and devices, where these types are supported by the CUDA C++ Standard Library.
These variants will perform an update to the atomic according to op with value of the sum of input values of all threads in the group.
Previous value of the atomic will be combined with the result of scan by each thread and returned.
Following pseudocode illustrates how the update variant of scan works: /* inclusive_scan_update behaves as the following block, except both reduce and inclusive_scan is calculated simultaneously.
auto total = reduce(group, val, op); TyVal old; if (group.thread_rank() == selected_thread) { atomicaly { old = atomic.load(); atomic.store(op(old, total)); } } old = group.shfl(old, selected_thread); return op(inclusive_scan(group, val, op), old); */ Codegen Requirements: Compute Capability 5.0 minimum, C++11.
Example: #include #include #include namespace cg = cooperative_groups ; __global__ void kernel () { auto thread_block = cg :: this_thread_block (); auto tile = cg :: tiled_partition ( thread_block ); unsigned int val = cg :: inclusive_scan ( tile , tile .
thread_rank (), val ); } /* prints for each group: 0: 0 1: 1 2: 3 3: 6 4: 10 5: 15 6: 21 7: 28 */ Example of stream compaction using exclusive_scan: #include #include namespace cg = cooperative_groups ;   put data from input into output only if it passes test_fn predicate template __device__ int stream_compaction ( Group & g , Data * input , int count , TyFn && test_fn , Data * output ) { int per_thread = count / g .
thread_rank () * per_thread , count ); int my_count = min ( per_thread , count - thread_start );   get all passing items from my part of the input   into a contagious part of the array and count them.
int i = thread_start ; while ( i #include namespace cg = cooperative_groups ;   Buffer partitioning is static to make the example easier to follow,   but any arbitrary dynamic allocation scheme can be implemented by replacing this function.
__device__ int calculate_buffer_space_needed ( cg :: thread_block_tile & tile ) { return tile .
thread_rank () % 2 + 1 ; } __device__ int my_thread_data ( int i ) { return i ; } __global__ void kernel () { __shared__ extern int buffer []; __shared__ cuda :: atomic buffer_used ; auto block = cg :: this_thread_block (); auto tile = cg :: tiled_partition ( block ); buffer_used = 0 ; block .
sync ();   each thread calculates buffer size it needs int buf_needed = calculate_buffer_space_needed ( tile );   scan over the needs of each thread, result for each thread is an offset   of that thread’s part of the buffer.
buffer_used is atomically updated with   the sum of all thread's inputs, to correctly offset other tile’s allocations int buf_offset = cg :: exclusive_scan_update ( tile , buffer_used , buf_needed );   each thread fills its own part of the buffer with thread specific data for ( int i = 0 ; i void invoke_one ( const Group & group , Fn && fn , Args && ...
args ) -> decltype ( fn ( args ...)); invoke_one selects a single arbitrary thread from the calling group and uses that thread to call the supplied invocable fn with the supplied arguments args .
In case of invoke_one_broadcast the result of the call is also distributed to all threads in the group and returned from this collective.
Calling group can be synchronized with the selected thread before and/or after it calls the supplied invocable.
It means that communication within the calling group is not allowed inside the supplied invocable body, otherwise forward progress is not guaranteed.
Communication with threads outside of the calling group is allowed in the body of the supplied invocable.
On devices with Compute Capability 9.0 or higher hardware acceleration might be used to select the thread when called with explicit group types .
group : All group types are valid for invoke_one , coalesced_group and thread_block_tile are valid for invoke_one_broadcast .
args : Parameter pack of types matching types of parameters of the supplied invocable fn .
In case of invoke_one_broadcast the return type of the supplied invocable fn must satisfy the below requirements: Qualifies as trivially copyable i.e.
is_trivially_copyable::value == true sizeof(T) #include namespace cg = cooperative_groups ; template __device__ unsigned int atomicAddOneRelaxed ( cuda :: atomic & atomic ) { auto g = cg :: coalesced_threads (); auto prev = cg :: invoke_one_broadcast ( g , [ & ] () { return atomic .
Grid Synchronization  Prior to the introduction of Cooperative Groups, the CUDA programming model only allowed synchronization between thread blocks at a kernel completion boundary.
The kernel boundary carries with it an implicit invalidation of state, and with it, potential performance implications.
For example, in certain use cases, applications have a large number of small kernels, with each kernel representing a stage in a processing pipeline.
The presence of these kernels is required by the current CUDA programming model to ensure that the thread blocks operating on one pipeline stage have produced data before the thread block operating on the next pipeline stage is ready to consume it.
In such cases, the ability to provide global inter thread block synchronization would allow the application to be restructured to have persistent thread blocks, which are able to synchronize on the device when a given stage is complete.
To synchronize across the grid, from within a kernel, you would simply use the grid.sync() function: grid_group grid = this_grid (); grid .
sync (); And when launching the kernel it is necessary to use, instead of the >> execution configuration syntax, the cudaLaunchCooperativeKernel CUDA runtime launch API or the CUDA driver equivalent .
Example: To guarantee co-residency of the thread blocks on the GPU, the number of blocks launched needs to be carefully considered.
For example, as many blocks as there are SMs can be launched as follows: int dev = 0 ; cudaDeviceProp deviceProp ; cudaGetDeviceProperties ( & deviceProp , dev );   initialize, then launch cudaLaunchCooperativeKernel (( void * ) my_kernel , deviceProp .
multiProcessorCount , numThreads , args ); Alternatively, you can maximize the exposed parallelism by calculating how many blocks can fit simultaneously per-SM using the occupancy calculator as follows:  / This will launch a grid that can maximally fill the GPU, on the default stream with kernel arguments int numBlocksPerSm = 0 ;   Number of threads my_kernel will be launched with int numThreads = 128 ; cudaDeviceProp deviceProp ; cudaGetDeviceProperties ( & deviceProp , dev ); cudaOccupancyMaxActiveBlocksPerMultiprocessor ( & numBlocksPerSm , my_kernel , numThreads , 0 );   launch void * kernelArgs [] = { /* add kernel args */ }; dim3 dimBlock ( numThreads , 1 , 1 ); dim3 dimGrid ( deviceProp .
multiProcessorCount * numBlocksPerSm , 1 , 1 ); cudaLaunchCooperativeKernel (( void * ) my_kernel , dimGrid , dimBlock , kernelArgs ); It is good practice to first ensure the device supports cooperative launches by querying the device attribute cudaDevAttrCooperativeLaunch : int dev = 0 ; int supportsCoopLaunch = 0 ; cudaDeviceGetAttribute ( & supportsCoopLaunch , cudaDevAttrCooperativeLaunch , dev ); which will set supportsCoopLaunch to 1 if the property is supported on device 0.
In addition, you need to be running on either of these: The Linux platform without MPS The Linux platform with MPS and on a device with compute capability 7.0 or higher The latest Windows platform 8.8.
Multi-Device Synchronization  In order to enable synchronization across multiple devices with Cooperative Groups, use of the cudaLaunchCooperativeKernelMultiDevice CUDA API is required.
This, a significant departure from existing CUDA APIs, will allow a single host thread to launch a kernel across multiple devices.
In addition to the constraints and guarantees made by cudaLaunchCooperativeKernel , this API has additional semantics: This API will ensure that a launch is atomic, i.e.
if the API call succeeds, then the provided number of thread blocks will launch on all specified devices.
No explicit checks are done by the driver in this regard because it is largely not feasible.
All devices being targeted by this launch must be of the same compute capability - major and minor versions.
The block size, grid size and amount of shared memory per grid must be the same across all devices.
Note that this means the maximum number of blocks that can be launched per device will be limited by the device with the least number of SMs.
Any user defined __device__ , __constant__ or __managed__ device global variables present in the module that owns the CUfunction being launched are independently instantiated on every device.
Deprecation Notice: cudaLaunchCooperativeKernelMultiDevice has been deprecated in CUDA 11.3 for all devices.
Example of an alternative approach can be found in the multi device conjugate gradient sample.
Optimal performance in multi-device synchronization is achieved by enabling peer access via cuCtxEnablePeerAccess or cudaDeviceEnablePeerAccess for all participating devices.
The launch parameters should be defined using an array of structs (one per device), and launched with cudaLaunchCooperativeKernelMultiDevice Example: cudaDeviceProp deviceProp ; cudaGetDeviceCount ( & numGpus );   Per device launch parameters cudaLaunchParams * launchParams = ( cudaLaunchParams * ) malloc ( sizeof ( cudaLaunchParams ) * numGpus ); cudaStream_t * streams = ( cudaStream_t * ) malloc ( sizeof ( cudaStream_t ) * numGpus );   The kernel arguments are copied over during launch   Its also possible to have individual copies of kernel arguments per device, but   the signature and name of the function/kernel must be the same.
void * kernelArgs [] = { /* Add kernel arguments */ }; for ( int i = 0 ; i >> ( data ); tail_launch >> ( data ); } } void host_launch ( int * data ) { parent_launch >> ( data ); } 9.2.2.1.2.
Zero Copy Memory  Zero-copy system memory has identical coherence and consistency guarantees to global memory, and follows the semantics detailed above.
A kernel may not allocate or free zero-copy memory, but may use pointers to zero-copy passed in from the host program. 9.2.2.1.3. Constant Memory  Constants may not be modified from the device.
They may only be modified from the host, but the behavior of modifying a constant from the host while there is a concurrent grid that access that constant at any point during its lifetime is undefined. 9.2.2.1.4. Shared and Local Memory  Shared and Local memory is private to a thread block or thread, respectively, and is not visible or coherent between parent and child.
Behavior is undefined when an object in one of these locations is referenced outside of the scope within which it belongs, and may cause an error.
The NVIDIA compiler will attempt to warn if it can detect that a pointer to local or shared memory is being passed as an argument to a kernel launch.
At runtime, the programmer may use the __isGlobal() intrinsic to determine whether a pointer references global memory and so may safely be passed to a child launch.
Note that calls to cudaMemcpy*Async() or cudaMemset*Async() may invoke new child kernels on the device in order to preserve stream semantics.
As such, passing shared or local memory pointers to these APIs is illegal and will return an error. 9.2.2.1.5. Local Memory  Local memory is private storage for an executing thread, and is not visible outside of that thread.
It is illegal to pass a pointer to local memory as a launch argument when launching a child kernel.
For example the following is illegal, with undefined behavior if x_array is accessed by child_launch : int x_array [ 10 ];   Creates x_array in parent's local memory child_launch >> ( x_array ); It is sometimes difficult for a programmer to be aware of when a variable is placed into local memory by the compiler.
As a general rule, all storage passed to a child kernel should be allocated explicitly from the global-memory heap, either with cudaMalloc() , new() or by declaring __device__ storage at global scope.
For example:   Correct - "value" is global storage __device__ int value ; __device__ void x () { value = 5 ; child >> ( & value ); }   Invalid - "value" is local storage __device__ void y () { int value = 5 ; child >> ( & value ); } 9.2.2.1.6.
Texture Memory  Writes to the global memory region over which a texture is mapped are incoherent with respect to texture accesses.
Coherence for texture memory is enforced at the invocation of a child grid and when a child grid completes.
This means that writes to memory prior to a child kernel launch are reflected in texture memory accesses of the child.
Similarly to Global Memory above, writes to memory by a child are never guaranteed to be reflected in the texture memory accesses by a parent.
The only way to access the modifications made by the threads in the child grid before the parent grid exits is via a kernel launched into the cudaStreamTailLaunch stream.
Concurrent accesses by parent and child may result in inconsistent data. 9.3. Programming Interface  9.3.1.
CUDA C++ Reference  This section describes changes and additions to the CUDA C++ language extensions for supporting Dynamic Parallelism .
The language interface and API available to CUDA kernels using CUDA C++ for Dynamic Parallelism, referred to as the Device Runtime , is substantially like that of the CUDA Runtime API available on the host.
Where possible the syntax and semantics of the CUDA Runtime API have been retained in order to facilitate ease of code reuse for routines that may run in either the host or device environments.
This enables each thread to make unique, dynamic decisions regarding what kernel or operation to execute next.
There are no synchronization requirements between threads within a block to execute any of the provided device runtime APIs, which enables the device runtime API functions to be called in arbitrarily divergent kernel code without deadlock. 9.3.1.1. Device-Side Kernel Launch  Kernels may be launched from the device using the standard CUDA >> syntax: kernel_name >> ([ kernel arguments ]); Dg is of type dim3 and specifies the dimensions and size of the grid Db is of type dim3 and specifies the dimensions and size of each thread block Ns is of type size_t and specifies the number of bytes of shared memory that is dynamically allocated per thread block for this call in addition to statically allocated memory.
S is an optional argument that defaults to the NULL stream. 9.3.1.1.1. Launches are Asynchronous  Identical to host-side launches, all device-side kernel launches are asynchronous with respect to the launching thread.
That is to say, the >> launch command will return immediately and the launching thread will continue to execute until it hits an implicit launch-synchronization point (such as at a kernel launched into the cudaStreamTailLaunch stream).
The child grid launch is posted to the device and will execute independently of the parent thread.
The child grid may begin execution at any time after launch, but is not guaranteed to begin execution until the launching thread reaches an implicit launch-synchronization point. 9.3.1.1.2. Launch Environment Configuration  All global device configuration settings (for example, shared memory and L1 cache size as returned from cudaDeviceGetCacheConfig() , and device limits returned from cudaDeviceGetLimit() ) will be inherited from the parent.
For host-launched kernels, per-kernel configurations set from the host will take precedence over the global setting.
It is not possible to reconfigure a kernel’s environment from the device. 9.3.1.2. Streams  Both named and unnamed (NULL) streams are available from the device runtime.
Named streams may be used by any thread within a grid, but stream handles may not be passed to other child/parent kernels.
Similar to host-side launch, work launched into separate streams may run concurrently, but actual concurrency is not guaranteed.
Programs that depend upon concurrency between child kernels are not supported by the CUDA programming model and will have undefined behavior.
The host-side NULL stream’s cross-stream barrier semantic is not supported on the device (see below for details).
In order to retain semantic compatibility with the host runtime, all device streams must be created using the cudaStreamCreateWithFlags() API, passing the cudaStreamNonBlocking flag.
The cudaStreamCreate() call is a host-runtime- only API and will fail to compile for the device.
As cudaStreamSynchronize() and cudaStreamQuery() are unsupported by the device runtime, a kernel launched into the cudaStreamTailLaunch stream should be used instead when the application needs to know that stream-launched child kernels have completed. 9.3.1.2.1. The Implicit (NULL) Stream  Within a host program, the unnamed (NULL) stream has additional barrier synchronization semantics with other streams (see Default Stream for details).
The device runtime offers a single implicit, unnamed stream shared between all threads in a thread block, but as all named streams must be created with the cudaStreamNonBlocking flag, work launched into the NULL stream will not insert an implicit dependency on pending work in any other streams (including NULL streams of other thread blocks). 9.3.1.2.2. The Fire-and-Forget Stream  The fire-and-forget named stream ( cudaStreamFireAndForget ) allows the user to launch fire-and-forget work with less boilerplate and without stream tracking overhead.
It is functionally identical to, but faster than, creating a new stream per launch, and launching into that stream.
Fire-and-forget launches are immediately scheduled for launch without any dependency on the completion of previously launched grids.
No other grid launches can depend on the completion of a fire-and-forget launch, except through the implicit synchronization at the end of the parent grid.
So a tail launch or the next grid in parent grid’s stream won’t launch before a parent grid’s fire-and-forget work has completed.
The fire-and-forget stream is not supported when compiled with CUDA_FORCE_CDP1_IF_SUPPORTED defined.
Fire-and-forget stream usage requires compilation to be in 64-bit mode. 9.3.1.2.3. The Tail Launch Stream  The tail launch named stream ( cudaStreamTailLaunch ) allows a grid to schedule a new grid for launch after its completion.
It should be possible to to use a tail launch to achieve the same functionality as a cudaDeviceSynchronize() in most cases.
All non-tail launch work launched by a grid is implicitly synchronized before the tail stream is kicked off.
A parent grid’s tail launch does not launch until the parent grid and all work launched by the parent grid to ordinary streams or per-thread or fire-and-forget streams have completed.
If two grids are launched to the same grid’s tail launch stream, the later grid does not launch until the earlier grid and all its descendent work has completed.
); } Grids launched into the tail launch stream will not launch until the completion of all work by the parent grid, including all other grids (and their descendants) launched by the parent in all non-tail launched streams, including work executed or launched after the tail launch.
) } The next grid in the parent grid’s stream will not be launched before a parent grid’s tail launch work has completed.
In other words, the tail launch stream behaves as if it were inserted between its parent grid and the next grid in its parent grid’s stream.
In this example, C1 and C2 will launch concurrently after P's completion __global__ void T ( ...
The tail launch stream is not supported when compiled with CUDA_FORCE_CDP1_IF_SUPPORTED defined.
Tail launch stream usage requires compilation to be in 64-bit mode. 9.3.1.3. Events  Only the inter-stream synchronization capabilities of CUDA events are supported.
This means that cudaStreamWaitEvent() is supported, but cudaEventSynchronize() , cudaEventElapsedTime() , and cudaEventQuery() are not.
As cudaEventElapsedTime() is not supported, cudaEvents must be created via cudaEventCreateWithFlags() , passing the cudaEventDisableTiming flag.
As with named streams, event objects may be shared between all threads within the grid which created them but are local to that grid and may not be passed to other kernels.
Event handles are not guaranteed to be unique between grids, so using an event handle within a grid that did not create it will result in undefined behavior. 9.3.1.4. Synchronization  It is up to the program to perform sufficient inter-thread synchronization, for example via a CUDA Event, if the calling thread is intended to synchronize with child grids invoked from other threads.
As it is not possible to explicitly synchronize child work from a parent thread, there is no way to guarantee that changes occuring in child grids are visible to threads within the parent grid. 9.3.1.5. Device Management  Only the device on which a kernel is running will be controllable from that kernel.
This means that device APIs such as cudaSetDevice() are not supported by the device runtime.
The active device as seen from the GPU (returned from cudaGetDevice() ) will have the same device number as seen from the host system.
The cudaDeviceGetAttribute() call may request information about another device as this API allows specification of a device ID as a parameter of the call.
Note that the catch-all cudaGetDeviceProperties() API is not offered by the device runtime - properties must be queried individually. 9.3.1.6. Memory Declarations  9.3.1.6.1.
Device and Constant Memory  Memory declared at file scope with __device__ or __constant__ memory space specifiers behaves identically when using the device runtime.
All kernels may read or write device variables, whether the kernel was initially launched by the host or device runtime.
Equivalently, all kernels will have the same view of __constant__ s as declared at the module scope. 9.3.1.6.2. Textures and Surfaces  CUDA supports dynamically created texture and surface objects 14 , where a texture object may be created on the host, passed to a kernel, used by that kernel, and then destroyed from the host.
The device runtime does not allow creation or destruction of texture or surface objects from within device code, but texture and surface objects created from the host may be used and passed around freely on the device.
Regardless of where they are created, dynamically created texture objects are always valid and may be passed to child kernels from a parent.
Note The device runtime does not support legacy module-scope (i.e., Fermi-style) textures and surfaces within a kernel launched from the device.
Module-scope (legacy) textures may be created from the host and used in device code as for any kernel, but may only be used by a top-level kernel (i.e., the one which is launched from the host). 9.3.1.6.3. Shared Memory Variable Declarations  In CUDA C++ shared memory can be declared either as a statically sized file-scope or function-scoped variable, or as an extern variable with the size determined at runtime by the kernel’s caller via a launch configuration argument.
__global__ void permute ( int n , int * data ) { extern __shared__ int smem []; if ( n >> ( n / 2 , data ); permute >> ( n / 2 , data + n / 2 ); } } void host_launch ( int * data ) { permute >> ( 256 , data ); } 9.3.1.6.4.
Symbol Addresses  Device-side symbols (i.e., those marked __device__ ) may be referenced from within a kernel simply via the & operator, as all global-scope device variables are in the kernel’s visible address space.
This also applies to __constant__ symbols, although in this case the pointer will reference read-only data.
Given that device-side symbols can be referenced directly, those CUDA runtime APIs which reference symbols (e.g., cudaMemcpyToSymbol() or cudaGetSymbolAddress() ) are redundant and hence not supported by the device runtime.
Note this implies that constant data cannot be altered from within a running kernel, even ahead of a child kernel launch, as references to __constant__ space are read-only. 9.3.1.7. API Errors and Launch Failures  As usual for the CUDA runtime, any function may return an error code.
The last error code returned is recorded and may be retrieved via the cudaGetLastError() call.
Errors are recorded per-thread, so that each thread can identify the most recent error that it has generated.
Similar to a host-side launch, device-side launches may fail for many reasons (invalid arguments, etc).
The user must call cudaGetLastError() to determine if a launch generated an error, however lack of an error after launch does not imply the child kernel completed successfully.
For device-side exceptions, e.g., access to an invalid address, an error in a child grid will be returned to the host. 9.3.1.7.1. Launch Setup APIs  Kernel launch is a system-level mechanism exposed through the device runtime library, and as such is available directly from PTX via the underlying cudaGetParameterBuffer() and cudaLaunchDevice() APIs.
It is permitted for a CUDA application to call these APIs itself, with the same requirements as for PTX.
In both cases, the user is then responsible for correctly populating all necessary data structures in the correct format according to specification.
As with host-side launch, the device-side operator >> maps to underlying kernel launch APIs.
This is so that users targeting PTX will be able to enact a launch, and so that the compiler front-end can translate >> into these calls.
Table 9 New Device-only Launch Implementation Functions  Runtime API Launch Functions Description of Difference From Host Runtime Behaviour (behavior is identical if no description) cudaGetParameterBuffer Generated automatically from >> .
The APIs for these launch functions are different to those of the CUDA Runtime API, and are defined as follows: extern device cudaError_t cudaGetParameterBuffer ( void ** params ); extern __device__ cudaError_t cudaLaunchDevice ( void * kernel , void * params , dim3 gridDim , dim3 blockDim , unsigned int sharedMemSize = 0 , cudaStream_t stream = 0 ); 9.3.1.8.
API Reference  The portions of the CUDA Runtime API supported in the device runtime are detailed here.
Host and device runtime APIs have identical syntax; semantics are the same except where indicated.
The following table provides an overview of the API relative to the version available from the host.
Device-side Launch from PTX  This section is for the programming language and compiler implementers who target Parallel Thread Execution (PTX) and plan to support Dynamic Parallelism in their language.
It provides the low-level details related to supporting kernel launches at the PTX level. 9.3.2.1. Kernel Launch APIs  Device-side kernel launches can be implemented using the following two APIs accessible from PTX: cudaLaunchDevice() and cudaGetParameterBuffer() .
cudaLaunchDevice() launches the specified kernel with the parameter buffer that is obtained by calling cudaGetParameterBuffer() and filled with the parameters to the launched kernel.
The parameter buffer can be NULL, i.e., no need to invoke cudaGetParameterBuffer() , if the launched kernel does not take any parameters. 9.3.2.1.1. cudaLaunchDevice  At the PTX level, cudaLaunchDevice() needs to be declared in one of the two forms shown below before it is used.
b64 stream ) ; The CUDA-level declaration below is mapped to one of the aforementioned PTX-level declarations and is found in the system header file cuda_device_runtime_api.h .
The function is defined in the cudadevrt system library, which must be linked with a program in order to use device-side kernel launch functionality.
CUDA-level declaration of cudaLaunchDevice() extern "C" __device__ cudaError_t cudaLaunchDevice ( void * func , void * parameterBuffer , dim3 gridDimension , dim3 blockDimension , unsigned int sharedMemSize , cudaStream_t stream ); The first parameter is a pointer to the kernel to be is launched, and the second parameter is the parameter buffer that holds the actual parameters to the launched kernel.
Other parameters specify the launch configuration, i.e., as grid dimension, block dimension, shared memory size, and the stream associated with the launch (please refer to Execution Configuration for the detailed description of launch configuration. 9.3.2.1.2. cudaGetParameterBuffer  cudaGetParameterBuffer() needs to be declared at the PTX level before it’s used.
The PTX-level declaration must be in one of the two forms given below, depending on address size:   PTX-level Declaration of cudaGetParameterBuffer() when .address_size is 64 .
b64 size ) ; The following CUDA-level declaration of cudaGetParameterBuffer() is mapped to the aforementioned PTX-level declaration:   CUDA-level Declaration of cudaGetParameterBuffer() extern "C" __device__ void * cudaGetParameterBuffer ( size_t alignment , size_t size ); The first parameter specifies the alignment requirement of the parameter buffer and the second parameter the size requirement in bytes.
In the current implementation, the parameter buffer returned by cudaGetParameterBuffer() is always guaranteed to be 64- byte aligned, and the alignment requirement parameter is ignored.
However, it is recommended to pass the correct alignment requirement value - which is the largest alignment of any parameter to be placed in the parameter buffer - to cudaGetParameterBuffer() to ensure portability in the future. 9.3.2.2. Parameter Buffer Layout  Parameter reordering in the parameter buffer is prohibited, and each individual parameter placed in the parameter buffer is required to be aligned.
That is, each parameter must be placed at the n th byte in the parameter buffer, where n is the smallest multiple of the parameter size that is greater than the offset of the last byte taken by the preceding parameter.
For a more detailed description of PTX code generated by the CUDA compiler, please refer to the PTX-3.5 specification. 9.3.3. Toolkit Support for Dynamic Parallelism  9.3.3.1.
Including Device Runtime API in CUDA Code  Similar to the host-side runtime API, prototypes for the CUDA device runtime API are included automatically during program compilation.
There is no need to include cuda_device_runtime_api.h explicitly. 9.3.3.2. Compiling and Linking  When compiling and linking CUDA programs using dynamic parallelism with nvcc , the program will automatically link against the static device runtime library libcudadevrt .
The device runtime is offered as a static library ( cudadevrt.lib on Windows, libcudadevrt.a under Linux), against which a GPU application that uses the device runtime must be linked.
A device runtime program may be compiled and linked in a single step, if all required source files can be specified from the command line: $ nvcc -arch=sm_75 -rdc=true hello_world.cu -o hello -lcudadevrt It is also possible to compile CUDA .cu source files first to object files, and then link these together in a two-stage process: $ nvcc -arch=sm_75 -dc hello_world.cu -o hello_world.o $ nvcc -arch=sm_75 -rdc=true hello_world.o -o hello -lcudadevrt Please see the Using Separate Compilation section of The CUDA Driver Compiler NVCC guide for more details. 9.4. Programming Guidelines  9.4.1.
API level device management, kernel launching, device memcpy, stream management, and event management are exposed from the device runtime.
Programming for the device runtime should be familiar to someone who already has experience with CUDA.
Device runtime syntax and semantics are largely the same as that of the host API, with any exceptions detailed earlier in this document.
The following example shows a simple Hello World program incorporating dynamic parallelism: #include __global__ void childKernel () { printf ( "Hello " ); } __global__ void tailKernel () { printf ( "World!   " ); } __global__ void parentKernel () {   launch child childKernel >> (); if ( cudaSuccess != cudaGetLastError ()) { return ; }   launch tail into cudaStreamTailLaunch stream   implicitly synchronizes: waits for child to complete tailKernel >> (); } int main ( int argc , char * argv []) {   launch parent parentKernel >> (); if ( cudaSuccess != cudaGetLastError ()) { return 1 ; }   wait for parent to complete if ( cudaSuccess != cudaDeviceSynchronize ()) { return 2 ; } return 0 ; } This program may be built in a single step from the command line as follows: $ nvcc -arch=sm_75 -rdc=true hello_world.cu -o hello -lcudadevrt 9.4.2.
Dynamic-parallelism-enabled Kernel Overhead  System software which is active when controlling dynamic launches may impose an overhead on any kernel which is running at the time, whether or not it invokes kernel launches of its own.
This overhead arises from the device runtime’s execution tracking and management software and may result in decreased performance.
This overhead is, in general, incurred for applications that link against the device runtime library. 9.4.3. Implementation Restrictions and Limitations  Dynamic Parallelism guarantees all semantics described in this document, however, certain hardware and software resources are implementation-dependent and limit the scale, performance and other properties of a program which uses the device runtime.
Memory Footprint  The device runtime system software reserves memory for various management purposes, in particular a reservation for tracking pending grid launches.
Configuration controls are available to reduce the size of this reservation in exchange for certain launch limitations.
See Configuration Options , below, for details. 9.4.3.1.2. Pending Kernel Launches  When a kernel is launched, all associated configuration and parameter data is tracked until the kernel completes.
The size of the fixed-size launch pool is configurable by calling cudaDeviceSetLimit() from the host and specifying cudaLimitDevRuntimePendingLaunchCount . 9.4.3.1.3. Configuration Options  Resource allocation for the device runtime system software is controlled via the cudaDeviceSetLimit() API from the host program.
Limits must be set before any kernel is launched, and may not be changed while the GPU is actively running programs.
The following named limits may be set: Limit Behavior cudaLimitDevRuntimePendingLaunchCount Controls the amount of memory set aside for buffering kernel launches and events which have not yet begun to execute, due either to unresolved dependencies or lack of execution resources.
When the buffer is full, an attempt to allocate a launch slot during a device side kernel launch will fail and return cudaErrorLaunchOutOfResources , while an attempt to allocate an event slot will fail and return cudaErrorMemoryAllocation .
Applications may increase the number of launch and/or event slots by setting cudaLimitDevRuntimePendingLaunchCount .
The CUDA driver automatically increases the per-thread stack size for each kernel launch as needed.
To set the per-thread stack size to a different value, cudaDeviceSetLimit() can be called to set this limit.
The stack will be immediately resized, and if necessary, the device will block until all preceding requested tasks are complete.
cudaDeviceGetLimit() can be called to get the current per-thread stack size. 9.4.3.1.4. Memory Allocation and Lifetime  cudaMalloc() and cudaFree() have distinct semantics between the host and device environments.
When invoked from the host, cudaMalloc() allocates a new region from unused device memory.
When invoked from the device runtime these functions map to device-side malloc() and free() .
This implies that within the device environment the total allocatable memory is limited to the device malloc() heap size, which may be smaller than the available unused device memory.
Also, it is an error to invoke cudaFree() from the host program on a pointer which was allocated by cudaMalloc() on the device or vice-versa.
cudaMalloc() on Host cudaMalloc() on Device cudaFree() on Host Supported Not Supported cudaFree() on Device Not Supported Supported Allocation limit Free device memory cudaLimitMallocHeapSize 9.4.3.1.5.
The device runtime may reschedule thread blocks onto different SMs in order to more efficiently manage resources.
As such, it is unsafe to rely upon %smid or %warpid remaining unchanged across the lifetime of a thread or thread block. 9.4.3.1.6. ECC Errors  No notification of ECC errors is available to code within a CUDA kernel.
Any ECC errors which arise during execution of a nested program will either generate an exception or continue execution (depending upon error and configuration). 9.5. CDP2 vs CDP1  This section summarises the differences between, and the compatibility and interoperability of, the new (CDP2) and legacy (CDP1) CUDA Dynamic Parallelism interfaces.
It also shows how to opt-out of the CDP2 interface on devices of compute capability less than 9.0. 9.5.1. Differences Between CDP1 and CDP2  Explicit device-side synchronization is no longer possible with CDP2 or on devices of compute capability 9.0 or higher.
Attempting to query or set cudaLimitDevRuntimeSyncDepth (or CU_LIMIT_DEV_RUNTIME_SYNC_DEPTH ) with CDP2 or on devices of compute capability 9.0 or higher results in cudaErrorUnsupportedLimit .
CDP2 no longer has a virtualized pool for pending launches that don’t fit in the fixed-sized pool.
cudaLimitDevRuntimePendingLaunchCount must be set to be large enough to avoid running out of launch slots.
For CDP2, there is a limit to the total number of events existing at once (note that events are destroyed only after a launch completes), equal to twice the pending launch count.
cudaLimitDevRuntimePendingLaunchCount must be set to be large enough to avoid running out of event slots.
Streams are tracked per grid with CDP2 or on devices of compute capability 9.0 or higher, not per thread block.
CDP2 introduces the tail launch ( cudaStreamTailLaunch ) and fire-and-forget ( cudaStreamFireAndForget ) named streams.
CDP2 is supported only under 64-bit compilation mode. 9.5.2. Compatibility and Interoperability  CDP2 is the default.
Functions can be compiled with -DCUDA_FORCE_CDP1_IF_SUPPORTED to opt-out of using CDP2 on devices of compute capability less than 9.0.
Function compiler with CUDA 12.0 and newer (default) Function compiled with pre-CUDA 12.0 or with CUDA 12.0 and newer with -DCUDA_FORCE_CDP1_IF_SUPPORTED specified Compilation Compile error if device code references cudaDeviceSynchronize .
Compile error if device code references cudaDeviceSynchronize and code is compiled for sm_90 or newer.
Compute capability >> ( data ); cudaDeviceSynchronize (); } __syncthreads (); } void host_launch ( int * data ) { parent_launch >> ( data ); } 9.6.1.2.1.2.
Zero-copy system memory has identical coherence and consistency guarantees to global memory, and follows the semantics detailed above. 9.6.1.2.1.3. Constant Memory (CDP1)  See Constant Memory , above, for CDP2 version of document.
Constants are immutable and may not be modified from the device, even between parent and child launches.
That is to say, the value of all __constant__ variables must be set from the host prior to launch.
Constant memory is inherited automatically by all child kernels from their respective parents.
Taking the address of a constant memory object from within a kernel thread has the same semantics as for all CUDA programs, and passing that pointer from parent to child or from a child to parent is naturally supported. 9.6.1.2.1.4. Shared and Local Memory (CDP1)  See Shared and Local Memory , above, for CDP2 version of document.
Shared and Local memory is private to a thread block or thread, respectively, and is not visible or coherent between parent and child. 9.6.1.2.1.5. Local Memory (CDP1)  See Local Memory , above, for CDP2 version of document.
Local memory is private storage for an executing thread, and is not visible outside of that thread.
For example:   Correct - "value" is global storage __device__ int value ; __device__ void x () { value = 5 ; child >> ( & value ); }   Invalid - "value" is local storage __device__ void y () { int value = 5 ; child >> ( & value ); } 9.6.1.2.1.6.
Writes to the global memory region over which a texture is mapped are incoherent with respect to texture accesses.
Similarly, writes to memory by a child will be reflected in the texture memory accesses by a parent, but only after the parent synchronizes on the child’s completion.
using cudaDeviceSynchronize() in device code) is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release. 9.6.2. Programming Interface (CDP1)  See Programming Interface , above, for CDP2 version of document.
9.6.2.1. CUDA C++ Reference (CDP1)  See CUDA C++ Reference , above, for CDP2 version of document.
This section describes changes and additions to the CUDA C++ language extensions for supporting Dynamic Parallelism . 9.6.2.1.1. Device-Side Kernel Launch (CDP1)  See Device-Side Kernel Launch , above, for CDP2 version of document.
Kernels may be launched from the device using the standard CUDA >> syntax: kernel_name >> ([ kernel arguments ]); Dg is of type dim3 and specifies the dimensions and size of the grid Db is of type dim3 and specifies the dimensions and size of each thread block Ns is of type size_t and specifies the number of bytes of shared memory that is dynamically allocated per thread block for this call and addition to statically allocated memory.
The stream must have been allocated in the same thread block where the call is being made.
S is an optional argument that defaults to 0. 9.6.2.1.1.1. Launches are Asynchronous (CDP1)  See Launches are Asynchronous , above, for CDP2 version of document.
Identical to host-side launches, all device-side kernel launches are asynchronous with respect to the launching thread.
That is to say, the >> launch command will return immediately and the launching thread will continue to execute until it hits an explicit launch-synchronization point such as cudaDeviceSynchronize() .
The grid launch is posted to the device and will execute independently of the parent thread.
The child grid may begin execution at any time after launch, but is not guaranteed to begin execution until the launching thread reaches an explicit launch-synchronization point. 9.6.2.1.1.2. Launch Environment Configuration (CDP1)  See Launch Environment Configuration , above, for CDP2 version of document.
All global device configuration settings (for example, shared memory and L1 cache size as returned from cudaDeviceGetCacheConfig() , and device limits returned from cudaDeviceGetLimit() ) will be inherited from the parent. 9.6.2.1.2. Streams (CDP1)  See Streams , above, for CDP2 version of document.
Named streams may be used by any thread within a thread-block, but stream handles may not be passed to other blocks or child/parent kernels.
In other words, a stream should be treated as private to the block in which it is created.
Stream handles are not guaranteed to be unique between blocks, so using a stream handle within a block that did not allocate it will result in undefined behavior.
As cudaStreamSynchronize() and cudaStreamQuery() are unsupported by the device runtime, cudaDeviceSynchronize() should be used instead when the application needs to know that stream-launched child kernels have completed. 9.6.2.1.2.1. The Implicit (NULL) Stream (CDP1)  See The Implicit (NULL) Stream , above, for CDP2 version of document.
Within a host program, the unnamed (NULL) stream has additional barrier synchronization semantics with other streams (see Default Stream for details).
The device runtime offers a single implicit, unnamed stream shared between all threads in a block, but as all named streams must be created with the cudaStreamNonBlocking flag, work launched into the NULL stream will not insert an implicit dependency on pending work in any other streams (including NULL streams of other thread blocks). 9.6.2.1.3. Events (CDP1)  See Events , above, for CDP2 version of document.
As for all device runtime objects, event objects may be shared between all threads within the thread-block which created them but are local to that block and may not be passed to other kernels, or between blocks within the same kernel.
Event handles are not guaranteed to be unique between blocks, so using an event handle within a block that did not create it will result in undefined behavior. 9.6.2.1.4. Synchronization (CDP1)  See Synchronization , above, for CDP2 version of document.
The cudaDeviceSynchronize() function will synchronize on all work launched by any thread in the thread-block up to the point where cudaDeviceSynchronize() was called.
Note that cudaDeviceSynchronize() may be called from within divergent code (see Block Wide Synchronization (CDP1) ).
It is up to the program to perform sufficient additional inter-thread synchronization, for example via a call to __syncthreads() , if the calling thread is intended to synchronize with child grids invoked from other threads. 9.6.2.1.4.1. Block Wide Synchronization (CDP1)  See CUDA Dynamic Parallelism , above, for CDP2 version of document.
In particular, without explicit synchronization via a __syncthreads() directive the calling thread can make no assumptions about what work has been launched by any thread other than itself.
For example if multiple threads within a block are each launching work and synchronization is desired for all this work at once (perhaps because of event-based dependencies), it is up to the program to guarantee that this work is submitted by all threads before calling cudaDeviceSynchronize() .
Because the implementation is permitted to synchronize on launches from any thread in the block, it is quite possible that simultaneous calls to cudaDeviceSynchronize() by multiple threads will drain all work in the first call and then have no effect for the later calls. 9.6.2.1.5. Device Management (CDP1)  See Device Management , above, for CDP2 version of document.
Only the device on which a kernel is running will be controllable from that kernel. 9.6.2.1.6. Memory Declarations (CDP1)  See Memory Declarations , above, for CDP2 version of document.
9.6.2.1.6.1. Device and Constant Memory (CDP1)  See Device and Constant Memory , above, for CDP2 version of document.
Memory declared at file scope with __device__ or __constant__ memory space specifiers behaves identically when using the device runtime. 9.6.2.1.6.2. Textures and Surfaces (CDP1)  See Textures and Surfaces , above, for CDP2 version of document.
CUDA supports dynamically created texture and surface objects 14 , where a texture object may be created on the host, passed to a kernel, used by that kernel, and then destroyed from the host. 9.6.2.1.6.3. Shared Memory Variable Declarations (CDP1)  See Shared Memory Variable Declarations , above, for CDP2 version of document.
In CUDA C++ shared memory can be declared either as a statically sized file-scope or function-scoped variable, or as an extern variable with the size determined at runtime by the kernel’s caller via a launch configuration argument.
__global__ void permute ( int n , int * data ) { extern __shared__ int smem []; if ( n >> ( n / 2 , data ); permute >> ( n / 2 , data + n / 2 ); } } void host_launch ( int * data ) { permute >> ( 256 , data ); } 9.6.2.1.6.4.
Device-side symbols (i.e., those marked __device__ ) may be referenced from within a kernel simply via the & operator, as all global-scope device variables are in the kernel’s visible address space. 9.6.2.1.7. API Errors and Launch Failures (CDP1)  See API Errors and Launch Failures , above, for CDP2 version of document.
For device-side exceptions, e.g., access to an invalid address, an error in a child grid will be returned to the host instead of being returned by the parent’s call to cudaDeviceSynchronize() . 9.6.2.1.7.1. Launch Setup APIs (CDP1)  See Launch Setup APIs , above, for CDP2 version of document.
Kernel launch is a system-level mechanism exposed through the device runtime library, and as such is available directly from PTX via the underlying cudaGetParameterBuffer() and cudaLaunchDevice() APIs.
Table 11 New Device-only Launch Implementation Functions  Runtime API Launch Functions Description of Difference From Host Runtime Behaviour (behavior is identical if no description) cudaGetParameterBuffer Generated automatically from >> .
The APIs for these launch functions are different to those of the CUDA Runtime API, and are defined as follows: extern device cudaError_t cudaGetParameterBuffer ( void ** params ); extern __device__ cudaError_t cudaLaunchDevice ( void * kernel , void * params , dim3 gridDim , dim3 blockDim , unsigned int sharedMemSize = 0 , cudaStream_t stream = 0 ); 9.6.2.1.8.
The table below provides an overview of the API relative to the version available from the host.
Table 12 Supported API Functions  Runtime API Functions Details cudaDeviceSynchronize Synchronizes on work launched from thread’s own block only.
Warning: Note that calling this API from device code is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release.
Device-side Launch from PTX (CDP1)  See Device-side Launch from PTX , above, for CDP2 version of document.
This section is for the programming language and compiler implementers who target Parallel Thread Execution (PTX) and plan to support Dynamic Parallelism in their language. 9.6.2.2.1. Kernel Launch APIs (CDP1)  See Kernel Launch APIs , above, for CDP2 version of document.
Device-side kernel launches can be implemented using the following two APIs accessible from PTX: cudaLaunchDevice() and cudaGetParameterBuffer() . 9.6.2.2.1.1. cudaLaunchDevice (CDP1)  See cudaLaunchDevice , above, for CDP2 version of document.
At the PTX level, cudaLaunchDevice() needs to be declared in one of the two forms shown below before it is used.
b32 stream ) ; The CUDA-level declaration below is mapped to one of the aforementioned PTX-level declarations and is found in the system header file cuda_device_runtime_api.h .
The layout of the parameter buffer is explained in Parameter Buffer Layout (CDP1) , below. 9.6.2.2.1.2. cudaGetParameterBuffer (CDP1)  See cudaGetParameterBuffer , above, for CDP2 version of document.
The PTX-level declaration must be in one of the two forms given below, depending on address size:   PTX-level Declaration of cudaGetParameterBuffer() when .address_size is 64   When .address_size is 64 .
b64 size ) ;   PTX-level Declaration of cudaGetParameterBuffer() when .address_size is 32 .
b32 size ) ; The following CUDA-level declaration of cudaGetParameterBuffer() is mapped to the aforementioned PTX-level declaration:   CUDA-level Declaration of cudaGetParameterBuffer() extern "C" __device__ void * cudaGetParameterBuffer ( size_t alignment , size_t size ); The first parameter specifies the alignment requirement of the parameter buffer and the second parameter the size requirement in bytes. 9.6.2.2.2. Parameter Buffer Layout (CDP1)  See Parameter Buffer Layout , above, for CDP2 version of document.
Parameter reordering in the parameter buffer is prohibited, and each individual parameter placed in the parameter buffer is required to be aligned. 9.6.2.3. Toolkit Support for Dynamic Parallelism (CDP1)  See Toolkit Support for Dynamic Parallelism , above, for CDP2 version of document.
9.6.2.3.1. Including Device Runtime API in CUDA Code (CDP1)  See Including Device Runtime API in CUDA Code , above, for CDP2 version of document.
Similar to the host-side runtime API, prototypes for the CUDA device runtime API are included automatically during program compilation. 9.6.2.3.2. Compiling and Linking (CDP1)  See Compiling and Linking , above, for CDP2 version of document.
When compiling and linking CUDA programs using dynamic parallelism with nvcc , the program will automatically link against the static device runtime library libcudadevrt . 9.6.3. Programming Guidelines (CDP1)  See Programming Guidelines , above, for CDP2 version of document.
The following example shows a simple Hello World program incorporating dynamic parallelism: #include __global__ void childKernel () { printf ( "Hello " ); } __global__ void parentKernel () {   launch child childKernel >> (); if ( cudaSuccess != cudaGetLastError ()) { return ; }   wait for child to complete if ( cudaSuccess != cudaDeviceSynchronize ()) { return ; } printf ( "World!   " ); } int main ( int argc , char * argv []) {   launch parent parentKernel >> (); if ( cudaSuccess != cudaGetLastError ()) { return 1 ; }   wait for parent to complete if ( cudaSuccess != cudaDeviceSynchronize ()) { return 2 ; } return 0 ; } This program may be built in a single step from the command line as follows: $ nvcc -arch=sm_75 -rdc=true hello_world.cu -o hello -lcudadevrt 9.6.3.2.
Performance (CDP1)  See Performance , above, for CDP2 version of document. 9.6.3.2.1. Synchronization (CDP1)  See CUDA Dynamic Parallelism , above, for CDP2 version of document.
Warning Explicit synchronization with child kernels from a parent block (such as using cudaDeviceSynchronize() in device code) is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release.
Synchronization by one thread may impact the performance of other threads in the same Thread Block , even when those other threads do not call cudaDeviceSynchronize() themselves.
In general the implicit synchronization of child kernels done when a thread block ends is more efficient compared to calling cudaDeviceSynchronize() explicitly.
It is therefore recommended to only call cudaDeviceSynchronize() if it is needed to synchronize with a child kernel before a thread block ends. 9.6.3.2.2. Dynamic-parallelism-enabled Kernel Overhead (CDP1)  See Dynamic-parallelism-enabled Kernel Overhead , above, for CDP2 version of document.
System software which is active when controlling dynamic launches may impose an overhead on any kernel which is running at the time, whether or not it invokes kernel launches of its own.
This overhead arises from the device runtime’s execution tracking and management software and may result in decreased performance for example, library calls when made from the device compared to from the host side. 9.6.3.3. Implementation Restrictions and Limitations (CDP1)  See Implementation Restrictions and Limitations , above, for CDP2 version of document.
Dynamic Parallelism guarantees all semantics described in this document, however, certain hardware and software resources are implementation-dependent and limit the scale, performance and other properties of a program which uses the device runtime. 9.6.3.3.1. Runtime (CDP1)  See Runtime , above, for CDP2 version of document.
9.6.3.3.1.1. Memory Footprint (CDP1)  See Memory Footprint , above, for CDP2 version of document.
The device runtime system software reserves memory for various management purposes, in particular one reservation which is used for saving parent-grid state during synchronization, and a second reservation for tracking pending grid launches.
Configuration controls are available to reduce the size of these reservations in exchange for certain launch limitations.
The majority of reserved memory is allocated as backing-store for parent kernel state, for use when synchronizing on a child launch.
Conservatively, this memory must support storing of state for the maximum number of live threads possible on the device.
This means that each parent generation at which cudaDeviceSynchronize() is callable may require up to 860MB of device memory, depending on the device configuration, which will be unavailable for program use even if it is not all consumed. 9.6.3.3.1.2. Nesting and Synchronization Depth (CDP1)  See CUDA Dynamic Parallelism , above, for CDP2 version of document.
Using the device runtime, one kernel may launch another kernel, and that kernel may launch another, and so on.
Each subordinate launch is considered a new nesting level , and the total number of levels is the nesting depth of the program.
The synchronization depth is defined as the deepest level at which the program will explicitly synchronize on a child launch.
Typically this is one less than the nesting depth of the program, but if the program does not need to call cudaDeviceSynchronize() at all levels then the synchronization depth might be substantially different to the nesting depth.
The overall maximum nesting depth is limited to 24, but practically speaking the real limit will be the amount of memory required by the system for each new level (see Memory Footprint (CDP1) above).
Note that this may also apply to cudaMemcpyAsync() , which might itself generate a kernel launch.
This maximum synchronization depth (and hence reserved storage) may be controlled by calling cudaDeviceSetLimit() and specifying cudaLimitDevRuntimeSyncDepth .
The number of levels to be supported must be configured before the top-level kernel is launched from the host, in order to guarantee successful execution of a nested program.
Calling cudaDeviceSynchronize() at a depth greater than the specified maximum synchronization depth will return an error.
An optimization is permitted where the system detects that it need not reserve space for the parent’s state in cases where the parent kernel never calls cudaDeviceSynchronize() .
In this case, because explicit parent/child synchronization never occurs, the memory footprint required for a program will be much less than the conservative maximum.
Such a program could specify a shallower maximum synchronization depth to avoid over-allocation of backing store. 9.6.3.3.1.3. Pending Kernel Launches (CDP1)  See Pending Kernel Launches , above, for CDP2 version of document.
When a kernel is launched, all associated configuration and parameter data is tracked until the kernel completes.
The launch pool is divided into a fixed-size pool and a virtualized pool with lower performance.
The device runtime system software will try to track launch data in the fixed-size pool first.
The virtualized pool will be used to track new launches when the fixed-size pool is full. 9.6.3.3.1.4. Configuration Options (CDP1)  See Configuration Options , above, for CDP2 version of document.
Resource allocation for the device runtime system software is controlled via the cudaDeviceSetLimit() API from the host program.
The following named limits may be set: Limit Behavior cudaLimitDevRuntimeSyncDepth Sets the maximum depth at which cudaDeviceSynchronize() may be called.
Launches may be performed deeper than this, but explicit synchronization deeper than this limit will return the cudaErrorLaunchMaxDepthExceeded .
cudaLimitDevRuntimePendingLaunchCount Controls the amount of memory set aside for buffering kernel launches which have not yet begun to execute, due either to unresolved dependencies or lack of execution resources.
When the buffer is full, the device runtime system software will attempt to track new pending launches in a lower performance virtualized buffer.
when all available heap space is consumed, launches will not occur, and the thread’s last error will be set to cudaErrorLaunchPendingCountExceeded .
The default pending launch count is 2048 launches. 9.6.3.3.1.5. Memory Allocation and Lifetime (CDP1)  See Memory Allocation and Lifetime , above, for CDP2 version of document.
cudaMalloc() and cudaFree() have distinct semantics between the host and device environments.
cudaMalloc() on Host cudaMalloc() on Device cudaFree() on Host Supported Not Supported cudaFree() on Device Not Supported Supported Allocation limit Free device memory cudaLimitMallocHeapSize 9.6.3.3.1.6.
Note that in PTX %smid and %warpid are defined as volatile values. 9.6.3.3.1.7. ECC Errors (CDP1)  See ECC Errors , above, for CDP2 version of document.
14 ( 1 , 2 , 3 ) Dynamically created texture and surface objects are an addition to the CUDA memory model introduced with CUDA 5.0.
Introduction  The Virtual Memory Management APIs provide a way for the application to directly manage the unified virtual address space that CUDA provides to map physical memory to virtual addresses accessible by the GPU.
Introduced in CUDA 10.2, these APIs additionally provide a new way to interop with other processes and graphics APIs like OpenGL and Vulkan, as well as provide newer memory attributes that a user can tune to fit their applications.
Historically, memory allocation calls (such as cudaMalloc() ) in the CUDA programming model have returned a memory address that points to the GPU memory.
In order to increase an allocation’s size, the user had to explicitly allocate a larger buffer, copy data from the initial allocation, free it and then continue to keep track of the newer allocation’s address.
This often leads to lower performance and higher peak memory utilization for applications.
Essentially, users had a malloc-like interface for allocating GPU memory, but did not have a corresponding realloc to complement it.
The Virtual Memory Management APIs decouple the idea of an address and memory and allow the application to handle them separately.
The APIs allow applications to map and unmap memory from a virtual address range as they see fit.
In the case of enabling peer device access to memory allocations by using cudaEnablePeerAccess , all past and future user allocations are mapped to the target peer device.
This lead to users unwittingly paying runtime cost of mapping all cudaMalloc allocations to peer devices.
However, in most situations applications communicate by sharing only a few allocations with another device and not all allocations are required to be mapped to all the devices.
With Virtual Memory Management, applications can specifically choose certain allocations to be accessible from target devices.
The CUDA Virtual Memory Management APIs expose fine grained control to the user for managing the GPU memory in applications.
It provides APIs that let users: Place memory allocated on different devices into a contiguous VA range.
In order to allocate memory, the Virtual Memory Management programming model exposes the following functionality: Allocating physical memory.
Note that the suite of APIs described in this section require a system that supports UVA. 10.2. Query for Support  Before attempting to use Virtual Memory Management APIs, applications must ensure that the devices they want to use support CUDA Virtual Memory Management.
The following code sample shows querying for Virtual Memory Management support: int deviceSupportsVmm ; CUresult result = cuDeviceGetAttribute ( & deviceSupportsVmm , CU_DEVICE_ATTRIBUTE_VIRTUAL_MEMORY_MANAGEMENT_SUPPORTED , device ); if ( deviceSupportsVmm != 0 ) {   `device` supports Virtual Memory Management } 10.3.
Allocating Physical Memory  The first step in memory allocation using Virtual Memory Management APIs is to create a physical memory chunk that will provide a backing for the allocation.
The function argument CUmemGenericAllocationHandle describes the properties of the memory to allocate such as the location of the allocation, if the allocation is going to be shared to another process (or other Graphics APIs), or the physical attributes of the memory to be allocated.
Users must ensure the requested allocation’s size must be aligned to appropriate granularity.
Information regarding an allocation’s granularity requirements can be queried using cuMemGetAllocationGranularity .
The following code snippet shows allocating physical memory with cuMemCreate : CUmemGenericAllocationHandle allocatePhysicalMemory ( int device , size_t size ) { CUmemAllocationProp prop = {}; prop .
id = device ; cuMemGetAllocationGranularity ( & granularity , & prop , CU_MEM_ALLOC_GRANULARITY_MINIMUM );   Ensure size matches granularity requirements for the allocation size_t padded_size = ROUND_UP ( size , granularity );   Allocate physical memory CUmemGenericAllocationHandle allocHandle ; cuMemCreate ( & allocHandle , padded_size , & prop , 0 ); return allocHandle ; } The memory allocated by cuMemCreate is referenced by the CUmemGenericAllocationHandle it returns.
This is a departure from the cudaMalloc-style of allocation, which returns a pointer to the GPU memory, which was directly accessible by CUDA kernel executing on the device.
The memory allocated cannot be used for any operations other than querying properties using cuMemGetAllocationPropertiesFromHandle .
In order to make this memory accessible, applications must map this memory into a VA range reserved by cuMemAddressReserve and provide suitable access rights to it.
Applications must free the allocated memory using the cuMemRelease API. 10.3.1. Shareable Memory Allocations  With cuMemCreate users now have the facility to indicate to CUDA, at allocation time, that they have earmarked a particular allocation for Inter process communication and graphics interop purposes.
Applications can do this by setting CUmemAllocationProp::requestedHandleTypes to a platform-specific field.
On Windows, when CUmemAllocationProp::requestedHandleTypes is set to CU_MEM_HANDLE_TYPE_WIN32 applications must also specify an LPSECURITYATTRIBUTES attribute in CUmemAllocationProp::win32HandleMetaData .
This security attribute defines the scope of which exported allocations may be transferred to other processes.
The CUDA Virtual Memory Management API functions do not support the legacy interprocess communication functions with their memory.
Instead, they expose a new mechanism for interprocess communication that uses OS-specific handles.
Applications can obtain these OS-specific handles corresponding to the allocations by using cuMemExportToShareableHandle .
The handles thus obtained can be transferred by using the usual OS native mechanisms for inter process communication.
The recipient process should import the allocation by using cuMemImportFromShareableHandle .
Users must ensure they query for support of the requested handle type before attempting to export memory allocated with cuMemCreate .
The following code snippet illustrates query for handle type support in a platform-specific way.
int deviceSupportsIpcHandle ; #if defined(__linux__) cuDeviceGetAttribute ( & deviceSupportsIpcHandle , CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED , device )); #else cuDeviceGetAttribute ( & deviceSupportsIpcHandle , CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_WIN32_HANDLE_SUPPORTED , device )); #endif Users should set the CUmemAllocationProp::requestedHandleTypes appropriately as shown below: #if defined(__linux__) prop .
#endif The memMapIpcDrv sample can be used as an example for using IPC with Virtual Memory Management allocations. 10.3.2. Memory Type  Before CUDA 10.2, applications had no user-controlled way of allocating any special type of memory that certain devices may support.
With cuMemCreate , applications can additionally specify memory type requirements using the CUmemAllocationProp::allocFlags to opt into any specific memory features.
Applications must also ensure that the requested memory type is supported on the device of allocation. 10.3.2.1. Compressible Memory  Compressible memory can be used to accelerate accesses to data with unstructured sparsity and other compressible data patterns.
Compression can save DRAM bandwidth, L2 read bandwidth and L2 capacity depending on the data being operated on.
Applications that want to allocate compressible memory on devices that support Compute Data Compression can do so by setting CUmemAllocationProp::allocFlags::compressionType to CU_MEM_ALLOCATION_COMP_GENERIC .
Users must query if device supports Compute Data Compression by using CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED .
The following code snippet illustrates querying compressible memory support cuDeviceGetAttribute .
int compressionSupported = 0 ; cuDeviceGetAttribute ( & compressionSupported , CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED , device ); On devices that support Compute Data Compression, users must opt in at allocation time as shown below: prop .
compressionType = CU_MEM_ALLOCATION_COMP_GENERIC ; Due to various reasons such as limited HW resources, the allocation may not have compression attributes, the user is expected to query back the properties of the allocated memory using cuMemGetAllocationPropertiesFromHandle and check for compression attribute.
CUmemAllocationPropPrivate allocationProp = {}; cuMemGetAllocationPropertiesFromHandle ( & allocationProp , allocationHandle ); if ( allocationProp .
compressionType == CU_MEM_ALLOCATION_COMP_GENERIC ) {   Obtained compressible memory allocation } 10.4.
Reserving a Virtual Address Range  Since with Virtual Memory Management the notions of address and memory are distinct, applications must carve out an address range that can hold the memory allocations made by cuMemCreate .
The address range reserved must be at least as large as the sum of the sizes of all the physical memory allocations the user plans to place in them.
Applications can reserve a virtual address range by passing appropriate parameters to cuMemAddressReserve .
The address range obtained will not have any device or host physical memory associated with it.
The reserved virtual address range can be mapped to memory chunks belonging to any device in the system, thus providing the application a continuous VA range backed and mapped by memory belonging to different devices.
Applications are expected to return the virtual address range back to CUDA using cuMemAddressFree .
These functions are conceptually similar to mmap/munmap (on Linux) or VirtualAlloc/VirtualFree (on Windows) functions.
The following code snippet illustrates the usage for the function: CUdeviceptr ptr ;   `ptr` holds the returned start of virtual address range reserved.
CUresult result = cuMemAddressReserve ( & ptr , size , 0 , 0 , 0 );   alignment = 0 for default alignment 10.5.
Virtual Aliasing Support  The Virtual Memory Management APIs provide a way to create multiple virtual memory mappings or “proxies” to the same allocation using multiple calls to cuMemMap with different virtual addresses, so-called virtual aliasing.
Unless otherwise noted in the PTX ISA, writes to one proxy of the allocation are considered inconsistent and incoherent with any other proxy of the same memory until the writing device operation (grid launch, memcpy, memset, and so on) completes.
Grids present on the GPU prior to a writing device operation but reading after the writing device operation completes are also considered to have inconsistent and incoherent proxies.
For example, the following snippet is considered undefined, assuming device pointers A and B are virtual aliases of the same memory allocation: __global__ void foo ( char * A , char * B ) { * A = 0x1 ; printf ( "%d   " , * B );   Undefined behavior! *B can take on either   the previous value or some value in-between.
} The following is defined behavior, assuming these two kernels are ordered monotonically (by streams or events).
__global__ void foo1 ( char * A ) { * A = 0x1 ; } __global__ void foo2 ( char * B ) { printf ( "%d   " , * B );   *B == *A == 0x1 assuming foo2 waits for foo1   to complete before launching } cudaMemcpyAsync ( B , input , size , stream1 );   Aliases are allowed at   operation boundaries foo1 >> ( A );   allowing foo1 to access A.
cudaEventRecord ( event , stream1 ); cudaStreamWaitEvent ( stream2 , event ); foo2 >> ( B ); cudaStreamWaitEvent ( stream3 , event ); cudaMemcpyAsync ( output , B , size , stream3 );   Both launches of foo2 and   cudaMemcpy (which both   read) wait for foo1 (which writes)   to complete before proceeding 10.6.
Mapping Memory  The allocated physical memory and the carved out virtual address space from the previous two sections represent the memory and address distinction introduced by the Virtual Memory Management APIs.
For the allocated memory to be useable, the user must first place the memory in the address space.
The address range obtained from cuMemAddressReserve and the physical allocation obtained from cuMemCreate or cuMemImportFromShareableHandle must be associated with each other by using cuMemMap .
Users can associate allocations from multiple devices to reside in contiguous virtual address ranges as long as they have carved out enough address space.
In order to decouple the physical allocation and the address range, users must unmap the address of the mapping by using cuMemUnmap .
Users can map and unmap memory to the same address range as many times as they want, as long as they ensure that they don’t attempt to create mappings on VA range reservations that are already mapped.
The following code snippet illustrates the usage for the function: CUdeviceptr ptr ;   `ptr`: address in the address range previously reserved by cuMemAddressReserve.
Controlling Access Rights  The Virtual Memory Management APIs enable applications to explicitly protect their VA ranges with access control mechanisms.
Mapping the allocation to a region of the address range using cuMemMap does not make the address accessible, and would result in a program crash if accessed by a CUDA kernel.
Users must specifically select access control using the cuMemSetAccess function, which allows or restricts access for specific devices to a mapped address range.
The following code snippet illustrates the usage for the function: void setAccessOnDevice ( int device , CUdeviceptr ptr , size_t size ) { CUmemAccessDesc accessDesc = {}; accessDesc .
flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE ;   Make the address accessible cuMemSetAccess ( ptr , size , & accessDesc , 1 ); } The access control mechanism exposed with Virtual Memory Management allows users to be explicit about which allocations they want to share with other peer devices on the system.
As specified earlier, cudaEnablePeerAccess forces all prior and future cudaMalloc’d allocations to be mapped to the target peer device.
This can be convenient in many cases as user doesn’t have to worry about tracking the mapping state of every allocation to every device in the system.
But for users concerned with performance of their applications this approach has performance implications .
With access control at allocation granularity Virtual Memory Management exposes a mechanism to have peer mappings with minimal overhead.
The vectorAddMMAP sample can be used as an example for using the Virtual Memory Management APIs. 11. Stream Ordered Memory Allocator  11.1.
Introduction  Managing memory allocations using cudaMalloc and cudaFree causes GPU to synchronize across all executing CUDA streams.
The Stream Order Memory Allocator enables applications to order memory allocation and deallocation with other work launched into a CUDA stream such as kernel launches and asynchronous copies.
This improves application memory use by taking advantage of stream-ordering semantics to reuse memory allocations.
The allocator also allows applications to control the allocator’s memory caching behavior.
When set up with an appropriate release threshold, the caching behavior allows the allocator to avoid expensive calls into the OS when the application indicates it is willing to accept a bigger memory footprint.
For many applications, the Stream Ordered Memory Allocator reduces the need for custom memory management abstractions, and makes it easier to create high-performance custom memory management for applications that need it.
For applications and libraries that already have custom memory allocators, adopting the Stream Ordered Memory Allocator enables multiple libraries to share a common pool of memory managed by the driver, thus reducing excess memory consumption.
Additionally, the driver can perform optimizations based on its awareness of the allocator and other stream management APIs.
Finally, Nsight Compute and the Next-Gen CUDA debugger is aware of the allocator as part of their CUDA 11.3 toolkit support. 11.2. Query for Support  The user can determine whether or not a device supports the stream ordered memory allocator by calling cudaDeviceGetAttribute() with the device attribute cudaDevAttrMemoryPoolsSupported .
Starting with CUDA 11.3, IPC memory pool support can be queried with the cudaDevAttrMemoryPoolSupportedHandleTypes device attribute.
Previous drivers will return cudaErrorInvalidValue as those drivers are unaware of the attribute enum.
int driverVersion = 0 ; int deviceSupportsMemoryPools = 0 ; int poolSupportedHandleTypes = 0 ; cudaDriverGetVersion ( & driverVersion ); if ( driverVersion >= 11020 ) { cudaDeviceGetAttribute ( & deviceSupportsMemoryPools , cudaDevAttrMemoryPoolsSupported , device ); } if ( deviceSupportsMemoryPools != 0 ) {   `device` supports the Stream Ordered Memory Allocator } if ( driverVersion >= 11030 ) { cudaDeviceGetAttribute ( & poolSupportedHandleTypes , cudaDevAttrMemoryPoolSupportedHandleTypes , device ); } if ( poolSupportedHandleTypes & cudaMemHandleTypePosixFileDescriptor ) {   Pools on the specified device can be created with posix file descriptor-based IPC } Performing the driver version check before the query avoids hitting a cudaErrorInvalidValue error on drivers where the attribute was not yet defined.
One can use cudaGetLastError to clear the error instead of avoiding it. 11.3. API Fundamentals (cudaMallocAsync and cudaFreeAsync)  The APIs cudaMallocAsync and cudaFreeAsync form the core of the allocator.
Both APIs accept stream arguments to define when the allocation will become and stop being available for use.
The pointer value returned by cudaMallocAsync is determined synchronously and is available for constructing future work.
It is important to note that cudaMallocAsync ignores the current device/context when determining where the allocation will reside.
Instead, cudaMallocAsync determines the resident device based on the specified memory pool or the supplied stream.
The simplest use pattern is when the memory is allocated, used, and freed back into the same stream.
void * ptr ; size_t size = 512 ; cudaMallocAsync ( & ptr , size , cudaStreamPerThread );   do work using the allocation kernel >> ( ptr , ...);   An asynchronous free can be specified without synchronizing the cpu and GPU cudaFreeAsync ( ptr , cudaStreamPerThread ); When using an allocation in a stream other than the allocating stream, the user must guarantee that the access will happen after the allocation operation, otherwise the behavior is undefined.
The user may make this guarantee either by synchronizing the allocating stream, or by using CUDA events to synchronize the producing and consuming streams.
The user must guarantee that the free operation happens after the allocation operation and any use of the allocation.
Also, any use of the allocation after the free operation starts results in undefined behavior.
Events and/or stream synchronizing operations should be used to guarantee any access to the allocation on other streams is complete before the freeing stream begins the free operation.
cudaMallocAsync ( & ptr , size , stream1 ); cudaEventRecord ( event1 , stream1 );  stream2 must wait for the allocation to be ready before accessing cudaStreamWaitEvent ( stream2 , event1 ); kernel >> ( ptr , ...); cudaEventRecord ( event2 , stream2 );   stream3 must wait for stream2 to finish accessing the allocation before   freeing the allocation cudaStreamWaitEvent ( stream3 , event2 ); cudaFreeAsync ( ptr , stream3 ); The user can free allocations allocated with cudaMalloc() with cudaFreeAsync() .
The user must make the same guarantees about accesses being complete before the free operation begins.
cudaMalloc ( & ptr , size ); kernel >> ( ptr , ...); cudaFreeAsync ( ptr , stream ); The user can free memory allocated with cudaMallocAsync with cudaFree() .
When freeing such allocations through the cudaFree() API, the driver assumes that all accesses to the allocation are complete and performs no further synchronization.
The user can use cudaStreamQuery / cudaStreamSynchronize / cudaEventQuery / cudaEventSynchronize / cudaDeviceSynchronize to guarantee that the appropriate asynchronous work is complete and that the GPU will not try to access the allocation.
cudaMallocAsync ( & ptr , size , stream ); kernel >> ( ptr , ...);   synchronize is needed to avoid prematurely freeing the memory cudaStreamSynchronize ( stream ); cudaFree ( ptr ); 11.4.
Memory Pools and the cudaMemPool_t  Memory pools encapsulate virtual address and physical memory resources that are allocated and managed according to the pools attributes and properties.
In the absence of a specified memory pool, cudaMallocAsync uses the current memory pool of the supplied stream’s device.
The current memory pool for a device may be set with cudaDeviceSetMempool and queried with cudaDeviceGetMempool .
By default (in the absence of a cudaDeviceSetMempool call), the current memory pool is the default memory pool of a device.
The API cudaMallocFromPoolAsync and c++ overloads of cudaMallocAsync allow a user to specify the pool to be used for an allocation without setting it as the current pool.
The APIs cudaDeviceGetDefaultMempool and cudaMemPoolCreate give users handles to memory pools.
So allocating without specifying a memory pool will always yield an allocation local to the stream’s device.
Note cudaMemPoolSetAttribute and cudaMemPoolGetAttribute control the attributes of the memory pools. 11.5. Default/Implicit Pools  The default memory pool of a device may be retrieved with the cudaDeviceGetDefaultMempool API.
Allocations from the default memory pool of a device are non-migratable device allocation located on that device.
The accessibility of the default memory pool may be modified with cudaMemPoolSetAccess and queried by cudaMemPoolGetAccess .
Since the default pools do not need to be explicitly created, they are sometimes referred to as implicit pools.
The default memory pool of a device does not support IPC. 11.6. Explicit Pools  The API cudaMemPoolCreate creates an explicit pool.
This allows applications to request properties for their allocation beyond what is provided by the default/implict pools.
These include properties such as IPC capability, maximum pool size, allocations resident on a specific CPU NUMA node on supported platforms etc.
create a pool similar to the implicit pool on device 0 int device = 0 ; cudaMemPoolProps poolProps = { }; poolProps .
type = cudaMemLocationTypeDevice ; cudaMemPoolCreate ( & memPool , & poolProps )); The following code snippet illustrates an example of creating an IPC capable memory pool on a valid CPU NUMA node.
create a pool resident on a CPU NUMA node that is capable of IPC sharing (via a file descriptor).
handleType = cudaMemHandleTypePosixFileDescriptor ; cudaMemPoolCreate ( & ipcMemPool , & poolProps )); 11.7.
Physical Page Caching Behavior  By default, the allocator tries to minimize the physical memory owned by a pool.
To minimize the OS calls to allocate and free physical memory, applications must configure a memory footprint for each pool.
Applications can do this with the release threshold attribute ( cudaMemPoolAttrReleaseThreshold ).
The release threshold is the amount of memory in bytes a pool should hold onto before trying to release memory back to the OS.
When more than the release threshold bytes of memory are held by the memory pool, the allocator will try to release memory back to the OS on the next call to stream, event or device synchronize.
Setting the release threshold to UINT64_MAX will prevent the driver from attempting to shrink the pool after every synchronization.
Cuuint64_t setVal = UINT64_MAX ; cudaMemPoolSetAttribute ( memPool , cudaMemPoolAttrReleaseThreshold , & setVal ); Applications that set cudaMemPoolAttrReleaseThreshold high enough to effectively disable memory pool shrinking may wish to explicitly shrink a memory pool’s memory footprint.
When trimming a memory pool’s footprint, the minBytesToKeep parameter allows an application to hold onto an amount of memory it expects to need in a subsequent phase of execution.
Cuuint64_t setVal = UINT64_MAX ; cudaMemPoolSetAttribute ( memPool , cudaMemPoolAttrReleaseThreshold , & setVal );   application phase needing a lot of memory from the stream ordered allocator for ( i = 0 ; i >> ( ptrs ,...); for ( j = 0 ; j reserved ); cudaMemPoolGetAttribute ( memPool , cudaMemPoolAttrReservedMemHigh , statistics -> reservedHigh ); cudaMemPoolGetAttribute ( memPool , cudaMemPoolAttrUsedMemCurrent , statistics -> used ); cudaMemPoolGetAttribute ( memPool , cudaMemPoolAttrUsedMemHigh , statistics -> usedHigh ); }   resetting the watermarks will make them take on the current value.
void resetStatistics ( cudaMemoryPool_t memPool ) { cuuint64_t value = 0 ; cudaMemPoolSetAttribute ( memPool , cudaMemPoolAttrReservedMemHigh , & value ); cudaMemPoolSetAttribute ( memPool , cudaMemPoolAttrUsedMemHigh , & value ); } 11.9.
Memory Reuse Policies  In order to service an allocation request, the driver attempts to reuse memory that was previously freed via cudaFreeAsync() before attempting to allocate more memory from the OS.
For example, memory freed in a stream can immediately be reused for a subsequent allocation request in the same stream.
Similarly, when a stream is synchronized with the CPU, the memory that was previously freed in that stream becomes available for reuse for an allocation in any stream.
The pool attributes cudaMemPoolReuseFollowEventDependencies , cudaMemPoolReuseAllowOpportunistic , and cudaMemPoolReuseAllowInternalDependencies control these policies.
Upgrading to a newer CUDA driver may change, enhance, augment and/or reorder the reuse policies. 11.9.1. cudaMemPoolReuseFollowEventDependencies  Before allocating more physical GPU memory, the allocator examines dependency information established by CUDA events and tries to allocate from memory freed in another stream.
cudaMallocAsync ( & ptr , size , originalStream ); kernel >> ( ptr , ...); cudaFreeAsync ( ptr , originalStream ); cudaEventRecord ( event , originalStream );   waiting on the event that captures the free in another stream   allows the allocator to reuse the memory to satisfy   a new allocation request in the other stream when   cudaMemPoolReuseFollowEventDependencies is enabled.
cudaStreamWaitEvent ( otherStream , event ); cudaMallocAsync ( & ptr2 , size , otherStream ); 11.9.2.
cudaMemPoolReuseAllowOpportunistic  According to the cudaMemPoolReuseAllowOpportunistic policy, the allocator examines freed allocations to see if the free’s stream order semantic has been met (such as the stream has passed the point of execution indicated by the free).
When this is disabled, the allocator will still reuse memory made available when a stream is synchronized with the CPU.
Disabling this policy does not stop the cudaMemPoolReuseFollowEventDependencies from applying.
cudaMallocAsync ( & ptr , size , originalStream ); kernel >> ( ptr , ...); cudaFreeAsync ( ptr , originalStream );   after some time, the kernel finishes running wait ( 10 );   When cudaMemPoolReuseAllowOpportunistic is enabled this allocation request   can be fulfilled with the prior allocation based on the progress of originalStream.
cudaMemPoolReuseAllowInternalDependencies  Failing to allocate and map more physical memory from the OS, the driver will look for memory whose availability depends on another stream’s pending progress.
If such memory is found, the driver will insert the required dependency into the allocating stream and reuse the memory.
cudaMallocAsync ( & ptr , size , originalStream ); kernel >> ( ptr , ...); cudaFreeAsync ( ptr , originalStream );   When cudaMemPoolReuseAllowInternalDependencies is enabled   and the driver fails to allocate more physical memory, the driver may   effectively perform a cudaStreamWaitEvent in the allocating stream   to make sure that future work in ‘otherStream’ happens after the work   in the original stream that would be allowed to access the original allocation.
Disabling Reuse Policies  While the controllable reuse policies improve memory reuse, users may want to disable them.
Allowing opportunistic reuse (such as cudaMemPoolReuseAllowOpportunistic ) introduces run to run variance in allocation patterns based on the interleaving of CPU and GPU execution.
Internal dependency insertion (such as cudaMemPoolReuseAllowInternalDependencies ) can serialize work in unexpected and potentially non-deterministic ways when the user would rather explicitly synchronize an event or stream on allocation failure. 11.10. Device Accessibility for Multi-GPU Support  Just like allocation accessibility controlled through the virtual memory management APIs, memory pool allocation accessibility does not follow cudaDeviceEnablePeerAccess or cuCtxEnablePeerAccess .
Instead, the API cudaMemPoolSetAccess modifies what devices can access allocations from a pool.
To enable access from other devices, the accessing device must be peer capable with the memory pool’s device; check with cudaDeviceCanAccessPeer .
If the peer capability is not checked, the set access may fail with cudaErrorInvalidDevice .
If no allocations had been made from the pool, the cudaMemPoolSetAccess call may succeed even when the devices are not peer capable; in this case, the next allocation from the pool will fail.
It is worth noting that cudaMemPoolSetAccess affects all allocations from the memory pool, not just future ones.
Also the accessibility reported by cudaMemPoolGetAccess applies to all allocations from the pool, not just future ones.
It is recommended that the accessibility settings of a pool for a given GPU not be changed frequently; once a pool is made accessible from a given GPU, it should remain accessible from that GPU for the lifetime of the pool.
snippet showing usage of cudaMemPoolSetAccess: cudaError_t setAccessOnDevice ( cudaMemPool_t memPool , int residentDevice , int accessingDevice ) { cudaMemAccessDesc accessDesc = {}; accessDesc .
flags = cudaMemAccessFlagsProtReadWrite ; int canAccess = 0 ; cudaError_t error = cudaDeviceCanAccessPeer ( & canAccess , accessingDevice , residentDevice ); if ( error != cudaSuccess ) { return error ; } else if ( canAccess == 0 ) { return cudaErrorPeerAccessUnsupported ; }   Make the address accessible return cudaMemPoolSetAccess ( memPool , & accessDesc , 1 ); } 11.11.
IPC Memory Pools  IPC capable memory pools allow easy, efficient and secure sharing of GPU memory between processes.
CUDA’s IPC memory pools provide the same security benefits as CUDA’s virtual memory management APIs.
The processes first need to share access to the pool, then share specific allocations from that pool.
The second phase coordinates what virtual addresses are used in each process and when mappings need to be valid in the importing process. 11.11.1. Creating and Sharing IPC Memory Pools  Sharing access to a pool involves retrieving an OS native handle to the pool (with the cudaMemPoolExportToShareableHandle() API), transferring the handle to the importing process using the usual OS native IPC mechanisms, and creating an imported memory pool (with the cudaMemPoolImportFromShareableHandle() API).
For cudaMemPoolExportToShareableHandle to succeed, the memory pool had to be created with the requested handle type specified in the pool properties structure.
Please reference samples for the appropriate IPC mechanisms to transfer the OS native handle between processes.
in exporting process   create an exportable IPC capable pool on device 0 cudaMemPoolProps poolProps = { }; poolProps .
type = cudaMemLocationTypeDevice ;   Setting handleTypes to a non zero value will make the pool exportable (IPC capable) poolProps .
handleTypes = CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR ; cudaMemPoolCreate ( & memPool , & poolProps ));   FD based handles are integer types int fdHandle = 0 ;   Retrieve an OS native handle to the pool.
cudaMemPoolExportToShareableHandle ( & fdHandle , memPool , CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR , 0 );   The handle must be sent to the importing process with the appropriate   OS specific APIs.
in importing process int fdHandle ;   The handle needs to be retrieved from the exporting process with the   appropriate OS specific APIs.
cudaMemPoolImportFromShareableHandle ( & importedMemPool , ( void * ) fdHandle , CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR , 0 ); 11.11.2.
Set Access in the Importing Process  Imported memory pools are initially only accessible from their resident device.
The importing process needs to enable access (with cudaMemPoolSetAccess ) from any GPU it plans to access the memory from.
If the imported memory pool belongs to a non-visible device in the importing process, the user must use the cudaMemPoolSetAccess API to enable access from the GPUs the allocations will be used on. 11.11.3. Creating and Sharing Allocations from an Exported Pool  Once the pool has been shared, allocations made with cudaMallocAsync() from the pool in the exporting process can be shared with other processes that have imported the pool.
Since the pool’s security policy is established and verified at the pool level, the OS does not need extra bookkeeping to provide security for specific pool allocations; In other words, the opaque cudaMemPoolPtrExportData required to import a pool allocation may be sent to the importing process using any mechanism.
While allocations may be exported and even imported without synchronizing with the allocating stream in any way, the importing process must follow the same rules as the exporting process when accessing the allocation.
Namely, access to the allocation must happen after the stream ordering of the allocation operation in the allocating stream.
The two following code snippets show cudaMemPoolExportPointer() and cudaMemPoolImportPointer() sharing the allocation with an IPC event used to guarantee that the allocation isn’t accessed in the importing process before the allocation is ready.
preparing an allocation in the exporting process cudaMemPoolPtrExportData exportData ; cudaEvent_t readyIpcEvent ; cudaIpcEventHandle_t readyIpcEventHandle ;   ipc event for coordinating between processes   cudaEventInterprocess flag makes the event an ipc event   cudaEventDisableTiming is set for performance reasons cudaEventCreate ( & readyIpcEvent , cudaEventDisableTiming | cudaEventInterprocess )   allocate from the exporting mem pool cudaMallocAsync ( & ptr , size , exportMemPool , stream );   event for sharing when the allocation is ready.
cudaEventRecord ( readyIpcEvent , stream ); cudaMemPoolExportPointer ( & exportData , ptr ); cudaIpcGetEventHandle ( & readyIpcEventHandle , readyIpcEvent );   Share IPC event and pointer export data with the importing process using   any mechanism.
Here we copy the data into shared memory shmem -> ptrData = exportData ; shmem -> readyIpcEventHandle = readyIpcEventHandle ;   signal consumers data is ready   Importing an allocation cudaMemPoolPtrExportData * importData = & shmem -> prtData ; cudaEvent_t readyIpcEvent ; cudaIpcEventHandle_t * readyIpcEventHandle = & shmem -> readyIpcEventHandle ;   Need to retrieve the ipc event handle and the export data from the   exporting process using any mechanism.
Here we are using shmem and just   need synchronization to make sure the shared memory is filled in.
cudaIpcOpenEventHandle ( & readyIpcEvent , readyIpcEventHandle );   import the allocation.
cudaMemPoolImportPointer ( & ptr , importedMemPool , importData );   Wait for the prior stream operations in the allocating stream to complete before   using the allocation in the importing process.
cudaStreamWaitEvent ( stream , readyIpcEvent ); kernel >> ( ptr , ...); When freeing the allocation, the allocation needs to be freed in the importing process before it is freed in the exporting process.
The following code snippet demonstrates the use of CUDA IPC events to provide the required synchronization between the cudaFreeAsync operations in both processes.
Access to the allocation from the importing process is obviously restricted by the free operation in the importing process side.
It is worth noting that cudaFree can be used to free the allocation in both processes and that other stream synchronization APIs may be used instead of CUDA IPC events.
The free must happen in importing process before the exporting process kernel >> ( ptr , ...);   Last access in importing process cudaFreeAsync ( ptr , stream );   Access not allowed in the importing process after the free cudaIpcEventRecord ( finishedIpcEvent , stream );   Exporting process   The exporting process needs to coordinate its free with the stream order   of the importing process’s free.
cudaStreamWaitEvent ( stream , finishedIpcEvent ); kernel >> ( ptrInExportingProcess , ...);   The free in the importing process doesn’t stop the exporting process   from using the allocation.
IPC Export Pool Limitations  IPC pools currently do not support releasing physical blocks back to the OS.
As a result the cudaMemPoolTrimTo API acts as a no-op and the cudaMemPoolAttrReleaseThreshold effectively gets ignored.
This behavior is controlled by the driver, not the runtime and may change in a future driver update. 11.11.5. IPC Import Pool Limitations  Allocating from an import pool is not allowed; specifically, import pools cannot be set current and cannot be used in the cudaMallocFromPoolAsync API.
The resource usage stat attribute queries only reflect the allocations imported into the process and the associated physical memory. 11.12. Synchronization API Actions  One of the optimizations that comes with the allocator being part of the CUDA driver is integration with the synchronize APIs.
When the user requests that the CUDA driver synchronize, the driver waits for asynchronous work to complete.
Before returning, the driver will determine what frees the synchronization guaranteed to be completed.
These allocations are made available for allocation regardless of specified stream or disabled allocation policies.
The driver also checks cudaMemPoolAttrReleaseThreshold here and releases any excess physical memory that it can. 11.13. Addendums  11.13.1.
cudaMemcpyAsync Current Context/Device Sensitivity  In the current CUDA driver, any async memcpy involving memory from cudaMallocAsync should be done using the specified stream’s context as the calling thread’s current context.
This is not necessary for cudaMemcpyPeerAsync , as the device primary contexts specified in the API are referenced instead of the current context. 11.13.2. cuPointerGetAttribute Query  Invoking cuPointerGetAttribute on an allocation after invoking cudaFreeAsync on it results in undefined behavior.
Specifically, it does not matter if an allocation is still accessible from a given stream: the behavior is still undefined. 11.13.3. cuGraphAddMemsetNode  cuGraphAddMemsetNode does not work with memory allocated via the stream ordered allocator.
However, memsets of the allocations can be stream captured. 11.13.4. Pointer Attributes  The cuPointerGetAttributes query works on stream ordered allocations.
Since stream ordered allocations are not context associated, querying CU_POINTER_ATTRIBUTE_CONTEXT will succeed but return NULL in *data .
The attribute CU_POINTER_ATTRIBUTE_DEVICE_ORDINAL can be used to determine the location of the allocation: this can be useful when selecting a context for making p2h2p copies using cudaMemcpyPeerAsync .
The attribute CU_POINTER_ATTRIBUTE_MEMPOOL_HANDLE was added in CUDA 11.3 and can be useful for debugging and for confirming which pool an allocation comes from before doing IPC. 12. Graph Memory Nodes  12.1.
Graph memory nodes have GPU ordered lifetime semantics, which dictate when memory is allowed to be accessed on the device.
These GPU ordered lifetime semantics enable driver-managed memory reuse, and match those of the stream ordered allocation APIs cudaMallocAsync and cudaFreeAsync , which may be captured when creating a graph.
Graph allocations have fixed addresses over the life of a graph including repeated instantiations and launches.
This allows the memory to be directly referenced by other operations within the graph without the need of a graph update, even when CUDA changes the backing physical memory.
Within a graph, allocations whose graph ordered lifetimes do not overlap may use the same underlying physical memory.
CUDA may reuse the same physical memory for allocations across multiple graphs, aliasing virtual address mappings according to the GPU ordered lifetime semantics.
For example when different graphs are launched into the same stream, CUDA may virtually alias the same physical memory to satisfy the needs of allocations which have single-graph lifetimes. 12.2. Support and Compatibility  Graph memory nodes require an 11.4 capable CUDA driver and support for the stream ordered allocator on the GPU.
int driverVersion = 0 ; int deviceSupportsMemoryPools = 0 ; int deviceSupportsMemoryNodes = 0 ; cudaDriverGetVersion ( & driverVersion ); if ( driverVersion >= 11020 ) {   avoid invalid value error in cudaDeviceGetAttribute cudaDeviceGetAttribute ( & deviceSupportsMemoryPools , cudaDevAttrMemoryPoolsSupported , device ); } deviceSupportsMemoryNodes = ( driverVersion >= 11040 ) && ( deviceSupportsMemoryPools != 0 ); Doing the attribute query inside the driver version check avoids an invalid value return code on 11.0 and 11.1 drivers.
Be aware that the compute sanitizer emits warnings when it detects CUDA returning error codes, and a version check before reading the attribute will avoid this.
Graph memory nodes are only supported on driver versions 11.4 and newer. 12.3. API Fundamentals  Graph memory nodes are graph nodes representing either memory allocation or free actions.
While these virtual addresses are fixed for the lifetime of the allocation node, the allocation contents are not persistent past the freeing operation and may be overwritten by accesses referring to a different allocation.
A graph allocation’s lifetime, which differs from the node’s lifetime, begins when GPU execution reaches the allocating graph node and ends when one of the following occurs: GPU execution reaches the freeing graph node GPU execution reaches the freeing cudaFreeAsync() stream call immediately upon the freeing call to cudaFree() Note Graph destruction does not automatically free any live graph-allocated memory, even though it ends the lifetime of the allocation node.
The allocation must subsequently be freed in another graph, or using cudaFreeAsync() /cudaFree() .
Just like other graph nodes , graph memory nodes are ordered within a graph by dependency edges.
A program must guarantee that operations accessing graph memory: are ordered after the allocation node are ordered before the operation freeing the memory Graph allocation lifetimes begin and usually end according to GPU execution (as opposed to API invocation).
GPU ordering is the order that work runs on the GPU as opposed to the order that the work is enqueued or described.
Graph Node APIs  Graph memory nodes may be explicitly created with the memory node creation APIs, cudaGraphAddMemAllocNode and cudaGraphAddMemFreeNode .
The address allocated by cudaGraphAddMemAllocNode is returned to the user in the dptr field of the passed CUDA_MEM_ALLOC_NODE_PARAMS structure.
All operations using graph allocations inside the allocating graph must be ordered after the allocating node.
Similarly, any free nodes must be ordered after all uses of the allocation within the graph.
Kernel nodes a , b , and c are ordered after the allocation node and before the free node such that the kernels can access the allocation.
Kernel node e is not ordered after the alloc node and therefore cannot safely access the memory.
Kernel node d is not ordered before the free node, therefore it cannot safely access the memory.
Since the dependency of node b on node a establishes an indirect dependency, the free node does not need to explicitly depend on node a.
dependencies[0] = b; dependencies[1] = c; cudaGraphAddMemFreeNode(&freeNode, graph, dependencies, 2, params.dptr);   free node does not depend on kernel node d, so it must not access the freed graph allocation.
cudaGraphAddKernelNode(&d, graph, &c, 1, &nodeParams);   node e does not depend on the allocation node, so it must not access the allocation.
Stream Capture  Graph memory nodes can be created by capturing the corresponding stream ordered allocation and free calls cudaMallocAsync and cudaFreeAsync .
In this case, the virtual addresses returned by the captured allocation API can be used by other operations inside the graph.
Since the stream ordered dependencies will be captured into the graph, the ordering requirements of the stream ordered allocation APIs guarantee that the graph memory nodes will be properly ordered with respect to the captured stream operations (for correctly written stream code).
Ignoring kernel nodes d and e , for clarity, the following code snippet shows how to use stream capture to create the graph from the previous figure: cudaMallocAsync(&dptr, size, stream1); kernel_A>>(dptr, ...);   Fork into stream2 cudaEventRecord(event1, stream1); cudaStreamWaitEvent(stream2, event1); kernel_B>>(dptr, ...);   event dependencies translated into graph dependencies, so the kernel node created by the capture of kernel C will depend on the allocation node created by capturing the cudaMallocAsync call.
kernel_C>>(dptr, ...);   Join stream2 back to origin stream (stream1) cudaEventRecord(event2, stream2); cudaStreamWaitEvent(stream1, event2);   Free depends on all work accessing the memory.
cudaFreeAsync(dptr, stream1);   End capture in the origin stream cudaStreamEndCapture(stream1, &graph); 12.3.3.
Accessing and Freeing Graph Memory Outside of the Allocating Graph  Graph allocations do not have to be freed by the allocating graph.
When a graph does not free an allocation, that allocation persists beyond the execution of the graph and can be accessed by subsequent CUDA operations.
These allocations may be accessed in another graph or directly using a stream operation as long as the accessing operation is ordered after the allocation through CUDA events and other stream ordering mechanisms.
An allocation may subsequently be freed by regular calls to cudaFree , cudaFreeAsync , or by the launch of another graph with a corresponding free node, or a subsequent launch of the allocating graph (if it was instantiated with the cudaGraphInstantiateFlagAutoFreeOnLaunch flag).
It is illegal to access memory after it has been freed - the free operation must be ordered after all operations accessing the memory using graph dependencies, CUDA events, and other stream ordering mechanisms.
Note Because graph allocations may share underlying physical memory with each other, the Virtual Aliasing Support rules relating to consistency and coherency must be considered.
Simply put, the free operation must be ordered after the full device operation (for example, compute kernel / memcpy) completes.
Specifically, out of band synchronization - for example a handshake through memory as part of a compute kernel that accesses the graph-allocated memory - is not sufficient for providing ordering guarantees between the memory writes to graph memory and the free operation of that graph memory.
The following code snippets demonstrate accessing graph allocations outside of the allocating graph with ordering properly established by: using a single stream, using events between streams, and using events baked into the allocating and freeing graph.
cudaEvent_t streamUseDoneEvent;   event indicating when the stream operations are done with the allocation.
Contents of allocating graph with event record node cudaGraphAddMemAllocNode(&allocNode, allocGraph, NULL, 0, ¶ms); dptr = params.dptr;   note: this event record node depends on the alloc node cudaGraphAddEventRecordNode(&recordNode, allocGraph, &allocNode, 1, allocEvent); cudaGraphInstantiate(&allocGraphExec, allocGraph, NULL, NULL, 0);   contents of consuming/freeing graph with event wait nodes cudaGraphAddEventWaitNode(&streamUseDoneEventNode, waitAndFreeGraph, NULL, 0, streamUseDoneEvent); cudaGraphAddEventWaitNode(&allocReadyEventNode, waitAndFreeGraph, NULL, 0, allocEvent); nodeParams->kernelParams[0] = params.dptr;   The allocReadyEventNode provides ordering with the alloc node for use in a consuming graph.
cudaGraphAddKernelNode(&kernelNode, waitAndFreeGraph, &allocReadyEventNode, 1, &nodeParams);   The free node has to be ordered after both external and internal users.
dependencies[0] = kernelNode; dependencies[1] = streamUseDoneEventNode; cudaGraphAddMemFreeNode(&freeNode, waitAndFreeGraph, &dependencies, 2, dptr); cudaGraphInstantiate(&waitAndFreeGraphExec, waitAndFreeGraph, NULL, NULL, 0); cudaGraphLaunch(allocGraphExec, allocStream);   establish the dependency of stream2 on the event node satisfies the ordering requirement cudaStreamWaitEvent(stream2, allocEvent); kernel>> (dptr, …); cudaStreamRecordEvent(streamUseDoneEvent, stream2);   the event wait node in the waitAndFreeGraphExec establishes the dependency on the “readyForFreeEvent” that is needed to prevent the kernel running in stream two from accessing the allocation after the free node in execution order.
cudaGraphInstantiateFlagAutoFreeOnLaunch  Under normal circumstances, CUDA will prevent a graph from being relaunched if it has unfreed memory allocations because multiple allocations at the same address will leak memory.
Instantiating a graph with the cudaGraphInstantiateFlagAutoFreeOnLaunch flag allows the graph to be relaunched while it still has unfreed allocations.
In this case, the launch automatically inserts an asynchronous free of the unfreed allocations.
At each iteration, a producer graph creates several allocations, and, depending on runtime conditions, a varying set of consumers accesses those allocations.
This type of variable execution sequence means that consumers cannot free the allocations because a subsequent consumer may require access.
Auto free on launch means that the launch loop does not need to track the producer’s allocations - instead, that information remains isolated to the producer’s creation and destruction logic.
In general, auto free on launch simplifies an algorithm which would otherwise need to free all the allocations owned by a graph before each relaunch.
Note The cudaGraphInstantiateFlagAutoFreeOnLaunch flag does not change the behavior of graph destruction.
The application must explicitly free the unfreed memory in order to avoid memory leaks, even for graphs instantiated with the flag.
The following code shows the use of cudaGraphInstantiateFlagAutoFreeOnLaunch to simplify a single-producer / multiple-consumer algorithm:   Create producer graph which allocates memory and populates it with data cudaStreamBeginCapture ( cudaStreamPerThread , cudaStreamCaptureModeGlobal ); cudaMallocAsync ( & data1 , blocks * threads , cudaStreamPerThread ); cudaMallocAsync ( & data2 , blocks * threads , cudaStreamPerThread ); produce >> ( data1 , data2 ); ...
cudaStreamEndCapture ( cudaStreamPerThread , & graph ); cudaGraphInstantiateWithFlags ( & producer , graph , cudaGraphInstantiateFlagAutoFreeOnLaunch ); cudaGraphDestroy ( graph );   Create first consumer graph by capturing an asynchronous library call cudaStreamBeginCapture ( cudaStreamPerThread , cudaStreamCaptureModeGlobal ); consumerFromLibrary ( data1 , cudaStreamPerThread ); cudaStreamEndCapture ( cudaStreamPerThread , & graph ); cudaGraphInstantiateWithFlags ( & consumer1 , graph , 0 );  regular instantiation cudaGraphDestroy ( graph );   Create second consumer graph cudaStreamBeginCapture ( cudaStreamPerThread , cudaStreamCaptureModeGlobal ); consume2 >> ( data2 ); ...
cudaStreamEndCapture ( cudaStreamPerThread , & graph ); cudaGraphInstantiateWithFlags ( & consumer2 , graph , 0 ); cudaGraphDestroy ( graph );   Launch in a loop bool launchConsumer2 = false ; do { cudaGraphLaunch ( producer , myStream ); cudaGraphLaunch ( consumer1 , myStream ); if ( launchConsumer2 ) { cudaGraphLaunch ( consumer2 , myStream ); } } while ( determineAction ( & launchConsumer2 )); cudaFreeAsync ( data1 , myStream ); cudaFreeAsync ( data2 , myStream ); cudaGraphExecDestroy ( producer ); cudaGraphExecDestroy ( consumer1 ); cudaGraphExecDestroy ( consumer2 ); 12.4.
Optimized Memory Reuse  CUDA reuses memory in two ways: Virtual and physical memory reuse within a graph is based on virtual address assignment, like in the stream ordered allocator.
Physical memory reuse between graphs is done with virtual aliasing: different graphs can map the same physical memory to their unique virtual addresses. 12.4.1. Address Reuse within a Graph  CUDA may reuse memory within a graph by assigning the same virtual address ranges to different allocations whose lifetimes do not overlap.
Since virtual addresses may be reused, pointers to different allocations with disjoint lifetimes are not guaranteed to be unique.
The following figure shows adding a new allocation node (2) that can reuse the address freed by a dependent node (1).
Figure 29 Adding New Alloc Node 2  The following figure shows adding a new alloc node (4).
The new alloc node is not dependent on the free node (2) so cannot reuse the address from the associated alloc node (2).
If the alloc node (2) used the address freed by free node (1), the new alloc node 3 would need a new address.
Physical Memory Management and Sharing  CUDA is responsible for mapping physical memory to the virtual address before the allocating node is reached in GPU order.
As an optimization for memory footprint and mapping overhead, multiple graphs may use the same physical memory for distinct allocations if they will not run simultaneously; however, physical pages cannot be reused if they are bound to more than one executing graph at the same time, or to a graph allocation which remains unfreed.
CUDA may update physical memory mappings at any time during graph instantiation, launch, or execution.
CUDA may also introduce synchronization between future graph launches in order to prevent live graph allocations from referring to the same physical memory.
As for any allocate-free-allocate pattern, if a program accesses a pointer outside of an allocation’s lifetime, the erroneous access may silently read or write live data owned by another allocation (even if the virtual address of the allocation is unique).
Since the graphs in the same stream never run concurrently, CUDA can and should use the same physical memory to satisfy all the allocations.
Performance Considerations  When multiple graphs are launched into the same stream, CUDA attempts to allocate the same physical memory to them because the execution of these graphs cannot overlap.
Physical mappings for a graph are retained between launches as an optimization to avoid the cost of remapping.
If, at a later time, one of the graphs is launched such that its execution may overlap with the others (for example if it is launched into a different stream) then CUDA must perform some remapping because concurrent graphs require distinct memory to avoid data corruption.
In general, remapping of graph memory in CUDA is likely caused by these operations: Changing the stream into which a graph is launched A trim operation on the graph memory pool, which explicitly frees unused memory (discussed in Physical Memory Footprint ) Relaunching a graph while an unfreed allocation from another graph is mapped to the same memory will cause a remap of memory before relaunch Remapping must happen in execution order, but after any previous execution of that graph is complete (otherwise memory that is still in use could be unmapped).
Due to this ordering dependency, as well as because mapping operations are OS calls, mapping operations can be relatively expensive.
Applications can avoid this cost by launching graphs containing allocation memory nodes consistently into the same stream. 12.5.1. First Launch / cudaGraphUpload  Physical memory cannot be allocated or mapped during graph instantiation because the stream in which the graph will execute is unknown.
Calling cudaGraphUpload can separate out the cost of allocation from the launch by performing all mappings for that graph immediately and associating the graph with the upload stream.
If the graph is then launched into the same stream, it will launch without any additional remapping.
Using different streams for graph upload and graph launch behaves similarly to switching streams, likely resulting in remap operations.
In addition, unrelated memory pool management is permitted to pull memory from an idle stream, which could negate the impact of the uploads. 12.6. Physical Memory Footprint  The pool-management behavior of asynchronous allocation means that destroying a graph which contains memory nodes (even if their allocations are free) will not immediately return physical memory to the OS for use by other processes.
To explicitly release memory back to the OS, an application should use the cudaDeviceGraphMemTrim API.
cudaDeviceGraphMemTrim will unmap and release any physical memory reserved by graph memory nodes that is not actively in use.
Allocations that have not been freed and graphs that are scheduled or running are considered to be actively using the physical memory and will not be impacted.
Use of the trim API will make physical memory available to other allocation APIs and other applications or processes, but will cause CUDA to reallocate and remap memory when the trimmed graphs are next launched.
CUDA allows applications to query their graph memory footprint through the cudaDeviceGetGraphMemAttribute API.
Querying the attribute cudaGraphMemAttrReservedMemCurrent returns the amount of physical memory reserved by the driver for graph allocations in the current process.
Querying cudaGraphMemAttrUsedMemCurrent returns the amount of physical memory currently mapped by at least one graph.
Either of these attributes can be used to track when new physical memory is acquired by CUDA for the sake of an allocating graph.
Both of these attributes are useful for examining how much memory is saved by the sharing mechanism. 12.7. Peer Access  Graph allocations can be configured for access from multiple GPUs, in which case CUDA will map the allocations onto the peer GPUs as required.
CUDA allows graph allocations requiring different mappings to reuse the same virtual address.
When this occurs, the address range is mapped onto all GPUs required by the different allocations.
This means an allocation may sometimes allow more peer access than was requested during its creation; however, relying on these extra mappings is still an error. 12.7.1. Peer Access with Graph Node APIs  The cudaGraphAddMemAllocNode API accepts mapping requests in the accessDescs array field of the node parameters structures.
The poolProps.location embedded structure specifies the resident device for the allocation.
Access from the allocating GPU is assumed to be needed, thus the application does not need to specify an entry for the resident device in the accessDescs array.
cudaMemAllocNodeParams params = {}; params.poolProps.allocType = cudaMemAllocationTypePinned; params.poolProps.location.type = cudaMemLocationTypeDevice;   specify device 1 as the resident device params.poolProps.location.id = 1; params.bytesize = size;   allocate an allocation resident on device 1 accessible from device 1 cudaGraphAddMemAllocNode(&allocNode, graph, NULL, 0, ¶ms); accessDescs[2];   boilerplate for the access descs (only ReadWrite and Device access supported by the add node api) accessDescs[0].flags = cudaMemAccessFlagsProtReadWrite; accessDescs[0].location.type = cudaMemLocationTypeDevice; accessDescs[1].flags = cudaMemAccessFlagsProtReadWrite; accessDescs[1].location.type = cudaMemLocationTypeDevice;   access being requested for device 0 & 2.
accessDescs[0].location.id = 0; accessDescs[1].location.id = 2;   access request array has 2 entries.
params.accessDescCount = 2; params.accessDescs = accessDescs;   allocate an allocation resident on device 1 accessible from devices 0, 1 and 2.
Peer Access with Stream Capture  For stream capture, the allocation node records the peer accessibility of the allocating pool at the time of the capture.
Altering the peer accessibility of the allocating pool after a cudaMallocFromPoolAsync call is captured does not affect the mappings that the graph will make for the allocation.
boilerplate for the access descs (only ReadWrite and Device access supported by the add node api) accessDesc.flags = cudaMemAccessFlagsProtReadWrite; accessDesc.location.type = cudaMemLocationTypeDevice; accessDesc.location.id = 1;   let memPool be resident and accessible on device 0 cudaStreamBeginCapture(stream); cudaMallocAsync(&dptr1, size, memPool, stream); cudaStreamEndCapture(stream, &graph1); cudaMemPoolSetAccess(memPool, &accessDesc, 1); cudaStreamBeginCapture(stream); cudaMallocAsync(&dptr2, size, memPool, stream); cudaStreamEndCapture(stream, &graph2);  The graph node allocating dptr1 would only have the device 0 accessibility even though memPool now has device 1 accessibility.
The graph node allocating dptr2 will have device 0 and device 1 accessibility, since that was the pool accessibility at the time of the cudaMallocAsync call. 13. Mathematical Functions  The reference manual lists, along with their description, all the functions of the C/C++ standard library mathematical functions that are supported in device code, as well as all intrinsic functions (that are only supported in device code).
For further information on the definition of the Unit in the Last Place (ULP), please see Jean-Michel Muller’s paper On the definition of ulp(x) , RR-5504, LIP RR-2005-09, INRIA, LIP.
Mathematical functions supported in device code do not set the global errno variable, nor report any floating-point exceptions to indicate errors; thus, if error diagnostic mechanisms are required, the user should implement additional screening for inputs and outputs of the functions.
The user must not pass uninitialized parameters to the Mathematical functions as this may result in undefined behavior: functions are inlined in the user program and thus are subject to compiler optimizations. 13.1. Standard Functions  The functions from this section can be used in both host and device code.
This section specifies the error bounds of each function when executed on the device and also when executed on the host in the case where the host does not supply the function.
The error bounds are generated from extensive but not exhaustive tests, so they are not guaranteed bounds.
Single-Precision Floating-Point Functions Addition and multiplication are IEEE-compliant, so have a maximum error of 0.5 ulp.
The recommended way to round a single-precision floating-point operand to an integer, with the result being a single-precision floating-point number is rintf() , not roundf() .
The reason is that roundf() maps to a 4-instruction sequence on the device, whereas rintf() maps to a single instruction.
The maximum error is stated as the absolute value of the difference in ulps between the result returned by the CUDA library function and a correctly rounded single-precision result obtained according to the round-to-nearest ties-to-even rounding mode.
 Function Maximum ulp error x+y 0 (IEEE-754 round-to-nearest-even) x*y 0 (IEEE-754 round-to-nearest-even) x/y 0 for compute capability \(\ge 2\) when compiled with -prec-div=true 2 (full range), otherwise 1/x 0 for compute capability \(\ge 2\) when compiled with -prec-div=true 1 (full range), otherwise rsqrtf(x) 1/sqrtf(x) 2 (full range) Applies to 1/sqrtf(x) only when it is converted to rsqrtf(x) by the compiler.
sqrtf(x) 0 when compiled with -prec-sqrt=true Otherwise 1 for compute capability \(\ge 5.2\) and 3 for older architectures cbrtf(x) 1 (full range) rcbrtf(x) 1 (full range) hypotf(x,y) 3 (full range) rhypotf(x,y) 2 (full range) norm3df(x,y,z) 3 (full range) rnorm3df(x,y,z) 2 (full range) norm4df(x,y,z,t) 3 (full range) rnorm4df(x,y,z,t) 2 (full range) normf(dim,arr) An error bound cannot be provided because a fast algorithm is used with accuracy loss due to round-off. . rnormf(dim,arr) An error bound cannot be provided because a fast algorithm is used with accuracy loss due to round-off.
Among these functions are the less accurate, but faster versions of some of the functions of Standard Functions .
The compiler has an option ( -use_fast_math ) that forces each function in Table 15 to compile to its intrinsic counterpart.
In addition to reducing the accuracy of the affected functions, it may also cause some differences in special case handling.
A more robust approach is to selectively replace mathematical function calls by calls to intrinsic functions only where it is merited by the performance gains and where changed properties such as reduced accuracy and different special case handling can be tolerated.
Table 15 Functions Affected by -use_fast_math  Operator/Function Device Function x/y __fdividef(x,y) sinf(x) __sinf(x) cosf(x) __cosf(x) tanf(x) __tanf(x) sincosf(x,sptr,cptr) __sincosf(x,sptr,cptr) logf(x) __logf(x) log2f(x) __log2f(x) log10f(x) __log10f(x) expf(x) __expf(x) exp10f(x) __exp10f(x) powf(x,y) __powf(x,y) Single-Precision Floating-Point Functions __fadd_[rn,rz,ru,rd]() and __fmul_[rn,rz,ru,rd]() map to addition and multiplication operations that the compiler never merges into FMADs.
By contrast, additions and multiplications generated from the ‘*’ and ‘+’ operators will frequently be combined into FMADs.
Functions suffixed with _ru operate using the round up (to positive infinity) rounding mode.
Functions suffixed with _rd operate using the round down (to negative infinity) rounding mode.
The accuracy of floating-point division varies depending on whether the code is compiled with -prec-div=false or -prec-div=true .
When the code is compiled with -prec-div=false , both the regular division / operator and __fdividef(x,y) have the same accuracy, but for 2 126 2.
__dsqrt_[rn,rz,ru,rd](x) IEEE-compliant. 14. C++ Language Support  As described in Compilation with NVCC , CUDA source files compiled with nvcc can include a mix of host code and device code.
The CUDA front-end compiler aims to emulate the host compiler behavior with respect to C++ input code.
The input source code is processed according to the C++ ISO/IEC 14882:2003, C++ ISO/IEC 14882:2011, C++ ISO/IEC 14882:2014 or C++ ISO/IEC 14882:2017 specifications, and the CUDA front-end compiler aims to emulate any host compiler divergences from the ISO specification.
In addition, the supported language is extended with CUDA-specific constructs described in this document 13 , and is subject to the restrictions described below.
C++11 Language Features , C++14 Language Features and C++17 Language Features provide support matrices for the C++11, C++14, C++17 and C++20 features, respectively.
Code Samples gives code samples. 14.1. C++11 Language Features  The following table lists new language features that have been accepted into the C++11 standard.
The “Proposal” column provides a link to the ISO C++ committee proposal that describes the feature, while the “Available in nvcc (device code)” column indicates the first version of nvcc that contains an implementation of this feature (if it has been implemented) for device code.
C++14 Language Features  The following table lists new language features that have been accepted into the C++14 standard.
Table 19 C++14 Language Features  Language Feature C++14 Proposal Available in nvcc (device code) Tweak to certain C++ contextual conversions N3323 9.0 Binary literals N3472 9.0 Functions with deduced return type N3638 9.0 Generalized lambda capture (init-capture) N3648 9.0 Generic (polymorphic) lambda expressions N3649 9.0 Variable templates N3651 9.0 Relaxing requirements on constexpr functions N3652 9.0 Member initializers and aggregates N3653 9.0 Clarifying memory allocation N3664 Sized deallocation N3778 [[deprecated]] attribute N3760 9.0 Single-quotation-mark as a digit separator N3781 9.0 14.3.
C++17 Language Features  All C++17 language features are supported in nvcc version 11.0 and later, subject to restrictions described here . 14.4. C++20 Language Features  All C++20 language features are supported in nvcc version 12.0 and later, subject to restrictions described here .
Host Compiler Extensions  Host compiler specific language extensions are not supported in device code.
__int128 type is supported in device code when compiled in conjunction with a host compiler that supports it.
A constant expression of __float128 type may be processed by the compiler in a floating point representation with lower precision. 14.5.2. Preprocessor Symbols  14.5.2.1.
__CUDA_ARCH__  The type signature of the following entities shall not depend on whether __CUDA_ARCH__ is defined or not, or on a particular value of __CUDA_ARCH__ : __global__ functions and function templates __device__ and __constant__ variables textures and surfaces Example: #if !defined(__CUDA_ARCH__) typedef int mytype ; #else typedef double mytype ; #endif __device__ mytype xxx ;   error: xxx's type depends on __CUDA_ARCH__ __global__ void foo ( mytype in ,   error: foo's type depends on __CUDA_ARCH__ mytype * ptr ) { * ptr = in ; } If a __global__ function template is instantiated and launched from the host, then the function template must be instantiated with the same template arguments irrespective of whether __CUDA_ARCH__ is defined and regardless of the value of __CUDA_ARCH__ .
Example: __device__ int result ; template __global__ void kern ( T in ) { result = in ; } __host__ __device__ void foo ( void ) { #if !defined(__CUDA_ARCH__) kern >> ( 1 );   error: "kern" instantiation only   when __CUDA_ARCH__ is undefined! #endif } int main ( void ) { foo (); cudaDeviceSynchronize (); return 0 ; } In separate compilation mode, the presence or absence of a definition of a function or variable with external linkage shall not depend on whether __CUDA_ARCH__ is defined or on a particular value of __CUDA_ARCH__ 14 .
Example: #if !defined(__CUDA_ARCH__) void foo ( void ) { }   error: The definition of foo()   is only present when __CUDA_ARCH__   is undefined #endif In separate compilation, __CUDA_ARCH__ must not be used in headers such that different objects could contain different behavior.
If a weak function or template function is defined in a header and its behavior depends on __CUDA_ARCH__ , then the instances of that function in the objects could conflict if the objects are compiled for different compute arch.
For example, if an a.h contains: template __device__ T * getptr ( void ) { #if __CUDA_ARCH__ == 700 return NULL ; /* no address */ #else __shared__ T arr [ 256 ]; return arr ; #endif } Then if a.cu and b.cu both include a.h and instantiate getptr for the same type, and b.cu expects a non-NULL address, and compile with: nvcc –arch=compute_70 –dc a.cu nvcc –arch=compute_80 –dc b.cu nvcc –arch=sm_80 a.o b.o At link time only one version of the getptr is used, so the behavior would depend on which version is chosen.
To avoid this, either a.cu and b.cu must be compiled for the same compute arch, or __CUDA_ARCH__ should not be used in the shared header function.
The compiler does not guarantee that a diagnostic will be generated for the unsupported uses of __CUDA_ARCH__ described above. 14.5.3. Qualifiers  14.5.3.1.
Device Memory Space Specifiers  The __device__ , __shared__ , __managed__ and __constant__ memory space specifiers are not allowed on: class , struct , and union data members, formal parameters, non-extern variable declarations within a function that executes on the host.
The __device__ , __constant__ and __managed__ memory space specifiers are not allowed on variable declarations that are neither extern nor static within a function that executes on the device.
A __device__ , __constant__ , __managed__ or __shared__ variable definition cannot have a class type with a non-empty constructor or a non-empty destructor.
A constructor for a class type is considered empty at a point in the translation unit, if it is either a trivial constructor or it satisfies all of the following conditions: The constructor function has been defined.
The constructor function has no parameters, the initializer list is empty and the function body is an empty compound statement.
Its class has no virtual functions, no virtual base classes and no non-static data member initializers.
For all the nonstatic data members of its class that are of class type (or array thereof), the default constructors can be considered empty.
A destructor for a class is considered empty at a point in the translation unit, if it is either a trivial destructor or it satisfies all of the following conditions: The destructor function has been defined.
For all the nonstatic data members of its class that are of class type (or array thereof), the destructor can be considered empty.
When compiling in the whole program compilation mode (see the nvcc user manual for a description of this mode), __device__ , __shared__ , __managed__ and __constant__ variables cannot be defined as external using the extern keyword.
The only exception is for dynamically allocated __shared__ variables as described in index.html#__shared__ .
When compiling in the separate compilation mode (see the nvcc user manual for a description of this mode), __device__ , __shared__ , __managed__ and __constant__ variables can be defined as external using the extern keyword.
nvlink will generate an error when it cannot find a definition for an external variable (unless it is a dynamically allocated __shared__ variable). 14.5.3.2. __managed__ Memory Space Specifier  Variables marked with the __managed__ memory space specifier (“managed” variables) have the following restrictions: The address of a managed variable is not a constant expression.
The address or value of a managed variable shall not be used when the CUDA runtime may not be in a valid state, including the following cases: In static/dynamic initialization or destruction of an object with static or thread local storage duration.
In code that executes after exit() has been called (for example, a function marked with gcc’s “ __attribute__((destructor)) ”).
In code that executes when CUDA runtime may not be initialized (for example, a function marked with gcc’s “ __attribute__((constructor)) ”).
A managed variable cannot be used as an unparenthesized id-expression argument to a decltype() expression.
Managed variables have the same coherence and consistency behavior as specified for dynamically allocated managed memory.
When a CUDA program containing managed variables is run on an execution platform with multiple GPUs, the variables are allocated only once, and not per GPU.
A managed variable declaration without the extern linkage is not allowed within a function that executes on the host.
A managed variable declaration without the extern or static linkage is not allowed within a function that executes on the device.
Volatile Qualifier  The compiler is free to optimize reads and writes to global or shared memory (for example, by caching global reads into registers or L1 cache) as long as it respects the memory ordering semantics of memory fence functions ( Memory Fence Functions ) and memory visibility semantics of synchronization functions ( Synchronization Functions ).
These optimizations can be disabled using the volatile keyword: If a variable located in global or shared memory is declared as volatile, the compiler assumes that its value can be changed or used at any time by another thread and therefore any reference to this variable compiles to an actual memory read or write instruction. 14.5.4. Pointers  Dereferencing a pointer either to global or shared memory in code that is executed on the host, or to host memory in code that is executed on the device results in an undefined behavior, most often in a segmentation fault and application termination.
The address obtained by taking the address of a __device__ , __shared__ or __constant__ variable can only be used in device code.
The address of a __device__ or __constant__ variable obtained through cudaGetSymbolAddress() as described in Device Memory can only be used in host code. 14.5.5. Operators  14.5.5.1.
Assignment Operator  __constant__ variables can only be assigned from the host code through runtime functions ( Device Memory ); they cannot be assigned from the device code.
It is not allowed to assign values to any of the built-in variables defined in Built-in Variables . 14.5.5.2. Address Operator  It is not allowed to take the address of any of the built-in variables defined in Built-in Variables .
14.5.6. Run Time Type Information (RTTI)  The following RTTI-related features are supported in host code, but not in device code.
Exception Handling  Exception handling is only supported in host code, but not in device code.
Exception specification is not supported for __global__ functions. 14.5.8. Standard Library  Standard libraries are only supported in host code, but not in device code, unless specified otherwise.
14.5.9. Namespace Reservations  Unless an exception is otherwise noted, it is undefined behavior to add any declarations or definitions to cuda:: , nv:: , cooperative_groups:: or any namespace nested within.
Examples: namespace cuda {   Bad: class declaration added to namespace cuda struct foo {};   Bad: function definition added to namespace cuda cudaStream_t make_stream (){ cudaStream_t s ; cudaStreamCreate ( & s ); return s ; } }   namespace cuda namespace cuda { namespace utils {   Bad: function definition added to namespace nested within cuda cudaStream_t make_stream (){ cudaStream_t s ; cudaStreamCreate ( & s ); return s ; } }   namespace utils }   namespace cuda namespace utils { namespace cuda {   Okay: namespace cuda may be used nested within a non-reserved namespace cudaStream_t make_stream (){ cudaStream_t s ; cudaStreamCreate ( & s ); return s ; } }   namespace cuda }   namespace utils   Bad: Equivalent to adding symbols to namespace cuda at global scope using namespace utils ; 14.5.10.
External Linkage  A call within some device code of a function declared with the extern qualifier is only allowed if the function is defined within the same compilation unit as the device code, i.e., a single file or several files linked together with relocatable device code and nvlink. 14.5.10.2. Implicitly-declared and explicitly-defaulted functions  Let F denote a function that is either implicitly-declared or is explicitly-defaulted on its first declaration The execution space specifiers ( __host__ , __device__ ) for F are the union of the execution space specifiers of all the functions that invoke it (note that a __global__ caller will be treated as a __device__ caller for this analysis).
For example: class Base { int x ; public : __host__ __device__ Base ( void ) : x ( 10 ) {} }; class Derived : public Base { int y ; }; class Other : public Base { int z ; }; __device__ void foo ( void ) { Derived D1 ; Other D2 ; } __host__ void bar ( void ) { Other D3 ; } Here, the implicitly-declared constructor function “Derived::Derived” will be treated as a __device__ function, since it is invoked only from the __device__ function “foo”.
The implicitly-declared constructor function “Other::Other” will be treated as a __host__ __device__ function, since it is invoked both from a __device__ function “foo” and a __host__ function “bar”.
In addition, if F is a virtual destructor, then the execution spaces of each virtual destructor D overridden by F are added to the set of execution spaces for F , if D is either not implicitly defined or is explicitly defaulted on a declaration other than its first declaration.
For example: struct Base1 { virtual __host__ __device__ ~ Base1 () { } }; struct Derived1 : Base1 { };   implicitly-declared virtual destructor   ~Derived1 has __host__ __device__   execution space specifiers struct Base2 { virtual __device__ ~ Base2 (); }; __device__ Base2 ::~ Base2 () = default ; struct Derived2 : Base2 { };   implicitly-declared virtual destructor   ~Derived2 has __device__ execution   space specifiers 14.5.10.3.
Function Parameters  __global__ function parameters are passed to the device via constant memory and are limited to 32,764 bytes starting with Volta, and 4 KB on older architectures.
In separate compilation mode, if a __device__ or __global__ function is ODR-used in a particular translation unit, then the parameter and return types of the function must be complete in that translation unit.
Example:  first.cu: struct S ; __device__ void foo ( S );   error: type 'S' is incomplete __device__ auto * ptr = foo ; int main () { }  second.cu: struct S { int x ; }; __device__ void foo ( S ) { }  compiler invocation $nvcc -std=c++14 -rdc=true first.cu second.cu -o first nvlink error : Prototype doesn't match for '_Z3foo1S' in '/tmp/tmpxft_00005c8c_00000000-18_second.o', first defined in '/tmp/tmpxft_00005c8c_00000000-18_second.o' nvlink fatal : merge_elf failed 14.5.10.3.1.
__global__ Function Argument Processing  When a __global__ function is launched from device code, each argument must be trivially copyable and trivially destructible.
When a __global__ function is launched from host code, each argument type is allowed to be non-trivially copyable or non-trivially-destructible, but the processing for such types does not follow the standard C++ model, as described below.
The workflow diverges from standard C++ in two areas: Memcpy instead of copy constructor invocation When lowering a __global__ function launch from host code, the compiler generates stub functions that copy the parameters one or more times by value, before eventually using memcpy to copy the arguments to the __global__ function’s parameter memory on the device.
This occurs even if an argument was non-trivially-copyable, and therefore may break programs where the copy constructor has side effects.
Example: #include struct S { int x ; int * ptr ; __host__ __device__ S () { } __host__ __device__ S ( const S & ) { ptr = & x ; } }; __global__ void foo ( S in ) {   this assert may fail, because the compiler   generated code will memcpy the contents of "in"   from host to kernel parameter memory, so the   "in.ptr" is not initialized to "&in.x" because   the copy constructor is skipped.
x ); } int main () { S tmp ; foo >> ( tmp ); cudaDeviceSynchronize (); } Example: #include __managed__ int counter ; struct S1 { S1 () { } S1 ( const S1 & ) { ++ counter ; } }; __global__ void foo ( S1 ) { /* this assertion may fail, because the compiler generates stub functions on the host for a kernel launch, and they may copy the argument by value more than once.
*/ assert ( counter == 1 ); } int main () { S1 V ; foo >> ( V ); cudaDeviceSynchronize (); } Destructor may be invoked before the ``__global__`` function has finished Kernel launches are asynchronous with host execution.
As a result, if a __global__ function argument has a non-trivial destructor, the destructor may execute in host code even before the __global__ function has finished execution.
Example: struct S { int * ptr ; S () : ptr ( nullptr ) { } S ( const S & ) { cudaMallocManaged ( & ptr , sizeof ( int )); } ~ S () { cudaFree ( ptr ); } }; __global__ void foo ( S in ) {  error: This store may write to memory that has already been   freed (see below).
ptr ) = 4 ; } int main () { S V ; /* The object 'V' is first copied by value to a compiler-generated * stub function that does the kernel launch, and the stub function * bitwise copies the contents of the argument to kernel parameter * memory.
* As a result, S::~S() will execute when the stub function returns, releasing allocated memory, even though the kernel may not have finished execution.
Toolkit and Driver Compatibility  Developers must use the 12.1 Toolkit and r530 driver or higher to compile, launch, and debug kernels that accept parameters larger than 4KB.
If such kernels are launched on older drivers, CUDA will issue the error CUDA_ERROR_NOT_SUPPORTED . 14.5.10.3.3. Link Compatibility across Toolkit Revisions  When linking device objects, if at least one device object contains a kernel with a parameter larger than 4KB, the developer must recompile all objects from their respective device sources with the 12.1 toolkit or higher before linking them together.
Failure to do so will result in a linker error. 14.5.10.4. Static Variables within Function  Variable memory space specifiers are allowed in the declaration of a static variable V within the immediate or nested block scope of a function F where: F is a __global__ or __device__ -only function.
If no explicit memory space specifier is present in the declaration of V , an implicit __device__ specifier is assumed during device compilation.
V has the same initialization restrictions as a variable with the same memory space specifiers declared in namespace scope for example a __device__ variable cannot have a ‘non-empty’ constructor (see Device Memory Space Specifiers ).
#ifdef __CUDA_ARCH__ static __device__ int d1 ;   OK, declaration is only visible during device   compilation (__CUDA_ARCH__ is defined) #else static int d0 ;   OK, declaration is only visible during host   compilation (__CUDA_ARCH__ is not defined) #endif static __device__ int d2 ;   error: __device__ variable inside   a host function during host compilation   i.e.
when __CUDA_ARCH__ is not defined static __shared__ int i2 ;   error: __shared__ variable inside   a host function during host compilation   i.e.
Function Pointers  The address of a __global__ function taken in host code cannot be used in device code (e.g.
Similarly, the address of a __global__ function taken in device code cannot be used in host code.
It is not allowed to take the address of a __device__ function in host code. 14.5.10.6. Function Recursion  __global__ functions do not support recursion.
14.5.10.7. Friend Functions  A __global__ function or function template cannot be defined in a friend declaration.
Example: struct S1_t { friend __global__ void foo1 ( void );   OK: not a definition template friend __global__ void foo2 ( void );   OK: not a definition friend __global__ void foo3 ( void ) { }   error: definition in friend declaration template friend __global__ void foo4 ( void ) { }   error: definition in friend declaration }; 14.5.10.8.
Operator Function  An operator function cannot be a __global__ function. 14.5.10.9. Allocation and Deallocation Functions  A user-defined operator new , operator new[] , operator delete , or operator delete[] cannot be used to replace the corresponding __host__ or __device__ builtins provided by the compiler.
Data Members  Static data members are not supported except for those that are also const-qualified (see Const-qualified variables ). 14.5.11.2. Function Members  Static member functions cannot be __global__ functions.
14.5.11.3. Virtual Functions  When a function in a derived class overrides a virtual function in a base class, the execution space specifiers (i.e., __host__ , __device__ ) on the overridden and overriding functions must match.
It is not allowed to pass as an argument to a __global__ function an object of a class with virtual functions.
If an object is created in host code, invoking a virtual function for that object in device code has undefined behavior.
If an object is created in device code, invoking a virtual function for that object in host code has undefined behavior.
Example: struct S1 { virtual __host__ __device__ void foo () { } }; __managed__ S1 * ptr1 , * ptr2 ; __managed__ __align__ ( 16 ) char buf1 [ 128 ]; __global__ void kern () { ptr1 -> foo ();   error: virtual function call on a object   created in host code.
ptr2 = new ( buf1 ) S1 (); } int main ( void ) { void * buf ; cudaMallocManaged ( & buf , sizeof ( S1 ), cudaMemAttachGlobal ); ptr1 = new ( buf ) S1 (); kern >> (); cudaDeviceSynchronize (); ptr2 -> foo ();   error: virtual function call on an object   created in device code. } 14.5.11.4. Virtual Base Classes  It is not allowed to pass as an argument to a __global__ function an object of a class derived from virtual base classes.
14.5.11.5. Anonymous Unions  Member variables of a namespace scope anonymous union cannot be referenced in a __global__ or __device__ function.
14.5.11.6. Windows-Specific  The CUDA compiler follows the IA64 ABI for class layout, while the Microsoft host compiler does not.
Let T denote a pointer to member type, or a class type that satisfies any of the following conditions: T has virtual functions.
All direct and indirect base classes B of T are empty and the type of the first field F of T uses B in its definition, such that B is laid out at offset 0 in the definition of F .
The CUDA compiler may compute the class layout and size differently than the Microsoft host compiler for the type C .
As long as the type C is used exclusively in host or device code, the program should work correctly.
Passing an object of type C between host and device code has undefined behavior, for example, as an argument to a __global__ function or through cudaMemcpy*() calls.
Accessing an object of type C or any subobject in device code, or invoking a member function in device code, has undefined behavior if the object is created in host code.
Accessing an object of type C or any subobject in host code, or invoking a member function in host code, has undefined behavior if the object is created in device code 18 . 14.5.12. Templates  A type or template cannot be used in the type, non-type or template template argument of a __global__ function template instantiation or a __device__/__constant__ variable instantiation if either: The type or template is defined within a __host__ or __host__ __device__ .
The type or template is a class member with private or protected access and its parent class is not defined within a __device__ or __global__ function.
Example: template __global__ void myKernel ( void ) { } class myClass { private : struct inner_t { }; public : static void launch ( void ) {   error: inner_t is used in template argument   but it is private myKernel >> (); } };   C++14 only template __device__ T d1 ; template __device__ T1 d2 ; void fn () { struct S1_t { };   error (C++14 only): S1_t is local to the function fn d1 = {}; auto lam1 = [] { };   error (C++14 only): a closure type cannot be used for   instantiating a variable template d2 = 10 ; } 14.5.13.
Digraphs are not supported on Windows. 14.5.14. Const-qualified variables  Let ‘V’ denote a namespace scope variable or a class static member variable that has const qualified type and does not have execution space annotations (for example, __device__, __constant__, __shared__ ).
The value of V may be directly used in device code, if V has been initialized with a constant expression before the point of use, the type of V is not volatile-qualified, and it has one of the following types: built-in floating point type except when the Microsoft compiler is used as the host compiler, built-in integral type.
Example: const int xxx = 10 ; struct S1_t { static const int yyy = 20 ; }; extern const int zzz ; const float www = 5.0 ; __device__ void foo ( void ) { int local1 [ xxx ];   OK int local2 [ S1_t :: yyy ];   OK int val1 = xxx ;   OK int val2 = S1_t :: yyy ;   OK int val3 = zzz ;   error: zzz not initialized with constant   expression at the point of use.
const int & val3 = xxx ;   error: reference to host variable const int * val4 = & xxx ;   error: address of host variable const float val5 = www ;   OK except when the Microsoft compiler is used as   the host compiler.
Long Double  The use of long double type is not supported in device code. 14.5.16. Deprecation Annotation  nvcc supports the use of deprecated attribute when using gcc , clang , xlC , icc or pgcc host compilers, and the use of deprecated declspec when using the cl.exe host compiler.
It also supports the [[deprecated]] standard attribute when the C++14 dialect has been enabled.
The CUDA frontend compiler will generate a deprecation diagnostic for a reference to a deprecated entity from within the body of a __device__ , __global__ or __host__ __device__ function when __CUDA_ARCH__ is defined (i.e., during device compilation phase).
Other references to deprecated entities will be handled by the host compiler, e.g., a reference from within a __host__ function.
The CUDA frontend compiler does not support the #pragma gcc diagnostic or #pragma warning mechanisms supported by various host compilers.
Therefore, deprecation diagnostics generated by the CUDA frontend compiler are not affected by these pragmas, but diagnostics generated by the host compiler will be affected.
To suppress the warning for device-code, user can use NVIDIA specific pragma #pragma nv_diag_suppress .
The nvcc flag -Wno-deprecated-declarations can be used to suppress all deprecation warnings, and the flag -Werror=deprecated-declarations can be used to turn deprecation warnings into errors. 14.5.17. Noreturn Annotation  nvcc supports the use of noreturn attribute when using gcc , clang , xlC , icc or pgcc host compilers, and the use of noreturn declspec when using the cl.exe host compiler.
It also supports the [[noreturn]] standard attribute when the C++11 dialect has been enabled.
The attribute/declspec can be used in both host and device code. 14.5.18. [[likely]] / [[unlikely]] Standard Attributes  These attributes are accepted in all configurations that support the C++ standard attribute syntax.
The attributes can be used to hint to the device compiler optimizer whether a statement is more or less likely to be executed compared to any alternative path that does not include the statement.
Example: __device__ int foo ( int x ) { if ( i __global__ void foo ( T in ) { }; template struct S1_t { }; void bar ( void ) { auto temp1 = [] { }; foo >> ( temp1 );   error: lambda closure type used in   template type argument foo >> ( S1_t ());   error: lambda closure type used in   template type argument } 14.5.22.2.
std::initializer_list  By default, the CUDA compiler will implicitly consider the member functions of std::initializer_list to have __host__ __device__ execution space specifiers, and therefore they can be invoked directly from device code.
The nvcc flag --no-host-device-initializer-list will disable this behavior; member functions of std::initializer_list will then be considered as __host__ functions and will not be directly invokable from device code.
Example: #include __device__ int foo ( std :: initializer_list in ); __device__ void bar ( void ) { foo ({ 4 , 5 , 6 });   (a) initializer list containing only   constant expressions.
int i = 4 ; foo ({ i , 5 , 6 });   (b) initializer list with at least one   non-constant element.
This form may have better performance than (a). } 14.5.22.3. Rvalue references  By default, the CUDA compiler will implicitly consider std::move and std::forward function templates to have __host__ __device__ execution space specifiers, and therefore they can be invoked directly from device code.
The nvcc flag --no-host-device-move-forward will disable this behavior; std::move and std::forward will then be considered as __host__ functions and will not be directly invokable from device code. 14.5.22.4. Constexpr functions and function templates  By default, a constexpr function cannot be called from a function with incompatible execution space 22 .
When this flag is specified, host code can invoke a __device__ constexpr function and device code can invoke a __host__ constexpr function.
nvcc will define the macro __CUDACC_RELAXED_CONSTEXPR__ when --expt-relaxed-constexpr has been specified.
Note that a function template instantiation may not be a constexpr function even if the corresponding template is marked with the keyword constexpr (C++11 Standard Section [dcl.constexpr.p6] ). 14.5.22.5. Constexpr variables  Let ‘V’ denote a namespace scope variable or a class static member variable that has been marked constexpr and that does not have execution space annotations (e.g., __device__, __constant__, __shared__ ).
If V is of scalar type 24 other than long double and the type is not volatile-qualified, the value of V can be directly used in device code.
In addition, if V is of a non-scalar type then scalar elements of V can be used inside a constexpr __device__ or __host__ __device__ function, if the call to the function is a constant expression 25 .
Example: constexpr int xxx = 10 ; constexpr int yyy = xxx + 4 ; struct S1_t { static constexpr int qqq = 100 ; }; constexpr int host_arr [] = { 1 , 2 , 3 }; constexpr __device__ int get ( int idx ) { return host_arr [ idx ]; } __device__ int foo ( int idx ) { int v1 = xxx + yyy + S1_t :: qqq ;   OK const int & v2 = xxx ;   error: reference to host constexpr   variable const int * v3 = & xxx ;   error: address of host constexpr   variable const int & v4 = S1_t :: qqq ;   error: reference to host constexpr   variable const int * v5 = & S1_t :: qqq ;   error: address of host constexpr   variable v1 += get ( 2 );   OK: 'get(2)' is a constant   expression.
v1 += get ( idx );   error: 'get(idx)' is not a constant   expression v1 += host_arr [ 2 ];   error: 'host_arr' does not have   scalar type.
Inline namespaces  For an input CUDA translation unit, the CUDA compiler may invoke the host compiler for compiling the host code within the translation unit.
In the code passed to the host compiler, the CUDA compiler will inject additional compiler generated code, if the input CUDA translation unit contained a definition of any of the following entities: __global__ function or function template instantiation __device__ , __constant__ variables with surface or texture type The compiler generated code contains a reference to the defined entity.
If the entity is defined within an inline namespace and another entity of the same name and type signature is defined in an enclosing namespace, this reference may be considered ambiguous by the host compiler and host compilation will fail.
This limitation can be avoided by using unique names for such entities defined within an inline namespace.
Example: __device__ int Gvar ; inline namespace N1 { __device__ int Gvar ; }   __global__ void foo ( void );   error __global__ void bar ( void ) { }   error template <> __global__ void foo ( void ) { }   error __device__ int x1b ;   error __constant__ int x2b ;   error __shared__ int x3b ;   error texture q2 ;   error surface s2 ;   error } }; 14.5.22.7.
thread_local  The thread_local storage specifier is not allowed in device code. 14.5.22.8. __global__ functions and function templates  If the closure type associated with a lambda expression is used in a template argument of a __global__ function template instantiation, the lambda expression must either be defined in the immediate or nested block scope of a __device__ or __global__ function, or must be an extended lambda .
Example: template __global__ void kernel ( T in ) { } __device__ void foo_device ( void ) {   All kernel instantiations in this function   are valid, since the lambdas are defined inside   a __device__ function.
kernel >> ( [] __device__ { } ); kernel >> ( [] __host__ __device__ { } ); kernel >> ( [] { } ); } auto lam1 = [] { }; auto lam2 = [] __host__ __device__ { }; void foo_host ( void ) {   OK: instantiated with closure type of an extended __device__ lambda kernel >> ( [] __device__ { } );   OK: instantiated with closure type of an extended __host__ __device__   lambda kernel >> ( [] __host__ __device__ { } );   error: unsupported: instantiated with closure type of a lambda   that is not an extended lambda kernel >> ( [] { } );   error: unsupported: instantiated with closure type of a lambda   that is not an extended lambda kernel >> ( lam1 );   error: unsupported: instantiated with closure type of a lambda   that is not an extended lambda kernel >> ( lam2 ); } A __global__ function or function template cannot be declared as constexpr .
A __global__ function or function template cannot have a parameter of type std::initializer_list or va_list .
A variadic __global__ function template has the following restrictions: Only a single pack parameter is allowed.
Pack > __global__ void foo1 ( Wrapper );   error: pack parameter is not last in parameter list template class Wrapper > __global__ void foo2 ( Wrapper );   error: multiple parameter packs template class Wrapper1 , template class Wrapper2 > __global__ void foo3 ( Wrapper1 , Wrapper2 ); 14.5.22.9.
__managed__ and __shared__ variables  `__managed__ and __shared__ variables cannot be marked with the keyword constexpr . 14.5.22.10. Defaulted functions  Execution space specifiers on a function that is explicitly-defaulted on its first declaration are ignored by the CUDA compiler.
Instead, the CUDA compiler will infer the execution space specifiers as described in Implicitly-declared and explicitly-defaulted functions .
Execution space specifiers are not ignored if the function is explicitly-defaulted, but not on its first declaration.
Example: struct S1 {   warning: __host__ annotation is ignored on a function that   is explicitly-defaulted on its first declaration __host__ S1 () = default ; }; __device__ void foo1 () {  note: __device__ execution space is derived for S1::S1   based on implicit call from within __device__ function   foo1 S1 s1 ; } struct S2 { __host__ S2 (); };  note: S2::S2 is not defaulted on its first declaration, and   its execution space is fixed to __host__ based on its   first declaration.
S2 :: S2 () = default ; __device__ void foo2 () {   error: call from __device__ function 'foo2' to   __host__ function 'S2::S2' S2 s2 ; } 14.5.23.
C++14 Features  C++14 features enabled by default by the host compiler are also supported by nvcc.
Passing nvcc -std=c++14 flag turns on all C++14 features and also invokes the host preprocessor, compiler and linker with the corresponding C++14 dialect option 26 .
This section describes the restrictions on the supported C++14 features. 14.5.23.1. Functions with deduced return type  A __global__ function cannot have a deduced return type.
If a __device__ function has deduced return type, the CUDA frontend compiler will change the function declaration to have a void return type, before invoking the host compiler.
This may cause issues for introspecting the deduced return type of the __device__ function in host code.
Thus, the CUDA compiler will issue compile-time errors for referencing such deduced return type outside device function bodies, except if the reference is absent when __CUDA_ARCH__ is undefined.
Examples: __device__ auto fn1 ( int x ) { return x ; } __device__ decltype ( auto ) fn2 ( int x ) { return x ; } __device__ void device_fn1 () {   OK int ( * p1 )( int ) = fn1 ; }   error: referenced outside device function bodies decltype ( fn1 ( 10 )) g1 ; void host_fn1 () {   error: referenced outside device function bodies int ( * p1 )( int ) = fn1 ; struct S_local_t {   error: referenced outside device function bodies decltype ( fn2 ( 10 )) m1 ; S_local_t () : m1 ( 10 ) { } }; }   error: referenced outside device function bodies template void host_fn2 () { } template struct S1_t { };   error: referenced outside device function bodies struct S1_derived_t : S1_t { }; 14.5.23.2.
Variable templates  A __device__/__constant__ variable template cannot have a const qualified type when using the Microsoft host compiler.
Examples:   error: a __device__ variable template cannot   have a const qualified type on Windows template __device__ const T d1 ( 2 ); int * const x = nullptr ;   error: a __device__ variable template cannot   have a const qualified type on Windows template __device__ T * const d2 ( x );   OK template __device__ const T * d3 ; __device__ void fn () { int t1 = d1 ; int * const t2 = d2 ; const int * t3 = d3 ; } 14.5.24.
C++17 Features  C++17 features enabled by default by the host compiler are also supported by nvcc.
Passing nvcc -std=c++17 flag turns on all C++17 features and also invokes the host preprocessor, compiler and linker with the corresponding C++17 dialect option 27 .
This section describes the restrictions on the supported C++17 features. 14.5.24.1. Inline Variable  A namespace scope inline variable declared with __device__ or __constant__ or __managed__ memory space specifier must have internal linkage, if the code is compiled with nvcc in whole program compilation mode.
Examples: inline __device__ int xxx ;  error when compiled with nvcc in  whole program compilation mode.
static inline __device__ int yyy ;   ok: internal linkage namespace { inline __device__ int zzz ;   ok: internal linkage } When using g++ host compiler, an inline variable declared with __managed__ memory space specifier may not be visible to the debugger. 14.5.24.2. Structured Binding  A structured binding cannot be declared with a variable memory space specifier.
Example: struct S { int x ; int y ; }; __device__ auto [ a1 , b1 ] = S { 4 , 5 };   error 14.5.25.
C++20 Features  C++20 features enabled by default by the host compiler are also supported by nvcc.
Passing nvcc -std=c++20 flag turns on all C++20 features and also invokes the host preprocessor, compiler and linker with the corresponding C++20 dialect option 28 .
This section describes the restrictions on the supported C++20 features. 14.5.25.1. Module support  Modules are not supported in CUDA C++, in either host or device code.
Uses of the module , export and import keywords are diagnosed as errors. 14.5.25.2. Coroutine support  Coroutines are not supported in device code.
Uses of the co_await , co_yield and co_return keywords in the scope of a device function are diagnosed as error during device compilation. 14.5.25.3. Three-way comparison operator  The three-way comparison operator is supported in both host and device code, but some uses implicitly rely on functionality from the Standard Template Library provided by the host implementation.
Uses of those operators may require specifying the flag --expt-relaxed-constexpr to silence warnings and the functionality requires that the host implementation satisfies the requirements of device code.
Instances of nvstd::function in device code cannot be initialized with the address of a __host__ function or with a functor whose operator() is a __host__ function.
nvstd::function instances cannot be passed from host code to device code (and vice versa) at run time.
nvstd::function cannot be used in the parameter type of a __global__ function, if the __global__ function is launched from host code.
Extended Lambdas  The nvcc flag '--extended-lambda' allows explicit execution space annotations in a lambda expression 29 .
The execution space annotations should be present after the ‘lambda-introducer’ and before the optional ‘lambda-declarator’.
nvcc will define the macro __CUDACC_EXTENDED_LAMBDA__ when the '--extended-lambda' flag has been specified.
An ‘extended __device__ lambda’ is a lambda expression that is annotated explicitly with ‘ __device__ ’, and is defined within the immediate or nested block scope of a __host__ or __host__ __device__ function.
An ‘extended __host__ __device__ lambda’ is a lambda expression that is annotated explicitly with both ‘ __host__ ’ and ‘ __device__ ’, and is defined within the immediate or nested block scope of a __host__ or __host__ __device__ function.
An ‘extended lambda’ denotes either an extended __device__ lambda or an extended __host__ __device__ lambda.
Extended lambdas can be used in the type arguments of __global__ function template instantiation .
If the execution space annotations are not explicitly specified, they are computed based on the scopes enclosing the closure class associated with the lambda, as described in the section on C++11 support.
The execution space annotations are applied to all methods of the closure class associated with the lambda.
auto lam1 = [] { }; auto lam2 = [] __device__ { }; auto lam3 = [] __host__ __device__ { }; auto lam4 = [] __host__ { }; }   lam1 and lam2 are not extended lambdas because they are not defined   within a __host__ or __host__ __device__ function.
Extended Lambda Type Traits  The compiler provides type traits to detect closure types for extended lambdas at compile time: __nv_is_extended_device_lambda_closure_type(type) : If ‘type’ is the closure class created for an extended __device__ lambda, then the trait is true, otherwise it is false.
__nv_is_extended_device_lambda_with_preserved_return_type(type) : If ‘type’ is the closure class created for an extended __device__ lambda and the lambda is defined with trailing return type (with restriction), then the trait is true, otherwise it is false.
If the trailing return type definition refers to any lambda parameter name, the return type is not preserved.
__nv_is_extended_host_device_lambda_closure_type(type) : If ‘type’ is the closure class created for an extended __host__ __device__ lambda, then the trait is true, otherwise it is false.
These traits can be used in all compilation modes, irrespective of whether lambdas or extended lambdas are enabled 30 .
static_assert ( IS_D_LAMBDA ( decltype ( lam5 )), "" ); static_assert ( ! IS_DPRT_LAMBDA ( decltype ( lam5 )), "" ); static_assert ( ! IS_HD_LAMBDA ( decltype ( lam5 )), "" ); } 14.7.2.
Extended Lambda Restrictions  The CUDA compiler will replace an extended lambda expression with an instance of a placeholder type defined in namespace scope, before invoking the host compiler.
The template argument of the placeholder type requires taking the address of a function enclosing the original extended lambda expression.
This is required for the correct execution of any __global__ function template whose template argument involves the closure type of an extended lambda.
By definition, the extended lambda is present within the immediate or nested block scope of a __host__ or __host__ __device__ function.
If this function is not the operator() of a lambda expression, then it is considered the enclosing function for the extended lambda.
Otherwise, the extended lambda is defined within the immediate or nested block scope of the operator() of one or more enclosing lambda expressions.
If the outermost such lambda expression is defined in the immediate or nested block scope of a function F , then F is the computed enclosing function, else the enclosing function does not exist.
Example: void foo ( void ) {   enclosing function for lam1 is "foo" auto lam1 = [] __device__ { }; auto lam2 = [] { auto lam3 = [] {   enclosing function for lam4 is "foo" auto lam4 = [] __host__ __device__ { }; }; }; } auto lam6 = [] {   enclosing function for lam7 does not exist auto lam7 = [] __host__ __device__ { }; }; Here are the restrictions on extended lambdas: An extended lambda cannot be defined inside another extended lambda expression.
Example: void foo ( void ) { auto lam1 = [] __host__ __device__ {   error: extended lambda defined within another extended lambda auto lam2 = [] __host__ __device__ { }; }; } An extended lambda cannot be defined inside a generic lambda expression.
Example: void foo ( void ) { auto lam1 = [] ( auto ) {   error: extended lambda defined within a generic lambda auto lam2 = [] __host__ __device__ { }; }; } If an extended lambda is defined within the immediate or nested block scope of one or more nested lambda expression, the outermost such lambda expression must be defined inside the immediate or nested block scope of a function.
Example: auto lam1 = [] {   error: outer enclosing lambda is not defined within a   non-lambda-operator() function.
auto lam2 = [] __host__ __device__ { }; }; The enclosing function for the extended lambda must be named and its address can be taken.
If the enclosing function is a class member, then the following conditions must be satisfied: All classes enclosing the member function must have a name.
All enclosing classes must not have private or protected access within their respective parent classes.
Example: void foo ( void ) {   OK auto lam1 = [] __device__ { return 0 ; }; {   OK auto lam2 = [] __device__ { return 0 ; };   OK auto lam3 = [] __device__ __host__ { return 0 ; }; } } struct S1_t { S1_t ( void ) {   Error: cannot take address of enclosing function auto lam4 = [] __device__ { return 0 ; }; } }; class C0_t { void foo ( void ) {   Error: enclosing function has private access in parent class auto temp1 = [] __device__ { return 10 ; }; } struct S2_t { void foo ( void ) {   Error: enclosing class S2_t has private access in its   parent class auto temp1 = [] __device__ { return 10 ; }; } }; }; It must be possible to take the address of the enclosing routine unambiguously, at the point where the extended lambda has been defined.
Example: template struct A { typedef void Bar ; void test (); }; template <> struct A { }; template void A :: test () { /* In code sent to host compiler, nvcc will inject an address expression here, of the form: (void (A ::*)(void))(&A::test)) However, the class typedef 'Bar' (to void) shadows the template argument 'Bar', causing the address expression in A::test to actually refer to: (void (A ::*)(void))(&A::test)) ..which doesn't take the address of the enclosing routine 'A::test' correctly.
Example: void foo ( void ) { struct S1_t { void bar ( void ) {   Error: bar is member of a class that is local to a function.
auto lam4 = [] __host__ __device__ { return 0 ; }; } }; } The enclosing function for an extended lambda cannot have deduced return type.
auto lam1 = [] __host__ __device__ { return 0 ; }; } __host__ __device__ extended lambdas cannot be generic lambdas.
Example: void foo ( void ) {   Error: __host__ __device__ extended lambdas cannot be   generic lambdas.
auto lam1 = [] __host__ __device__ ( auto i ) { return i ; };   Error: __host__ __device__ extended lambdas cannot be   generic lambdas.
i ) { return sizeof ...( i ); }; } If the enclosing function is an instantiation of a function template or a member function template, and/or the function is a member of a class template, the template(s) must satisfy the following constraints: The template must have at most one variadic parameter, and it must be listed last in the template parameter list.
The template instantiation argument types cannot involve types that are either local to a function (except for closure types for extended lambdas), or are private or protected class members.
Example: template __global__ void kern ( T in ) { in (); } template struct foo {}; template class T , typename ...
P2 > void bar1 ( const T , const T ) {   Error: enclosing function has multiple parameter packs auto lam1 = [] __device__ { return 10 ; }; } template class T , typename ...
P1 , typename T2 > void bar2 ( const T , T2 ) {   Error: for enclosing function, the   parameter pack is not last in the template parameter list.
auto lam1 = [] __device__ { return 10 ; }; } template void bar3 ( void ) {   Error: for enclosing function, the second template   parameter is not named.
auto lam1 = [] __device__ { return 10 ; }; } int main () { foo f1 ; foo f2 ; bar1 ( f1 , f2 ); bar2 ( f1 , 10 ); bar3 (); } Example: template __global__ void kern ( T in ) { in (); } template void bar4 ( void ) { auto lam1 = [] __device__ { return 10 ; }; kern >> ( lam1 ); } struct C1_t { struct S1_t { }; friend int main ( void ); }; int main () { struct S1_t { };   Error: enclosing function for device lambda in bar4   is instantiated with a type local to main.
bar4 ();   Error: enclosing function for device lambda in bar4   is instantiated with a type that is a private member   of a class.
bar4 (); } With Visual Studio host compilers, the enclosing function must have external linkage.
The restriction is present because this host compiler does not support using the address of non-extern linkage functions as template arguments, which is needed by the CUDA compiler transformations to support extended lambdas.
With Visual Studio host compilers, an extended lambda shall not be defined within the body of an ‘if-constexpr’ block.
An extended lambda has the following restrictions on captured variables: In the code sent to the host compiler, the variable may be passed by value to a sequence of helper functions before being used to direct-initialize the field of the class type used to represent the closure type for the extended lambda 31 .
A variable of array type cannot be captured if the number of array dimensions is greater than 7.
For a variable of array type, in the code sent to the host compiler, the closure type’s array field is first default-initialized, and then each element of the array field is copy-assigned from the corresponding element of the captured array variable.
Therefore, the array element type must be default-constructible and copy-assignable in host code.
The type of the captured variable cannot involve types that are either local to a function (except for closure types of extended lambdas), or are private or protected class members.
For a __host__ __device__ extended lambda, the types used in the return or parameter types of the lambda expression’s operator() cannot involve types that are either local to a function (except for closure types of extended lambdas), or are private or protected class members.
Init-capture is supported for __device__ extended lambdas, except when the init-capture is of array type or of type std::initializer_list .
The constexpr and consteval specifier cannot be used in the declaration of an extended lambda.
A variable cannot be implicitly captured inside an if-constexpr block lexically nested inside an extended lambda, unless it has already been implicitly captured earlier outside the if-constexpr block or appears in the explicit capture list for the extended lambda (see example below).
Example void foo ( void ) {   OK: an init-capture is allowed for an   extended __device__ lambda.
auto lam1 = [ x = 1 ] __device__ () { return x ; };   Error: an init-capture is not allowed for   an extended __host__ __device__ lambda.
auto lam2 = [ x = 1 ] __host__ __device__ () { return x ; }; int a = 1 ;   Error: an extended __device__ lambda cannot capture   variables by reference.
auto lam3 = [ & a ] __device__ () { return a ; };   Error: by-reference capture is not allowed   for an extended __device__ lambda.
auto lam4 = [ & x = a ] __device__ () { return x ; }; struct S1_t { }; S1_t s1 ;   Error: a type local to a function cannot be used in the type   of a captured variable.
auto lam6 = [ s1 ] __device__ () { };   Error: an init-capture cannot be of type std::initializer_list.
auto lam7 = [ x = { 11 }] __device__ () { }; std :: initializer_list b = { 11 , 22 , 33 };   Error: an init-capture cannot be of type std::initializer_list.
auto lam8 = [ x = b ] __device__ () { };   Error scenario (lam9) and supported scenarios (lam10, lam11)   for capture within 'if-constexpr' block int yyy = 4 ; auto lam9 = [ = ] __device__ { int result = 0 ; if constexpr ( false ) {  Error: An extended __device__ lambda cannot first-capture   'yyy' in constexpr-if context result += yyy ; } return result ; }; auto lam10 = [ yyy ] __device__ { int result = 0 ; if constexpr ( false ) {  OK: 'yyy' already listed in explicit capture list for the extended lambda result += yyy ; } return result ; }; auto lam11 = [ = ] __device__ { int result = yyy ; if constexpr ( false ) {  OK: 'yyy' already implicit captured outside the 'if-constexpr' block result += yyy ; } return result ; }; } When parsing a function, the CUDA compiler assigns a counter value to each extended lambda within that function.
Hence, whether or not an extended lambda is defined within a function should not depend on a particular value of __CUDA_ARCH__ , or on __CUDA_ARCH__ being undefined.
Example template __global__ void kernel ( T in ) { in (); } __host__ __device__ void foo ( void ) {   Error: the number and relative declaration   order of extended lambdas depends on   __CUDA_ARCH__ #if defined(__CUDA_ARCH__) auto lam1 = [] __device__ { return 0 ; }; auto lam1b = [] __host___ __device__ { return 10 ; }; #endif auto lam2 = [] __device__ { return 4 ; }; kernel >> ( lam2 ); } As described above, the CUDA compiler replaces a __device__ extended lambda defined in a host function with a placeholder type defined in namespace scope.
Unless the trait __nv_is_extended_device_lambda_with_preserved_return_type() returns true for the closure type of the extended lambda, the placeholder type does not define a operator() function equivalent to the original lambda declaration.
An attempt to determine the return type or parameter types of the operator() function of such a lambda may therefore work incorrectly in host code, as the code processed by the host compiler will be semantically different than the input code processed by the CUDA compiler.
However, it is OK to introspect the return type or parameter types of the operator() function within device code.
Note that this restriction does not apply to __host__ __device__ extended lambdas, or to __device__ extended lambdas for which the trait __nv_is_extended_device_lambda_with_preserved_return_type() returns true.
static_assert ( ! __nv_is_extended_device_lambda_with_preserved_return_type ( decltype ( lam4 )), "" ); } For an extended device lambda: - Introspecting the parameter type of operator() is only supported in device code.
- Introspecting the return type of operator() is supported only in device code, unless the trait function __nv_is_extended_device_lambda_with_preserved_return_type() returns true.
If the functor object represented by an extended lambda is passed from host to device code (e.g., as the argument of a __global__ function), then any expression in the body of the lambda expression that captures variables must be remain unchanged irrespective of whether the __CUDA_ARCH__ macro is defined, and whether the macro has a particular value.
This restriction arises because the lambda’s closure class layout depends on the order in which captured variables are encountered when the compiler processes the lambda expression; the program may execute incorrectly if the closure class layout differs in device and host compilation.
Example __device__ int result ; template __global__ void kernel ( T in ) { result = in (); } void foo ( void ) { int x1 = 1 ; auto lam1 = [ = ] __host__ __device__ {   Error: "x1" is only captured when __CUDA_ARCH__ is defined.
#ifdef __CUDA_ARCH__ return x1 + 1 ; #else return 10 ; #endif }; kernel >> ( lam1 ); } As described previously, the CUDA compiler replaces an extended __device__ lambda expression with an instance of a placeholder type in the code sent to the host compiler.
This placeholder type does not define a pointer-to-function conversion operator in host code, however the conversion operator is provided in device code.
Example template __global__ void kern ( T in ) { int ( * fp )( double ) = in ;   OK: conversion in device code is supported fp ( 0 ); auto lam1 = []( double ) { return 1 ; };   OK: conversion in device code is supported fp = lam1 ; fp ( 0 ); } void foo ( void ) { auto lam_d = [] __device__ ( double ) { return 1 ; }; auto lam_hd = [] __host__ __device__ ( double ) { return 1 ; }; kern >> ( lam_d ); kern >> ( lam_hd );   OK : conversion for __host__ __device__ lambda is supported   in host code int ( * fp )( double ) = lam_hd ;   Error: conversion for __device__ lambda is not supported in   host code.
int ( * fp2 )( double ) = lam_d ; } As described previously, the CUDA compiler replaces an extended __device__ or __host__ __device__ lambda expression with an instance of a placeholder type in the code sent to the host compiler.
As a result, some standard C++ type traits may return different results for the closure type of the extended lambda, in the CUDA frontend compiler versus the host compiler.
The following type traits are affected: std::is_trivially_copyable , std::is_trivially_constructible , std::is_trivially_copy_constructible , std::is_trivially_move_constructible , std::is_trivially_destructible .
Care must be taken that the results of these type traits are not used in __global__ function template instantiation or in __device__ / __constant__ / __managed__ variable template instantiation.
Example template void __global__ foo () { printf ( "hi" ); } template void dolaunch () {   ERROR: this kernel launch may fail, because CUDA frontend compiler   and host compiler may disagree on the result of   std::is_trivially_copyable() trait on the closure type of the   extended lambda foo :: value >>> (); cudaDeviceSynchronize (); } int main () { int x = 0 ; auto lam1 = [ = ] __host__ __device__ () { return x ; }; dolaunch (); } The CUDA compiler will generate compiler diagnostics for a subset of cases described in 1-12; no diagnostic will be generated for cases 13-17, but the host compiler may fail to compile the generated code. 14.7.3. Notes on __host__ __device__ lambdas  Unlike __device__ lambdas, __host__ __device__ lambdas can be called from host code.
As described earlier, the CUDA compiler replaces an extended lambda expression defined in host code with an instance of a named placeholder type.
The placeholder type for an extended __host__ __device__ lambda invokes the original lambda’s operator() with an indirect function call 30 .
The presence of the indirect function call may cause an extended __host__ __device__ lambda to be less optimized by the host compiler than lambdas that are implicitly or explicitly __host__ only.
In the latter case, the host compiler can easily inline the body of the lambda into the calling context.
But in case of an extended __host__ __device__ lambda, the host compiler encounters the indirect function call and may not be able to easily inline the original __host__ __device__ lambda body. 14.7.4. *this Capture By Value  When a lambda is defined within a non-static class member function, and the body of the lambda refers to a class member variable, C++11/C++14 rules require that the this pointer of the class is captured by value, instead of the referenced member variable.
If the lambda is an extended __device__ or __host__ __device__ lambda defined in a host function, and the lambda is executed on the GPU, accessing the referenced member variable on the GPU will cause a run time error if the this pointer points to host memory.
Example: #include template __global__ void foo ( T in ) { printf ( "   value = %d" , in ()); } struct S1_t { int xxx ; __host__ __device__ S1_t ( void ) : xxx ( 10 ) { }; void doit ( void ) { auto lam1 = [ = ] __device__ {   reference to "xxx" causes   the 'this' pointer (S1_t*) to be captured by value return xxx + 1 ; };   Kernel launch fails at run time because 'this->xxx'   is not accessible from the GPU foo >> ( lam1 ); cudaDeviceSynchronize (); } }; int main ( void ) { S1_t s1 ; s1 .
In this mode, the compiler makes a copy of the object denoted by “*this” instead of capturing the pointer this by value.
The “*this” capture mode is described in more detail here: http: www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0018r3.html .
The CUDA compiler supports the “*this” capture mode for lambdas defined within __device__ and __global__ functions and for extended __device__ lambdas defined in host code, when the --extended-lambda nvcc flag is used.
Here’s the above example modified to use “*this” capture mode: #include template __global__ void foo ( T in ) { printf ( "   value = %d" , in ()); } struct S1_t { int xxx ; __host__ __device__ S1_t ( void ) : xxx ( 10 ) { }; void doit ( void ) {   note the "*this" capture specification auto lam1 = [ = , * this ] __device__ {   reference to "xxx" causes   the object denoted by '*this' to be captured by   value, and the GPU code will access copy_of_star_this->xxx return xxx + 1 ; };   Kernel launch succeeds foo >> ( lam1 ); cudaDeviceSynchronize (); } }; int main ( void ) { S1_t s1 ; s1 .
doit (); } “*this” capture mode is not allowed for unannotated lambdas defined in host code, or for extended __host__ __device__ lambdas.
Additional Notes  ADL Lookup : As described earlier, the CUDA compiler will replace an extended lambda expression with an instance of a placeholder type, before invoking the host compiler.
One template argument of the placeholder type uses the address of the function enclosing the original lambda expression.
This may cause additional namespaces to participate in argument dependent lookup (ADL), for any host function call whose argument types involve the closure type of the extended lambda expression.
Example: namespace N1 { struct S1_t { }; template void foo ( T ); }; namespace N2 { template int foo ( T ); template void doit ( T in ) { foo ( in ); } } void bar ( N1 :: S1_t in ) { /* extended __device__ lambda.
In the code sent to the host compiler, this is replaced with the placeholder type instantiation expression ' __nv_dl_wrapper_t > { }' As a result, the namespace 'N1' participates in ADL lookup of the call to "foo" in the body of N2::doit, causing ambiguity.
*/ auto lam1 = [ = ] __device__ { }; N2 :: doit ( lam1 ); } In the example above, the CUDA compiler replaced the extended lambda with a placeholder type that involves the N1 namespace.
As a result, the namespace N1 participates in the ADL lookup for foo(in) in the body of N2::doit , and host compilation fails because multiple overload candidates N1::foo and N2::foo are found. 14.8. Code Samples  14.8.1.
Data Aggregation Class  class PixelRGBA { public : __device__ PixelRGBA () : r_ ( 0 ), g_ ( 0 ), b_ ( 0 ), a_ ( 0 ) { } __device__ PixelRGBA ( unsigned char r , unsigned char g , unsigned char b , unsigned char a = 255 ) : r_ ( r ), g_ ( g ), b_ ( b ), a_ ( a ) { } private : unsigned char r_ , g_ , b_ , a_ ; friend PixelRGBA operator + ( const PixelRGBA & , const PixelRGBA & ); }; __device__ PixelRGBA operator + ( const PixelRGBA & p1 , const PixelRGBA & p2 ) { return PixelRGBA ( p1 .
Derived Class  __device__ void * operator new ( size_t bytes , MemoryPool & p ); __device__ void operator delete ( void * , MemoryPool & p ); class Shape { public : __device__ Shape ( void ) { } __device__ void putThis ( PrintBuffer * p ) const ; __device__ virtual void Draw ( PrintBuffer * p ) const { p -> put ( "Shapeless" ); } __device__ virtual ~ Shape () {} }; class Point : public Shape { public : __device__ Point () : x ( 0 ), y ( 0 ) {} __device__ Point ( int ix , int iy ) : x ( ix ), y ( iy ) { } __device__ void PutCoord ( PrintBuffer * p ) const ; __device__ void Draw ( PrintBuffer * p ) const ; __device__ ~ Point () {} private : int x , y ; }; __device__ Shape * GetPointObj ( MemoryPool & pool ) { Shape * shape = new ( pool ) Point ( rand ( -20 , 10 ), rand ( -100 , -20 )); return shape ; } 14.8.3.
Class Template  template class myValues { T values [ MAX_VALUES ]; public : __device__ myValues ( T clear ) { ...
} }; template void __global__ useValues ( T * memoryBuffer ) { myValues myLocation ( 0 ); ...
useValues >> ( buffer ); ... } 14.8.4. Function Template  template __device__ bool func ( T x ) { ...
return (...); } template <> __device__ bool func ( T x )   Specialization { return true ; }   Explicit argument specification bool result = func ( 0.5 );   Implicit argument deduction int x = 1 ; bool result = func ( x ); 14.8.5.
Functor Class  class Add { public : __device__ float operator () ( float a , float b ) const { return a + b ; } }; class Sub { public : __device__ float operator () ( float a , float b ) const { return a - b ; } };   Device code template __global__ void VectorOperation ( const float * A , const float * B , float * C , unsigned int N , O op ) { unsigned int iElement = blockDim .
16 This does not apply to entities that may be defined in more than one translation unit, such as compiler generated template instantiations.
17 The intent is to allow variable memory space specifiers for static variables in a __host__ __device__ function during device compilation, but disallow it during host compilation 18 One way to debug suspected layout mismatch of a type C is to use printf to output the values of sizeof(C) and offsetof(C, field) in host and device code.
19 Note that this may negatively impact compile time due to presence of extra declarations.
20 At present, the -std=c++11 flag is supported only for the following host compilers : gcc version >= 4.7, clang, icc >= 15, and xlc >= 13.1 21 including operator() 22 The restrictions are the same as with a non-constexpr callee function.
24 C++ Standard Section [basic.types] 25 C++ Standard Section [expr.const] 26 At present, the -std=c++14 flag is supported only for the following host compilers : gcc version >= 5.1, clang version >= 3.7 and icc version >= 17 27 At present, the -std=c++17 flag is supported only for the following host compilers : gcc version >= 7.0, clang version >= 8.0, Visual Studio version >= 2017, pgi compiler version >= 19.0, icc compiler version >= 19.0 28 At present, the -std=c++20 flag is supported only for the following host compilers : gcc version >= 10.0, clang version >= 10.0, Visual Studio Version >= 2022 and nvc++ version >= 20.7.
31 In contrast, the C++ standard specifies that the captured variable is used to direct-initialize the field of the closure type. 15. Texture Fetching  This section gives the formula used to compute the value returned by the texture functions of Texture Functions depending on the various attributes of the texture object (see Texture and Surface Memory ).
The texture bound to the texture object is represented as an array T of N texels for a one-dimensional texture, N x M texels for a two-dimensional texture, N x M x L texels for a three-dimensional texture.
It is fetched using non-normalized texture coordinates x , y , and z , or the normalized texture coordinates x/N , y/M , and z/L as described in Texture Memory .
Texture Memory explained how out-of-range coordinates are remapped to the valid range based on the addressing mode. 15.1. Nearest-Point Sampling  In this filtering mode, the value returned by the texture fetch is tex(x)=T[i] for a one-dimensional texture, tex(x,y)=T[i,j] for a two-dimensional texture, tex(x,y,z)=T[i,j,k] for a three-dimensional texture, where i=floor(x) , j=floor(y) , and k=floor(z) .
Table Lookup  A table lookup TL(x) where x spans the interval [0,R] can be implemented as TL(x)=tex((N-1)/R)x+0.5) in order to ensure that TL(0)=T[0] and TL(R)=T[N-1] .
Figure 34 illustrates the use of texture filtering to implement a table lookup with R=4 or R=1 from a one-dimensional texture with N=4 .
Compute Capabilities  The general specifications and features of a compute device depend on its compute capability (see Compute Capability ).
Table 20 and Table 21 show the features and technical specifications associated with each compute capability that is currently supported.
Sections Compute Capability 5.x , Compute Capability 6.x , Compute Capability 7.x , Compute Capability 8.x and Compute Capability 9.0 give more details on the architecture of devices of compute capabilities 5.x, 6.x, 7.x, 8.x and 9.0 respectively. 16.1. Feature Availability  A compute feature is introduced with a compute architecture with the intention that the feature will be available on all subsequent architectures.
This is shown in Table 20 by the “yes” for availability of a feature on compute capabilities subsequent to its introduction.
Highly specialized compute features that are introduced with an architecture may not be guaranteed to be available on all subsequent compute capabilities.
These features target acceleration of specialized operations which are not intended for all classes of compute capabilities (denoted by the compute capability’s minor number) or are likely to significantly change on future generations (denoted by the compute capability’s major number).
There are potentially two sets of compute features for a given compute capability: Compute Capability #.# : The predominant set of compute features that are introduced with the intent to be available for subsequent compute architectures.
Compute Capability #.#a : A small and highly specialized set of features that are introduced to accelerate specialized operations, which are not guaranteed to be available or might change significantly on subsequent compute architecture.
A feature which appears in device code must be available for the targeted compute capability.
For example: The compute_90 compilation target allows use of Compute Capability 9.0 features but does not allow use of Compute Capability 9.0a features.
Floating-Point Standard  All compute devices follow the IEEE 754-2008 standard for binary floating-point arithmetic with the following deviations: There is no dynamically configurable rounding mode; however, most of the operations support multiple IEEE rounding modes, exposed via device intrinsics.
There is no mechanism for detecting that a floating-point exception has occurred and all operations behave as if the IEEE-754 exceptions are always masked, and deliver the masked response as defined by IEEE-754 if there is an exceptional event.
For the same reason, while SNaN encodings are supported, they are not signaling and are handled as quiet.
The result of a single-precision floating-point operation involving one or more input NaNs is the quiet NaN of bit pattern 0x7fffffff.
Double-precision floating-point absolute value and negation are not compliant with IEEE-754 with respect to NaNs; these are passed through unchanged.
Code must be compiled with -ftz=false , -prec-div=true , and -prec-sqrt=true to ensure IEEE compliance (this is the default setting; see the nvcc user manual for description of these compilation flags).
Regardless of the setting of the compiler flag -ftz , atomic single-precision floating-point adds on global memory always operate in flush-to-zero mode, i.e., behave equivalent to FADD.F32.FTZ.RN , atomic single-precision floating-point adds on shared memory always operate with denormal support, i.e., behave equivalent to FADD.F32.RN .
In accordance to the IEEE-754R standard, if one of the input parameters to fminf() , fmin() , fmaxf() , or fmax() is NaN, but not the other, the result is the non-NaN parameter.
The conversion of a floating-point value to an integer value in the case where the floating-point value falls outside the range of the integer format is left undefined by IEEE-754.
The behavior of integer division by zero and integer overflow is left undefined by IEEE-754.
For compute devices, there is no mechanism for detecting that such integer operation exceptions have occurred.
https: developer.nvidia.com/content/precision-performance-floating-point-and-ieee-754-compliance-nvidia-gpus includes more information on the floating point accuracy and compliance of NVIDIA GPUs. 16.4. Compute Capability 5.x  16.4.1.
Architecture  An SM consists of: 128 CUDA cores for arithmetic operations (see Arithmetic Instructions for throughputs of arithmetic operations), 32 special function units for single-precision floating-point transcendental functions, 4 warp schedulers.
When an SM is given warps to execute, it first distributes them among the four schedulers.
Then, at every instruction issue time, each scheduler issues one instruction for one of its assigned warps that is ready to execute, if any.
An SM has: a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory, a unified L1/texture cache of 24 KB used to cache reads from global memory, 64 KB of shared memory for devices of compute capability 5.0 or 96 KB of shared memory for devices of compute capability 5.2.
The unified L1/texture cache is also used by the texture unit that implements the various addressing modes and data filtering mentioned in Texture and Surface Memory .
There is also an L2 cache shared by all SMs that is used to cache accesses to local or global memory, including temporary register spills.
Applications may query the L2 cache size by checking the l2CacheSize device property (see Device Enumeration ).
The cache behavior (e.g., whether reads are cached in both the unified L1/texture cache and L2 or in L2 only) can be partially configured on a per-access basis using modifiers to the load instruction. 16.4.2. Global Memory  Global memory accesses are always cached in L2.
Data that is read-only for the entire lifetime of the kernel can also be cached in the unified L1/texture cache described in the previous section by reading it using the __ldg() function (see Read-Only Data Cache Load Function ).
When the compiler detects that the read-only condition is satisfied for some data, it will use __ldg() to read it.
The compiler might not always be able to detect that the read-only condition is satisfied for some data.
Marking pointers used for loading such data with both the const and __restrict__ qualifiers increases the likelihood that the compiler will detect the read-only condition.
Data that is not read-only for the entire lifetime of the kernel cannot be cached in the unified L1/texture cache for devices of compute capability 5.0.
For devices of compute capability 5.2, it is, by default, not cached in the unified L1/texture cache, but caching may be enabled using the following mechanisms: Perform the read using inline assembly with the appropriate modifier as described in the PTX reference manual; Compile with the -Xptxas -dlcm=ca compilation flag, in which case all reads are cached, except reads that are performed using inline assembly with a modifier that disables caching; Compile with the -Xptxas -fscm=ca compilation flag, in which case all reads are cached, including reads that are performed using inline assembly regardless of the modifier used.
When caching is enabled using one of the three mechanisms listed above, devices of compute capability 5.2 will cache global memory reads in the unified L1/texture cache for all kernel launches except for the kernel launches for which thread blocks consume too much of the SM’s register file.
These exceptions are reported by the profiler. 16.4.3. Shared Memory  Shared memory has 32 banks that are organized such that successive 32-bit words map to successive banks.
A shared memory request for a warp does not generate a bank conflict between two threads that access any address within the same 32-bit word (even though the two addresses fall in the same bank).
In that case, for read accesses, the word is broadcast to the requesting threads and for write accesses, each address is written by only one of the threads (which thread performs the write is undefined).
Figure 23 shows some examples of memory read accesses that involve the broadcast mechanism.
Middle Conflict-free access since threads 3, 4, 6, 7, and 9 access the same word within bank 5.
Right Conflict-free broadcast access (threads access the same word within a bank). 16.5. Compute Capability 6.x  16.5.1.
Architecture  An SM consists of: 64 (compute capability 6.0) or 128 (6.1 and 6.2) CUDA cores for arithmetic operations, 16 (6.0) or 32 (6.1 and 6.2) special function units for single-precision floating-point transcendental functions, 2 (6.0) or 4 (6.1 and 6.2) warp schedulers.
An SM has: a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory, a unified L1/texture cache for reads from global memory of size 24 KB (6.0 and 6.2) or 48 KB (6.1), a shared memory of size 64 KB (6.0 and 6.2) or 96 KB (6.1). 16.5.2. Global Memory  Global memory behaves the same way as in devices of compute capability 5.x (See Global Memory ).
16.5.3. Shared Memory  Shared memory behaves the same way as in devices of compute capability 5.x (See Shared Memory ).
Architecture  An SM consists of: 64 FP32 cores for single-precision arithmetic operations, 32 FP64 cores for double-precision arithmetic operations, 34 64 INT32 cores for integer math, 8 mixed-precision Tensor Cores for deep learning matrix arithmetic 16 special function units for single-precision floating-point transcendental functions, 4 warp schedulers.
An SM has: a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory, a unified data cache and shared memory with a total size of 128 KB ( Volta ) or 96 KB ( Turing ).
Shared memory is partitioned out of unified data cache, and can be configured to various sizes (See Shared Memory .) The remaining data cache serves as an L1 cache and is also used by the texture unit that implements the various addressing and data filtering modes mentioned in Texture and Surface Memory . 16.6.2. Independent Thread Scheduling  The Volta architecture introduces Independent Thread Scheduling among threads in a warp, enabling intra-warp synchronization patterns previously unavailable and simplifying code changes when porting CPU code.
However, this can lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity of previous hardware architectures.
For applications using warp intrinsics ( __shfl* , __any , __all , __ballot ), it is necessary that developers port their code to the new, safe, synchronizing counterpart, with the *_sync suffix.
The new warp intrinsics take in a mask of threads that explicitly define which lanes (threads of a warp) must participate in the warp intrinsic.
Since the intrinsics are available with CUDA 9.0+, (if necessary) code can be executed conditionally with the following preprocessor macro: #if defined(CUDART_VERSION) && CUDART_VERSION >= 9000   *_sync intrinsic #endif These intrinsics are available on all architectures, not just Volta or Turing , and in most cases a single code-base will suffice for all architectures.
Note, however, that for Pascal and earlier architectures, all threads in mask must execute the same warp intrinsic instruction in convergence, and the union of all values in mask must be equal to the warp’s active mask.
The following code pattern is valid on Volta , but not on Pascal or earlier architectures.
if ( tid % warpSize threshold ); if ( warpLane == 0 ) { output [ i / 32 ] = bitPack ; } } This code is invalid because CUDA does not guarantee that the warp will diverge ONLY at the loop condition.
When divergence happens for other reasons, conflicting results will be computed for the same 32-bit output element by different subsets of threads in the warp.
A correct code might use a non-divergent loop condition together with __ballot_sync() to safely enumerate the set of threads in the warp participating in the threshold calculation as follows.
for ( int i = warpLane ; i - warpLane threshold ); if ( warpLane == 0 ) { output [ i / 32 ] = bitPack ; } } } Discovery Pattern demonstrates a valid use case for __activemask() .
If applications have warp-synchronous codes, they will need to insert the new __syncwarp() warp-wide barrier synchronization instruction between any steps where data is exchanged between threads via global or shared memory.
Assumptions that code is executed in lockstep or that reads/writes from separate threads are visible across a warp without synchronization are invalid.
__shared__ float s_buff [ BLOCK_SIZE ]; s_buff [ tid ] = val ; __syncthreads ();   Inter-warp reduction for ( int i = BLOCK_SIZE / 2 ; i >= 32 ; i /= 2 ) { if ( tid >> (...); In addition to an integer percentage, several convenience enums are provided as listed in the code comments above.
Where a chosen integer percentage does not map exactly to a supported capacity (SM 7.0 devices support shared capacities of 0, 8, 16, 32, 64, or 96 KB), the next larger capacity is used.
For instance, in the example above, 50% of the 96 KB maximum is 48 KB, which is not a supported shared memory capacity.
Compute capability 7.x devices allow a single thread block to address the full capacity of shared memory: 96 KB on Volta , 64 KB on Turing .
Kernels relying on shared memory allocations over 48 KB per block are architecture-specific, as such they must use dynamic shared memory (rather than statically sized arrays) and require an explicit opt-in using cudaFuncSetAttribute() as follows.
}   Host code int maxbytes = 98304 ;   96 KB cudaFuncSetAttribute ( MyKernel , cudaFuncAttributeMaxDynamicSharedMemorySize , maxbytes ); MyKernel >> (...); Otherwise, shared memory behaves the same way as for devices of compute capability 5.x (See Shared Memory ). 16.7. Compute Capability 8.x  16.7.1.
An SM has: a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory, a unified data cache and shared memory with a total size of 192 KB for devices of compute capability 8.0 and 8.7 (1.5x Volta ’s 128 KB capacity) and 128 KB for devices of compute capabilities 8.6 and 8.9.
Shared memory is partitioned out of the unified data cache, and can be configured to various sizes (see Shared Memory section). 16.7.2. Global Memory  Global memory behaves the same way as for devices of compute capability 5.x (See Global Memory ).
16.7.3. Shared Memory  Similar to the Volta architecture , the amount of the unified data cache reserved for shared memory is configurable on a per kernel basis.
For the NVIDIA Ampere GPU architecture , the unified data cache has a size of 192 KB for devices of compute capability 8.0 and 8.7 and 128 KB for devices of compute capabilities 8.6 and 8.9.
The shared memory capacity can be set to 0, 8, 16, 32, 64, 100, 132 or 164 KB for devices of compute capability 8.0 and 8.7, and to 0, 8, 16, 32, 64 or 100 KB for devices of compute capabilities 8.6 and 8.9.
An application can set the carveout , i.e., the preferred shared memory capacity, with the cudaFuncSetAttribute() .
cudaFuncSetAttribute ( kernel_name , cudaFuncAttributePreferredSharedMemoryCarveout , carveout ); The API can specify the carveout either as an integer percentage of the maximum supported shared memory capacity of 164 KB for devices of compute capability 8.0 and 8.7 and 100 KB for devices of compute capabilities 8.6 and 8.9 respectively, or as one of the following values: {cudaSharedmemCarveoutDefault , cudaSharedmemCarveoutMaxL1 , or cudaSharedmemCarveoutMaxShared .
When using a percentage, the carveout is rounded up to the nearest supported shared memory capacity.
For example, for devices of compute capability 8.0, 50% will map to a 100 KB carveout instead of an 82 KB one.
Setting the cudaFuncAttributePreferredSharedMemoryCarveout is considered a hint by the driver; the driver may choose a different configuration, if needed.
Devices of compute capability 8.0 and 8.7 allow a single thread block to address up to 163 KB of shared memory, while devices of compute capabilities 8.6 and 8.9 allow up to 99 KB of shared memory.
Kernels relying on shared memory allocations over 48 KB per block are architecture-specific, and must use dynamic shared memory rather than statically sized shared memory arrays.
These kernels require an explicit opt-in by using cudaFuncSetAttribute() to set the cudaFuncAttributeMaxDynamicSharedMemorySize ; see Shared Memory for the Volta architecture.
Note that the maximum amount of shared memory per thread block is smaller than the maximum shared memory partition available per SM.
The 1 KB of shared memory not made available to a thread block is reserved for system use. 16.8. Compute Capability 9.0  16.8.1.
Architecture  A Streaming Multiprocessor (SM) consists of: 128 FP32 cores for single-precision arithmetic operations, 64 FP64 cores for double-precision arithmetic operations, 64 INT32 cores for integer math, 4 mixed-precision fourth-generation Tensor Cores supporting the new FP8 input type in either E4M3 or E5M2 for exponent (E) and mantissa (M), half-precision (fp16), __nv_bfloat16 , tf32 , INT8 and double precision (fp64) matrix arithmetic (see Warp Matrix Functions for details) with sparsity support, 16 special function units for single-precision floating-point transcendental functions, 4 warp schedulers.
An SM has: a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory, a unified data cache and shared memory with a total size of 256 KB for devices of compute capability 9.0 (1.33x NVIDIA Ampere GPU Architecture ’s 192 KB capacity). 16.8.2. 16.8.3. Shared Memory  Similar to the NVIDIA Ampere GPU architecture , the amount of the unified data cache reserved for shared memory is configurable on a per kernel basis.
For the NVIDIA H100 Tensor Core GPU architecture , the unified data cache has a size of 256 KB for devices of compute capability 9.0.
As with the NVIDIA Ampere GPU architecture , an application can configure its preferred shared memory capacity, i.e., the carveout .
Devices of compute capability 9.0 allow a single thread block to address up to 227 KB of shared memory.
32 above 48 KB requires dynamic shared memory 33 2 FP64 cores for double-precision arithmetic operations for devices of compute capabilities 7.5 34 2 FP64 cores for double-precision arithmetic operations for devices of compute capabilities 7.5 16.8.4.
Features Accelerating Specialized Computations  The NVIDIA Hopper GPU architecture includes features to accelerate matrix multiply-accumulate (MMA) computations with: asynchronous execution of MMA instructions MMA instructions acting on large matrices spanning a warp-group dynamic reassignment of register capacity among warp-groups to support even larger matrices, and operand matrices accessed directly from shared memory This feature set is only available within the CUDA compilation toolchain through inline PTX.
It is strongly recommended that applications utilize this complex feature set through CUDA-X libraries such as cuBLAS, cuDNN, or cuFFT.
It is strongly recommended that device kernels utilize this complex feature set through CUTLASS , a collection of CUDA C++ template abstractions for implementing high-performance matrix-multiplication (GEMM) and related computations at all levels and scales within CUDA. 17. Driver API  This section assumes knowledge of the concepts described in CUDA Runtime .
The driver API is implemented in the cuda dynamic library ( cuda.dll or cuda.so ) which is copied on the system during the installation of the device driver.
It is a handle-based, imperative API: Most objects are referenced by opaque handles that may be specified to functions to manipulate the objects.
Table 22 Objects Available in the CUDA Driver API  Object Handle Description Device CUdevice CUDA-enabled device Context CUcontext Roughly equivalent to a CPU process Module CUmodule Roughly equivalent to a dynamic library Function CUfunction Kernel Heap memory CUdeviceptr Pointer to device memory CUDA array CUarray Opaque container for one-dimensional or two-dimensional data on the device, readable via texture or surface references Texture object CUtexref Object that describes how to interpret texture memory data Surface reference CUsurfref Object that describes how to read or write CUDA arrays Stream CUstream Object that describes a CUDA stream Event CUevent Object that describes a CUDA event The driver API must be initialized with cuInit() before any function from the driver API is called.
A CUDA context must then be created that is attached to a specific device and made current to the calling host thread as detailed in Context .
Within a CUDA context, kernels are explicitly loaded as PTX or binary objects by the host code as described in Module .
Any application that wants to run on future device architectures must load PTX , not binary code.
This is because binary code is architecture-specific and therefore incompatible with future architectures, whereas PTX code is compiled to binary code at load time by the device driver.
Here is the host code of the sample from Kernels written using the driver API: int main () { int N = ...; size_t size = N * sizeof ( float );   Allocate input vectors h_A and h_B in host memory float * h_A = ( float * ) malloc ( size ); float * h_B = ( float * ) malloc ( size );   Initialize input vectors ...
Initialize cuInit ( 0 );   Get number of devices supporting CUDA int deviceCount = 0 ; cuDeviceGetCount ( & deviceCount ); if ( deviceCount == 0 ) { printf ( "There is no device supporting CUDA.
} Full code can be found in the vectorAddDrv CUDA sample. 17.1. Context  A CUDA context is analogous to a CPU process.
All resources and actions performed within the driver API are encapsulated inside a CUDA context, and the system automatically cleans up these resources when the context is destroyed.
Besides objects such as modules and texture or surface references, each context has its own distinct address space.
As a result, CUdeviceptr values from different contexts reference different memory locations.
When a context is created with cuCtxCreate( ), it is made current to the calling host thread.
CUDA functions that operate in a context (most functions that do not involve device enumeration or context management) will return CUDA_ERROR_INVALID_CONTEXT if a valid context is not current to the thread.
The context is then “floating” and may be pushed as the current context for any host thread.
A context is destroyed when the usage count goes to 0 when calling cuCtxDetach() or cuCtxDestroy() .
The driver API is interoperable with the runtime and it is possible to access the primary context (see Initialization ) managed by the runtime from the driver API via cuDevicePrimaryCtxRetain() .
Usage count facilitates interoperability between third party authored code operating in the same context.
For example, if three libraries are loaded to use the same context, each library would call cuCtxAttach() to increment the usage count and cuCtxDetach() to decrement the usage count when the library is done using the context.
For most libraries, it is expected that the application will have created a context before loading or initializing the library; that way, the application can create the context using its own heuristics, and the library simply operates on the context handed to it.
Libraries that wish to create their own contexts - unbeknownst to their API clients who may or may not have created contexts of their own - would use cuCtxPushCurrent() and cuCtxPopCurrent() as illustrated in the following figure.
Module  Modules are dynamically loadable packages of device code and data, akin to DLLs in Windows, that are output by nvcc (see Compilation with NVCC ).
The names for all symbols, including functions, global variables, and texture or surface references, are maintained at module scope so that modules written by independent third parties may interoperate in the same CUDA context.
Linker Output:   %s   " , walltime , info_log ); cuModuleLoadData ( cuModule , cubin ); cuLinkDestroy ( linkState ); Full code can be found in the ptxjit CUDA sample. 17.3. Kernel Execution  cuLaunchKernel() launches a kernel with a given execution configuration.
Parameters are passed either as an array of pointers (next to last parameter of cuLaunchKernel() ) where the nth pointer corresponds to the nth parameter and points to a region of memory from which the parameter is copied, or as one of the extra options (last parameter of cuLaunchKernel() ).
When parameters are passed as an extra option (the CU_LAUNCH_PARAM_BUFFER_POINTER option), they are passed as a pointer to a single buffer where parameters are assumed to be properly offset with respect to each other by matching the alignment requirement for each parameter type in device code.
Alignment requirements in device code for the built-in vector types are listed in Table 5 .
For all other basic types, the alignment requirement in device code matches the alignment requirement in host code and can therefore be obtained using __alignof() .
The only exception is when the host compiler aligns double and long long (and long on a 64-bit system) on a one-word boundary instead of a two-word boundary (for example, using gcc ’s compilation flag -mno-align-double ) since in device code these types are always aligned on a two-word boundary.
CUdeviceptr is an integer, but represents a pointer, so its alignment requirement is __alignof(void*) .
The following code sample uses a macro ( ALIGN_UP() ) to adjust the offset of each parameter to meet its alignment requirement and another macro ( ADD_TO_PARAM_BUFFER() ) to add each parameter to the parameter buffer passed to the CU_LAUNCH_PARAM_BUFFER_POINTER option.
The alignment requirement of a structure that contains built-in vector types, CUdeviceptr , or non-aligned double and long long , might therefore differ between device code and host code.
The following structure, for example, is not padded at all in host code, but it is padded in device code with 12 bytes after field f since the alignment requirement for field f4 is 16.
Interoperability between Runtime and Driver APIs  An application can mix runtime API code with driver API code.
If a context is created and made current via the driver API, subsequent runtime calls will pick up this context instead of creating a new one.
If the runtime is initialized (implicitly as mentioned in CUDA Runtime ), cuCtxGetCurrent() can be used to retrieve the context created during initialization.
The implicitly created context from the runtime is called the primary context (see Initialization ).
CUdeviceptr can be cast to regular pointers and vice-versa: CUdeviceptr devPtr ; float * d_data ;   Allocation using driver API cuMemAlloc ( & devPtr , size ); d_data = ( float * ) devPtr ;   Allocation using runtime API cudaMalloc ( & d_data , size ); devPtr = ( CUdeviceptr ) d_data ; In particular, this means that applications written using the driver API can invoke libraries written using the runtime API (such as cuFFT, cuBLAS, …).
All functions from the device and version management sections of the reference manual can be used interchangeably. 17.5. Driver Entry Point Access  17.5.1.
Introduction  The Driver Entry Point Access APIs provide a way to retrieve the address of a CUDA driver function.
Starting from CUDA 11.3, users can call into available CUDA driver APIs using function pointers obtained from these APIs.
These APIs provide functionality similar to their counterparts, dlsym on POSIX platforms and GetProcAddress on Windows.
The provided APIs will let users: Retrieve the address of a driver function using the CUDA Driver API.
For more details, see Retrieve per-thread default stream versions Access new CUDA features on older toolkits but with a newer driver. 17.5.2. Driver Function Typedefs  To help retrieve the CUDA Driver API entry points, the CUDA Toolkit provides access to headers containing the function pointer definitions for all CUDA driver APIs.
These headers are installed with the CUDA Toolkit and are made available in the toolkit’s include/ directory.
The table below summarizes the header files containing the typedefs for each CUDA API header file.
Table 23 Typedefs header files for CUDA driver APIs  API header file API Typedef header file cuda.h cudaTypedefs.h cudaGL.h cudaGLTypedefs.h cudaProfiler.h cudaProfilerTypedefs.h cudaVDPAU.h cudaVDPAUTypedefs.h cudaEGL.h cudaEGLTypedefs.h cudaD3D9.h cudaD3D9Typedefs.h cudaD3D10.h cudaD3D10Typedefs.h cudaD3D11.h cudaD3D11Typedefs.h The above headers do not define actual function pointers themselves; they define the typedefs for function pointers.
For example, cudaTypedefs.h has the below typedefs for the driver API cuMemAlloc : typedef CUresult ( CUDAAPI * PFN_cuMemAlloc_v3020 )( CUdeviceptr_v2 * dptr , size_t bytesize ); typedef CUresult ( CUDAAPI * PFN_cuMemAlloc_v2000 )( CUdeviceptr_v1 * dptr , unsigned int bytesize ); CUDA driver symbols have a version based naming scheme with a _v* extension in its name except for the first version.
When the signature or the semantics of a specific CUDA driver API changes, we increment the version number of the corresponding driver symbol.
In the case of the cuMemAlloc driver API, the first driver symbol name is cuMemAlloc and the next symbol name is cuMemAlloc_v2 .
The typedef for the first version which was introduced in CUDA 2.0 (2000) is PFN_cuMemAlloc_v2000 .
The typedef for the next version which was introduced in CUDA 3.2 (3020) is PFN_cuMemAlloc_v3020 .
The typedefs can be used to more easily define a function pointer of the appropriate type in code: PFN_cuMemAlloc_v3020 pfn_cuMemAlloc_v2 ; PFN_cuMemAlloc_v2000 pfn_cuMemAlloc_v1 ; The above method is preferable if users are interested in a specific version of the API.
Additionally, the headers have predefined macros for the latest version of all driver symbols that were available when the installed CUDA toolkit was released; these typedefs do not have a _v* suffix.
For CUDA 11.3 toolkit, cuMemAlloc_v2 was the latest version and so we can also define its function pointer as below: PFN_cuMemAlloc pfn_cuMemAlloc ; 17.5.3.
Driver Function Retrieval  Using the Driver Entry Point Access APIs and the appropriate typedef, we can get the function pointer to any CUDA driver API. 17.5.3.1. Using the driver API  The driver API requires CUDA version as an argument to get the ABI compatible version for the requested driver symbol.
For example, consider the versions of cuStreamBeginCapture and their corresponding typedefs from cudaTypedefs.h :   cuda.h CUresult CUDAAPI cuStreamBeginCapture ( CUstream hStream ); CUresult CUDAAPI cuStreamBeginCapture_v2 ( CUstream hStream , CUstreamCaptureMode mode );   cudaTypedefs.h typedef CUresult ( CUDAAPI * PFN_cuStreamBeginCapture_v10000 )( CUstream hStream ); typedef CUresult ( CUDAAPI * PFN_cuStreamBeginCapture_v10010 )( CUstream hStream , CUstreamCaptureMode mode ); From the above typedefs in the code snippet, version suffixes _v10000 and _v10010 indicate that the above APIs were introduced in CUDA 10.0 and CUDA 10.1 respectively.
#include   Declare the entry points for cuStreamBeginCapture PFN_cuStreamBeginCapture_v10000 pfn_cuStreamBeginCapture_v1 ; PFN_cuStreamBeginCapture_v10010 pfn_cuStreamBeginCapture_v2 ;   Get the function pointer to the cuStreamBeginCapture driver symbol cuGetProcAddress ( "cuStreamBeginCapture" , & pfn_cuStreamBeginCapture_v1 , 10000 , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus );   Get the function pointer to the cuStreamBeginCapture_v2 driver symbol cuGetProcAddress ( "cuStreamBeginCapture" , & pfn_cuStreamBeginCapture_v2 , 10010 , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); Referring to the code snippet above, to retrieve the address to the _v1 version of the driver API cuStreamBeginCapture , the CUDA version argument should be exactly 10.0 (10000).
Similarly, the CUDA version for retrieving the address to the _v2 version of the API should be 10.1 (10010).
Specifying a higher CUDA version for retrieving a specific version of a driver API might not always be portable.
For example, using 11030 here would still return the _v2 symbol, but if a hypothetical _v3 version is released in CUDA 11.3, the cuGetProcAddress API would start returning the newer _v3 symbol instead when paired with a CUDA 11.3 driver.
Since the ABI and function signatures of the _v2 and _v3 symbols might differ, calling the _v3 function using the _v10010 typedef intended for the _v2 symbol would exhibit undefined behavior.
To retrieve the latest version of a driver API for a given CUDA Toolkit, we can also specify CUDA_VERSION as the version argument and use the unversioned typedef to define the function pointer.
Since _v2 is the latest version of the driver API cuStreamBeginCapture in CUDA 11.3, the below code snippet shows a different method to retrieve it.
Assuming we are using CUDA 11.3 Toolkit #include   Declare the entry point PFN_cuStreamBeginCapture pfn_cuStreamBeginCapture_latest ;   Intialize the entry point.
Specifying CUDA_VERSION will give the function pointer to the   cuStreamBeginCapture_v2 symbol since it is latest version on CUDA 11.3.
cuGetProcAddress ( "cuStreamBeginCapture" , & pfn_cuStreamBeginCapture_latest , CUDA_VERSION , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); Note that requesting a driver API with an invalid CUDA version will return an error CUDA_ERROR_NOT_FOUND .
In the above code examples, passing in a version less than 10000 (CUDA 10.0) would be invalid. 17.5.3.2. Using the runtime API  The runtime API cudaGetDriverEntryPoint uses the CUDA runtime version to get the ABI compatible version for the requested driver symbol.
In the below code snippet, the minimum CUDA runtime version required would be CUDA 11.2 as cuMemAllocAsync was introduced then.
#include   Declare the entry point PFN_cuMemAllocAsync pfn_cuMemAllocAsync ;   Intialize the entry point.
Assuming CUDA runtime version >= 11.2 cudaGetDriverEntryPoint ( "cuMemAllocAsync" , & pfn_cuMemAllocAsync , cudaEnableDefault , & driverStatus );   Call the entry point if ( driverStatus == cudaDriverEntryPointSuccess && pfn_cuMemAllocAsync ) { pfn_cuMemAllocAsync (...); } The runtime API cudaGetDriverEntryPointByVersion uses the user provided CUDA version to get the ABI compatible version for the requested driver symbol.
This allows more specific control over the requested ABI version. 17.5.3.3. Retrieve per-thread default stream versions  Some CUDA driver APIs can be configured to have default stream or per-thread default stream semantics.
Driver APIs having per-thread default stream semantics are suffixed with _ptsz or _ptds in their name.
For example, cuLaunchKernel has a per-thread default stream variant named cuLaunchKernel_ptsz .
With the Driver Entry Point Access APIs, users can request for the per-thread default stream version of the driver API cuLaunchKernel instead of the default stream version.
Configuring the CUDA driver APIs for default stream or per-thread default stream semantics affects the synchronization behavior.
The default stream or per-thread default stream versions of a driver API can be obtained by one of the following ways: Use the compilation flag --default-stream per-thread or define the macro CUDA_API_PER_THREAD_DEFAULT_STREAM to get per-thread default stream behavior.
Force default stream or per-thread default stream behavior using the flags CU_GET_PROC_ADDRESS_LEGACY_STREAM/cudaEnableLegacyStream or CU_GET_PROC_ADDRESS_PER_THREAD_DEFAULT_STREAM/cudaEnablePerThreadDefaultStream respectively. 17.5.3.4. Access new CUDA features  It is always recommended to install the latest CUDA toolkit to access new CUDA driver features, but if for some reason, a user does not want to update or does not have access to the latest toolkit, the API can be used to access new CUDA features with only an updated CUDA driver.
For discussion, let us assume the user is on CUDA 11.3 and wants to use a new driver API cuFoo available in the CUDA 12.0 driver.
The below code snippet illustrates this use-case: int main () {   Assuming we have CUDA 12.0 driver installed.
Manually define the prototype as cudaTypedefs.h in CUDA 11.3 does not have the cuFoo typedef typedef CUresult ( CUDAAPI * PFN_cuFoo )(...); PFN_cuFoo pfn_cuFoo = NULL ; CUdriverProcAddressQueryResult driverStatus ;   Get the address for cuFoo API using cuGetProcAddress.
Specify CUDA version as   12000 since cuFoo was introduced then or get the driver version dynamically   using cuDriverGetVersion int driverVersion ; cuDriverGetVersion ( & driverVersion ); CUresult status = cuGetProcAddress ( "cuFoo" , & pfn_cuFoo , driverVersion , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); if ( status == CUDA_SUCCESS && pfn_cuFoo ) { pfn_cuFoo (...); } else { printf ( "Cannot retrieve the address to cuFoo - driverStatus = %d.
Potential Implications with cuGetProcAddress  Below is a set of concrete and theoretical examples of potential issues with cuGetProcAddress and cudaGetDriverEntryPoint . 17.5.4.1. Implications with cuGetProcAddress vs Implicit Linking  cuDeviceGetUuid was introduced in CUDA 9.2.
To preserve minor version compatibility, cuDeviceGetUuid will not be version bumped to cuDeviceGetUuid_v2 in cuda.h until CUDA 12.0.
This means that calling it by obtaining a function pointer to it via cuGetProcAddress might have different behavior.
Example using the API directly: #include CUuuid uuid ; CUdevice dev ; CUresult status ; status = cuDeviceGet ( & dev , 0 );   Get device 0   handle status status = cuDeviceGetUuid ( & uuid , dev )   Get uuid of device 0 In this example, assume the user is compiling with CUDA 11.4.
Now an example of using cuGetProcAddress : #include CUuuid uuid ; CUdevice dev ; CUresult status ; CUdriverProcAddressQueryResult driverStatus ; status = cuDeviceGet ( & dev , 0 );   Get device 0   handle status PFN_cuDeviceGetUuid pfn_cuDeviceGetUuid ; status = cuGetProcAddress ( "cuDeviceGetUuid" , & pfn_cuDeviceGetUuid , CUDA_VERSION , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); if ( CUDA_SUCCESS == status && pfn_cuDeviceGetUuid ) {   pfn_cuDeviceGetUuid points to ? ?? } In this example, assume the user is compiling with CUDA 11.4.
Calling the function pointer will then invoke the new _v2 function, not the same cuDeviceGetUuid as shown in the previous example. 17.5.4.2. Compile Time vs Runtime Version Usage in cuGetProcAddress  Let’s take the same issue and make one small tweak.
The last example used the compile time constant of CUDA_VERSION to determine which function pointer to obtain.
More complications arise if the user queries the driver version dynamically using cuDriverGetVersion or cudaDriverGetVersion to pass to cuGetProcAddress .
Example: #include CUuuid uuid ; CUdevice dev ; CUresult status ; int cudaVersion ; CUdriverProcAddressQueryResult driverStatus ; status = cuDeviceGet ( & dev , 0 );   Get device 0   handle status status = cuDriverGetVersion ( & cudaVersion );   handle status PFN_cuDeviceGetUuid pfn_cuDeviceGetUuid ; status = cuGetProcAddress ( "cuDeviceGetUuid" , & pfn_cuDeviceGetUuid , cudaVersion , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); if ( CUDA_SUCCESS == status && pfn_cuDeviceGetUuid ) {   pfn_cuDeviceGetUuid points to ? In this example, assume the user is compiling with CUDA 11.3.
The user would debug, test, and deploy this application with the known behavior of getting cuDeviceGetUuid (not the _v2 version).
Since CUDA has guaranteed ABI compatibility between minor versions, this same application is expected to run after the driver is upgraded to CUDA 11.4 (without updating the toolkit and runtime) without requiring recompilation.
This will have undefined behavior though, because now the typedef for PFN_cuDeviceGetUuid will still be of the signature for the original version, but since cudaVersion would now be 11040 (CUDA 11.4), cuGetProcAddress would return the function pointer to the _v2 version, meaning calling it might have undefined behavior.
Note in this case the original (not the _v2 version) typedef looks like: typedef CUresult ( CUDAAPI * PFN_cuDeviceGetUuid_v9020 )( CUuuid * uuid , CUdevice_v1 dev ); But the _v2 version typedef looks like: typedef CUresult ( CUDAAPI * PFN_cuDeviceGetUuid_v11040 )( CUuuid * uuid , CUdevice_v1 dev ); So in this case, the API/ABI is going to be the same and the runtime API call will likely not cause issues–only the potential for unknown uuid return.
In Implications to API/ABI , we discuss a more problematic case of API/ABI compatibility. 17.5.4.3. API Version Bumps with Explicit Version Checks  Above, was a specific concrete example.
Now for instance let’s use a theoretical example that still has issues with compatibility across driver versions.
Example: CUresult cuFoo ( int bar );   Introduced in CUDA 11.4 CUresult cuFoo_v2 ( int bar );   Introduced in CUDA 11.5 CUresult cuFoo_v3 ( int bar , void * jazz );   Introduced in CUDA 11.6 typedef CUresult ( CUDAAPI * PFN_cuFoo_v11040 )( int bar ); typedef CUresult ( CUDAAPI * PFN_cuFoo_v11050 )( int bar ); typedef CUresult ( CUDAAPI * PFN_cuFoo_v11060 )( int bar , void * jazz ); Notice that the API has been modified twice since original creation in CUDA 11.4 and the latest in CUDA 11.6 also modified the API/ABI interface to the function.
The usage in user code compiled against CUDA 11.5 is: #include #include CUresult status ; int cudaVersion ; CUdriverProcAddressQueryResult driverStatus ; status = cuDriverGetVersion ( & cudaVersion );   handle status PFN_cuFoo_v11040 pfn_cuFoo_v11040 ; PFN_cuFoo_v11050 pfn_cuFoo_v11050 ; if ( cudaVersion = CUDA 11.5 version we can use the second version status = cuGetProcAddress ( "cuFoo" , & pfn_cuFoo_v11050 , cudaVersion , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus );   Handle status and validating pfn_cuFoo_v11050 } In this example, without updates for the new typedef in CUDA 11.6 and recompiling the application with those new typedefs and case handling, the application will get the cuFoo_v3 function pointer returned and any usage of that function would then cause undefined behavior.
The point of this example was to illustrate that even explicit version checks for cuGetProcAddress may not safely cover the minor version bumps within a CUDA major release. 17.5.4.4. Issues with Runtime API Usage  The above examples were focused on the issues with the Driver API usage for obtaining the function pointers to driver APIs.
Now we will discuss the potential issues with the Runtime API usage for cudaApiGetDriverEntryPoint .
#include #include #include CUresult status ; cudaError_t error ; int driverVersion , runtimeVersion ; CUdriverProcAddressQueryResult driverStatus ;   Ask the runtime for the function PFN_cuDeviceGetUuid pfn_cuDeviceGetUuidRuntime ; error = cudaGetDriverEntryPoint ( "cuDeviceGetUuid" , & pfn_cuDeviceGetUuidRuntime , cudaEnableDefault , & driverStatus ); if ( cudaSuccess == error && pfn_cuDeviceGetUuidRuntime ) {   pfn_cuDeviceGetUuid points to ? The function pointer in this example is even more complicated than the driver only examples above because there is no control over which version of the function to obtain; it will always get the API for the current CUDA Runtime version.
See the following table for more information: Static Runtime Version Linkage Driver Version Installed V11.3 V11.4 V11.3 v1 v1x V11.4 v1 v2 V11.3 => 11.3 CUDA Runtime and Toolkit (includes header files cuda.h and cudaTypedefs.h) V11.4 => 11.4 CUDA Runtime and Toolkit (includes header files cuda.h and cudaTypedefs.h) v1 => cuDeviceGetUuid v2 => cuDeviceGetUuid_v2 x => Implies the typedef function pointer won't match the returned function pointer.
In these cases, the typedef at compile time using a CUDA 11.4 runtime, would match the _v2 version, but the returned function pointer would be the original (non _v2) function.
The problem in the table comes in with a newer CUDA 11.4 Runtime and Toolkit and older driver (CUDA 11.3) combination, labeled as v1x in the above.
This combination would have the driver returning the pointer to the older function (non _v2), but the typedef used in the application would be for the new function pointer. 17.5.4.5. Issues with Runtime API and Dynamic Versioning  More complications arise when we consider different combinations of the CUDA version with which an application is compiled, CUDA runtime version, and CUDA driver version that an application dynamically links against.
Because of that, notice the number of cases where the typedef does not match the actual version returned/used. 17.5.4.6. Issues with Runtime API allowing CUDA Version  Unless specified otherwise, the CUDA runtime API cudaGetDriverEntryPointByVersion will have similar implications as the driver entry point cuGetProcAddress since it allows for the user to request a specific CUDA driver version.
17.5.4.7. Implications to API/ABI  In the above examples using cuDeviceGetUuid , the implications of the mismatched API are minimal, and may not be entirely noticeable to many users as the _v2 was added to support Multi-Instance GPU (MIG) mode.
So, on a system without MIG, the user might not even realize they are getting a different API.
More problematic is an API which changes its application signature (and hence ABI) such as cuCtxCreate .
The _v2 version, introduced in CUDA 3.2 is currently used as the default cuCtxCreate when using cuda.h but now has a newer version introduced in CUDA 11.4 ( cuCtxCreate_v3 ).
So, in some of the cases above, where the typedef to the function pointer doesn’t match the returned function pointer, there is a chance for non-obvious ABI incompatibility which would lead to undefined behavior.
For example, assume the following code compiled against a CUDA 11.3 toolkit with a CUDA 11.4 driver installed: PFN_cuCtxCreate cuUnknown ; CUdriverProcAddressQueryResult driverStatus ; status = cuGetProcAddress ( "cuCtxCreate" , ( void ** ) & cuUnknown , cudaVersion , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); if ( CUDA_SUCCESS == status && cuUnknown ) { status = cuUnknown ( & ctx , 0 , dev ); } Running this code where cudaVersion is set to anything >=11040 (indicating CUDA 11.4) could have undefined behavior due to not having adequately supplied all the parameters required for the _v3 version of the cuCtxCreate_v3 API. 17.5.5. Determining cuGetProcAddress Failure Reasons  There are two types of errors with cuGetProcAddress.
The second error type encodes in the CUdriverProcAddressQueryResult *symbolStatus and can be used to help distinguish potential issues with the driver not being able to find the symbol requested.
In the example, specifying cudaVersion as anything 11030 or less and when running against a CUDA driver >= CUDA 11.4 would give this result of CU_GET_PROC_ADDRESS_VERSION_NOT_SUFFICIENT .
The second case with the return code CU_GET_PROC_ADDRESS_SYMBOL_NOT_FOUND indicates that the symbol was not found when searching in the CUDA driver.
This can be due to a few reasons such as unsupported CUDA function due to older driver as well as just having a typo.
In the latter, similar to the last example if the user had put symbol as CUDeviceGetExecAffinitySupport - notice the capital CU to start the string - cuGetProcAddress would not be able to find the API because the string doesn’t match.
In the former case an example might be the user developing an application against a CUDA driver supporting the new API, and deploying the application against an older CUDA driver.
Using the last example, if the developer developed against CUDA 11.4 or later but was deployed against a CUDA 11.3 driver, during their development they may have had a succesful cuGetProcAddress , but when deploying an application running against a CUDA 11.3 driver the call would no longer work with the CU_GET_PROC_ADDRESS_SYMBOL_NOT_FOUND returned in driverStatus . 18. CUDA Environment Variables  The following table lists the CUDA environment variables.
Environment variables related to the Multi-Process Service are documented in the Multi-Process Service section of the GPU Deployment and Management guide.
Table 24 CUDA Environment Variables  Variable Values Description Device Enumeration and Properties CUDA_VISIBLE_DEVICES A comma-separated sequence of GPU identifiers MIG support: MIG-  GPU identifiers are given as integer indices or as UUID strings.
GPU UUID strings should follow the same format as given by nvidia-smi , such as GPU-8932f937-d72c-4106-c12f-20bd9faed9f6.
However, for convenience, abbreviated forms are allowed; simply specify enough digits from the beginning of the GPU UUID to uniquely identify that GPU in the target system.
For example, CUDA_VISIBLE_DEVICES=GPU-8932f937 may be a valid way to refer to the above GPU UUID, assuming no other GPU in the system shares this prefix.
Only the devices whose index is present in the sequence are visible to CUDA applications and they are enumerated in the order of the sequence.
If one of the indices is invalid, only the devices whose index precedes the invalid index are visible to CUDA applications.
For example, setting CUDA_VISIBLE_DEVICES to 2,1 causes device 0 to be invisible and device 2 to be enumerated before device 1.
Setting CUDA_VISIBLE_DEVICES to 0,2,-1,1 causes devices 0 and 2 to be visible and device 1 to be invisible.
MIG format starts with MIG keyword and GPU UUID should follow the same format as given by nvidia-smi .
CUDA_MANAGED_FORCE_DEVICE_ALLOC 0 or 1 (default is 0) Forces the driver to place all managed allocations in device memory.
CUDA_DEVICE_ORDER FASTEST_FIRST, PCI_BUS_ID, (default is FASTEST_FIRST) FASTEST_FIRST causes CUDA to enumerate the available devices in fastest to slowest order using a simple heuristic.
Compilation CUDA_CACHE_DISABLE 0 or 1 (default is 0) Disables caching (when set to 1) or enables caching (when set to 0) for just-in-time-compilation.
CUDA_CACHE_PATH filepath Specifies the folder where the just-in-time compiler caches binary codes; the default values are: on Windows, %APPDATA%\NVIDIA\ComputeCache on Linux, ~/.nv/ComputeCache CUDA_CACHE_MAXSIZE integer (default is 1073741824 (1 GiB) for desktop/server platforms and 268435456 (256 MiB) for embedded platforms and the maximum is 4294967296 (4 GiB)) Specifies the size in bytes of the cache used by the just-in-time compiler.
Older binary codes are evicted from the cache to make room for newer binary codes if needed.
CUDA_FORCE_PTX_JIT 0 or 1 (default is 0) When set to 1, forces the device driver to ignore any binary code embedded in an application (see Application Compatibility ) and to just-in-time compile embedded PTX code instead.
This environment variable can be used to validate that PTX code is embedded in an application and that its just-in-time compilation works as expected to guarantee application forward compatibility with future architectures (see Just-in-Time Compilation ).
CUDA_DISABLE_PTX_JIT 0 or 1 (default is 0) When set to 1, disables the just-in-time compilation of embedded PTX code and use the compatible binary code embedded in an application (see Application Compatibility ).
If a kernel does not have embedded binary code or the embedded binary was compiled for an incompatible architecture, then it will fail to load.
This environment variable can be used to validate that an application has the compatible SASS code generated for each kernel.
CUDA_FORCE_JIT 0 or 1 (default is 0) When set to 1, forces the device driver to ignore any binary code embedded in an application (see Application Compatibility ) and to just-in-time compile embedded PTX code instead.
CUDA_DISABLE_JIT 0 or 1 (default is 0) When set to 1, disables the just-in-time compilation of embedded PTX code and use the compatible binary code embedded in an application (see Application Compatibility ).
Execution CUDA_LAUNCH_BLOCKING 0 or 1 (default is 0) Disables (when set to 1) or enables (when set to 0) asynchronous kernel launches.
CUDA_DEVICE_MAX_CONNECTIONS 1 to 32 (default is 8) Sets the number of compute and copy engine concurrent connections (work queues) from the host to each device of compute capability 3.5 and above.
CUDA_AUTO_BOOST 0 or 1 Overrides the autoboost behavior set by the –auto-boost-default option of nvidia-smi.
If an application requests via this environment variable a behavior that is different from nvidia-smi’s, its request is honored if there is no other application currently running on the same GPU that successfully requested a different behavior, otherwise it is ignored.
CUDA_SCALE_LAUNCH_QUEUES “0.25x”, “0.5x”, “2x” or “4x” Scales the size of the queues available for launching work by a fixed multiplier.
cuda-gdb (on Linux platform) CUDA_DEVICE_WAITS_ON_EXCEPTION 0 or 1 (default is 0) When set to 1, a CUDA application will halt when a device exception occurs, allowing a debugger to be attached for further debugging.
MPS service (on Linux platform) CUDA_DEVICE_DEFAULT_PERSISTING_L2_CACHE_PERCENTAGE_LIMIT Percentage value (between 0 - 100, default is 0) Devices of compute capability 8.x allow, a portion of L2 cache to be set-aside for persisting data accesses to global memory.
When using CUDA MPS service, the set-aside size can only be controlled using this environment variable, before starting CUDA MPS control daemon.
I.e., the environment variable should be set before running the command nvidia-cuda-mps-control -d .
Module loading CUDA_MODULE_LOADING DEFAULT, LAZY, EAGER (default is LAZY) Specifies the module loading mode for the application.
When set to EAGER, all kernels and data from a cubin, fatbin or a PTX file are fully loaded upon corresponding cuModuleLoad* and cuLibraryLoad* API call.
When set to LAZY, loading of specific kernels is delayed to the point a CUfunc handle is extracted with cuModuleGetFunction or cuKernelGetFunction API calls and data from the cubin is loaded at load of first kernel in the cubin or at first access of variables in the cubin.
CUDA_MODULE_DATA_LOADING DEFAULT, LAZY, EAGER (default is LAZY) Specifies the data loading mode for the application.
When set to EAGER, all data from a cubin, fatbin or a PTX file are fully loaded to memory upon corresponding cuLibraryLoad* .
Data loading behavior is inherited from CUDA_MODULE_LOADING if this environment variable is not set.
Pre-loading dependent libraries CUDA_FORCE_PRELOAD_LIBRARIES 0 or 1 (default is 0) When set to 1, forces the driver to preload the libraries required for NVVM and PTX just-in-time compilation during driver initialization.
This will increase the memory footprint and the time taken for CUDA driver initialization.
This environment variable needs to be set to avoid certain deadlock situations involving multiple CUDA threads.
CUDA Graphs CUDA_GRAPHS_USE_NODE_PRIORITY 0 or 1 Overrides the cudaGraphInstantiateFlagUseNodePriority flag on graph instantiation.
When set to 1, the flag will be set for all graphs and when set to 0, the flag will be cleared for all graphs. 19. Unified Memory Programming  Note This chapter applies to devices with compute capability 5.0 or higher unless stated otherwise.
For devices with compute capability lower than 5.0, refer to the CUDA toolkit documentation for CUDA 11.8.
This documentation on Unified Memory is divided into 3 parts: General description of unified memory Unified Memory on devices with full CUDA Unified Memory support Unified Memory on devices without full CUDA Unified Memory support 19.1.
Unified Memory Introduction  CUDA Unified Memory provides all processors with: a single unified memory pool, that is, a single pointer value enables all processors in the system (all CPUs, all GPUs, etc.) to access this memory with all of their native memory operations (pointer dereferenes, atomics, etc.).
Unified Memory improves GPU programming in several ways: Producitivity : GPU programs may access Unified Memory from GPU and CPU threads concurrently without needing to create separate allocations ( cudaMalloc() ) and copy memory manually back and forth ( cudaMemcpy*() ).
Performance : Data access speed may be maximized by migrating data towards processors that access it most frequently.
Applications can trigger manual migration of data and may use hints to control migration heuristics.
Total system memory usage may be reduced by avoiding duplicating memory on both CPUs and GPUs.
Functionality : it enables GPU programs to work on data that exceeds the GPU memory’s capacity.
With CUDA Unified Memory, data movement still takes place, and hints may improve performance.
These hints are not required for correctness or functionality, that is, programmers may focus on parallelizing their applications across GPUs and CPUs first, and worry about data-movement later in the development cycle as a performance optimzation.
Note that the physical location of data is invisible to a program and may be changed at any time, but accesses to the data’s virtual address will remain valid and coherent from any processor regardless of locality.
There are two main ways to obtain CUDA Unified Memory: System-Allocated Memory : memory allocated on the host with system APIs: stack variables, global-/file-scope variables, malloc() / mmap() (see System Allocator for in-depth examples), thread locals, etc.
CUDA APIs that explicitly allocate Unified Memory : memory allocated with, for example, cudaMallocManaged() , are available on more systems and may perform better than System-Allocated Memory. 19.1.1. System Requirements for Unified Memory  The following table shows the different levels of support for CUDA Unified Memory, the device properties required to detect these levels of support and links to the documentation specific to each level of support: Table 25 Overview of levels of unified memory support  Unified Memory Support Level System device properties Further documentation Full CUDA Unified Memory: all memory has full support.
Set to 1: pageableMemoryAccess Systems with hardware acceleration also have the following properties set to 1: hostNativeAtomicSupported , pageableMemoryAccessUsesHostPageTables , directManagedMemAccessFromHost Unified Memory on devices with full CUDA Unified Memory support Only CUDA Managed Memory has full support.
Set to 1: concurrentManagedAccess Set to 0: pageableMemoryAccess Unified Memory on devices with only CUDA Managed Memory support CUDA Managed Memory without full support: unified addressing but no concurrent access.
Set to 1: managedMemory Set to 0: concurrentManagedAccess Unified Memory on Windows or devices with compute capability 5.x CUDA for Tegra Memory Management Unified Memory on Tegra No Unified Memory support.
Set to 0: managedMemory CUDA for Tegra Memory Management The behavior of an application that attempts to use Unified Memory on a system that does not support it is undefined.
The following properties enable CUDA applications to check the level of system support for Unified Memory, and to be portable between systems with different levels of support: pageableMemoryAccess : This property is set to 1 on systems with CUDA Unified Memory support where all threads may access System-Allocated Memory and CUDA Managed Memory.
These systems include NVIDIA Grace Hopper, IBM Power9 + Volta, and modern Linux systems with HMM enabled (see next bullet), among others.
Linux HMM requires Linux kernel version 6.1.24+, 6.2.11+ or 6.3+, devices with compute capability 7.5 or higher and a CUDA driver version 535+ installed with Open Kernel Modules .
concurrentManagedAccess : This property is set to 1 on systems with full CUDA Managed Memory support.
When this property is set to 0, there is only partial support for Unified Memory in CUDA Managed Memory.
A program may query the level of GPU support for CUDA Unified Memory, by querying the attributes in Table Overview of levels of unified memory support above using cudaGetDeviceProperties() . 19.1.2. Programming Model  With CUDA Unified Memory, separate allocations between host and device, and explicit memory transfers between them, are no longer required.
Programs may allocate Unified Memory in the following ways: System-Allocation APIs : on systems with full CUDA Unified Memory support via any system allocation of the host process (C’s malloc() , C++’s new operator, POSIX’s mmap and so on).
CUDA Managed Memory Allocation APIs : via the cudaMallocManaged() API which is syntactically similar to cudaMalloc() .
CUDA Managed Variables : variables declared with __managed__ , which are semantically similar to a __device__ variable.
Most examples in this chapter provide at least two versions, one using CUDA Managed Memory and one using System-Allocated Memory.
ret may be used without a separate host_ret allocation and no copy routine is required, greatly simplifying and reducing the size of the program.
Managed Memory : data allocation changed to use cudaMallocManaged() , which returns a pointer valid from both host and device code. 19.1.2.1. Allocation APIs for System-Allocated Memory  On systems with full CUDA Unified Memory support , all memory is unified memory.
This includes memory allocated with system allocation APIs, such as malloc() , mmap() , C++ new() operator, and also automatic variables on CPU thread stacks, thread locals, global variables, and so on.
System-Allocated Memory may be popullated on first touch, depending on the API and system settings used.
First touch means that: - The allocation APIs allocate virtual memory and return immediately, and - physical memory is populated when a thread accesses the memory for the first time.
Usually, the physical memory will be chosen “close” to the processor that thread is running on.
For example, - GPU thread accesses it first: physical GPU memory of GPU that thread runs on is chosen.
- CPU thread accesses it first: physical CPU memory in the memory NUMA node of the CPU core that thread runs on is chosen.
CUDA Unified Memory Hint and Prefetch APIs, cudaMemAdvise and cudaMemPreftchAsync , may be used on System-Allocated Memory.
__global__ void printme ( char * str ) { printf ( str ); } int main () {   Allocate 100 bytes of memory, accessible to both Host and Device code char * s = ( char * ) malloc ( 100 );   Physical allocation placed in CPU memory because host accesses "s" first strncpy ( s , "Hello Unified Memory   " , 99 );   Here we pass "s" to a kernel without explicitly copying printme >> ( s ); cudaDeviceSynchronize ();   Free as for normal CUDA allocations cudaFree ( s ); return 0 ; } 19.1.2.2.
Allocation API for CUDA Managed Memory: cudaMallocManaged()  On systems with CUDA Managed Memory support, unified memory may be allocated using: __host__ cudaError_t cudaMallocManaged ( void ** devPtr , size_t size ); This API is syntactically identical to cudaMalloc() : it allocates size bytes of managed memory and sets devPtr to refer to the allocation.
On systems with full CUDA Managed Memory support , managed memory allocations may be accessed concurrently by all CPUs and GPUs in the system.
Replacing host calls to cudaMalloc() with cudaMallocManaged() , does not impact program semantics on these systems; device code is not able to call cudaMallocManaged() .
The following example shows the use of cudaMallocManaged() : __global__ void printme ( char * str ) { printf ( str ); } int main () {   Allocate 100 bytes of memory, accessible to both Host and Device code char * s ; cudaMallocManaged ( & s , 100 );   Note direct Host-code use of "s" strncpy ( s , "Hello Unified Memory   " , 99 );   Here we pass "s" to a kernel without explicitly copying printme >> ( s ); cudaDeviceSynchronize ();   Free as for normal CUDA allocations cudaFree ( s ); return 0 ; } Note For systems that support CUDA Managed Memory allocations, but do not provide full support, see Coherency and Concurrency .
Implementation details (may change any time): Devices of compute capability 5.x allocate CUDA Managed Memory on the GPU.
Devices of compute capability 6.x and greater populate the memory on first touch, just like System-Allocated Memory APIs. 19.1.2.3. Global-Scope Managed Variables Using __managed__  CUDA __managed__ variables behave as if they were allocated via cudaMallocManaged() (see Explicit Allocation Using cudaMallocManaged() ).
They simplify programs with global variables, making it particularly easy to exchange data between host and device without manual allocations or copying.
On systems with full CUDA Unified Memory support , file-scope or global-scope variables cannot be directly accessed by device code.
But a pointer to these variables may be passed to the kernel as an argument, see System Allocator for examples.
System Allocator __global__ void write_value ( int * ptr , int v ) { * ptr = v ; } int main () {   Requires System-Allocated Memory support int value ; write_value >> ( & value , 1 );   Synchronize required   (before, cudaMemcpy was synchronizing) cudaDeviceSynchronize (); printf ( "value = %d   " , value ); return 0 ; } Managed __global__ void write_value ( int * ptr , int v ) { * ptr = v ; }   Requires CUDA Managed Memory support __managed__ int value ; int main () { write_value >> ( & value , 1 );   Synchronize required   (before, cudaMemcpy was synchronizing) cudaDeviceSynchronize (); printf ( "value = %d   " , value ); return 0 ; } Note the absence of explicit cudaMemcpy() commands and the fact that the return array ret is visible on both CPU and GPU.
CUDA __managed__ variable implies __device__ and is equivalent to __managed__ __device__ , which is also allowed.
Accessing __managed__ variables can trigger CUDA context creation if a context for the current device hasn’t already been created.
In the example above, accessing x before the kernel launch triggers context creation on device 0.
C++ objects declared as __managed__ are subject to certain specific constraints, particularly where static initializers are concerned.
Note For devices with CUDA Managed Memory without full support , visibility of __managed__ variables for asynchronous operations executing in CUDA streams is discussed in the section on Managing Data Visibility and Concurrent CPU + GPU Access with Streams . 19.1.2.4. Difference between Unified Memory and Mapped Memory  The main difference between Unified Memory and CUDA Mapped Memory is that CUDA Mapped Memory does not guarantee that all kinds of memory accesses (for example atomics) are supported on all systems, while Unified Memory does.
The limited set of memory operations that are guaranteed to be portably supported by CUDA Mapped Memory is available on more systems than Unified Memory. 19.1.2.5. Pointer Attributes  CUDA Programs may check whether a pointer addresses a CUDA Managed Memory allocation by calling cudaPointerGetAttributes() and testing whether the pointer attribute value is cudaMemoryTypeManaged .
This API returns cudaMemoryTypeHost for system-allocated memory that has been registered with cudaHostRegister() and cudaMemoryTypeUnregistered for system-allocated memory that CUDA is unaware of.
Pointer attributes do not state where the memory resides, they state how the memory was allocated or registered.
The following example shows how to detect the type of pointer at runtime: char const * kind ( cudaPointerAttributes a , bool pma , bool cma ) { switch ( a .
Data Usage Hints  When multiple processors simultaneously access the same data, cudaMemAdvise may be used to hint how the data at [devPtr, devPtr + count) will be accessed: cudaError_t cudaMemAdvise ( const void * devPtr , size_t count , enum cudaMemoryAdvise advice , int device ); Where advice may take the following values: cudaMemAdviseSetReadMostly : This implies that the data is mostly going to be read from and only occasionally written to.
Example: void test_advise_managed ( cudaStream_t stream ) { char * dataPtr ; size_t dataSize = 64 * TPB ;   16 KiB   Allocate memory using cudaMallocManaged   (malloc may be used on systems with full CUDA Unified memory support) cudaMallocManaged ( & dataPtr , dataSize );   Set the advice on the memory region cudaMemAdvise ( dataPtr , dataSize , cudaMemAdviseSetReadMostly , myGpuId ); int outerLoopIter = 0 ; while ( outerLoopIter >> (( const char * ) dataPtr , dataSize ); innerLoopIter ++ ; } outerLoopIter ++ ; } cudaFree ( dataPtr ); } cudaMemAdviseSetPreferredLocation : In general, any memory may be migrated at any time to any location, for example, when a given processor is running out of physical memory.
This hint tells the system that migrating this memory region away from its preferred location is undesired, by setting the preferred location for the data to be the physical memory belonging to device.
Passing in a value of cudaCpuDeviceId for device sets the preferred location as CPU memory.
Other hints, like cudaMemPrefetchAsync , may override this hint, leading the memory to be migrated away from its preferred location.
cudaMemAdviseSetAccessedBy : In some systems, it may be beneficial for performance to establish a mapping into memory before accessing the data from a given processor.
This hint tells the system that the data will be frequently accessed by device , enabling the system to assume that creating these mappings pays off.
This hint does not imply where the data should reside, but it can be combined with cudaMemAdviseSetPreferredLocation to specify that.
Each advice can be also unset by using one of the following values: cudaMemAdviseUnsetReadMostly , cudaMemAdviseUnsetPreferredLocation and cudaMemAdviseUnsetAccessedBy . 19.1.2.8.3. Querying Data Usage Attributes on Managed Memory  A program can query memory range attributes assigned through cudaMemAdvise or cudaMemPrefetchAsync on CUDA Managed Memory by using the following API: cudaMemRangeGetAttribute ( void * data , size_t dataSize , enum cudaMemRangeAttribute attribute , const void * devPtr , size_t count ); This function queries an attribute of the memory range starting at devPtr with a size of count bytes.
The memory range must refer to managed memory allocated via cudaMallocManaged or declared via __managed__ variables.
It is possible to query the following attributes: cudaMemRangeAttributeReadMostly : the result returned will be 1 if the entire memory range has the cudaMemAdviseSetReadMostly attribute set, or 0 otherwise.
cudaMemRangeAttributePreferredLocation : the result returned will be a GPU device id or cudaCpuDeviceId if the entire memory range has the corresponding processor as preferred location, otherwise cudaInvalidDeviceId will be returned.
An application can use this query API to make decision about staging data through CPU or GPU depending on the preferred location attribute of the managed pointer.
Note that the actual location of the memory range at the time of the query may be different from the preferred location.
cudaMemRangeAttributeAccessedBy : will return the list of devices that have that advise set for that memory range.
cudaMemRangeAttributeLastPrefetchLocation : will return the last location to which the memory range was prefetched explicitly using cudaMemPrefetchAsync .
Note that this simply returns the last location that the application requested to prefetch the memory range to.
It gives no indication as to whether the prefetch operation to that location has completed or even begun.
Additionally, multiple attributes can be queried by using corresponding cudaMemRangeGetAttributes function. 19.2. Unified memory on devices with full CUDA Unified Memory support  19.2.1.
System-Allocated Memory: in-depth examples  Systems with full CUDA Unified Memory support allow the device to access any memory owned by the host process interacting with the device.
The next three tabs show various ways a file-scope or global-scope variable can be accessed from the device.
Note that for the extern variable, it could be declared and its memory owned and managed by a third-party library, which does not interact with CUDA at all.
Also note that stack variables as well as file-scope and global-scope variables can only be accessed through a pointer by the GPU.
In this specific example, this is convenient because the character array is already declared as a pointer: const char* .
However, consider the following example with a global-scope integer:   this variable is declared at global scope int global_variable ; __global__ void kernel_uncompilable () {   this causes a compilation error: global (__host__) variables must not   be accessed from __device__ / __global__ code printf ( "%d   " , global_variable ); }   On systems with pageableMemoryAccess set to 1, we can access the address   of a global variable.
The below kernel takes that address as an argument __global__ void kernel ( int * global_variable_addr ) { printf ( "%d   " , * global_variable_addr ); } int main () { kernel >> ( & global_variable ); ...
return 0 ; } In the example above, we need to ensure to pass a pointer to the global variable to the kernel instead of directly accessing the global variable in the kernel.
This is because global variables without the __managed__ specifier are declared as __host__ -only by default, thus most compilers won’t allow using these variables directly in device code as of now. 19.2.1.1. File-backed Unified Memory  Since systems with full CUDA Unified Memory support allow the device to access any memory owned by the host process, they can directly access file-backed memory.
Here, we show a modified version of the initial example shown in the previous section to use file-backed memory in order to print a string from the GPU, read directly from an input file.
In the following example, the memory is backed by a physical file, but the example applies to memory-backed files, too, as shown in the section on Inter-Process Communication with Unified Memory .
__global__ void kernel ( const char * type , const char * data ) { static const int n_char = 8 ; printf ( "%s - first %d characters: '" , type , n_char ); for ( int i = 0 ; i = 0 , "Invalid file handle" ); struct stat file_stat ; int status = fstat ( fd , & file_stat ); ASSERT ( status >= 0 , "Invalid file stats" ); char * mapped = ( char * ) mmap ( 0 , file_stat .
st_size , PROT_READ , MAP_PRIVATE , fd , 0 ); ASSERT ( mapped != MAP_FAILED , "Cannot map file into memory" ); kernel >> ( "file-backed" , mapped ); ASSERT ( cudaDeviceSynchronize () == cudaSuccess , "CUDA failed with '%s'" , cudaGetErrorString ( cudaGetLastError ())); ASSERT ( munmap ( mapped , file_stat .
st_size ) == 0 , "Cannot unmap file" ); ASSERT ( close ( fd ) == 0 , "Cannot close file" ); } Note that on systems without the hostNativeAtomicSupported property, including systems with Linux HMM enabled , atomic accesses to file-backed memory are not supported. 19.2.1.2. Inter-Process Communication (IPC) with Unified Memory  Note As of now, using IPC with Unified Memory can have significant performance implications.
Many applications prefer to manage one GPU per process, but still need to use Unified Memory, for example for over-subscription, and access it from multiple GPUs.
CUDA IPC (see Interprocess Communication ) does not support Managed Memory: handles to this type of memory may not be shared through any of the mechanisms discussed in this section.
On systems with full CUDA Unified Memory support , System-Allocated Memory is Inter-Process Communication (IPC) capable.
Once access to System-Allocated Memory has been shared with other processes, the same Unified Memory Programming Model applies, similar to File-backed Unified Memory .
See the following references for more information on various ways of creating IPC-capable System-Allocated Memory under Linux: mmap with MAP_SHARED POSIX IPC APIs Linux memfd_create Note that it is not possible to share memory between different hosts and their devices using this technique. 19.2.2. Performance Tuning  In order to achieve good performance with Unified Memory, it is important to: Understand how paging works on your system, and how to avoid unnecessary page faults.
As general advice, Unified Memory Performance Hints might provide improved performance, but using them incorrectly might degrade performance compared to the default behavior.
Also note that any hint has a performance cost associated with it on the host, thus useful hints must at the very least improve performance enough to overcome this cost. 19.2.2.1. Memory Paging and Page Sizes  Many of the sections for unified memory performance tuning assume prior knowledge on virtual addressing, memory pages and page sizes.
This section attempts to define all necessary terms and explain why paging matters for performance.
All currently supported systems for Unified Memory use a virtual address space: this means that memory addresses used by an application represent a virtual location which might be mapped to a physical location where the memory actually resides.
All currently supported processors, including both CPUs and GPUs, additionally use memory paging .
Because all systems use a virtual address space, there are two types of memory pages: Virtual pages: this represents a fixed-size contiguous chunk of virtual memory per process tracked by the operating system, which can be mapped into physical memory.
Note that the virtual page is linked to the mapping : for example, a single virtual address might be mapped into physical memory using different page sizes.
Physical pages: this represents a fixed-size contiguous chunk of memory the processor’s main Memory Management Unit (MMU) supports and into which a virtual page can be mapped.
Arm CPUs support multiple physical page sizes - 4KiB, 16KiB, 32KiB and 64KiB - depending on the exact CPU.
Finally, NVIDIA GPUs support multiple physical page sizes, but prefer 2MiB physical pages or larger.
The default page size of virtual pages usually corresponds to the physical page size, but an application may use different page sizes as long as they are supported by the operating system and the hardware.
Typically, supported virtual page sizes must be powers of 2 and multiples of the physical page size.
The logical entity tracking the mapping of virtual pages into physical pages will be referred to as a page table , and each mapping of a given virtual page with a given virtual size to physical pages is called a page table entry (PTE) .
All supported processors provide specific caches for the page table to speed up the translation of virtual addresses to physical addresses.
There are two important aspects for performance tuning of applications: the choice of virtual page size, whether the system offers a combined page table used by both CPUs and GPUs, or separate page tables for each CPU and GPU individually. 19.2.2.1.1. Choosing the right page size  In general, small page sizes lead to less (virtual) memory fragmentation but more TLB misses, whereas larger page sizes lead to more memory fragmentation but less TLB misses.
Additionally, memory migration is generally more expensive with larger page sizes compared to smaller page sizes, because we typically migrate full memory pages.
One important aspect for performance tuning is that TLB misses are generally significantly more expensive on the GPU compared to the CPU.
This means that if a GPU thread frequently accesses random locations of Unified Memory mapped using a small enough page size, it might be significantly slower compared to the same accesses to Unified Memory mapped using a large enough page size.
While a similar effect might occur for a CPU thread randomly accessing a large area of memory mapped using a small page size, the slowdown is less pronounced, meaning that the application might want to trade-off this slowdown with having less memory fragmentation.
Note that in general, applications should not tune their performance to the physical page size of a given processor, since physical page sizes are subject to change depending on the hardware.
The advice above only applies to virtual page sizes. 19.2.2.1.2. CPU and GPU page tables: hardware coherency vs.
software coherency  Note In the remainder of the performance tuning documentation, we will refer to systems with a combined page table for both CPUs and GPUs as hardware coherent systems.
Systems with separate page tables for CPUs and GPUs are referred to as software coherent .
Hardware coherent systems such as NVIDIA Grace Hopper offer a logically combined page table for both CPUs and GPUs.
This is important because in order to access System-Allocated Memory from the GPU , the GPU uses whichever page table entry was created by the CPU for the requested memory.
If that page table entry uses the default CPU page size of 4KiB or 64KiB, accesses to large virtual memory areas will cause significant TLB misses, thus significant slowdowns.
See the section on configuring huge pages for examples on how to ensure System-Allocated Memory uses large enough page sizes to avoid this type of issue.
On the other hand, on systems where the CPUs and GPUs each have their own logical page table, different performance tuning aspects should be considered: in order to guarantee coherency , these systems usually use page faults in case a processor accesses a memory address mapped into the physical memory of a different processor.
Such a page fault means that: it needs to be ensured that the currently owning processor (where the physical page currently resides) cannot access this page anymore, either by deleting the page table entry or updating it.
it needs to be ensured that the processor requesting access can access this page, either by creating a new page table entry or updating and existing entry, such that it becomes valid/active.
the physical page backing this virtual page must be moved/migrated to the processor requesting access: this can be an expensive operation, and the amount of work is proportional to the page size.
Overall, hardware coherent systems provide significant performance benefits compared to software coherent systems in cases where frequent concurrent accesses to the same memory page are made by both CPU and GPU threads: less page-faults: these systems do not need to use page-faults for emulating coherency or migrating memory, less contention: these systems are coherent at cache-line granularity instead of page-size granularity, that is, when there is contention from multiple processors within a cache line, only the cache line is exchanged which is much smaller than the smallest page-size, and when the different processors access different cache-lines within a page, then there is no contention.
This impacts the performance of the following scenarios: Atomic updates to the same address concurrently from both CPUs and GPUs.
Signaling a GPU thread from a CPU thread or vice-versa. 19.2.2.2. Direct Unified Memory Access from host  Some devices have hardware support for coherent reads, stores and atomic accesses from the host on GPU-resident unified memory.
Note that all hardware coherent systems have this attribute set for NVLink-connected devices.
On these systems, the host has direct access to GPU-resident memory without page faults and data migration (see Data Usage Hints for more details on memory usage hints).
Note that with CUDA Managed Memory, the cudaMemAdviseSetAccessedBy hint with cudaCpuDeviceId is necessary to enable this direct access without page faults.
Consider an example code below: System Allocator __global__ void write ( int * ret , int a , int b ) { ret [ threadIdx .
x ; } void test_malloc () { int * ret = ( int * ) malloc ( 1000 * sizeof ( int ));   for shared page table systems, the following hint is not necesary cudaMemAdvise ( ret , 1000 * sizeof ( int ), cudaMemAdviseSetAccessedBy , cudaCpuDeviceId ); write >> ( ret , 10 , 100 );   pages populated in GPU memory cudaDeviceSynchronize (); for ( int i = 0 ; i >> ( ret , 10 , 100 );   directManagedMemAccessFromHost=1: GPU accesses GPU memory without migrations cudaDeviceSynchronize ();   directManagedMemAccessFromHost=0: GPU faults and triggers host-to-device migrations free ( ret ); } Managed __global__ void write ( int * ret , int a , int b ) { ret [ threadIdx .
x ; } void test_managed () { int * ret ; cudaMallocManaged ( & ret , 1000 * sizeof ( int )); cudaMemAdvise ( ret , 1000 * sizeof ( int ), cudaMemAdviseSetAccessedBy , cudaCpuDeviceId );   set direct access hint write >> ( ret , 10 , 100 );   pages populated in GPU memory cudaDeviceSynchronize (); for ( int i = 0 ; i >> ( ret , 10 , 100 );   directManagedMemAccessFromHost=1: GPU accesses GPU memory without migrations cudaDeviceSynchronize ();   directManagedMemAccessFromHost=0: GPU faults and triggers host-to-device migrations cudaFree ( ret ); } After write kernel is completed, ret will be created and initialized in GPU memory.
This code will show different behavior depending on the system architecture and support of hardware coherency: On systems with directManagedMemAccessFromHost=1 : CPU accesses to the managed buffer will not trigger any migrations; the data will remain resident in GPU memory and any subsequent GPU kernels can continue to access it directly without inflicting faults or migrations.
On systems with directManagedMemAccessFromHost=0 : CPU accesses to the managed buffer will page fault and initiate data migration; any GPU kernel trying to access the same data first time will page fault and migrate pages back to GPU memory. 19.2.2.3. Host Native Atomics  Some devices, including NVLink-connected devices in hardware coherent systems , support hardware-accelerated atomic accesses to CPU-resident memory.
This implies that atomic accesses to host memory do not have to be emulated with a page fault.
For these devices, the attribute cudaDevAttrHostNativeAtomicSupported is set to 1. 19.3. Unified memory on devices without full CUDA Unified Memory support  19.3.1.
Unified memory on devices with only CUDA Managed Memory support  For devices with compute capability 6.x or higher but without pageable memory access , CUDA Managed Memory is fully supported and coherent.
The programming model and performance tuning of unified memory is largely similar to the model as described in Unified memory on devices with full CUDA Unified Memory support , with the notable exception that system allocators cannot be used to allocate memory.
Thus, the following list of sub-sections do not apply: System Allocator Hardware/Software Coherency 19.3.2.
Unified memory on Windows or devices with compute capability 5.x  Devices with compute capability lower than 6.0 or Windows platforms support CUDA Managed Memory v1.0 with limited support for data migration and coherency as well as memory oversubscription.
The following sub-sections describe in more detail how to use and optimize Managed Memory on these platforms. 19.3.2.1. Data Migration and Coherency  GPU architectures of compute capability lower than 6.0 do not support fine-grained movement of the managed data to GPU on-demand.
Whenever a GPU kernel is launched all managed memory generally has to be transferred to GPU memory to avoid faulting on memory access.
With compute capability 6.x a new GPU page faulting mechanism is introduced that provides more seamless Unified Memory functionality.
Combined with the system-wide virtual address space, page faulting provides several benefits.
First, page faulting means that the CUDA system software doesn’t need to synchronize all managed memory allocations to the GPU before each kernel launch.
If a kernel running on the GPU accesses a page that is not resident in its memory, it faults, allowing the page to be automatically migrated to the GPU memory on-demand.
Alternatively, the page may be mapped into the GPU address space for access over the PCIe or NVLink interconnects (mapping on access can sometimes be faster than migration).
Note that Unified Memory is system-wide: GPUs (and CPUs) can fault on and migrate memory pages either from CPU memory or from the memory of other GPUs in the system. 19.3.2.2. GPU Memory Oversubscription  Devices of compute capability lower than 6.0 cannot allocate more managed memory than the physical size of GPU memory.
19.3.2.3. Multi-GPU  On systems with devices of compute capabilities lower than 6.0 managed allocations are automatically visible to all GPUs in a system via the peer-to-peer capabilities of the GPUs.
Managed memory allocations behave similar to unmanaged memory allocated using cudaMalloc() : the current active device is the home for the physical allocation but other GPUs in the system will access the memory at reduced bandwidth over the PCIe bus.
On Linux the managed memory is allocated in GPU memory as long as all GPUs that are actively being used by a program have the peer-to-peer support.
If at any time the application starts using a GPU that doesn’t have peer-to-peer support with any of the other GPUs that have managed allocations on them, then the driver will migrate all managed allocations to system memory.
On Windows, if peer mappings are not available (for example, between GPUs of different architectures), then the system will automatically fall back to using zero-copy memory, regardless of whether both GPUs are actually used by a program.
If only one GPU is actually going to be used, it is necessary to set the CUDA_VISIBLE_DEVICES environment variable before launching the program.
This constrains which GPUs are visible and allows managed memory to be allocated in GPU memory.
Alternatively, on Windows users can also set CUDA_MANAGED_FORCE_DEVICE_ALLOC to a non-zero value to force the driver to always use device memory for physical storage.
When this environment variable is set to a non-zero value, all devices used in that process that support managed memory have to be peer-to-peer compatible with each other.
The error ::cudaErrorInvalidDevice will be returned if a device that supports managed memory is used and it is not peer-to-peer compatible with any of the other managed memory supporting devices that were previously used in that process, even if ::cudaDeviceReset has been called on those devices.
Note that starting from CUDA 8.0 CUDA_MANAGED_FORCE_DEVICE_ALLOC has no effect on Linux operating systems. 19.3.2.4. Coherency and Concurrency  Simultaneous access to managed memory on devices of compute capability lower than 6.0 is not possible, because coherence could not be guaranteed if the CPU accessed a Unified Memory allocation while a GPU kernel was active.
19.3.2.4.1. GPU Exclusive Access To Managed Memory  To ensure coherency on pre-6.x GPU architectures, the Unified Memory programming model puts constraints on data accesses while both the CPU and GPU are executing concurrently.
In effect, the GPU has exclusive access to all managed data while any kernel operation is executing, regardless of whether the specific kernel is actively using the data.
When managed data is used with cudaMemcpy*() or cudaMemset*() , the system may choose to access the source or destination from the host or the device, which will put constraints on concurrent CPU access to that data while the cudaMemcpy*() or cudaMemset*() is executing.
It is not permitted for the CPU to access any managed allocations or variables while the GPU is active for devices with concurrentManagedAccess property set to 0.
On these systems concurrent CPU/GPU accesses, even to different managed memory allocations, will cause a segmentation fault because the page is considered inaccessible to the CPU.
__device__ __managed__ int x , y = 2 ; __global__ void kernel () { x = 10 ; } int main () { kernel >> (); y = 20 ;   Error on GPUs not supporting concurrent access cudaDeviceSynchronize (); return 0 ; } In example above, the GPU program kernel is still active when the CPU touches y .
(Note how it occurs before cudaDeviceSynchronize() .) The code runs successfully on devices of compute capability 6.x due to the GPU page faulting capability which lifts all restrictions on simultaneous access.
However, such memory access is invalid on pre-6.x architectures even though the CPU is accessing different data than the GPU.
The program must explicitly synchronize with the GPU before accessing y : __device__ __managed__ int x , y = 2 ; __global__ void kernel () { x = 10 ; } int main () { kernel >> (); cudaDeviceSynchronize (); y = 20 ;   Success on GPUs not supporing concurrent access return 0 ; } As this example shows, on systems with pre-6.x GPU architectures, a CPU thread may not access any managed data in between performing a kernel launch and a subsequent synchronization call, regardless of whether the GPU kernel actually touches that same data (or any managed data at all).
The mere potential for concurrent CPU and GPU access is sufficient for a process-level exception to be raised.
Note that if memory is dynamically allocated with cudaMallocManaged() or cuMemAllocManaged() while the GPU is active, the behavior of the memory is unspecified until additional work is launched or the GPU is synchronized.
Attempting to access the memory on the CPU during this time may or may not cause a segmentation fault.
This does not apply to memory allocated using the flag cudaMemAttachHost or CU_MEM_ATTACH_HOST . 19.3.2.4.2. Explicit Synchronization and Logical GPU Activity  Note that explicit synchronization is required even if kernel runs quickly and finishes before the CPU touches y in the above example.
This aligns with the CUDA programming model, which specifies that a kernel can run at any time following a launch and is not guaranteed to have finished until the host issues a synchronization call.
This includes cudaDeviceSynchronize() ; cudaStreamSynchronize() and cudaStreamQuery() (provided it returns cudaSuccess and not cudaErrorNotReady ) where the specified stream is the only stream still executing on the GPU; cudaEventSynchronize() and cudaEventQuery() in cases where the specified event is not followed by any device work; as well as uses of cudaMemcpy() and cudaMemset() that are documented as being fully synchronous with respect to the host.
Dependencies created between streams will be followed to infer completion of other streams by synchronizing on a stream or event.
Dependencies can be created via cudaStreamWaitEvent() or implicitly when using the default (NULL) stream.
It is legal for the CPU to access managed data from within a stream callback, provided no other stream that could potentially be accessing managed data is active on the GPU.
In addition, a callback that is not followed by any device work can be used for synchronization: for example, by signaling a condition variable from inside the callback; otherwise, CPU access is valid only for the duration of the callback(s).
There are several important points of note: It is always permitted for the CPU to access non-managed zero-copy data while the GPU is active.
The GPU is considered active when it is running any kernel, even if that kernel does not make use of managed data.
If a kernel might use data, then access is forbidden, unless device property concurrentManagedAccess is 1.
There are no constraints on concurrent inter-GPU access of managed memory, other than those that apply to multi-GPU access of non-managed memory.
Note how the last point allows for races between GPU kernels, as is currently the case for non-managed GPU memory.
As mentioned previously, managed memory functions identically to non-managed memory from the perspective of the GPU.
The following code example illustrates these points: int main () { cudaStream_t stream1 , stream2 ; cudaStreamCreate ( & stream1 ); cudaStreamCreate ( & stream2 ); int * non_managed , * managed , * also_managed ; cudaMallocHost ( & non_managed , 4 );   Non-managed, CPU-accessible memory cudaMallocManaged ( & managed , 4 ); cudaMallocManaged ( & also_managed , 4 );   Point 1: CPU can access non-managed data.
kernel >> ( managed ); * non_managed = 1 ;   Point 2: CPU cannot access any managed data while GPU is busy,   unless concurrentManagedAccess = 1   Note we have not yet synchronized, so "kernel" is still active.
* also_managed = 2 ;   Will issue segmentation fault   Point 3: Concurrent GPU kernels can access the same data.
Managing Data Visibility and Concurrent CPU + GPU Access with Streams  Until now it was assumed that for SM architectures before 6.x: 1) any active kernel may use any managed memory, and 2) it was invalid to use managed memory from the CPU while a kernel is active.
Here we present a system for finer-grained control of managed memory designed to work on all devices supporting managed memory, including older architectures with concurrentManagedAccess equal to 0.
The CUDA programming model provides streams as a mechanism for programs to indicate dependence and independence among kernel launches.
Kernels launched into the same stream are guaranteed to execute consecutively, while kernels launched into different streams are permitted to execute concurrently.
Streams describe independence between work items and hence allow potentially greater efficiency through concurrency.
Unified Memory builds upon the stream-independence model by allowing a CUDA program to explicitly associate managed allocations with a CUDA stream.
In this way, the programmer indicates the use of data by kernels based on whether they are launched into a specified stream or not.
This enables opportunities for concurrency based on program-specific data access patterns.
The function to control this behavior is: cudaError_t cudaStreamAttachMemAsync ( cudaStream_t stream , void * ptr , size_t length = 0 , unsigned int flags = 0 ); The cudaStreamAttachMemAsync() function associates length bytes of memory starting from ptr with the specified stream .
(Currently, length must always be 0 to indicate that the entire region should be attached.) Because of this association, the Unified Memory system allows CPU access to this memory region so long as all operations in stream have completed, regardless of whether other streams are active.
In effect, this constrains exclusive ownership of the managed memory region by an active GPU to per-stream activity instead of whole-GPU activity.
Most importantly, if an allocation is not associated with a specific stream, it is visible to all running kernels regardless of their stream.
This is the default visibility for a cudaMallocManaged() allocation or a __managed__ variable; hence, the simple-case rule that the CPU may not touch the data while any kernel is running.
By associating an allocation with a specific stream, the program makes a guarantee that only kernels launched into that stream will touch that data.
No error checking is performed by the Unified Memory system: it is the programmer’s responsibility to ensure that guarantee is honored.
In addition to allowing greater concurrency, the use of cudaStreamAttachMemAsync() can (and typically does) enable data transfer optimizations within the Unified Memory system that may affect latencies and other overhead. 19.3.2.4.4. Stream Association Examples  Associating data with a stream allows fine-grained control over CPU + GPU concurrency, but what data is visible to which streams must be kept in mind when using devices of compute capability lower than 6.0.
Looking at the earlier synchronization example: __device__ __managed__ int x , y = 2 ; __global__ void kernel () { x = 10 ; } int main () { cudaStream_t stream1 ; cudaStreamCreate ( & stream1 ); cudaStreamAttachMemAsync ( stream1 , & y , 0 , cudaMemAttachHost ); cudaDeviceSynchronize ();   Wait for Host attachment to occur.
return 0 ; } Here we explicitly associate y with host accessibility, thus enabling access at all times from the CPU.
(As before, note the absence of cudaDeviceSynchronize() before the access.) Accesses to y by the GPU running kernel will now produce undefined results.
Note that associating a variable with a stream does not change the associating of any other variable.
For example, associating x with stream1 does not ensure that only x is accessed by kernels launched in stream1 , thus an error is caused by this code: __device__ __managed__ int x , y = 2 ; __global__ void kernel () { x = 10 ; } int main () { cudaStream_t stream1 ; cudaStreamCreate ( & stream1 ); cudaStreamAttachMemAsync ( stream1 , & x );   Associate “x” with stream1.
y = 20 ;   ERROR: “y” is still associated globally   with all streams by default return 0 ; } Note how the access to y will cause an error because, even though x has been associated with a stream, we have told the system nothing about who can see y .
The system therefore conservatively assumes that kernel might access it and prevents the CPU from doing so. 19.3.2.4.5. Stream Attach With Multithreaded Host Programs  The primary use for cudaStreamAttachMemAsync() is to enable independent task parallelism using CPU threads.
Typically in such a program, a CPU thread creates its own stream for all work that it generates because using CUDA’s NULL stream would cause dependencies between threads.
The default global visibility of managed data to any GPU stream can make it difficult to avoid interactions between CPU threads in a multi-threaded program.
Function cudaStreamAttachMemAsync() is therefore used to associate a thread’s managed allocations with that thread’s own stream, and the association is typically not changed for the life of the thread.
Such a program would simply add a single call to cudaStreamAttachMemAsync() to use unified memory for its data accesses:   This function performs some task, in its own private stream.
cudaStream_t stream ; cudaStreamCreate ( & stream );   Allocate some managed data and associate with our stream.
Note the use of the host-attach flag to cudaMallocManaged();   we then associate the allocation with our stream so that   our GPU kernel launches can access it.
int * data ; cudaMallocManaged (( void ** ) & data , length , cudaMemAttachHost ); cudaStreamAttachMemAsync ( stream , data ); cudaStreamSynchronize ( stream );   Iterate on the data in some way, using both Host & Device.
for ( int i = 0 ; i >> ( in , data , length ); cudaStreamSynchronize ( stream ); host_process ( data , length );   CPU uses managed data.
convert >> ( out , data , length ); } cudaStreamSynchronize ( stream ); cudaStreamDestroy ( stream ); cudaFree ( data ); } In this example, the allocation-stream association is established just once, and then data is used repeatedly by both the host and device.
The result is much simpler code than occurs with explicitly copying data between host and device, although the result is the same. 19.3.2.4.6. Advanced Topic: Modular Programs and Data Access Constraints  In the previous example cudaMallocManaged() specifies the cudaMemAttachHost flag, which creates an allocation that is initially invisible to device-side execution.
(The default allocation would be visible to all GPU kernels on all streams.) This ensures that there is no accidental interaction with another thread’s execution in the interval between the data allocation and when the data is acquired for a specific stream.
Without this flag, a new allocation would be considered in-use on the GPU if a kernel launched by another thread happens to be running.
This might impact the thread’s ability to access the newly allocated data from the CPU (for example, within a base-class constructor) before it is able to explicitly attach it to a private stream.
To enable safe independence between threads, therefore, allocations should be made specifying this flag.
Note An alternative would be to place a process-wide barrier across all threads after the allocation has been attached to the stream.
This would ensure that all threads complete their data/stream associations before any kernels are launched, avoiding the hazard.
A second barrier would be needed before the stream is destroyed because stream destruction causes allocations to revert to their default visibility.
The cudaMemAttachHost flag exists both to simplify this process, and because it is not always possible to insert global barriers where required. 19.3.2.4.7. Memcpy()/Memset() Behavior With Stream-associated Unified Memory  See Memcpy()/Memset() Behavior With Unified Memory for a general overview of cudaMemcpy* / cudaMemset* behavior on devices with concurrentManagedAccess set.
On devices where concurrentManagedAccess is not set, the following rules apply: If cudaMemcpyHostTo* is specified and the source data is unified memory, then it will be accessed from the host if it is coherently accessible from the host in the copy stream (1) ; otherwise it will be accessed from the device.
Similar rules apply to the destination when cudaMemcpy*ToHost is specified and the destination is unified memory.
If cudaMemcpyDeviceTo* is specified and the source data is unified memory, then it will be accessed from the device.
The source must be coherently accessible from the device in the copy stream (2) ; otherwise, an error is returned.
Similar rules apply to the destination when cudaMemcpy*ToDevice is specified and the destination is unified memory.
If cudaMemcpyDefault is specified, then unified memory will be accessed from the host either if it cannot be coherently accessed from the device in the copy stream (2) or if the preferred location for the data is cudaCpuDeviceId and it can be coherently accessed from the host in the copy stream (1) ; otherwise, it will be accessed from the device.
When using cudaMemset*() with unified memory, the data must be coherently accessible from the device in the stream being used for the cudaMemset() operation (2) ; otherwise, an error is returned.
When data is accessed from the device either by cudaMemcpy* or cudaMemset* , the stream of operation is considered to be active on the GPU.
During this time, any CPU access of data that is associated with that stream or data that has global visibility, will result in a segmentation fault if the GPU has a zero value for the device attribute concurrentManagedAccess .
The program must synchronize appropriately to ensure the operation has completed before accessing any associated data from the CPU.
Coherently accessible from the host in a given stream means that the memory neither has global visibility nor is it associated with the given stream.
Coherently accessible from the device in a given stream means that the memory either has global visibility or is associated with the given stream. 20. Lazy Loading  20.1.
 Lazy Loading delays loading of CUDA modules and kernels from program initalization closer to kernels execution.
If a program does not use every single kernel it has included, then some kernels will be loaded unneccesarily.
Most of the time, programs only use a small amount of kernels from libraries they include.
Thanks to Lazy Loading, programs are able to only load kernels they are actually going to use, saving time on initialization.
Firstly, CUDA Runtime will no longer load all modules during program initialization, with the exception of modules containing managed variables.
This optimization is only relevant to CUDA Runtime users, CUDA Driver users who use cuModuleLoad are unaffected.
The behavior for CUDA Driver users who use cuLibraryLoad to load module data into memory can be changed by setting the CUDA_MODULE_DATA_LOADING environment variable.
Secondly, loading a module ( cuModuleLoad*() family of functions) will not be loading kernels immediately, instead it will delay loading of a kernel until cuModuleGetFunction() is called.
There are certain exceptions here, some kernels have to be loaded during cuModuleLoad*() , such as kernels of which pointers are stored in global variables.
CUDA Runtime will only call cuModuleGetFunction() when a kernel is used/referenced for the first time.
Both of these optimizations are designed to be invisible to the user, assuming CUDA Programming Model is followed.
Upgrades to both might be necessary to utilize the feature. 20.2.1. Driver  Lazy Loading requires R515+ user-mode library, but it supports Forward Compatibility, meaning it can run on top of older kernel mode drivers.
Without R515+ user-mode library, Lazy Loading is not available in any shape or form, even if toolkit version is 11.7+. 20.2.2. Toolkit  Lazy Loading was introduced in CUDA 11.7, and received a significant upgrade in CUDA 11.8.
If your application uses CUDA Runtime, then in order to see benefits from Lazy Loading your application must use 11.7+ CUDA Runtime.
As CUDA Runtime is usually linked statically into programs and libraries, this means that you have to recompile your program with CUDA 11.7+ toolkit and use CUDA 11.7+ libraries.
Otherwise you will not see the benefits of Lazy Loading, even if your driver version supports it.
If only some of your libraries are 11.7+, you will only see benefits of Lazy Loading in those libraries.
Other libraries will still load everything eagerly. 20.2.3. Compiler  Lazy Loading does not require any compiler support.
Both SASS and PTX compiled with pre-11.7 compilers can be loaded with Lazy Loading enabled, and will see full benefits of the feature.
However, 11.7+ CUDA Runtime is still required, as described above. 20.3. Triggering loading of kernels in lazy mode  Loading kernels and variables happens automatically, without any need for explicit loading.
Simply launching a kernel or referencing a variable or a kernel will automatically load relevant modules and kernels.
However, if for any reason you wish to load a kernel without executing it or modifying it in any way, we recommend the following. 20.3.1. CUDA Driver API  Loading of kernels happens during cuModuleGetFunction() call.
This call is necessary even without Lazy Loading, as it is the only way to obtain a kernel handle.
However, you can also use this API to control with finer granularity when kernels are loaded. 20.3.2. CUDA Runtime API  CUDA Runtime API manages module management automatically, so we recommend simply using cudaFuncGetAttributes() to reference the kernel.
This will ensure that the kernel is loaded without changing the state. 20.4. Querying whether Lazy Loading is turned on  In order to check whether user enabled Lazy Loading, CUresult cuModuleGetLoadingMode ( CUmoduleLoadingMode* mode ) can be used.
#include "cuda.h" #include "assert.h" #include "iostream" int main () { CUmoduleLoadingMode mode ; assert ( CUDA_SUCCESS == cuInit ( 0 )); assert ( CUDA_SUCCESS == cuModuleGetLoadingMode ( & mode )); std :: cout << "CUDA Module Loading Mode is " << (( mode == CU_MODULE_LAZY_LOADING ) ? "lazy" : "eager" ) << std :: endl ; return 0 ; } 20.5.
Possible issues when adopting lazy loading  Lazy Loading is designed so that it should not require any modifications to applications to use it.
That said, there are some caveats, especially when applications are not fully compliant with CUDA Programming Model. 20.5.1. Concurrent execution  Loading kernels might require context synchronization.
Some programs incorrectly treat the possibility of concurrent execution of kernels as a guarantee.
In such cases, if program assumes that two kernels will be able to execute concurrently, and one of the kernels will not return without the other kernel executing, there is a possibility of a deadlock.
If this loading will require context synchronization, then we have a deadlock: kernel A is waiting for kernel B, but loading kernel B is stuck waiting for kernel A to finish to synchronize the context.
Such program is an anti-pattern, but if for any reason you want to keep it you can do the following: preload all kernels that you hope to execute concurrently prior to launching them run application with CUDA_MODULE_DATA_LOADING=EAGER to force loading data eagerly without forcing each function to load eagerly 20.5.2.
Allocators  Lazy Loading delays loading code from initialization phase of the program closer to execution phase.
to use it for its own allocator, then it might turn out that there will be no more memory left to load the kernels.
CUDA will need to allocate some memory to load each kernel, which usually happens at first launch time of each kernel.
If your application allocator greedily allocated everything, CUDA will fail to allocate memory.
Possible solutions: use cudaMallocAsync() instead of an allocator that allocates the entire VRAM on startup add some buffer to compensate for the delayed loading of kernels preload all kernels that will be used in the program before trying to initialize your allocator 20.5.3.
Autotuning  Some applications launch several kernels implementing the same functionality to determine which one is the fastest.
While it is overall advisable to run at least one warmup iteration, it becomes especially important with Lazy Loading.
Possible solutions: do at least one warmup interaction prior to measurement preload the benchmarked kernel prior to launching it 21.
Extended GPU Memory  The Extended GPU Memory (EGM) feature, utilizing the high-bandwidth NVLink-C2C, facilitates efficient access to all system memory by GPUs, in a single-node system.
EGM applies to integrated CPU-GPU NVIDIA systems by allowing physical memory allocation that can be accessed from any GPU thread within the setup.
EGM ensures that all GPUs can access its resources at the speed of either GPU-GPU NVLink or NVLink-C2C.
With EGM, GPU threads gain the capability to access all available memory resources, including CPU attached memory and HBM3, over the NVSwitch fabric. 21.1. Preliminaries  Before diving into API changes for EGM functionalities, we are going to cover currently supported topologies, identifier assignment, prerequisites for virtual memory management, and CUDA types for EGM.
21.1.1. EGM Platforms: System topology  Currently, EGM can be enabled in three platforms: (1) Single-Node, Single-GPU : Consists of an Arm-based CPU, CPU attached memory, and a GPU.
(2) Single-Node, Multi-GPU : Consists of fully connected four single-node, single-GPU platforms.
Note Using cgroups to limit available devices will block routing over EGM and cause performance issues.
Use CUDA_VISIBLE_DEVICES instead. 21.1.2.  NUMA (Non-Uniform Memory Access) is a memory architecture used in multi-processor computer systems such that the memory is divided into multiple nodes.
In such a system, NUMA divides the system into nodes and assigns a unique identifier ( numaID ) to every node.
Note that, this identifier is different from the ordinal of a device and it is associated with the closest host node.
In addition to the existing methods, the user can obtain the identifier of the host node ( numaID ) by calling cuDeviceGetAttribute with CU_DEVICE_ATTRIBUTE_HOST_NUMA_ID attribute type as follows: int numaId ; cuDeviceGetAttribute ( & numaId , CU_DEVICE_ATTRIBUTE_HOST_NUMA_ID , deviceOrdinal ); 21.1.3.
Allocators and EGM support  Mapping system memory as EGM does not cause any performance issues.
Currently, cuMemCreate and cudaMemPoolCreate allocators are supported with appropriate location type and NUMA identifiers. 21.1.4. Memory management extensions to current APIs  Currently, EGM memory can be mapped with Virtual Memory ( cuMemCreate ) or Stream Ordered Memory ( cudaMemPoolCreate ) allocators.
The user is responsible for allocating physical memory and mapping it to a virtual memory address space on all sockets.
Therefore we encourage the reader to see Chapter 3 Note We encourage readers to read CUDA Programming Guide’s Chapter 10 and Chapter 11 for a better understanding.
New CUDA property types have been added to APIs for allowing those approaches to understand allocation locations using NUMA-like node identifiers: CUDA Type Used with CU_MEM_LOCATION_TYPE_HOST_NUMA CUmemAllocationProp for cuMemCreate cudaMemLocationTypeHostNuma cudaMemPoolProps for cudaMemPoolCreate Note Please see CUDA Driver API and CUDA Runtime Data Types to find more about NUMA specific CUDA types. 21.2. Using the EGM Interface  21.2.1.
Single-Node, Single-GPU  Any of the existing CUDA host allocators as well as system allocated memory can be used to benefit from high-bandwidth C2C.
Note Refer to the tuning guide for more information about memory allocators and page sizes. 21.2.2. Single-Node, Multi-GPU  In a multi-GPU system, the user has to provide host information for the placement.
As we mentioned, a natural way to express that information would be by using NUMA node IDs and EGM follows this approach.
Therefore, using the cuDeviceGetAttribute function the user should be able to learn the closest NUMA node id.
Then the user can allocate and manage EGM memory using VMM (Virtual Memory Management) API or CUDA Memory Pool. 21.2.2.1. Using VMM APIs  The first step in memory allocation using Virtual Memory Management APIs is to create a physical memory chunk that will provide a backing for the allocation.
In EGM allocations the user has to explicitly provide CU_MEM_LOCATION_TYPE_HOST_NUMA as the location type and numaID as the location identifier.
The following code snippet shows allocating physical memory with cuMemCreate : CUmemAllocationProp prop {}; prop .
id = numaId ; size_t granularity = 0 ; cuMemGetAllocationGranularity ( & granularity , & prop , MEM_ALLOC_GRANULARITY_MINIMUM ); size_t padded_size = ROUND_UP ( size , granularity ); CUmemGenericAllocationHandle allocHandle ; cuMemCreate ( & allocHandle , padded_size , & prop , 0 ); After physical memory allocation, we have to reserve an address space and map it to a pointer.
These procedures do not have EGM-specific changes: CUdeviceptr dptr ; cuMemAddressReserve ( & dptr , padded_size , 0 , 0 , 0 ); cuMemMap ( dptr , padded_size , 0 , allocHandle , 0 ); Finally, the user has to explicitly protect mapped virtual address ranges.
Similar to the memory allocation, the user has to provide CU_MEM_LOCATION_TYPE_HOST_NUMA as the location type and numaId as the location identifier.
Following code snippet create an access descriptors for the host node and the GPU to give read and write access for the mapped memory to both of them: CUmemAccessDesc accessDesc [ 2 ]{{}}; accessDesc [ 0 ].
flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE ; cuMemSetAccess ( dptr , size , accessDesc , 2 ); 21.2.2.2.
Using CUDA Memory Pool  To define EGM, the user can create a memory pool on a node and give access to peers.
In this case, the user has to explicitly define cudaMemLocationTypeHostNuma as the location type and numaId as the location identifier.
The following code snippet shows creating a memory pool cudaMemPoolCreate : cudaSetDevice ( homeDevice ); cudaMemPoolProps props {}; props .
id = numaId ; cudaMemPoolCreate ( & memPool , & props ); Additionally, for direct connect peer access, it is also possible to use the existing peer access API, cudaMemPoolSetAccess .
An example for an accessingDevice is shown in the following code snippet: cudaMemAccessDesc desc {}; desc .
id = accessingDevice ; cudaMemPoolSetAccess ( memPool , & desc , 1 ); When the memory pool is created, and accesses are given, the user can set created memory pool to the residentDevice and start allocating memory using cudaMallocAsync : cudaDeviceSetMemPool ( residentDevice , memPool ); cudaMallocAsync ( & ptr , size , memPool , stream ); Note EGM is mapped with 2MB pages.
Therefore, users may encounter more TLB misses when accessing very large allocations. 21.2.3. Multi-Node, Single-GPU  Beyond memory allocation, remote peer access does not have EGM-specific modification and it follows CUDA inter process (IPC) protocol.
The user should allocate memory using cuMemCreate and again the user has to explicitly provide CU_MEM_LOCATION_TYPE_HOST_NUMA as the location type and numaID as the location identifier.
The following code snippet shows allocating physical memory on Node A: CUmemAllocationProp prop {}; prop .
id = numaId ; size_t granularity = 0 ; cuMemGetAllocationGranularity ( & granularity , & prop , MEM_ALLOC_GRANULARITY_MINIMUM ); size_t padded_size = ROUND_UP ( size , granularity ); size_t page_size = ...; assert ( padded_size % page_size == 0 ); CUmemGenericAllocationHandle allocHandle ; cuMemCreate ( & allocHandle , padded_size , & prop , 0 ); After creating allocation handle using cuMemCreate the user can export that handle to the other node, Node B, calling cuMemExportToShareableHandle : cuMemExportToShareableHandle ( & fabricHandle , allocHandle , CU_MEM_HANDLE_TYPE_FABRIC , 0 );   At this point, fabricHandle should be sent to Node B via TCP/IP.
On Node B, the handle can be imported using cuMemImportFromShareableHandle and treated as any other fabric handle   At this point, fabricHandle should be received from Node A via TCP/IP.
CUmemGenericAllocationHandle allocHandle ; cuMemImportFromShareableHandle ( & allocHandle , & fabricHandle , CU_MEM_HANDLE_TYPE_FABRIC ); When handle is imported at Node B, then the user can reserve an address space and map it locally in a regular fashion: size_t granularity = 0 ; cuMemGetAllocationGranularity ( & granularity , & prop , MEM_ALLOC_GRANULARITY_MINIMUM ); size_t padded_size = ROUND_UP ( size , granularity ); size_t page_size = ...; assert ( padded_size % page_size == 0 ); CUdeviceptr dptr ; cuMemAddressReserve ( & dptr , padded_size , 0 , 0 , 0 ); cuMemMap ( dptr , padded_size , 0 , allocHandle , 0 ); As the final step, the user should give appropriate accesses to each of the local GPUs at Node B.
An example code snippet that gives read and write access to eight local GPUs:   Give all 8 local GPUS access to exported EGM memory located on Node A.
flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE ; } cuMemSetAccess ( dptr , size , accessDesc , 8 ); 22.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 22.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 22.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates.
Recommendations for taking advantage of minor version compatibility in your application 16.
Preface v12.5 | PDF | Archive CUDA C++ Best Practices Guide The programming guide to using the CUDA Toolkit to obtain the best performance from NVIDIA GPUs.
 This Best Practices Guide is a manual to help developers obtain the best performance from NVIDIA ® CUDA ® GPUs.
It presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for CUDA-capable GPU architectures.
While the contents can be used as a reference manual, you should be aware that some topics are revisited in different contexts as various programming and configuration topics are explored.
As a result, it is recommended that first-time readers proceed through the guide sequentially.
This approach will greatly improve your understanding of effective programming practices and enable you to better use the guide for reference later.
 The discussions in this guide all use the C++ programming language, so you should be comfortable reading C++ code.
This guide refers to and relies on several other documents that you should have at your disposal for reference, all of which are available at no cost from the CUDA website https: docs.nvidia.com/cuda/ .
The following documents are especially important resources: CUDA Installation Guide CUDA C++ Programming Guide CUDA Toolkit Reference Manual In particular, the optimization section of this guide assumes that you have already successfully downloaded and installed the CUDA Toolkit (if not, please refer to the relevant CUDA Installation Guide for your platform) and that you have a basic familiarity with the CUDA C++ programming language and environment (if not, please refer to the CUDA C++ Programming Guide).
Assess, Parallelize, Optimize, Deploy  This guide introduces the Assess, Parallelize, Optimize, Deploy(APOD) design cycle for applications with the goal of helping application developers to rapidly identify the portions of their code that would most readily benefit from GPU acceleration, rapidly realize that benefit, and begin leveraging the resulting speedups in production as early as possible.
APOD is a cyclical process: initial speedups can be achieved, tested, and deployed with only minimal initial investment of time, at which point the cycle can begin again by identifying further optimization opportunities, seeing additional speedups, and then deploying the even faster versions of the application into production. 1.3.1. Assess  For an existing project, the first step is to assess the application to locate the parts of the code that are responsible for the bulk of the execution time.
Armed with this knowledge, the developer can evaluate these bottlenecks for parallelization and start to investigate GPU acceleration.
By understanding the end-user’s requirements and constraints and by applying Amdahl’s and Gustafson’s laws, the developer can determine the upper bound of performance improvement from acceleration of the identified portions of the application. 1.3.2. Parallelize  Having identified the hotspots and having done the basic exercises to set goals and expectations, the developer needs to parallelize the code.
Depending on the original code, this can be as simple as calling into an existing GPU-optimized library such as cuBLAS , cuFFT , or Thrust , or it could be as simple as adding a few preprocessor directives as hints to a parallelizing compiler.
On the other hand, some applications’ designs will require some amount of refactoring to expose their inherent parallelism.
As even CPU architectures will require exposing parallelism in order to improve or simply maintain the performance of sequential applications, the CUDA family of parallel programming languages (CUDA C++, CUDA Fortran, etc.) aims to make the expression of this parallelism as simple as possible, while simultaneously enabling operation on CUDA-capable GPUs designed for maximum parallel throughput. 1.3.3. Optimize  After each round of application parallelization is complete, the developer can move to optimizing the implementation to improve performance.
Since there are many possible optimizations that can be considered, having a good understanding of the needs of the application can help to make the process as smooth as possible.
However, as with APOD as a whole, program optimization is an iterative process (identify an opportunity for optimization, apply and test the optimization, verify the speedup achieved, and repeat), meaning that it is not necessary for a programmer to spend large amounts of time memorizing the bulk of all possible optimization strategies prior to seeing good speedups.
Optimizations can be applied at various levels, from overlapping data transfers with computation all the way down to fine-tuning floating-point operation sequences.
The available profiling tools are invaluable for guiding this process, as they can help suggest a next-best course of action for the developer’s optimization efforts and provide references into the relevant portions of the optimization section of this guide. 1.3.4. Deploy  Having completed the GPU acceleration of one or more components of the application it is possible to compare the outcome with the original expectation.
Recall that the initial assess step allowed the developer to determine an upper bound for the potential speedup attainable by accelerating given hotspots.
Before tackling other hotspots to improve the total speedup, the developer should consider taking the partially parallelized implementation and carry it through to production.
This is important for a number of reasons; for example, it allows the user to profit from their investment as early as possible (the speedup may be partial but is still valuable), and it minimizes risk for the developer and the user by providing an evolutionary rather than revolutionary set of changes to the application. 1.4. Recommendations and Best Practices  Throughout this guide, specific recommendations are made regarding the design and implementation of CUDA C++ code.
These recommendations are categorized by priority, which is a blend of the effect of the recommendation and its scope.
Actions that present substantial improvements for most CUDA applications have the highest priority, while small optimizations that affect only very specific situations are given a lower priority.
Before implementing lower priority recommendations, it is good practice to make sure all higher priority recommendations that are relevant have already been applied.
This approach will tend to provide the best results for the time invested and will avoid the trap of premature optimization.
The criteria of benefit and scope for establishing priority will vary depending on the nature of the program.
Regardless of this possibility, it is good practice to verify that no higher-priority recommendations have been overlooked before undertaking lower-priority items.
Production code should, however, systematically check the error code returned by each API call and check for failures in kernel launches by calling cudaGetLastError() . 1.5. Assessing Your Application  From supercomputers to mobile phones, modern processors increasingly rely on parallelism to provide performance.
The core computational unit, which includes control, arithmetic, registers and typically some cache, is replicated some number of times and connected to memory via a network.
As a result, all modern processors require parallel code in order to achieve good utilization of their computational power.
While processors are evolving to expose more fine-grained parallelism to the programmer, many existing applications have evolved either as serial codes or as coarse-grained parallel codes (for example, where the data is decomposed into regions processed in parallel, with sub-regions shared using MPI).
In order to profit from any modern processor architecture, GPUs included, the first steps are to assess the application to identify the hotspots, determine whether they can be parallelized, and understand the relevant workloads both now and in the future. 2. Heterogeneous Computing  CUDA programming involves running code on two different platforms concurrently: a host system with one or more CPUs and one or more CUDA-enabled NVIDIA GPU devices .
While NVIDIA GPUs are frequently associated with graphics, they are also powerful arithmetic engines capable of running thousands of lightweight threads in parallel.
This capability makes them well suited to computations that can leverage parallel execution.
However, the device is based on a distinctly different design from the host system, and it’s important to understand those differences and how they determine the performance of CUDA applications in order to use CUDA effectively. 2.1. Differences between Host and Device  The primary differences are in threading model and in separate physical memories: Threading resources Execution pipelines on host systems can support a limited number of concurrent threads.
For example, servers that have two 32 core processors can run only 64 threads concurrently (or small multiple of that if the CPUs support simultaneous multithreading).
By comparison, the smallest executable unit of parallelism on a CUDA device comprises 32 threads (termed a warp of threads).
Modern NVIDIA GPUs can support up to 2048 active threads concurrently per multiprocessor (see Features and Specifications of the CUDA C++ Programming Guide) On GPUs with 80 multiprocessors, this leads to more than 160,000 concurrently active threads.
The operating system must swap threads on and off CPU execution channels to provide multithreading capability.
In a typical system, thousands of threads are queued up for work (in warps of 32 threads each).
Because separate registers are allocated to all active threads, no swapping of registers or other state need occur when switching among GPU threads.
In short, CPU cores are designed to minimize latency for a small number of threads at a time each, whereas GPUs are designed to handle a large number of concurrent, lightweight threads in order to maximize throughput .
RAM The host system and the device each have their own distinct attached physical memories 1 .
As the host and device memories are separated, items in the host memory must occasionally be communicated between device memory and host memory as described in What Runs on a CUDA-Enabled Device? .
These are the primary hardware differences between CPU hosts and GPU devices with respect to parallel programming.
Applications composed with these differences in mind can treat the host and device together as a cohesive heterogeneous system wherein each processing unit is leveraged to do the kind of work it does best: sequential work on the host and parallel work on the device. 2.2.  The following issues should be considered when determining what parts of an application to run on the device: The device is ideally suited for computations that can be run on numerous data elements simultaneously in parallel.
This typically involves arithmetic on large data sets (such as matrices) where the same operation can be performed across thousands, if not millions, of elements at the same time.
This is a requirement for good performance on CUDA: the software must use a large number (generally thousands or tens of thousands) of concurrent threads.
The support for running numerous threads in parallel derives from CUDA’s use of a lightweight threading model described above.
(See Data Transfer Between Host and Device .) This cost has several ramifications: The complexity of operations should justify the cost of moving data to and from the device.
Code that transfers data for brief use by a small number of threads will see little or no performance benefit.
For example, transferring two matrices to the device to perform a matrix addition and then transferring the results back to the host will not realize much performance benefit.
For the preceding procedure, assuming matrices of size NxN, there are N 2 operations (additions) and 3N 2 elements transferred, so the ratio of operations to elements transferred is 1:3 or O(1).
For example, a matrix multiplication of the same matrices requires N 3 operations (multiply-add), so the ratio of operations to elements transferred is O(N), in which case the larger the matrix the greater the performance benefit.
The types of operations are an additional factor, as additions have different complexity profiles than, for example, trigonometric functions.
It is important to include the overhead of transferring data to and from the device in determining whether operations should be performed on the host or on the device.
Because transfers should be minimized, programs that run multiple kernels on the same data should favor leaving the data on the device between kernel calls, rather than transferring intermediate results to the host and then sending them back to the device for subsequent calculations.
So, in the previous example, had the two matrices to be added already been on the device as a result of some previous calculation, or if the results of the addition would be used in some subsequent calculation, the matrix addition should be performed locally on the device.
This approach should be used even if one of the steps in a sequence of calculations could be performed faster on the host.
Even a relatively slow kernel may be advantageous if it avoids one or more transfers between host and device memory.
Data Transfer Between Host and Device provides further details, including the measurements of bandwidth between the host and the device versus within the device proper.
For best performance, there should be some coherence in memory access by adjacent threads running on the device.
Certain memory access patterns enable the hardware to coalesce groups of reads or writes of multiple data items into one operation.
Data that cannot be laid out so as to enable coalescing , or that doesn’t have enough locality to use the L1 or texture caches effectively, will tend to see lesser speedups when used in computations on GPUs.
In general, they should be avoided, because compared to peak capabilities any architecture processes these memory access patterns at a low efficiency.
However, compared to cache based architectures, like CPUs, latency hiding architectures, like GPUs, tend to cope better with completely random memory access patterns.
1 On Systems on a Chip with integrated GPUs, such as NVIDIA® Tegra®, host and device memory are physically the same, but there is still a logical distinction between host and device memory.
Profile  Many codes accomplish a significant portion of the work with a relatively small amount of code.
Using a profiler, the developer can identify such hotspots and start to compile a list of candidates for parallelization. 3.1.1. Creating the Profile  There are many possible approaches to profiling the code, but in all cases the objective is the same: to identify the function or functions in which the application is spending most of its execution time.
Note High Priority: To maximize developer productivity, profile the application to determine hotspots and bottlenecks.
The most important consideration with any profiling activity is to ensure that the workload is realistic - i.e., that information gained from the test and decisions based upon that information are relevant to real data.
Using unrealistic workloads can lead to sub-optimal results and wasted effort both by causing developers to optimize for unrealistic problem sizes and by causing developers to concentrate on the wrong functions.
The following example is based on gprof , which is an open-source profiler for Linux platforms from the GNU Binutils collection.
$ gcc -O2 -g -pg myprog.c $ gprof ./a.out > profile.txt Each sample counts as 0.01 seconds.
% cumulative self self total time seconds seconds calls ms/call ms/call name 33.34 0.02 0.02 7208 0.00 0.00 genTimeStep 16.67 0.03 0.01 240 0.04 0.12 calcStats 16.67 0.04 0.01 8 1.25 1.25 calcSummaryData 16.67 0.05 0.01 7 1.43 1.43 write 16.67 0.06 0.01 mcount 0.00 0.06 0.00 236 0.00 0.00 tzset 0.00 0.06 0.00 192 0.00 0.00 tolower 0.00 0.06 0.00 47 0.00 0.00 strlen 0.00 0.06 0.00 45 0.00 0.00 strchr 0.00 0.06 0.00 1 0.00 50.00 main 0.00 0.06 0.00 1 0.00 0.00 memcpy 0.00 0.06 0.00 1 0.00 10.11 print 0.00 0.06 0.00 1 0.00 0.00 profil 0.00 0.06 0.00 1 0.00 50.00 report 3.1.2.
Identifying Hotspots  In the example above, we can clearly see that the function genTimeStep() takes one-third of the total running time of the application.
Understanding Scaling discusses the potential benefit we might expect from such parallelization.
It is worth noting that several of the other functions in the above example also take up a significant portion of the overall running time, such as calcStats() and calcSummaryData() .
However, since APOD is a cyclical process, we might opt to parallelize these functions in a subsequent APOD pass, thereby limiting the scope of our work in any given pass to a smaller set of incremental changes. 3.1.3. Understanding Scaling  The amount of performance benefit an application will realize by running on CUDA depends entirely on the extent to which it can be parallelized.
Code that cannot be sufficiently parallelized should run on the host, unless doing so would result in excessive transfers between the host and the device.
Note High Priority: To get the maximum benefit from CUDA, focus first on finding ways to parallelize sequential code.
By understanding how applications can scale it is possible to set expectations and plan an incremental parallelization strategy.
Strong Scaling and Amdahl’s Law describes strong scaling, which allows us to set an upper bound for the speedup with a fixed problem size.
Weak Scaling and Gustafson’s Law describes weak scaling, where the speedup is attained by growing the problem size.
In many applications, a combination of strong and weak scaling is desirable. 3.1.3.1. Strong Scaling and Amdahl’s Law  Strong scaling is a measure of how, for a fixed overall problem size, the time to solution decreases as more processors are added to a system.
An application that exhibits linear strong scaling has a speedup equal to the number of processors used.
Strong scaling is usually equated with Amdahl’s Law, which specifies the maximum speedup that can be expected by parallelizing portions of a serial program.
Essentially, it states that the maximum speedup S of a program is: \(S = \frac{1}{(1 - P) + \frac{P}{N}}\) Here P is the fraction of the total serial execution time taken by the portion of code that can be parallelized and N is the number of processors over which the parallel portion of the code runs.
The larger N is(that is, the greater the number of processors), the smaller the P/N fraction.
It can be simpler to view N as a very large number, which essentially transforms the equation into \(S = 1/(1 - P)\) .
Now, if 3/4 of the running time of a sequential program is parallelized, the maximum speedup over serial code is 1 / (1 - 3/4) = 4.
In reality, most applications do not exhibit perfectly linear strong scaling, even if they do exhibit some degree of strong scaling.
For most purposes, the key point is that the larger the parallelizable portion P is, the greater the potential speedup.
Conversely, if P is a small number (meaning that the application is not substantially parallelizable), increasing the number of processors N does little to improve performance.
Therefore, to get the largest speedup for a fixed problem size, it is worthwhile to spend effort on increasing P , maximizing the amount of code that can be parallelized. 3.1.3.2. Weak Scaling and Gustafson’s Law  Weak scaling is a measure of how the time to solution changes as more processors are added to a system with a fixed problem size per processor ; i.e., where the overall problem size increases as the number of processors is increased.
Weak scaling is often equated with Gustafson’s Law, which states that in practice, the problem size scales with the number of processors.
Because of this, the maximum speedup S of a program is: \(S = N + (1 - P)(1 - N)\) Here P is the fraction of the total serial execution time taken by the portion of code that can be parallelized and N is the number of processors over which the parallel portion of the code runs.
Another way of looking at Gustafson’s Law is that it is not the problem size that remains constant as we scale up the system but rather the execution time.
Note that Gustafson’s Law assumes that the ratio of serial to parallel execution remains constant, reflecting additional cost in setting up and handling the larger problem. 3.1.3.3. Applying Strong and Weak Scaling  Understanding which type of scaling is most applicable to an application is an important part of estimating speedup.
For some applications the problem size will remain constant and hence only strong scaling is applicable.
An example would be modeling how two molecules interact with each other, where the molecule sizes are fixed.
Examples include modeling fluids or structures as meshes or grids and some Monte Carlo simulations, where increasing the problem size provides increased accuracy.
Having understood the application profile, the developer should understand how the problem size would change if the computational performance changes and then apply either Amdahl’s or Gustafson’s Law to determine an upper bound for the speedup. 4. Parallelizing Your Application  Having identified the hotspots and having done the basic exercises to set goals and expectations, the developer needs to parallelize the code.
As even CPU architectures require exposing this parallelism in order to improve or simply maintain the performance of sequential applications, the CUDA family of parallel programming languages (CUDA C++, CUDA Fortran, etc.) 5.
While the details of how to apply these strategies to a particular application is a complex and problem-specific topic, the general themes listed here apply regardless of whether we are parallelizing code to run on for multicore CPUs or for use on CUDA GPUs. 5.1. Parallel Libraries  The most straightforward approach to parallelizing an application is to leverage existing libraries that take advantage of parallel architectures on our behalf.
The CUDA Toolkit includes a number of such libraries that have been fine-tuned for NVIDIA CUDA GPUs, such as cuBLAS , cuFFT , and so on.
The key here is that libraries are most useful when they match well with the needs of the application.
Applications already using other BLAS libraries can often quite easily switch to cuBLAS , for example, whereas applications that do little to no linear algebra will have little use for cuBLAS .
The same goes for other CUDA Toolkit libraries: cuFFT has an interface similar to that of FFTW , etc.
Also of note is the Thrust library, which is a parallel C++ template library similar to the C++ Standard Template Library.
Thrust provides a rich collection of data parallel primitives such as scan, sort, and reduce, which can be composed together to implement complex algorithms with concise, readable source code.
By describing your computation in terms of these high-level abstractions you provide Thrust with the freedom to select the most efficient implementation automatically.
As a result, Thrust can be utilized in rapid prototyping of CUDA applications, where programmer productivity matters most, as well as in production, where robustness and absolute performance are crucial. 5.2. Parallelizing Compilers  Another common approach to parallelization of sequential codes is to make use of parallelizing compilers.
Often this means the use of directives-based approaches, where the programmer uses a pragma or other similar notation to provide hints to the compiler about where parallelism can be found without needing to modify or adapt the underlying code itself.
By exposing parallelism to the compiler, directives allow the compiler to do the detailed work of mapping the computation onto the parallel architecture.
The OpenACC standard provides a set of compiler directives to specify loops and regions of code in standard C, C++ and Fortran that should be offloaded from a host CPU to an attached accelerator such as a CUDA GPU.
The details of managing the accelerator device are handled implicitly by an OpenACC-enabled compiler and runtime.
See http: www.openacc.org/ for details. 5.3. Coding to Expose Parallelism  For applications that need additional functionality or performance beyond what existing parallel libraries or parallelizing compilers can provide, parallel programming languages such as CUDA C++ that integrate seamlessly with existing sequential code are essential.
Once we have located a hotspot in our application’s profile assessment and determined that custom code is the best approach, we can use CUDA C++ to expose the parallelism in that portion of our code as a CUDA kernel.
We can then launch this kernel onto the GPU and retrieve the results without requiring major rewrites to the rest of our application.
This approach is most straightforward when the majority of the total running time of our application is spent in a few relatively isolated portions of the code.
More difficult to parallelize are applications with a very flat profile - i.e., applications where the time spent is spread out relatively evenly across a wide portion of the code base.
For the latter variety of application, some degree of code refactoring to expose the inherent parallelism in the application might be necessary, but keep in mind that this refactoring work will tend to benefit all future architectures, CPU and GPU alike, so it is well worth the effort should it become necessary. 6. Getting the Right Answer  Obtaining the right answer is clearly the principal goal of all computation.
On parallel systems, it is possible to run into difficulties not typically found in traditional serial-oriented programming.
These include threading issues, unexpected values due to the way floating-point values are computed, and challenges arising from differences in the way CPU and GPU processors operate.
This chapter examines issues that can affect the correctness of returned data and points to appropriate solutions. 6.1. Verification  6.1.1.
Reference Comparison  A key aspect of correctness verification for modifications to any existing program is to establish some mechanism whereby previous known-good reference outputs from representative inputs can be compared to new results.
After each change is made, ensure that the results match using whatever criteria apply to the particular algorithm.
Some will expect bitwise identical results, which is not always possible, especially where floating-point arithmetic is concerned; see Numerical Accuracy and Precision regarding numerical accuracy.
For other algorithms, implementations may be considered correct if they match the reference within some small epsilon.
Note that the process used for validating numerical results can easily be extended to validate performance results as well.
We want to ensure that each change we make is correct and that it improves performance (and by how much).
Checking these things frequently as an integral part of our cyclical APOD process will help ensure that we achieve the desired results as rapidly as possible. 6.1.2. Unit Testing  A useful counterpart to the reference comparisons described above is to structure the code itself in such a way that is readily verifiable at the unit level.
For example, we can write our CUDA kernels as a collection of many short __device__ functions rather than one large monolithic __global__ function; each device function can be tested independently before hooking them all together.
For example, many kernels have complex addressing logic for accessing memory in addition to their actual computation.
If we validate our addressing logic separately prior to introducing the bulk of the computation, then this will simplify any later debugging efforts.
(Note that the CUDA compiler considers any device code that does not contribute to a write to global memory as dead code subject to elimination, so we must at least write something out to global memory as a result of our addressing logic in order to successfully apply this strategy.) Going a step further, if most functions are defined as __host__ __device__ rather than just __device__ functions, then these functions can be tested on both the CPU and the GPU, thereby increasing our confidence that the function is correct and that there will not be any unexpected differences in the results.
If there are differences, then those differences will be seen early and can be understood in the context of a simple function.
As a useful side effect, this strategy will allow us a means to reduce code duplication should we wish to include both CPU and GPU execution paths in our application: if the bulk of the work of our CUDA kernels is done in __host__ __device__ functions, we can easily call those functions from both the host code and the device code without duplication. 6.2. Debugging  CUDA-GDB is a port of the GNU Debugger that runs on Linux and Mac; see: https: developer.nvidia.com/cuda-gdb .
The NVIDIA Nsight Visual Studio Edition for Microsoft Windows 7, Windows HPC Server 2008, Windows 8.1, and Windows 10 is available as a free plugin for Microsoft Visual Studio; see: https: developer.nvidia.com/nsight-visual-studio-edition .
Several third-party debuggers support CUDA debugging as well; see: https: developer.nvidia.com/debugging-solutions for more details. 6.3. Numerical Accuracy and Precision  Incorrect or unexpected results arise principally from issues of floating-point accuracy due to the way floating-point values are computed and stored.
Other peculiarities of floating-point arithmetic are presented in Features and Technical Specifications of the CUDA C++ Programming Guide as well as in a whitepaper and accompanying webinar on floating-point precision and performance available from https: developer.nvidia.com/content/precision-performance-floating-point-and-ieee-754-compliance-nvidia-gpus . 6.3.1. Double Precision  Devices of compute capability 1.3 and higher provide native support for double-precision floating-point values (that is, values 64 bits wide).
Results obtained using double-precision arithmetic will frequently differ from the same operation performed via single-precision arithmetic due to the greater precision of the former and due to rounding issues.
Therefore, it is important to be sure to compare values of like precision and to express the results within a certain tolerance rather than expecting them to be exact. 6.3.2. Floating Point Math Is not Associative  Each floating-point arithmetic operation involves a certain amount of rounding.
If A, B, and C are floating-point values, (A+B)+C is not guaranteed to equal A+(B+C) as it is in symbolic math.
When you parallelize computations, you potentially change the order of operations and therefore the parallel results might not match sequential results.
This limitation is not specific to CUDA, but an inherent part of parallel computation on floating-point values. 6.3.3. IEEE 754 Compliance  All CUDA compute devices follow the IEEE 754 standard for binary floating-point representation, with some small exceptions.
These exceptions, which are detailed in Features and Technical Specifications of the CUDA C++ Programming Guide, can lead to results that differ from IEEE 754 values computed on the host system.
One of the key differences is the fused multiply-add (FMA) instruction, which combines multiply-add operations into a single instruction execution.
Its result will often differ slightly from results obtained by doing the two operations separately. 6.3.4. x86 80-bit Computations  x86 processors can use an 80-bit double extended precision math when performing floating-point calculations.
The results of these calculations can frequently differ from pure 64-bit operations performed on the CUDA device.
To get a closer match between values, set the x86 host processor to use regular double or single precision (64 bits and 32 bits, respectively).
This is done with the FLDCW x86 assembly instruction or the equivalent operating system API. 7. Optimizing CUDA Applications  After each round of application parallelization is complete, the developer can move to optimizing the implementation to improve performance.
8. Performance Metrics  When attempting to optimize CUDA code, it pays to know how to measure performance accurately and to understand the role that bandwidth plays in performance measurement.
This chapter discusses how to correctly measure performance using CPU timers and CUDA events.
It then explores how bandwidth affects performance metrics and how to mitigate some of the challenges it poses. 8.1. Timing  CUDA calls and kernel executions can be timed using either CPU or GPU timers.
This section examines the functionality, advantages, and pitfalls of both approaches. 8.1.1. Using CPU Timers  Any CPU timer can be used to measure the elapsed time of a CUDA call or kernel execution.
The details of various CPU timing approaches are outside the scope of this document, but developers should always be aware of the resolution their timing calls provide.
When using CPU timers, it is critical to remember that many CUDA API functions are asynchronous; that is, they return control back to the calling CPU thread prior to completing their work.
All kernel launches are asynchronous, as are memory-copy functions with the Async suffix on their names.
Therefore, to accurately measure the elapsed time for a particular call or sequence of CUDA calls, it is necessary to synchronize the CPU thread with the GPU by calling cudaDeviceSynchronize() immediately before starting and stopping the CPU timer.
cudaDeviceSynchronize() blocks the calling CPU thread until all CUDA calls previously issued by the thread are completed.
Although it is also possible to synchronize the CPU thread with a particular stream or event on the GPU, these synchronization functions are not suitable for timing code in streams other than the default stream.
cudaStreamSynchronize() blocks the CPU thread until all CUDA calls previously issued into the given stream have completed.
cudaEventSynchronize() blocks until a given event in a particular stream has been recorded by the GPU.
Because the driver may interleave execution of CUDA calls from other non-default streams, calls in other streams may be included in the timing.
Because the default stream, stream 0, exhibits serializing behavior for work on the device (an operation in the default stream can begin only after all preceding calls in any stream have completed; and no subsequent operation in any stream can begin until it finishes), these functions can be used reliably for timing in the default stream.
Be aware that CPU-to-GPU synchronization points such as those mentioned in this section imply a stall in the GPU’s processing pipeline and should thus be used sparingly to minimize their performance impact. 8.1.2. Using CUDA GPU Timers  The CUDA event API provides calls that create and destroy events, record events (including a timestamp), and convert timestamp differences into a floating-point value in milliseconds.
How to time code using CUDA events cudaEvent_t start , stop ; float time ; cudaEventCreate ( & start ); cudaEventCreate ( & stop ); cudaEventRecord ( start , 0 ); kernel >> ( d_odata , d_idata , size_x , size_y , NUM_REPS ); cudaEventRecord ( stop , 0 ); cudaEventSynchronize ( stop ); cudaEventElapsedTime ( & time , start , stop ); cudaEventDestroy ( start ); cudaEventDestroy ( stop ); Here cudaEventRecord() is used to place the start and stop events into the default stream, stream 0.
The device will record a timestamp for the event when it reaches that event in the stream.
The cudaEventElapsedTime() function returns the time elapsed between the recording of the start and stop events.
This value is expressed in milliseconds and has a resolution of approximately half a microsecond.
Like the other calls in this listing, their specific operation, parameters, and return values are described in the CUDA Toolkit Reference Manual .
Note that the timings are measured on the GPU clock, so the timing resolution is operating-system-independent. 8.2. Bandwidth  Bandwidth - the rate at which data can be transferred - is one of the most important gating factors for performance.
As described in Memory Optimizations of this guide, bandwidth can be dramatically affected by the choice of memory in which data is stored, how the data is laid out and the order in which it is accessed, as well as other factors.
To measure performance accurately, it is useful to calculate theoretical and effective bandwidth.
When the latter is much lower than the former, design or implementation details are likely to reduce bandwidth, and it should be the primary goal of subsequent optimization efforts to increase it.
Note High Priority: Use the effective bandwidth of your computation as a metric when measuring performance and optimization benefits. 8.2.1. Theoretical Bandwidth Calculation  Theoretical bandwidth can be calculated using hardware specifications available in the product literature.
For example, the NVIDIA Tesla V100 uses HBM2 (double data rate) RAM with a memory clock rate of 877 MHz and a 4096-bit-wide memory interface.
Using these data items, the peak theoretical memory bandwidth of the NVIDIA Tesla V100 is 898 GB/s: \(\left.
\times (4096/8) \times 2  ight) \div 10^{9} = 898\text{GB/s}\) In this calculation, the memory clock rate is converted in to Hz, multiplied by the interface width (divided by 8, to convert bits to bytes) and multiplied by 2 due to the double data rate.
It is important to use the same divisor when calculating theoretical and effective bandwidth so that the comparison is valid.
Note On GPUs with GDDR memory with ECC enabled the available DRAM is reduced by 6.25% to allow for the storage of ECC bits.
Fetching ECC bits for each memory transaction also reduced the effective bandwidth by approximately 20% compared to the same GPU with ECC disabled, though the exact impact of ECC on bandwidth can be higher and depends on the memory access pattern.
HBM2 memories, on the other hand, provide dedicated ECC resources, allowing overhead-free ECC protection. 2 8.2.2. Effective Bandwidth Calculation  Effective bandwidth is calculated by timing specific program activities and by knowing how data is accessed by the program.
To do so, use this equation: \(\text{Effective\ bandwidth} = \left( {\left( B_{r} + B_{w}  ight) \div 10^{9}}  ight) \div \text{time}\) Here, the effective bandwidth is in units of GB/s, B r is the number of bytes read per kernel, B w is the number of bytes written per kernel, and time is given in seconds.
For example, to compute the effective bandwidth of a 2048 x 2048 matrix copy, the following formula could be used: \(\text{Effective\ bandwidth} = \left( {\left( 2048^{2} \times 4 \times 2  ight) \div 10^{9}}  ight) \div \text{time}\) The number of elements is multiplied by the size of each element (4 bytes for a float), multiplied by 2 (because of the read and write), divided by 10 9 (or 1,024 3 ) to obtain GB of memory transferred.
This number is divided by the time in seconds to obtain GB/s. 8.2.3. Throughput Reported by Visual Profiler  For devices with compute capability of 2.0 or greater, the Visual Profiler can be used to collect several different memory throughput measures.
The following throughput metrics can be displayed in the Details or Detail Graphs view: Requested Global Load Throughput Requested Global Store Throughput Global Load Throughput Global Store Throughput DRAM Read Throughput DRAM Write Throughput The Requested Global Load Throughput and Requested Global Store Throughput values indicate the global memory throughput requested by the kernel and therefore correspond to the effective bandwidth obtained by the calculation shown under Effective Bandwidth Calculation .
Because the minimum memory transaction size is larger than most word sizes, the actual memory throughput required for a kernel can include the transfer of data not used by the kernel.
For global memory accesses, this actual throughput is reported by the Global Load Throughput and Global Store Throughput values.
The actual memory throughput shows how close the code is to the hardware limit, and a comparison of the effective or requested bandwidth to the actual bandwidth presents a good estimate of how much bandwidth is wasted by suboptimal coalescing of memory accesses (see Coalesced Access to Global Memory ).
For global memory accesses, this comparison of requested memory bandwidth to actual memory bandwidth is reported by the Global Memory Load Efficiency and Global Memory Store Efficiency metrics.
2 As an exception, scattered writes to HBM2 see some overhead from ECC but much less than the overhead with similar access patterns on ECC-protected GDDR5 memory. 9. Memory Optimizations  Memory optimizations are the most important area for performance.
Bandwidth is best served by using as much fast memory and as little slow-access memory as possible.
This chapter discusses the various kinds of memory on the host and device and how best to set up data items to use the memory effectively. 9.1. Data Transfer Between Host and Device  The peak theoretical bandwidth between the device memory and the GPU is much higher (898 GB/s on the NVIDIA Tesla V100, for example) than the peak theoretical bandwidth between host memory and device memory (16 GB/s on the PCIe x16 Gen3).
Hence, for best overall application performance, it is important to minimize data transfer between the host and the device, even if that means running kernels on the GPU that do not demonstrate any speedup compared with running them on the host CPU.
Note High Priority: Minimize data transfer between the host and the device, even if it means running some kernels on the device that do not show performance gains when compared with running them on the host CPU.
Intermediate data structures should be created in device memory, operated on by the device, and destroyed without ever being mapped by the host or copied to host memory.
Also, because of the overhead associated with each transfer, batching many small transfers into one larger transfer performs significantly better than making each transfer separately, even if doing so requires packing non-contiguous regions of memory into a contiguous buffer and then unpacking after the transfer.
Finally, higher bandwidth between the host and the device is achieved when using page-locked (or pinned ) memory, as discussed in the CUDA C++ Programming Guide and the Pinned Memory section of this document. 9.1.1. Pinned Memory  Page-locked or pinned memory transfers attain the highest bandwidth between the host and the device.
On PCIe x16 Gen3 cards, for example, pinned memory can attain roughly 12 GB/s transfer rates.
The bandwidthTest CUDA Sample shows how to use these functions as well as how to measure memory transfer performance.
For regions of system memory that have already been pre-allocated, cudaHostRegister() can be used to pin the memory on-the-fly without the need to allocate a separate buffer and copy the data into it.
Excessive use can reduce overall system performance because pinned memory is a scarce resource, but how much is too much is difficult to know in advance.
Furthermore, the pinning of system memory is a heavyweight operation compared to most normal system memory allocations, so as with all optimizations, test the application and the systems it runs on for optimal performance parameters. 9.1.2. Asynchronous and Overlapping Transfers with Computation  Data transfers between the host and the device using cudaMemcpy() are blocking transfers; that is, control is returned to the host thread only after the data transfer is complete.
The cudaMemcpyAsync() function is a non-blocking variant of cudaMemcpy() in which control is returned immediately to the host thread.
In contrast with cudaMemcpy() , the asynchronous transfer version requires pinned host memory (see Pinned Memory ), and it contains an additional argument, a stream ID.
Operations in different streams can be interleaved and in some cases overlapped - a property that can be used to hide data transfers between the host and the device.
Asynchronous transfers enable overlap of data transfers with computation in two different ways.
On all CUDA-enabled devices, it is possible to overlap host computation with asynchronous data transfers and with device computations.
For example, Overlapping computation and data transfers demonstrates how host computation in the routine cpuFunction() is performed while data is transferred to the device and a kernel using the device is executed.
Overlapping computation and data transfers cudaMemcpyAsync ( a_d , a_h , size , cudaMemcpyHostToDevice , 0 ); kernel >> ( a_d ); cpuFunction (); The last argument to the cudaMemcpyAsync() function is the stream ID, which in this case uses the default stream, stream 0.
The kernel also uses the default stream, and it will not begin execution until the memory copy completes; therefore, no explicit synchronization is needed.
Because the memory copy and the kernel both return control to the host immediately, the host function cpuFunction() overlaps their execution.
In Overlapping computation and data transfers , the memory copy and kernel execution occur sequentially.
On devices that are capable of concurrent copy and compute, it is possible to overlap kernel execution on the device with data transfers between the host and the device.
Whether a device has this capability is indicated by the asyncEngineCount field of the cudaDeviceProp structure (or listed in the output of the deviceQuery CUDA Sample).
On devices that have this capability, the overlap once again requires pinned host memory, and, in addition, the data transfer and kernel must use different, non-default streams (streams with non-zero stream IDs).
Non-default streams are required for this overlap because memory copy, memory set functions, and kernel calls that use the default stream begin only after all preceding calls on the device (in any stream) have completed, and no operation on the device (in any stream) commences until they are finished.
Concurrent copy and execute cudaStreamCreate ( & stream1 ); cudaStreamCreate ( & stream2 ); cudaMemcpyAsync ( a_d , a_h , size , cudaMemcpyHostToDevice , stream1 ); kernel >> ( otherData_d ); In this code, two streams are created and used in the data transfer and kernel executions as specified in the last arguments of the cudaMemcpyAsync call and the kernel’s execution configuration.
Concurrent copy and execute demonstrates how to overlap kernel execution with asynchronous data transfer.
This technique could be used when the data dependency is such that the data can be broken into chunks and transferred in multiple stages, launching multiple kernels to operate on each chunk as it arrives.
The first segment shows the reference sequential implementation, which transfers and operates on an array of N floats (where N is assumed to be evenly divisible by nThreads).
Sequential copy and execute cudaMemcpy ( a_d , a_h , N * sizeof ( float ), dir ); kernel >> ( a_d ); Staged concurrent copy and execute shows how the transfer and kernel execution can be broken up into nStreams stages.
Staged concurrent copy and execute size = N * sizeof ( float ) / nStreams ; for ( i = 0 ; i >> ( a_d + offset ); } (In Staged concurrent copy and execute , it is assumed that N is evenly divisible by nThreads*nStreams .) Because execution within a stream occurs sequentially, none of the kernels will launch until the data transfers in their respective streams complete.
GPUs with a single copy engine can perform one asynchronous data transfer and execute kernels whereas GPUs with two copy engines can simultaneously perform one asynchronous data transfer from the host to the device, one asynchronous data transfer from the device to the host, and execute kernels.
The number of copy engines on a GPU is given by the asyncEngineCount field of the cudaDeviceProp structure, which is also listed in the output of the deviceQuery CUDA Sample.
(It should be mentioned that it is not possible to overlap a blocking transfer with an asynchronous transfer, because the blocking transfer occurs in the default stream, so it will not begin until all previous CUDA calls complete.
It will not allow any other CUDA call to begin until it has completed.) A diagram depicting the timeline of execution for the two code segments is shown in Figure 1 , and nStreams is equal to 4 for Staged concurrent copy and execute in the bottom half of the figure.
Timeline comparison for copy and kernel execution  Top Sequential Bottom Concurrent For this example, it is assumed that the data transfer and kernel execution times are comparable.
In such cases, and when the execution time ( tE ) exceeds the transfer time ( tT ), a rough estimate for the overall time is tE + tT/nStreams for the staged version versus tE + tT for the sequential version.
If the transfer time exceeds the execution time, a rough estimate for the overall time is tT + tE/nStreams . 9.1.3. Zero Copy  Zero copy is a feature that was added in version 2.2 of the CUDA Toolkit.
On integrated GPUs (i.e., GPUs with the integrated field of the CUDA device properties structure set to 1), mapped pinned memory is always a performance gain because it avoids superfluous copies as integrated GPU and CPU memory are physically the same.
Because the data is not cached on the GPU, mapped pinned memory should be read or written only once, and the global loads and stores that read and write the memory should be coalesced.
Zero copy can be used in place of streams because kernel-originated data transfers automatically overlap kernel execution without the overhead of setting up and determining the optimal number of streams.
Note Low Priority: Use zero-copy operations on integrated GPUs for CUDA Toolkit version 2.2 and later.
canMapHostMemory ) exit ( 0 ); cudaSetDeviceFlags ( cudaDeviceMapHost ); cudaHostAlloc ( & a_h , nBytes , cudaHostAllocMapped ); cudaHostGetDevicePointer ( & a_map , a_h , 0 ); kernel >> ( a_map ); In this code, the canMapHostMemory field of the structure returned by cudaGetDeviceProperties() is used to check that the device supports mapping host memory to the device’s address space.
Page-locked memory mapping is enabled by calling cudaSetDeviceFlags() with cudaDeviceMapHost .
Note that cudaSetDeviceFlags() must be called prior to setting a device or making a CUDA call that requires state (that is, essentially, before a context is created).
Page-locked mapped host memory is allocated using cudaHostAlloc() , and the pointer to the mapped device address space is obtained via the function cudaHostGetDevicePointer() .
In the code in Zero-copy host code , kernel() can reference the mapped pinned host memory using the pointer a_map in exactly the same was as it would if a_map referred to a location in device memory.
Note Mapped pinned host memory allows you to overlap CPU-GPU memory transfers with computation while avoiding the use of CUDA streams.
But since any repeated access to such memory areas causes repeated CPU-GPU transfers, consider creating a second area in device memory to manually cache the previously read host memory data. 9.1.4. Unified Virtual Addressing  Devices of compute capability 2.0 and later support a special addressing mode called Unified Virtual Addressing (UVA) on 64-bit Linux and Windows.
With UVA, the host memory and the device memories of all installed supported devices share a single virtual address space.
Prior to UVA, an application had to keep track of which pointers referred to device memory (and for which device) and which referred to host memory as a separate bit of metadata (or as hard-coded information in the program) for each pointer.
Using UVA, on the other hand, the physical memory space to which a pointer points can be determined simply by inspecting the value of the pointer using cudaPointerGetAttributes() .
Under UVA, pinned host memory allocated with cudaHostAlloc() will have identical host and device pointers, so it is not necessary to call cudaHostGetDevicePointer() for such allocations.
Host memory allocations pinned after-the-fact via cudaHostRegister() , however, will continue to have different device pointers than their host pointers, so cudaHostGetDevicePointer() remains necessary in that case.
UVA is also a necessary precondition for enabling peer-to-peer (P2P) transfer of data directly across the PCIe bus or NVLink for supported GPUs in supported configurations, bypassing host memory.
See the CUDA C++ Programming Guide for further explanations and software requirements for UVA and P2P. 9.2. Device Memory Spaces  CUDA devices use several memory spaces, which have different characteristics that reflect their distinct usages in CUDA applications.
These memory spaces include global, local, shared, texture, and registers, as shown in Figure 2 .
Memory spaces on a CUDA device  Of these different memory spaces, global memory is the most plentiful; see Features and Technical Specifications of the CUDA C++ Programming Guide for the amounts of memory available in each memory space at each compute capability level.
Global, local, and texture memory have the greatest access latency, followed by constant memory, shared memory, and the register file.
Salient Features of Device Memory  Memory Location on/off chip Cached Access Scope Lifetime Register On n/a R/W 1 thread Thread Local Off Yes†† R/W 1 thread Thread Shared On n/a R/W All threads in block Block Global Off † R/W All threads + host Host allocation Constant Off Yes R All threads + host Host allocation Texture Off Yes R All threads + host Host allocation † Cached in L1 and L2 by default on devices of compute capability 6.0 and 7.x; cached only in L2 by default on devices of lower compute capabilities, though some allow opt-in to caching in L1 as well via compilation flags.
†† Cached in L1 and L2 by default except on devices of compute capability 5.x; devices of compute capability 5.x cache locals only in L2.
In the case of texture access, if a texture reference is bound to a linear array in global memory, then the device code can write to the underlying array.
Texture references that are bound to CUDA arrays can be written to via surface-write operations by binding a surface to the same underlying CUDA array storage).
Reading from a texture while writing to its underlying global memory array in the same kernel launch should be avoided because the texture caches are read-only and are not invalidated when the associated global memory is modified. 9.2.1. Coalesced Access to Global Memory  A very important performance consideration in programming for CUDA-capable GPU architectures is the coalescing of global memory accesses.
Global memory loads and stores by threads of a warp are coalesced by the device into as few as possible transactions.
The access requirements for coalescing depend on the compute capability of the device and are documented in the CUDA C++ Programming Guide.
For devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp.
For certain devices of compute capability 5.2, L1-caching of accesses to global memory can be optionally enabled.
If L1-caching is enabled on these devices, the number of required transactions is equal to the number of required 128-byte aligned segments.
Note On devices of compute capability 6.0 or higher, L1-caching is the default, however the data access unit is 32-byte regardless of whether global loads are cached in L1 or not.
On devices with GDDR memory, accessing memory in a coalesced way is even more important when ECC is turned on.
Scattered accesses increase ECC memory transfer overhead, especially when writing data to global memory.
These examples assume compute capability 6.0 or higher and that accesses are for 4-byte words, unless otherwise noted. 9.2.1.1. A Simple Access Pattern  The first and simplest case of coalescing can be achieved by any CUDA-enabled device of compute capability 6.0 or higher: the k -th thread accesses the k -th word in a 32-byte aligned array.
For example, if the threads of a warp access adjacent 4-byte words (e.g., adjacent float values), four coalesced 32-byte transactions will service that memory access.
Coalesced access  This access pattern results in four 32-byte transactions, indicated by the red rectangles.
if several threads had accessed the same word or if some threads did not participate in the access), the full segment is fetched anyway.
Furthermore, if accesses by the threads of the warp had been permuted within or accross the four segments, still only four 32-byte transactions would have been performed by a device with compute capability 6.0 or higher. 9.2.1.2. A Sequential but Misaligned Access Pattern  If sequential threads in a warp access memory that is sequential but not aligned with a 32-byte segment, five 32-byte segments will be requested, as shown in Figure 4 .
Misaligned sequential addresses that fall within five 32-byte segments  Memory allocated through the CUDA Runtime API, such as via cudaMalloc() , is guaranteed to be aligned to at least 256 bytes.
Therefore, choosing sensible thread block sizes, such as multiples of the warp size (i.e., 32 on current GPUs), facilitates memory accesses by warps that are properly aligned.
(Consider what would happen to the memory addresses accessed by the second, third, and subsequent thread blocks if the thread block size was not a multiple of warp size, for example.) 9.2.1.3.
Effects of Misaligned Accesses  It is easy and informative to explore the ramifications of misaligned accesses using a simple copy kernel, such as the one in A copy kernel that illustrates misaligned accesses .
A copy kernel that illustrates misaligned accesses __global__ void offsetCopy ( float * odata , float * idata , int offset ) { int xid = blockIdx .
x + offset ; odata [ xid ] = idata [ xid ]; } In A copy kernel that illustrates misaligned accesses , data is copied from the input array idata to the output array, both of which exist in global memory.
The kernel is executed within a loop in host code that varies the parameter offset from 0 to 32.
Figure 4 corresponds to this misalignments) The effective bandwidth for the copy with various offsets on an NVIDIA Tesla V100 ( compute capability 7.0) is shown in Figure 5 .
Performance of offsetCopy kernel  For the NVIDIA Tesla V100, global memory accesses with no offset or with offsets that are multiples of 8 words result in four 32-byte transactions.
Otherwise, five 32-byte segments are loaded per warp, and we would expect approximately 4/5 th of the memory throughput achieved with no offsets.
In this particular example, the offset memory throughput achieved is, however, approximately 9/10 th , because adjacent warps reuse the cache lines their neighbors fetched.
It would have been more so if adjacent warps had not exhibited such a high degree of reuse of the over-fetched cache lines. 9.2.1.4. Strided Accesses  As seen above, in the case of misaligned sequential accesses, caches help to alleviate the performance impact.
It may be different with non-unit-strided accesses, however, and this is a pattern that occurs frequently when dealing with multidimensional data or matrices.
For this reason, ensuring that as much as possible of the data in each cache line fetched is actually used is an important part of performance optimization of memory accesses on these devices.
To illustrate the effect of strided access on effective bandwidth, see the kernel strideCopy() in A kernel to illustrate non-unit stride data copy , which copies data with a stride of stride elements between threads from idata to odata .
A kernel to illustrate non-unit stride data copy __global__ void strideCopy ( float * odata , float * idata , int stride ) { int xid = ( blockIdx .
x ) * stride ; odata [ xid ] = idata [ xid ]; } Figure 6 illustrates such a situation; in this case, threads within a warp access words in memory with a stride of 2.
This action leads to a load of eight L2 cache segments per warp on the Tesla V100 (compute capability 7.0).
Adjacent threads accessing memory with a stride of 2  A stride of 2 results in a 50% of load/store efficiency since half the elements in the transaction are not used and represent wasted bandwidth.
As the stride increases, the effective bandwidth decreases until the point where 32 32-byte segments are loaded for the 32 threads in a warp, as indicated in Figure 7 .
Performance of strideCopy kernel  As illustrated in Figure 7 , non-unit-stride global memory accesses should be avoided whenever possible.
One method for doing so utilizes shared memory, which is discussed in the next section. 9.2.2. L2 Cache  Starting with CUDA 11.0, devices of compute capability 8.0 and above have the capability to influence persistence of data in the L2 cache.
Because L2 cache is on-chip, it potentially provides higher bandwidth and lower latency accesses to global memory.
For more details refer to the L2 Access Management section in the CUDA C++ Programming Guide . 9.2.2.1. L2 Cache Access Window  When a CUDA kernel accesses a data region in the global memory repeatedly, such data accesses can be considered to be persisting .
On the other hand, if the data is only accessed once, such data accesses can be considered to be streaming .
A portion of the L2 cache can be set aside for persistent accesses to a data region in global memory.
If this set-aside portion is not used by persistent accesses, then streaming or normal data accesses can use it.
The L2 cache set-aside size for persisting accesses may be adjusted, within limits: cudaGetDeviceProperties ( & prop , device_id ); cudaDeviceSetLimit ( cudaLimitPersistingL2CacheSize , prop .
persistingL2CacheMaxSize ); /* Set aside max possible size of L2 cache for persisting accesses */ Mapping of user data to L2 set-aside portion can be controlled using an access policy window on a CUDA stream or CUDA graph kernel node.
cudaStreamAttrValue stream_attribute ;   Stream level attributes data structure stream_attribute .
hitRatio = 1.0 ;   Hint for L2 cache hit ratio for persisting accesses in the num_bytes region stream_attribute .
hitProp = cudaAccessPropertyPersisting ;   Type of access property on cache hit stream_attribute .
Set the attributes to a CUDA stream of type cudaStream_t cudaStreamSetAttribute ( stream , cudaStreamAttributeAccessPolicyWindow , & stream_attribute ); The access policy window requires a value for hitRatio and num_bytes .
Depending on the value of the num_bytes parameter and the size of L2 cache, one may need to tune the value of hitRatio to avoid thrashing of L2 cache lines. 9.2.2.2. Tuning the Access Window Hit-Ratio  The hitRatio parameter can be used to specify the fraction of accesses that receive the hitProp property.
For example, if the hitRatio value is 0.6, 60% of the memory accesses in the global memory region [ptr..ptr+num_bytes) have the persisting property and 40% of the memory accesses have the streaming property.
To understand the effect of hitRatio and num_bytes , we use a sliding window micro benchmark.
First, we set aside 30 MB of the L2 cache for persisting accesses using cudaDeviceSetLimit() , as discussed above.
Then, as shown in the figure below, we specify that the accesses to the first freqSize * sizeof(int) bytes of the memory region are persistent.
In our experiment, we vary the size of this persistent data region from 10 MB to 60 MB to model various scenarios where data fits in or exceeds the available L2 set-aside portion of 30 MB.
Accesses to the remaining data of the memory region (i.e., streaming data) are considered normal or streaming accesses and will thus use the remaining 10 MB of the non set-aside L2 portion (unless part of the L2 set-aside portion is unused).
Mapping Persistent data accesses to set-aside L2 in sliding window experiment  Consider the following kernel code and access window parameters, as the implementation of the sliding window experiment.
__global__ void kernel ( int * data_persistent , int * data_streaming , int dataSize , int freqSize ) { int tid = blockIdx .
x ; /*Each CUDA thread accesses one element in the persistent data section and one element in the streaming data section.
Because the size of the persistent memory region (freqSize * sizeof(int) bytes) is much smaller than the size of the streaming memory region (dataSize * sizeof(int) bytes), data in the persistent region is accessed more frequently*/ data_persistent [ tid % freqSize ] = 2 * data_persistent [ tid % freqSize ]; data_streaming [ tid % dataSize ] = 2 * data_streaming [ tid % dataSize ]; } stream_attribute .
num_bytes = freqSize * sizeof ( int );  Number of bytes for persisting accesses in range 10-60 MB stream_attribute .
When the persistent data region fits well into the 30 MB set-aside portion of the L2 cache, a performance increase of as much as 50% is observed.
However, once the size of this persistent data region exceeds the size of the L2 set-aside cache portion, approximately 10% performance drop is observed due to thrashing of L2 cache lines.
The performance of the sliding-window benchmark with fixed hit-ratio of 1.0  In order to optimize the performance, when the size of the persistent data is more than the size of the set-aside L2 cache portion, we tune the num_bytes and hitRatio parameters in the access window as below.
hitRatio = ( 20 * 1024 * 1024 ) / (( float ) freqSize * sizeof ( int ));  Such that up to 20MB of data is resident.
We fix the num_bytes in the access window to 20 MB and tune the hitRatio such that a random 20 MB of the total persistent data is resident in the L2 set-aside cache portion.
The remaining portion of this persistent data will be accessed using the streaming property.
The results are shown in the chart below, where we see good performance regardless of whether the persistent data fits in the L2 set-aside or not.
Shared Memory  Because it is on-chip, shared memory has much higher bandwidth and lower latency than local and global memory - provided there are no bank conflicts between the threads, as detailed in the following section. 9.2.3.1. Shared Memory and Memory Banks  To achieve high memory bandwidth for concurrent accesses, shared memory is divided into equally sized memory modules ( banks ) that can be accessed simultaneously.
Therefore, any memory load or store of n addresses that spans n distinct memory banks can be serviced simultaneously, yielding an effective bandwidth that is n times as high as the bandwidth of a single bank.
However, if multiple addresses of a memory request map to the same memory bank, the accesses are serialized.
The hardware splits a memory request that has bank conflicts into as many separate conflict-free requests as necessary, decreasing the effective bandwidth by a factor equal to the number of separate memory requests.
The one exception here is when multiple threads in a warp address the same shared memory location, resulting in a broadcast.
In this case, multiple broadcasts from different banks are coalesced into a single multicast from the requested shared memory locations to the threads.
To minimize bank conflicts, it is important to understand how memory addresses map to memory banks and how to optimally schedule memory requests.
On devices of compute capability 5.x or newer, each bank has a bandwidth of 32 bits every clock cycle, and successive 32-bit words are assigned to successive banks.
The warp size is 32 threads and the number of banks is also 32, so bank conflicts can occur between any threads in the warp.
See Compute Capability 5.x in the CUDA C++ Programming Guide for further details. 9.2.3.2. Shared Memory in Matrix Multiplication (C=AB)  Shared memory enables cooperation between threads in a block.
When multiple threads in a block use the same data from global memory, shared memory can be used to access the data from global memory only once.
Shared memory can also be used to avoid uncoalesced memory accesses by loading and storing data in a coalesced pattern from global memory and then reordering it in shared memory.
Aside from memory bank conflicts, there is no penalty for non-sequential or unaligned accesses by a warp in shared memory.
The use of shared memory is illustrated via the simple example of a matrix multiplication C = AB for the case with A of dimension Mxw, B of dimension wxN, and C of dimension MxN.
To keep the kernels simple, M and N are multiples of 32, since the warp size (w) is 32 for current devices.
Therefore, in terms of wxw tiles, A is a column matrix, B is a row matrix, and C is their outer product; see Figure 11 .
A grid of N/w by M/w blocks is launched, where each thread block calculates the elements of a different tile in C from a single tile of A and a single tile of B.
Block-column matrix (A) multiplied by block-row matrix (B) with resulting product matrix (C).
 To do this, the simpleMultiply kernel ( Unoptimized matrix multiplication ) calculates the output elements of a tile of matrix C.
Unoptimized matrix multiplication __global__ void simpleMultiply ( float * a , float * b , float * c , int N ) { int row = blockIdx .
x ; float sum = 0.0f ; for ( int i = 0 ; i __global__ void pipeline_kernel_sync ( T * global , uint64_t * clock , size_t copy_count ) { extern __shared__ char s []; T * shared = reinterpret_cast ( s ); uint64_t clock_start = clock64 (); for ( size_t i = 0 ; i ( clock ), clock_end - clock_start ); } template __global__ void pipeline_kernel_async ( T * global , uint64_t * clock , size_t copy_count ) { extern __shared__ char s []; T * shared = reinterpret_cast ( s ); uint64_t clock_start = clock64 ();  pipeline pipe; for ( size_t i = 0 ; i ( clock ), clock_end - clock_start ); } The synchronous version for the kernel loads an element from global memory to an intermediate register and then stores the intermediate register value to shared memory.
In the asynchronous version of the kernel, instructions to load from global memory and store directly into shared memory are issued as soon as __pipeline_memcpy_async() function is called.
The __pipeline_wait_prior(0) will wait until all the instructions in the pipe object have been executed.
Not using intermediate registers can help reduce register pressure and can increase kernel occupancy.
Data copied from global memory to shared memory using asynchronous copy instructions can be cached in the L1 cache or the L1 cache can be optionally bypassed.
If individual CUDA threads are copying elements of 16 bytes, the L1 cache can be bypassed.
Comparing Synchronous vs Asynchronous Copy from Global Memory to Shared Memory  We evaluate the performance of both kernels using elements of size 4B, 8B and 16B per thread i.e., using int , int2 and int4 for the template parameter.
We adjust the copy_count in the kernels such that each thread block copies from 512 bytes up to 48 MB.
Comparing Performance of Synchronous vs Asynchronous Copy from Global Memory to Shared Memory  From the performance chart, the following observations can be made for this experiment.
Best performance with synchronous copy is achieved when the copy_count parameter is a multiple of 4 for all three element sizes.
The async-copy does not require the copy_count parameter to be a multiple of 4, to maximize performance through compiler optimizations.
Overall, best performance is achieved when using asynchronous copies with an element of size 8 or 16 bytes. 9.2.4. Local Memory  Local memory is so named because its scope is local to the thread, not because of its physical location.
This is done by the nvcc compiler when it determines that there is insufficient register space to hold the variable.
Automatic variables that are likely to be placed in local memory are large structures or arrays that would consume too much register space and arrays that the compiler determines may be indexed dynamically.
Inspection of the PTX assembly code (obtained by compiling with -ptx or -keep command-line options to nvcc ) reveals whether a variable has been placed in local memory during the first compilation phases.
If it has, it will be declared using the .local mnemonic and accessed using the ld.local and st.local mnemonics.
If it has not, subsequent compilation phases might still decide otherwise, if they find the variable consumes too much register space for the targeted architecture.
There is no way to check this for a specific variable, but the compiler reports total local memory usage per kernel (lmem) when run with the --ptxas-options=-v option. 9.2.5. Texture Memory  The read-only texture memory space is cached.
Therefore, a texture fetch costs one device memory read only on a cache miss; otherwise, it just costs one read from the texture cache.
The texture cache is optimized for 2D spatial locality, so threads of the same warp that read texture addresses that are close together will achieve best performance.
Texture memory is also designed for streaming fetches with a constant latency; that is, a cache hit reduces DRAM bandwidth demand, but not fetch latency.
In certain addressing situations, reading device memory through texture fetching can be an advantageous alternative to reading device memory from global or constant memory. 9.2.5.1. Additional Texture Capabilities  If textures are fetched using tex1D() , tex2D() , or tex3D() rather than tex1Dfetch() , the hardware provides other capabilities that might be useful for some applications such as image processing, as shown in Table 4 .
Useful Features for tex1D(), tex2D(), and tex3D() Fetches  Feature Use Caveat Filtering Fast, low-precision interpolation between texels Valid only if the texture reference returns floating-point data Normalized texture coordinates Resolution-independent coding None Addressing modes Automatic handling of boundary cases 1 Can be used only with normalized texture coordinates 1 The automatic handling of boundary cases in the bottom row of Table 4 refers to how a texture coordinate is resolved when it falls outside the valid addressing range.
If x is the coordinate and N is the number of texels for a one-dimensional texture, then with clamp, x is replaced by 0 if x >> ( data_1 ); kernel2 >> ( data_2 ); 10.6.
Multiple contexts  CUDA work occurs within a process space for a particular GPU known as a context .
The context encapsulates kernel launches and memory allocations for that GPU as well as supporting constructs such as the page tables.
The context is explicit in the CUDA Driver API but is entirely implicit in the CUDA Runtime API, which creates and manages contexts automatically.
With the CUDA Driver API, a CUDA application process can potentially create more than one context for a given GPU.
If multiple CUDA application processes access the same GPU concurrently, this almost always implies multiple contexts, since a context is tied to a particular host process unless Multi-Process Service is in use.
While multiple contexts (and their associated resources such as global memory allocations) can be allocated concurrently on a given GPU, only one of these contexts can execute work at any given moment on that GPU; contexts sharing the same GPU are time-sliced.
Creating additional contexts incurs memory overhead for per-context data and time overhead for context switching.
Furthermore, the need for context switching can reduce utilization when work from several contexts could otherwise execute concurrently (see also Concurrent Kernel Execution ).
Therefore, it is best to avoid multiple contexts per GPU within the same CUDA application.
To assist with this, the CUDA Driver API provides methods to access and manage a special context on each GPU called the primary context .
These are the same contexts used implicitly by the CUDA Runtime when there is not already a current context for a thread.
When initializing the program/library CUcontext ctx ; cuDevicePrimaryCtxRetain ( & ctx , dev );   When the program/library launches work cuCtxPushCurrent ( ctx ); kernel >> (...); cuCtxPopCurrent ( & ctx );   When the program/library is finished with the context cuDevicePrimaryCtxRelease ( dev ); Note NVIDIA-SMI can be used to configure a GPU for exclusive process mode , which limits the number of contexts per GPU to one.
This context can be current to as many threads as desired within the creating process, and cuDevicePrimaryCtxRetain will fail if a non-primary context that was created with the CUDA driver API already exists on the device. 11. Instruction Optimization  Awareness of how instructions are executed often permits low-level optimizations that can be useful, especially in code that is run frequently (the so-called hot spot in a program).
Best practices suggest that this optimization be performed after all higher-level optimizations have been completed. 11.1. Arithmetic Instructions  Single-precision floats provide the best performance, and their use is highly encouraged.
The throughput of individual arithmetic operations is detailed in the CUDA C++ Programming Guide. 11.1.1. Division Modulo Operations  Note Low Priority: Use shift operations to avoid expensive division and modulo calculations.
Integer division and modulo operations are particularly costly and should be avoided or replaced with bitwise operations whenever possible: If \(n\) is a power of 2, ( \(i/n\) ) is equivalent to ( \(i \gg {log2}(n)\) ) and ( \(i\% n\) ) is equivalent to ( \(i\&\left( {n - 1}  ight)\) ).
(For further information, refer to Performance Guidelines in the CUDA C++ Programming Guide). 11.1.2. Loop Counters Signed vs.
Unsigned  Note Low Medium Priority: Use signed integers rather than unsigned integers as loop counters.
In the C language standard, unsigned integer overflow semantics are well defined, whereas signed integer overflow causes undefined results.
Therefore, the compiler can optimize more aggressively with signed arithmetic than it can with unsigned arithmetic.
This is of particular note with loop counters: since it is common for loop counters to have values that are always positive, it may be tempting to declare the counters as unsigned.
For example, consider the following code: for ( i = 0 ; i = 0, x != -0 , that is, signbit(x) == 0 .
Formulae for exponentiation by small fractions  Computation Formula x 1/9 r = rcbrt(rcbrt(x)) x -1/9 r = cbrt(rcbrt(x)) x 1/6 r = rcbrt(rsqrt(x)) x -1/6 r = rcbrt(sqrt(x)) x 1/4 r = rsqrt(rsqrt(x)) x -1/4 r = sqrt(rsqrt(x)) x 1/3 r = cbrt(x) x -1/3 r = rcbrt(x) x 1/2 r = sqrt(x) x -1/2 r = rsqrt(x) x 2/3 r = cbrt(x); r = r*r x -2/3 r = rcbrt(x); r = r*r x 3/4 r = sqrt(x); r = r*sqrt(r) x -3/4 r = rsqrt(x); r = r*sqrt(r) x 7/6 r = x*rcbrt(rsqrt(x)) x -7/6 r = (1/x) * rcbrt(sqrt(x)) x 5/4 r = x*rsqrt(rsqrt(x)) x -5/4 r = (1/x)*sqrt(rsqrt(x)) x 4/3 r = x*cbrt(x) x -4/3 r = (1/x)*rcbrt(x) x 3/2 r = x*sqrt(x) x -3/2 r = (1/x)*rsqrt(x) 11.1.6.
Math Libraries  Note Medium Priority: Use the fast math library whenever speed trumps precision.
They can be distinguished by their names: some have names with prepended underscores, whereas others do not (e.g., __functionName() versus functionName() ).
Functions following the __functionName() naming convention map directly to the hardware level.
Functions following functionName() naming convention are slower but have higher accuracy (e.g., sinf(x) and expf(x) ).
The throughput of __sinf(x) , __cosf(x) , and __expf(x) is much greater than that of sinf(x) , cosf(x) , and expf(x) .
The latter become even more expensive (about an order of magnitude slower) if the magnitude of the argument x needs to be reduced.
Moreover, in such cases, the argument-reduction code uses local memory, which can affect performance even more because of the high latency of local memory.
Note also that whenever sine and cosine of the same argument are computed, the sincos family of instructions should be used to optimize performance: __sincosf() for single-precision fast math (see next paragraph) sincosf() for regular single-precision sincos() for double precision The -use_fast_math compiler option of nvcc coerces every functionName() call to the equivalent __functionName() call.
It also disables single-precision denormal support and lowers the precision of single-precision division in general.
This is an aggressive optimization that can both reduce numerical accuracy and alter special case handling.
A more robust approach is to selectively introduce calls to fast intrinsic functions only if merited by performance gains and where altered behavior can be tolerated.
Note Medium Priority: Prefer faster, more specialized math functions over slower, more general ones when possible.
For small integer powers (e.g., x2 or x3 ), explicit multiplication is almost certainly faster than the use of general exponentiation routines such as pow() .
While compiler optimization improvements continually seek to narrow this gap, explicit multiplication (or the use of an equivalent purpose-built inline function or macro) can have a significant advantage.
This advantage is increased when several powers of the same base are needed (e.g., where both x2 and x5 are calculated in close proximity), as this aids the compiler in its common sub-expression elimination (CSE) optimization.
For exponentiation using base 2 or 10, use the functions exp2() or expf2() and exp10() or expf10() rather than the functions pow() or powf() .
Both pow() and powf() are heavy-weight functions in terms of register pressure and instruction count due to the numerous special cases arising in general exponentiation and the difficulty of achieving good accuracy across the entire ranges of the base and the exponent.
The functions exp2() , exp2f() , exp10() , and exp10f() , on the other hand, are similar to exp() and expf() in terms of performance, and can be as much as ten times faster than their pow() / powf() equivalents.
For exponentiation with an exponent of 1/3, use the cbrt() or cbrtf() function rather than the generic exponentiation functions pow() or powf() , as the former are significantly faster than the latter.
As a particular example, to evaluate the sine function in degrees instead of radians, use sinpi(x/180.0) .
Similarly, the single-precision functions sinpif() , cospif() , and sincospif() should replace calls to sinf() , cosf() , and sincosf() when the function argument is of the form π* .
(The performance advantage sinpi() has over sin() is due to simplified argument reduction; the accuracy advantage is because sinpi() multiplies by π only implicitly, effectively using an infinitely precise mathematical π rather than a single- or double-precision approximation thereof.) 11.1.7.
Precision-related Compiler Flags  By default, the nvcc compiler generates IEEE-compliant code, but it also provides options to generate code that somewhat less accurate but faster: -ftz=true (denormalized numbers are flushed to zero) -prec-div=false (less precise division) -prec-sqrt=false (less precise square root) Another, more aggressive, option is -use_fast_math , which coerces every functionName() call to the equivalent __functionName() call.
See Math Libraries . 11.2. Memory Instructions  Note High Priority: Minimize the use of global memory.
Memory instructions include any instruction that reads from or writes to shared, local, or global memory.
When accessing uncached local or global memory, there are hundreds of clock cycles of memory latency.
As an example, the assignment operator in the following sample code has a high throughput, but, crucially, there is a latency of hundreds of clock cycles to read data from global memory: __shared__ float shared [ 32 ]; __device__ float device [ 32 ]; shared [ threadIdx .
x ]; Much of this global memory latency can be hidden by the thread scheduler if there are sufficient independent arithmetic instructions that can be issued while waiting for the global memory access to complete.
However, it is best to avoid accessing global memory whenever possible. 12. Control Flow  12.1.
Branching and Divergence  Note High Priority: Avoid different execution paths within the same warp.
Flow control instructions ( if , switch , do , for , while ) can significantly affect the instruction throughput by causing threads of the same warp to diverge; that is, to follow different execution paths.
If this happens, the different execution paths must be executed separately; this increases the total number of instructions executed for this warp.
To obtain best performance in cases where the control flow depends on the thread ID, the controlling condition should be written so as to minimize the number of divergent warps.
This is possible because the distribution of the warps across the block is deterministic as mentioned in SIMT Architecture of the CUDA C++ Programming Guide.
A trivial example is when the controlling condition depends only on ( threadIdx / WSIZE ) where WSIZE is the warp size.
In this case, no warp diverges because the controlling condition is perfectly aligned with the warps.
For branches including just a few instructions, warp divergence generally results in marginal performance losses.
Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions.
Threads with a false predicate do not write results, and also do not evaluate addresses or read operands.
Starting with the Volta architecture, Independent Thread Scheduling allows a warp to remain diverged outside of the data-dependent conditional block.
An explicit __syncwarp() can be used to guarantee that the warp has reconverged for subsequent instructions. 12.2. Branch Predication  Note Low Priority: Make it easy for the compiler to use branch predication in lieu of loops or control statements.
Sometimes, the compiler may unroll loops or optimize out if or switch statements by using branch predication instead.
The programmer can also control loop unrolling using #pragma unroll For more information on this pragma, refer to the CUDA C++ Programming Guide.
When using branch predication, none of the instructions whose execution depends on the controlling condition is skipped.
Instead, each such instruction is associated with a per-thread condition code or predicate that is set to true or false according to the controlling condition.
Although each of these instructions is scheduled for execution, only the instructions with a true predicate are actually executed.
Instructions with a false predicate do not write results, and they also do not evaluate addresses or read operands.
The compiler replaces a branch instruction with predicated instructions only if the number of instructions controlled by the branch condition is less than or equal to a certain threshold. 13. Deploying CUDA Applications  Having completed the GPU acceleration of one or more components of the application it is possible to compare the outcome with the original expectation.
14. Understanding the Programming Environment  With each generation of NVIDIA processors, new features are added to the GPU that CUDA can leverage.
The first is the compute capability , and the second is the version number of the CUDA Runtime and CUDA Driver APIs. 14.1. CUDA Compute Capability  The compute capability describes the features of the hardware and reflects the set of instructions supported by the device as well as other specifications, such as the maximum number of threads per block and the number of registers per multiprocessor.
Higher compute capability versions are supersets of lower (that is, earlier) versions, so they are backward compatible.
The compute capability of the GPU in the device can be queried programmatically as illustrated in the deviceQuery CUDA Sample.
This information is obtained by calling cudaGetDeviceProperties() and accessing the information in the structure it returns.
Sample CUDA configuration data reported by deviceQuery  The major and minor revision numbers of the compute capability are shown on the seventh line of Figure 16 .
More details about the compute capabilities of various GPUs are in CUDA-Enabled GPUs and Compute Capabilities of the CUDA C++ Programming Guide.
In particular, developers should note the number of multiprocessors on the device, the number of registers and the amount of memory available, and any special capabilities of the device. 14.2. Additional Hardware Data  Certain hardware features are not described by the compute capability.
For example, the ability to overlap kernel execution with asynchronous data transfers between the host and the device is available on most but not all GPUs irrespective of the compute capability.
In such cases, call cudaGetDeviceProperties() to determine whether the device is capable of a certain feature.
For example, the asyncEngineCount field of the device property structure indicates whether overlapping kernel execution and data transfers is possible (and, if so, how many concurrent transfers are possible); likewise, the canMapHostMemory field indicates whether zero-copy data transfers can be performed. 14.3. Which Compute Capability Target  To target specific versions of NVIDIA hardware and CUDA software, use the -arch , -code , and -gencode options of nvcc .
Code that uses the warp shuffle operation, for example, must be compiled with -arch=sm_30 (or higher compute capability).
See Building for Maximum Compatibility for further discussion of the flags used for building code for multiple generations of CUDA-capable device simultaneously. 14.4. CUDA Runtime  The host runtime component of the CUDA software environment can be used only by host functions.
It provides functions to handle the following: Device management Context management Memory management Code module management Execution control Texture reference management Interoperability with OpenGL and Direct3D As compared to the lower-level CUDA Driver API, the CUDA Runtime greatly eases device management by providing implicit initialization, context management, and device code module management.
The C++ host code generated by nvcc utilizes the CUDA Runtime, so applications that link to this code will depend on the CUDA Runtime; similarly, any code that uses the cuBLAS , cuFFT , and other CUDA Toolkit libraries will also depend on the CUDA Runtime, which is used internally by these libraries.
The functions that make up the CUDA Runtime API are explained in the CUDA Toolkit Reference Manual.
The CUDA Runtime handles kernel loading and setting up kernel parameters and launch configuration before the kernel is launched.
The implicit driver version checking, code initialization, CUDA context management, CUDA module management (cubin to function mapping), kernel configuration, and parameter passing are all performed by the CUDA Runtime.
For more information on the Runtime API, refer to CUDA Runtime of the CUDA C++ Programming Guide. 15. CUDA Compatibility Developer’s Guide  CUDA Toolkit is released on a monthly release cadence to deliver new features, performance improvements, and critical bug fixes.
CUDA compatibility allows users to update the latest CUDA Toolkit software (including the compiler, libraries, and tools) without requiring update to the entire driver stack.
The CUDA software environment consists of three parts: CUDA Toolkit (libraries, CUDA runtime and developer tools) - SDK for developers to build CUDA applications.
On Linux systems, the CUDA driver and kernel mode components are delivered together in the NVIDIA display driver package.
Components of CUDA  The CUDA compiler (nvcc), provides a way to handle CUDA and non-CUDA code (by splitting and steering compilation), along with the CUDA runtime, is part of the CUDA compiler toolchain.
The CUDA Runtime API provides developers with high-level C++ interface for simplified management of devices, kernel executions etc., While the CUDA driver API provides ( CUDA Driver API ) a low-level programming interface for applications to target NVIDIA hardware.
Built on top of these technologies are CUDA libraries, some of which are included in the CUDA Toolkit, while others such as cuDNN may be released independently of the CUDA Toolkit. 15.1. CUDA Toolkit Versioning  Starting with CUDA 11, the toolkit versions are based on an industry-standard semantic versioning scheme: .X.Y.Z, where: .X stands for the major version - APIs have changed and binary compatibility is broken.
.Y stands for the minor version - Introduction of new APIs, deprecation of old APIs, and source compatibility might be broken but binary compatibility is maintained.
Compatibility of the CUDA platform is thus intended to address a few scenarios: NVIDIA driver upgrades to systems with GPUs running in production for enterprises or datacenters can be complex and may need advance planning.
Delays in rolling out new NVIDIA drivers could mean that users of such systems may not have access to new features available in CUDA releases.
Not requiring driver updates for new CUDA releases can mean that new versions of the software can be made available faster to users.
math libraries or deep learning frameworks) do not have a direct dependency on the CUDA runtime, compiler or driver.
In such cases, users or developers can still benefit from not having to upgrade the entire CUDA Toolkit or driver to use these libraries or frameworks.
Upgrading dependencies is error-prone and time consuming, and in some corner cases, can even change the semantics of a program.
Constantly recompiling with the latest CUDA Toolkit means forcing upgrades on the end-customers of an application product.
Package managers facilitate this process but unexpected issues can still arise and if a bug is found, it necessitates a repeat of the above upgrade process.
CUDA supports several compatibility choices: First introduced in CUDA 10, the CUDA Forward Compatible Upgrade is designed to allow users to get access to new CUDA features and run applications built with new CUDA releases on systems with older installations of the NVIDIA datacenter driver.
First introduced in CUDA 11.1, CUDA Enhanced Compatibility provides two benefits: By leveraging semantic versioning across components in the CUDA Toolkit, an application can be built for one CUDA minor release (for example 11.1) and work across all future minor releases within the major family (i.e.
The CUDA runtime has relaxed the minimum driver version check and thus no longer requires a driver upgrade when moving to a new minor release.
The CUDA driver ensures backward Binary Compatibility is maintained for compiled CUDA applications.
Applications compiled with CUDA toolkit versions as old as 3.2 will run on newer drivers. 15.2. Source Compatibility  We define source compatibility as a set of guarantees provided by the library, where a well-formed application built against a specific version of the library (using the SDK) will continue to build and run without errors when a newer version of the SDK is installed.
Both the CUDA driver and the CUDA runtime are not source compatible across the different SDK releases.
Therefore, an application that compiled successfully on an older version of the toolkit may require changes in order to compile against a newer version of the toolkit.
Developers are notified through deprecation and documentation mechanisms of any current or upcoming changes.
This does not mean that application binaries compiled using an older toolkit will not be supported anymore.
Application binaries rely on CUDA Driver API interface and even though the CUDA Driver API itself may also have changed across toolkit versions, CUDA guarantees Binary Compatibility of the CUDA Driver API interface. 15.3. Binary Compatibility  We define binary compatibility as a set of guarantees provided by the library, where an application targeting the said library will continue to work when dynamically linked against a different version of the library.
The CUDA Driver API has a versioned C-style ABI, which guarantees that applications that were running against an older driver (for example CUDA 3.2) will still run and function correctly against a modern driver (for example one shipped with CUDA 11.0).
This means that even though an application source might need to be changed if it has to be recompiled against a newer CUDA Toolkit in order to use the newer features, replacing the driver components installed in a system with a newer version will always support existing applications and its functions.
The CUDA Driver API thus is binary-compatible (the OS loader can pick up a newer version and the application continues to work) but not source-compatible (rebuilding your application against a newer SDK might require source changes).
CUDA Toolkit and Minimum Driver Versions  Before we proceed further on this topic, it’s important for developers to understand the concept of Minimum Driver Version and how that may affect them.
Each version of the CUDA Toolkit (and runtime) requires a minimum version of the NVIDIA driver.
Applications compiled against a CUDA Toolkit version will only run on systems with the specified minimum driver version for that toolkit version.
Prior to CUDA 11.0, the minimum driver version for a toolkit was the same as the driver shipped with that version of the CUDA Toolkit.
So, when an application is built with CUDA 11.0, it can only run on a system with an R450 or later driver.
If such an application is run on a system with the R418 driver installed, CUDA initialization will return an error as can be seen in the example below.
In this example, the deviceQuery sample is compiled with CUDA 11.1 and is run on a system with R418.
In this scenario, CUDA initialization returns an error due to the minimum driver requirement.
| |=+=+=| | 0 Tesla T4 On | 00000000:00:1E.0 Off | 0 | | N/A 42C P0 28W / 70W | 0MiB / 15079MiB | 0% Default | +-+-+-+ +-+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=| | No running processes found | +-+ $ samples/bin/x86_64/linux/release/deviceQuery samples/bin/x86_64/linux/release/deviceQuery Starting...
CUDA Device Query (Runtime API) version (CUDART static linking) cudaGetDeviceCount returned 3 -> initialization error Result = FAIL Refer to the CUDA Toolkit Release Notes for details for the minimum driver version and the version of the driver shipped with the toolkit. 15.3.1. CUDA Binary (cubin) Compatibility  A slightly related but important topic is one of application binary compatibility across GPU architectures in CUDA.
CUDA C++ provides a simple path for users familiar with the C++ programming language to easily write programs for execution by the device.
Kernels can be written using the CUDA instruction set architecture, called PTX, which is described in the PTX reference manual.
It is however usually more effective to use a high-level programming language such as C++.
In both cases, kernels must be compiled into binary code by nvcc (called cubins) to execute on the device.
Binary compatibility for cubins is guaranteed from one compute capability minor revision to the next one, but not from one compute capability minor revision to the previous one or across major compute capability revisions.
In other words, a cubin object generated for compute capability X.y will only execute on devices of compute capability X.z where z≥y .
To execute code on devices of specific compute capability, an application must load binary or PTX code that is compatible with this compute capability.
For portability, that is, to be able to execute code on future GPU architectures with higher compute capability (for which no binary code can be generated yet), an application must load PTX code that will be just-in-time compiled by the NVIDIA driver for these future devices.
More information on cubins, PTX and application compatibility can be found in the CUDA C++ Programming Guide . 15.4. CUDA Compatibility Across Minor Releases  By leveraging the semantic versioning, starting with CUDA 11, components in the CUDA Toolkit will remain binary compatible across the minor versions of the toolkit.
In order to maintain binary compatibility across minor versions, the CUDA runtime no longer bumps up the minimum driver version required for every minor release - this only happens when a major release is shipped.
One of the main reasons a new toolchain requires a new minimum driver is to handle the JIT compilation of PTX code and the JIT linking of binary code.
In this section, we will review the usage patterns that may require new user workflows when taking advantage of the compatibility features of the CUDA platform. 15.4.1. Existing CUDA Applications within Minor Versions of CUDA  $ nvidia - smi +-+ | NVIDIA - SMI 450.80.02 Driver Version : 450.80.02 CUDA Version : 11.0 | |-+-+-+ | GPU Name Persistence - M | Bus - Id Disp .
| |=+=+=| | 0 Tesla T4 On | 00000000 : 00 : 1 E .0 Off | 0 | | N / A 39 C P8 9 W / 70 W | 0 MiB / 15109 MiB | 0 % Default | | | | N / A | +-+-+-+ +-+ | Processes : | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=| | No running processes found | +-+ When our CUDA 11.1 application (i.e.
cudart 11.1 is statically linked) is run on the system, we see that it runs successfully even when the driver reports a 11.0 version - that is, without requiring the driver or other toolkit components to be updated on the system.
$ samples / bin / x86_64 / linux / release / deviceQuery samples / bin / x86_64 / linux / release / deviceQuery Starting ...
CUDA Device Query ( Runtime API ) version ( CUDART static linking ) Detected 1 CUDA Capable device ( s ) Device 0 : "Tesla T4" CUDA Driver Version / Runtime Version 11.0 / 11.1 CUDA Capability Major / Minor version number : 7.5 ... ... deviceQuery , CUDA Driver = CUDART , CUDA Driver Version = 11.0 , CUDA Runtime Version = 11.1 , NumDevs = 1 Result = PASS By using new CUDA versions, users can benefit from new CUDA programming model APIs, compiler optimizations and math library features.
The following sections discuss some caveats and considerations. 15.4.1.1. Handling New CUDA Features and Driver APIs  A subset of CUDA APIs don’t need a new driver and they can all be used without any driver dependencies.
For example, cuMemMap APIs or any of APIs introduced prior to CUDA 11.0, such as cudaDeviceSynchronize , do not require a driver upgrade.
To use other CUDA APIs introduced in a minor release (that require a new driver), one would have to implement fallbacks or fail gracefully.
This situation is not different from what is available today where developers use macros to compile out features based on CUDA versions.
Users should refer to the CUDA headers and documentation for new CUDA APIs introduced in a release.
When working with a feature exposed in a minor version of the toolkit, the feature might not be available at runtime if the application is running against an older CUDA driver.
Applications that do not check for CUDA API errors could at times run to completion without having noticed that the data calculated by the GPU is incomplete, invalid, or uninitialized.
Note The CUDA Toolkit Samples provide several helper functions for error checking with the various CUDA APIs; these helper functions are located in the samples/common/inc/helper_cuda.h file in the CUDA Toolkit. 16.3. Building for Maximum Compatibility  Each generation of CUDA-capable device has an associated compute capability version that indicates the feature set supported by the device (see CUDA Compute Capability ).
One or more compute capability versions can be specified to the nvcc compiler while building a file; compiling for the native compute capability for the target GPU(s) of the application is important to ensure that application kernels achieve the best possible performance and are able to use the features that are available on a given generation of GPU.
When an application is built for multiple compute capabilities simultaneously (using several instances of the -gencode flag to nvcc), the binaries for the specified compute capabilities are combined into the executable, and the CUDA Driver selects the most appropriate binary at runtime according to the compute capability of the present device.
If an appropriate native binary ( cubin ) is not available, but the intermediate PTX code (which targets an abstract virtual instruction set and is used for forward-compatibility) is available, then the kernel will be compiled Just In Time (JIT) (see Compiler JIT Cache Management Tools ) from the PTX to the native cubin for the device.
Unlike the CUDA Driver, the CUDA Runtime guarantees neither forward nor backward binary compatibility across versions.
It is therefore best to redistribute the CUDA Runtime library with the application when using dynamic linking or else to statically link against the CUDA Runtime.
This will ensure that the executable will be able to run even if the user does not have the same CUDA Toolkit installed that the application was built against.
Note When statically linking to the CUDA Runtime, multiple versions of the runtime can peacably coexist in the same application process simultaneously; for example, if an application uses one version of the CUDA Runtime, and a plugin to that application is statically linked to a different version, that is perfectly acceptable, as long as the installed NVIDIA Driver is sufficient for both.
Statically-linked CUDA Runtime The easiest option is to statically link against the CUDA Runtime.
Static linking makes the executable slightly larger, but it ensures that the correct version of runtime library functions are included in the application binary without requiring separate redistribution of the CUDA Runtime library.
Dynamically-linked CUDA Runtime If static linking against the CUDA Runtime is impractical for some reason, then a dynamically-linked version of the CUDA Runtime library is also available.
(This was the default and only option provided in CUDA versions 5.0 and earlier.) To use dynamic linking with the CUDA Runtime when using the nvcc from CUDA 5.5 or later to link the application, add the --cudart=shared flag to the link command line; otherwise the statically-linked CUDA Runtime library is used by default.
After the application is dynamically linked against the CUDA Runtime, this version of the runtime library should be bundled with the application.
It can be copied into the same directory as the application executable or into a subdirectory of that installation path.
Other CUDA Libraries Although the CUDA Runtime provides the option of static linking, some libraries included in the CUDA Toolkit are available only in dynamically-linked form.
As with the dynamically-linked version of the CUDA Runtime library , these libraries should be bundled with the application executable when distributing that application. 16.4.1. CUDA Toolkit Library Redistribution  The CUDA Toolkit’s End-User License Agreement (EULA) allows for redistribution of many of the CUDA libraries under certain terms and conditions.
This allows applications that depend on these libraries to redistribute the exact versions of the libraries against which they were built and tested, thereby avoiding any trouble for end users who might have a different version of the CUDA Toolkit (or perhaps none at all) installed on their machines.
Note This does not apply to the NVIDIA Driver; the end user must still download and install an NVIDIA Driver appropriate to their GPU(s) and operating system. 16.4.1.1. Which Files to Redistribute  When redistributing the dynamically-linked versions of one or more CUDA libraries, it is important to identify the exact files that need to be redistributed.
The following examples use the cuBLAS library from CUDA Toolkit 5.5 as an illustration: Linux In a shared library on Linux, there is a string field called the SONAME that indicates the binary compatibility level of the library.
The SONAME of the library against which the application was built must match the filename of the library that is redistributed with the application.
For example, in the standard CUDA Toolkit installation, the files libcublas.so and libcublas.so.5.5 are both symlinks pointing to a specific build of cuBLAS, which is named like libcublas.so.5.5.x , where x is the build number (e.g., libcublas.so.5.5.17 ).
However, the SONAME of this library is given as “ libcublas.so.5.5 ”: $ objdump -p /usr/local/cuda/lib64/libcublas.so | grep SONAME SONAME libcublas.so.5.5 Because of this, even if -lcublas (with no version number specified) is used when linking the application, the SONAME found at link time implies that “ libcublas.so.5.5 ” is the name of the file that the dynamic loader will look for when loading the application and therefore must be the name of the file (or a symlink to the same) that is redistributed with the application.
The ldd tool is useful for identifying the exact filenames of the libraries that the application expects to find at runtime as well as the path, if any, of the copy of that library that the dynamic loader would select when loading the application given the current library search path: $ ldd a.out | grep libcublas libcublas.so.5.5 => /usr/local/cuda/lib64/libcublas.so.5.5 Mac In a shared library on Mac OS X, there is a field called the install name that indicates the expected installation path and filename the library; the CUDA libraries also use this filename to indicate binary compatibility.
The value of this field is propagated into an application built against the library and is used to locate the library of the correct version at runtime.
For example, if the install name of the cuBLAS library is given as @rpath/libcublas.5.5.dylib , then the library is version 5.5 and the copy of this library redistributed with the application must be named libcublas.5.5.dylib , even though only -lcublas (with no version number specified) is used at link time.
Furthermore, this file should be installed into the @rpath of the application; see Where to Install Redistributed CUDA Libraries .
To view a library’s install name, use the otool -L command: $ otool -L a.out a.out: @rpath/libcublas.5.5.dylib (...) Windows The binary compatibility version of the CUDA libraries on Windows is indicated as part of the filename.
For example, a 64-bit application linked to cuBLAS 5.5 will look for cublas64_55.dll at runtime, so this is the file that should be redistributed with that application, even though cublas.lib is the file that the application is linked against.
To verify the exact DLL filename that the application expects to find at runtime, use the dumpbin tool from the Visual Studio command prompt: $ dumpbin /IMPORTS a.exe Microsoft (R) COFF/PE Dumper Version 10.00.40219.01 Copyright (C) Microsoft Corporation.
Dump of file a.exe File Type: EXECUTABLE IMAGE Section contains the following imports: ...
cublas64_55.dll ... 16.4.1.2. Where to Install Redistributed CUDA Libraries  Once the correct library files are identified for redistribution, they must be configured for installation into a location where the application will be able to find them.
On Windows, if the CUDA Runtime or other dynamically-linked CUDA Toolkit library is placed in the same directory as the executable, Windows will locate it automatically.
On Linux and Mac, the -rpath linker option should be used to instruct the executable to search its local path for these libraries before searching the system paths: Linux/Mac nvcc -I $(CUDA_HOME)/include -Xlinker "-rpath '$ORIGIN'" --cudart=shared -o myprogram myprogram.cu Windows nvcc.exe -ccbin "C:\vs2008\VC\bin" -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT" --cudart=shared -o "Release\myprogram.exe" "myprogram.cu" Note It may be necessary to adjust the value of -ccbin to reflect the location of your Visual Studio installation.
To specify an alternate path where the libraries will be distributed, use linker options similar to those below: Linux/Mac nvcc -I $(CUDA_HOME)/include -Xlinker "-rpath '$ORIGIN/lib'" --cudart=shared -o myprogram myprogram.cu Windows nvcc.exe -ccbin "C:\vs2008\VC\bin" -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT /DELAY" --cudart=shared -o "Release\myprogram.exe" "myprogram.cu" For Linux and Mac, the -rpath option is used as before.
For Windows, the /DELAY option is used; this requires that the application call SetDllDirectory() before the first call to any CUDA API function in order to specify the directory containing the CUDA DLLs.
Note For Windows 8, SetDefaultDLLDirectories() and AddDllDirectory() should be used instead of SetDllDirectory() .
Please see the MSDN documentation for these routines for more information. 17. Deployment Infrastructure Tools  17.1.
Nvidia-SMI  The NVIDIA System Management Interface ( nvidia-smi ) is a command line utility that aids in the management and monitoring of NVIDIA GPU devices.
This utility allows administrators to query GPU device state and, with the appropriate privileges, permits administrators to modify GPU device state.
nvidia-smi is targeted at Tesla and certain Quadro GPUs, though limited support is also available on other NVIDIA GPUs.
nvidia-smi ships with NVIDIA GPU display drivers on Linux, and with 64-bit Windows Server 2008 R2 and Windows 7.
nvidia-smi can output queried information as XML or as human-readable plain text either to standard output or to a file.
Please note that new versions of nvidia-smi are not guaranteed to be backward-compatible with previous versions. 17.1.1. Queryable state  ECC error counts Both correctable single-bit and detectable double-bit errors are reported.
GPU utilization Current utilization rates are reported for both the compute resources of the GPU and the memory interface.
Active compute process The list of active processes running on the GPU is reported, along with the corresponding process name/ID and allocated GPU memory.
Clocks and performance state Max and current clock rates are reported for several important clock domains, as well as the current GPU performance state ( pstate ).
Temperature and fan speed The current GPU core temperature is reported, along with fan speeds for products with active cooling.
Power management The current board power draw and power limits are reported for products that report these measurements.
Identification Various dynamic and static information is reported, including board serial numbers, PCI device IDs, VBIOS/Inforom version numbers and product names. 17.1.2. Modifiable state  ECC mode Enable and disable ECC reporting.
Compute mode Indicate whether compute processes can run on the GPU and whether they run exclusively or concurrently with other compute processes.
Persistence mode Indicate whether the NVIDIA driver stays loaded when no applications are connected to the GPU.
GPU reset Reinitialize the GPU hardware and software state via a secondary bus reset. 17.2. NVML  The NVIDIA Management Library (NVML) is a C-based interface that provides direct access to the queries and commands exposed via nvidia-smi intended as a platform for building 3rd-party system management applications.
The NVML API is shipped with the CUDA Toolkit (since version 8.0) and is also available standalone on the NVIDIA developer website as part of the GPU Deployment Kit through a single header file accompanied by PDF documentation, stub libraries, and sample applications; see https: developer.nvidia.com/gpu-deployment-kit .
These bindings expose the same features as the C-based interface and also provide backwards compatibility.
All of these products ( nvidia-smi , NVML, and the NVML language bindings) are updated with each new CUDA release and provide roughly the same functionality.
See https: developer.nvidia.com/nvidia-management-library-nvml for additional information. 17.3. Cluster Management Tools  Managing your GPU cluster will help achieve maximum GPU utilization and help you and your users extract the best possible performance.
For a listing of some of these tools, see https: developer.nvidia.com/cluster-management . 17.4. Compiler JIT Cache Management Tools  Any PTX device code loaded by an application at runtime is compiled further to binary code by the device driver.
Just-in-time compilation increases application load time but allows applications to benefit from latest compiler improvements.
It is also the only way for applications to run on devices that did not exist at the time the application was compiled.
When JIT compilation of PTX device code is used, the NVIDIA driver caches the resulting binary code on disk.
Some aspects of this behavior such as cache location and maximum cache size can be controlled via the use of environment variables; see Just in Time Compilation of the CUDA C++ Programming Guide. 17.5. CUDA_VISIBLE_DEVICES  It is possible to rearrange the collection of installed CUDA devices that will be visible to and enumerated by a CUDA application prior to the start of that application by way of the CUDA_VISIBLE_DEVICES environment variable.
Devices to be made visible to the application should be included as a comma-separated list in terms of the system-wide list of enumerable devices.
For example, to use only devices 0 and 2 from the system-wide list of devices, set CUDA_VISIBLE_DEVICES=0,2 before launching the application.
The application will then enumerate these devices as device 0 and device 1, respectively. 18. Recommendations and Best Practices  This chapter contains a summary of the recommendations for optimization that are explained in this document.
18.1. Overall Performance Optimization Strategies  Performance optimization revolves around three basic strategies: Maximizing parallel execution Optimizing memory usage to achieve maximum memory bandwidth Optimizing instruction usage to achieve maximum instruction throughput Maximizing parallel execution starts with structuring the algorithm in a way that exposes as much parallelism as possible.
Once the parallelism of the algorithm has been exposed, it needs to be mapped to the hardware as efficiently as possible.
The application should also maximize parallel execution at a higher level by explicitly exposing concurrent execution on the device through streams, as well as maximizing concurrent execution between the host and the device.
Optimizing memory usage starts with minimizing data transfers between the host and the device because those transfers have much lower bandwidth than internal device data transfers.
Kernel access to global memory also should be minimized by maximizing the use of shared memory on the device.
Sometimes, the best optimization might even be to avoid any data transfer in the first place by simply recomputing the data whenever it is needed.
The effective bandwidth can vary by an order of magnitude depending on the access pattern for each type of memory.
The next step in optimizing memory usage is therefore to organize memory accesses according to the optimal memory access patterns.
This optimization is especially important for global memory accesses, because latency of access costs hundreds of clock cycles.
Shared memory accesses, in counterpoint, are usually worth optimizing only when there exists a high degree of bank conflicts.
As for optimizing instruction usage, the use of arithmetic instructions that have low throughput should be avoided.
This suggests trading precision for speed when it does not affect the end result, such as using intrinsics instead of regular functions or single precision instead of double precision.
Finally, particular attention must be paid to control flow instructions due to the SIMT (single instruction multiple thread) nature of the device. 19. nvcc Compiler Switches  19.1.
nvcc  The NVIDIA nvcc compiler driver converts .cu files into C++ for the host system and CUDA assembly or binary instructions for the device.
It supports a number of command-line parameters, of which the following are especially useful for optimization and related best practices: -maxrregcount=N specifies the maximum number of registers kernels can use at a per-file level.
(See also the __launch_bounds__ qualifier discussed in Execution Configuration of the CUDA C++ Programming Guide to control the number of registers used on a per-kernel basis.) --ptxas-options=-v or -Xptxas=-v lists per-kernel register, shared, and constant memory usage.
-ftz=true (denormalized numbers are flushed to zero) -prec-div=false (less precise division) -prec-sqrt=false (less precise square root) -use_fast_math compiler option of nvcc coerces every functionName() call to the equivalent __functionName() call. 20. Notices  20.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 20.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 20.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Maxwell Compatibility v12.5 | PDF | Archive Maxwell Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Maxwell Architecture.
About this Document  This application note, Maxwell Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on GPUs based on the NVIDIA ® Maxwell Architecture.
This document provides guidance to developers who are already familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Maxwell. 1.2. Application Compatibility on Maxwell  The NVIDIA CUDA C++ compiler, nvcc , can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel.
Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number .
For example, cubin files that target compute capability 3.0 are supported on all compute-capability 3.x (Kepler) devices but are not supported on compute-capability 5.x (Maxwell) devices.
For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels.
Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forward-compatibility purposes.
Applications that already include PTX versions of their kernels should work as-is on Maxwell-based GPUs.
Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Maxwell-compatible PTX or cubins. 1.3. Verifying Maxwell Compatibility for Existing Applications  The first step is to check that Maxwell-compatible device code (at least PTX) is compiled in to the application.
The following sections show how to accomplish this for applications built with different CUDA Toolkit versions. 1.3.1. Applications Using CUDA Toolkit 5.5 or Earlier  CUDA applications built using CUDA Toolkit versions 2.1 through 5.5 are compatible with Maxwell as long as they are built to include PTX versions of their kernels.
To test that PTX JIT is working for your application, you can do the following: Download and install the latest driver from https: www.nvidia.com/drivers .
When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JIT-compile the PTX for each CUDA kernel that is used into native cubin code.
If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Maxwell compatibility.
Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable when you are done testing. 1.3.2. Applications Using CUDA Toolkit 6.0 or Later  CUDA applications built using CUDA Toolkit 6.0 or Later 1 are compatible with Maxwell as long as they are built to include kernels in either Maxwell-native cubin format (see Building Applications with Maxwell Support ) or PTX format (see Applications Using CUDA Toolkit 5.5 or Earlier ) or both.
1.4. Building Applications with Maxwell Support  When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available.
If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime will load the PTX and JIT-compile that PTX to the GPU’s native cubin format before launching it.
The method used to build your application with either native cubin or at least PTX support for Maxwell depend on the version of the CUDA Toolkit used.
The main advantages of providing native cubins are as follows: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX.
All kernels compiled into the application must have native binaries at load time or else they will be built just-in-time from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application.
Especially when using large libraries, this JIT compilation can take a significant amount of time.
The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a given user, but it is time best avoided whenever possible.
PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled code may be faster or of greater accuracy. 1.4.1. Applications Using CUDA Toolkit 5.5 or Earlier  The compilers included in CUDA Toolkit 5.5 or earlier generate cubin files native to earlier NVIDIA architectures such as Fermi and Kepler, but they cannot generate cubin files native to the Maxwell architecture.
To allow support for Maxwell and future architectures when using version 5.5 or earlier of the CUDA Toolkit, the compiler must generate a PTX version of each kernel.
Below are compiler settings that could be used to build mykernel.cu to run on Fermi or Kepler devices natively and on Maxwell devices via PTX JIT.
The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version.
The code= clause specifies the back-end compilation target and can either be cubin or PTX or both.
Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one must be PTX to provide Maxwell compatibility.
Windows nvcc.exe -ccbin "C:\vs2010\VC\bin" -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT" -gencode=arch=compute_20,code=sm_20 -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_35,code=compute_35 --compile -o "Release\mykernel.cu.obj" "mykernel.cu" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_20,code=sm_20 -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_35,code=compute_35 -O2 -o mykernel.o -c mykernel.cu Alternatively, you may be familiar with the simplified nvcc command-line option -arch=sm_XX , which is a shorthand equivalent to the following more explicit -gencode= command-line options used above.
-arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. 1.4.2. Applications Using CUDA Toolkit 6.0 or Later  With version 6.0 of the CUDA Toolkit, nvcc can generate cubin files native to the first-generation Maxwell architecture (compute capability 5.0); CUDA Toolkit 6.5 and later further add native support for second-generation Maxwell devices (compute capability 5.2).
When using CUDA Toolkit 6.x or Later, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below.
Windows nvcc.exe -ccbin "C:\vs2010\VC\bin" -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT" -gencode=arch=compute_20,code=sm_20 -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_52,code=compute_52 --compile -o "Release\mykernel.cu.obj" "mykernel.cu" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_20,code=sm_20 -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_52,code=compute_52 -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version.
Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. 2. Revision History  Version 1.0 Initial public release.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2014-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Pascal Compatibility v12.5 | PDF | Archive Pascal Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Pascal Architecture.
About this Document  This application note, Pascal Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on GPUs based on the NVIDIA ® Pascal Architecture.
This document provides guidance to developers who are already familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Pascal. 1.2. Application Compatibility on Pascal  The NVIDIA CUDA C++ compiler, nvcc , can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel.
Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number .
For example, cubin files that target compute capability 3.0 are supported on all compute-capability 3.x (Kepler) devices but are not supported on compute-capability 5.x (Maxwell) or 6.x (Pascal) devices.
For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels.
Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forward-compatibility purposes.
Applications that already include PTX versions of their kernels should work as-is on Pascal-based GPUs.
Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Pascal-compatible PTX or cubins. 1.3. Verifying Pascal Compatibility for Existing Applications  The first step is to check that Pascal-compatible device code (at least PTX) is compiled in to the application.
The following sections show how to accomplish this for applications built with different CUDA Toolkit versions. 1.3.1. Applications Using CUDA Toolkit 7.5 or Earlier  CUDA applications built using CUDA Toolkit versions 2.1 through 7.5 are compatible with Pascal as long as they are built to include PTX versions of their kernels.
To test that PTX JIT is working for your application, you can do the following: Download and install the latest driver from https: www.nvidia.com/drivers .
When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JIT-compile the PTX for each CUDA kernel that is used into native cubin code.
If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Pascal compatibility.
Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable when you are done testing. 1.3.2. Applications Using CUDA Toolkit 8.0  CUDA applications built using CUDA Toolkit 8.0 are compatible with Pascal as long as they are built to include kernels in either Pascal-native cubin format (see Building Applications with Pascal Support ) or PTX format (see Applications Using CUDA Toolkit 7.5 or Earlier ) or both.
1.4. Building Applications with Pascal Support  When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available.
If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime will load the PTX and JIT-compile that PTX to the GPU’s native cubin format before launching it.
The method used to build your application with either native cubin or at least PTX support for Pascal depend on the version of the CUDA Toolkit used.
The main advantages of providing native cubins are as follows: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX.
All kernels compiled into the application must have native binaries at load time or else they will be built just-in-time from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application.
Especially when using large libraries, this JIT compilation can take a significant amount of time.
The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a given user, but it is time best avoided whenever possible.
PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled code may be faster or of greater accuracy. 1.4.1. Applications Using CUDA Toolkit 7.5 or Earlier  The compilers included in CUDA Toolkit 7.5 or earlier generate cubin files native to earlier NVIDIA architectures such as Kepler and Maxwell, but they cannot generate cubin files native to the Pascal architecture.
To allow support for Pascal and future architectures when using version 7.5 or earlier of the CUDA Toolkit, the compiler must generate a PTX version of each kernel.
Below are compiler settings that could be used to build mykernel.cu to run on Kepler or Maxwell devices natively and on Pascal devices via PTX JIT.
The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version.
The code= clause specifies the back-end compilation target and can either be cubin or PTX or both.
Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one must be PTX to provide Pascal compatibility.
Windows nvcc.exe -ccbin "C:\vs2010\VC\bin" -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT" -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_52,code=compute_52 --compile -o "Release\mykernel.cu.obj" "mykernel.cu" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_52,code=compute_52 -O2 -o mykernel.o -c mykernel.cu Alternatively, you may be familiar with the simplified nvcc command-line option -arch=sm_XX , which is a shorthand equivalent to the following more explicit -gencode= command-line options used above.
-arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. 1.4.2. Applications Using CUDA Toolkit 8.0  With version 8.0 of the CUDA Toolkit, nvcc can generate cubin files native to the Pascal architectures (compute capability 6.0 and 6.1).
When using CUDA Toolkit 8.0, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below.
Windows nvcc.exe -ccbin "C:\vs2010\VC\bin" -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT" -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 --compile -o "Release\mykernel.cu.obj" "mykernel.cu" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version.
Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. 2. Revision History  Version 1.0 Initial public release.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2016-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Volta Compatibility v12.5 | PDF | Archive Volta Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Volta Architecture.
About this Document  This application note, Volta Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on GPUs based on the NVIDIA ® Volta Architecture.
This document provides guidance to developers who are already familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Volta. 1.2. Application Compatibility on Volta  The NVIDIA CUDA C++ compiler, nvcc , can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel.
Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number .
For example, cubin files that target compute capability 3.0 are supported on all compute-capability 3.x (Kepler) devices but are not supported on compute-capability 5.x (Maxwell) or 6.x (Pascal) devices.
For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels.
Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forward-compatibility purposes.
Applications that already include PTX versions of their kernels should work as-is on Volta-based GPUs.
Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Volta-compatible PTX or cubins. 1.3. Verifying Volta Compatibility for Existing Applications  The first step is to check that Volta-compatible device code (at least PTX) is compiled into the application.
The following sections show how to accomplish this for applications built with different CUDA Toolkit versions. 1.3.1. Applications Using CUDA Toolkit 8.0 or Earlier  CUDA applications built using CUDA Toolkit versions 2.1 through 8.0 are compatible with Volta as long as they are built to include PTX versions of their kernels.
To test that PTX JIT is working for your application, you can do the following: Download and install the latest driver from http: www.nvidia.com/drivers .
When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JIT-compile the PTX for each CUDA kernel that is used into native cubin code.
If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Volta compatibility.
Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable when you are done testing. 1.3.2. Applications Using CUDA Toolkit 9.0  CUDA applications built using CUDA Toolkit 9.0 are compatible with Volta as long as they are built to include kernels in either Volta-native cubin format (see Building Applications with Volta Support ) or PTX format (see Applications Using CUDA Toolkit 8.0 or Earlier ) or both.
1.4. Building Applications with Volta Support  When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available.
If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime will load the PTX and JIT-compile that PTX to the GPU’s native cubin format before launching it.
The method used to build your application with either native cubin or at least PTX support for Volta depend on the version of the CUDA Toolkit used.
The main advantages of providing native cubins are as follows: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX.
All kernels compiled into the application must have native binaries at load time or else they will be built just-in-time from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application.
Especially when using large libraries, this JIT compilation can take a significant amount of time.
The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a given user, but it is time best avoided whenever possible.
PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled code may be faster or of greater accuracy. 1.4.1. Applications Using CUDA Toolkit 8.0 or Earlier  The compilers included in CUDA Toolkit 8.0 or earlier generate cubin files native to earlier NVIDIA architectures such as Maxwell and Pascal, but they cannot generate cubin files native to the Volta architecture.
To allow support for Volta and future architectures when using version 8.0 or earlier of the CUDA Toolkit, the compiler must generate a PTX version of each kernel.
Below are compiler settings that could be used to build mykernel.cu to run on Maxwell or Pascal devices natively and on Volta devices via PTX JIT.
The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version.
The code= clause specifies the back-end compilation target and can either be cubin or PTX or both.
Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one must be PTX to provide Volta compatibility.
Windows nvcc.exe -ccbin "C:\vs2010\VC\bin" -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT" -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 --compile -o "Release\mykernel.cu.obj" "mykernel.cu" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 -O2 -o mykernel.o -c mykernel.cu Alternatively, you may be familiar with the simplified nvcc command-line option -arch=sm_XX , which is a shorthand equivalent to the following more explicit -gencode= command-line options used above.
-arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. 1.4.2. Applications Using CUDA Toolkit 9.0  With version 9.0 of the CUDA Toolkit, nvcc can generate cubin files native to the Volta architecture (compute capability 7.0).
When using CUDA Toolkit 9.0, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below.
Windows nvcc.exe -ccbin "C:\vs2010\VC\bin" -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT" -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 --compile -o "Release\mykernel.cu.obj" "mykernel.cu" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version.
Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures.
Any compute_2x and sm_2x flags need to be removed from your compiler commands. 1.4.3. Independent Thread Scheduling Compatibility  The Volta architecture introduces Independent Thread Scheduling among threads in a warp.
If the developer made assumptions about warp-synchronicity, 1 this feature can alter the set of threads participating in the executed code compared to previous architectures.
Please see Compute Capability 7.0 in the CUDA C++ Programming Guide for details and corrective actions.
To aid migration Volta developers can opt-in to the Pascal scheduling model with the following combination of compiler options.
nvcc -arch=compute_60 -code=sm_70 ... 2. Revision History  Version 1.0 Initial public release.
Version 1.1 Use CUDA C++ instead of CUDA C/C++ Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide. 3. Notices  3.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
1 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2017-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Turing Compatibility v12.5 | PDF | Archive Turing Compatibility Guide for CUDA Applications The guide to building CUDA applications for NVIDIA Turing GPUs.
About this Document  This application note, Turing Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on GPUs based on the NVIDIA ® Turing Architecture.
This document provides guidance to developers who are already familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Turing. 1.2. Application Compatibility on Turing  The NVIDIA CUDA C++ compiler, nvcc , can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel.
Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number .
For example, cubin files that target compute capability 3.0 are supported on all compute-capability 3.x (Kepler) devices but are not supported on compute-capability 5.x (Maxwell) or 6.x (Pascal) devices.
For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels.
Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forward-compatibility purposes.
Applications that already include PTX versions of their kernels should work as-is on Turing-based GPUs.
Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Turing-compatible PTX or cubins. 1.3. Compatibility between Volta and Turing  The Turing architecture is based on Volta’s Instruction Set Architecture ISA 7.0, extending it with new instructions.
As a consequence, any binary that runs on Volta will be able to run on Turing (forward compatibility), but a Turing binary will not be able to run on Volta.
Please note that Volta kernels using more than 64KB of shared memory (via the explicit opt-in, see CUDA C++ Programming Guide ) will not be able to launch on Turing, as they would exceed Turing’s shared memory capacity.
Most applications compiled for Volta should run efficiently on Turing, except if the application uses heavily the Tensor Cores, or if recompiling would allow use of new Turing-specific instructions.
Recompiling explicitly for Turing is thus recommended. 1.4. Verifying Turing Compatibility for Existing Applications  The first step is to check that Turing-compatible device code (at least PTX) is compiled into the application.
The following sections show how to accomplish this for applications built with different CUDA Toolkit versions. 1.4.1. Applications Using CUDA Toolkit 8.0 or Earlier  CUDA applications built using CUDA Toolkit versions 2.1 through 8.0 are compatible with Turing as long as they are built to include PTX versions of their kernels.
To test that PTX JIT is working for your application, you can do the following: Download and install the latest driver from https: www.nvidia.com/drivers .
When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JIT-compile the PTX for each CUDA kernel that is used into native cubin code.
If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Turing compatibility.
Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable when you are done testing. 1.4.2. Applications Using CUDA Toolkit 9.x  CUDA applications built using CUDA Toolkit 9.x are compatible with Turing as long as they are built to include kernels in either Volta-native cubin format (see Compatibility between Volta and Turing ) or PTX format (see Applications Using CUDA Toolkit 8.0 or Earlier ) or both.
1.4.3. Applications Using CUDA Toolkit 10.0  CUDA applications built using CUDA Toolkit 10.0 are compatible with Turing as long as they are built to include kernels in Volta-native or Turing-native cubin format (see Compatibility between Volta and Turing ), or PTX format (see Applications Using CUDA Toolkit 8.0 or Earlier ), or both.
1.5. Building Applications with Turing Support  When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available.
If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime will load the PTX and JIT-compile that PTX to the GPU’s native cubin format before launching it.
The method used to build your application with either native cubin or at least PTX support for Turing depend on the version of the CUDA Toolkit used.
The main advantages of providing native cubins are as follows: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX.
All kernels compiled into the application must have native binaries at load time or else they will be built just-in-time from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application.
Especially when using large libraries, this JIT compilation can take a significant amount of time.
The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a given user, but it is time best avoided whenever possible.
PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled code may be faster or of greater accuracy. 1.5.1. Applications Using CUDA Toolkit 8.0 or Earlier  The compilers included in CUDA Toolkit 8.0 or earlier generate cubin files native to earlier NVIDIA architectures such as Maxwell and Pascal, but they cannot generate cubin files native to Volta or Turing architecture.
To allow support for Volta, Turing and future architectures when using version 8.0 or earlier of the CUDA Toolkit, the compiler must generate a PTX version of each kernel.
Below are compiler settings that could be used to build mykernel.cu to run on Maxwell or Pascal devices natively and on Turing devices via PTX JIT.
The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version.
The code= clause specifies the back-end compilation target and can either be cubin or PTX or both.
Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one must be PTX to provide Turing compatibility.
Windows nvcc.exe -ccbin "C:\vs2010\VC\bin" -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT" -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 --compile -o "Release\mykernel.cu.obj" "mykernel.cu" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 -O2 -o mykernel.o -c mykernel.cu Alternatively, you may be familiar with the simplified nvcc command-line option -arch=sm_XX , which is a shorthand equivalent to the following more explicit -gencode= command-line options used above.
-arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. 1.5.2. Applications Using CUDA Toolkit 9.x  With versions 9.x of the CUDA Toolkit, nvcc can generate cubin files native to the Volta architecture (compute capability 7.0).
When using CUDA Toolkit 9.x, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below.
Windows nvcc.exe -ccbin "C:\vs2010\VC\bin" -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT" -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 --compile -o "Release\mykernel.cu.obj" "mykernel.cu" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version.
Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures.
Any compute_2x and sm_2x flags need to be removed from your compiler commands. 1.5.3. Applications Using CUDA Toolkit 10.0  With version 10.0 of the CUDA Toolkit, nvcc can generate cubin files native to the Turing architecture (compute capability 7.5).
When using CUDA Toolkit 10.0, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below.
If the developer made assumptions about warp-synchronicity, 1 this feature can alter the set of threads participating in the executed code compared to previous architectures.
Please see Compute Capability 7.0 in the CUDA C++ Programming Guide for details and corrective actions.
To aid migration Volta and Turing developers can opt-in to the Pascal scheduling model with the following combination of compiler options.
nvcc -arch=compute_60 -code=sm_70 ... 2. Revision History  Version 1.0 Initial public release.
Version 1.1 Use CUDA C++ instead of CUDA C/C++ Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide. 3. Notices  3.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
1 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2024, NVIDIA Corporation.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
NVIDIA Ampere GPU Architecture Compatibility v12.5 | PDF | Archive NVIDIA Ampere GPU Architecture Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Ampere GPU Architecture.
About this Document  This application note, NVIDIA Ampere GPU Architecture Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on the NVIDIA ® Ampere Architecture based GPUs.
This document provides guidance to developers who are familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with the NVIDIA Ampere GPU architecture. 1.2. Application Compatibility on the NVIDIA Ampere GPU Architecture  A CUDA application binary (with one or more GPU kernels) can contain the compiled GPU code in two forms, binary cubin objects and forward-compatible PTX assembly for each kernel.
A cubin generated for a certain compute capability is supported to run on any GPU with the same major revision and same or higher minor revision of compute capability.
For example, a cubin generated for compute capability 7.0 is supported to run on a GPU with compute capability 7.5, however a cubin generated for compute capability 7.5 is not supported to run on a GPU with compute capability 7.0, and a cubin generated with compute capability 7.x is not supported to run on a GPU with compute capability 8.x.
At the application load time, PTX is compiled to cubin and the cubin is used for kernel execution.
Meaning PTX is supported to run on any GPU with compute capability higher than the compute capability assumed for generation of that PTX.
For example, PTX code generated for compute capability 7.x is supported to run on compute capability 7.x or any higher revision (major or minor), including compute capability 8.x.
Therefore although it is optional, it is recommended that all applications should include PTX of the kernels to ensure forward-compatibility.
To read more about cubin and PTX compatibilities see Compilation with NVCC from the Programming Guide.
When a CUDA application launches a kernel on a GPU, the CUDA Runtime determines the compute capability of the GPU in the system and uses this information to find the best matching cubin or PTX version of the kernel.
If a cubin compatible with that GPU is present in the binary, the cubin is used as-is for execution.
Otherwise, the CUDA Runtime first generates compatible cubin by JIT-compiling 1 the PTX and then the cubin is used for the execution.
Application binaries that include PTX version of kernels, should work as-is on the NVIDIA Ampere architecture based GPUs.
However application binaries which do not include PTX (only include cubins), need to be rebuilt to run on the NVIDIA Ampere architecture based GPUs.
To know more about building compatible applications read Building Applications with the NVIDIA Ampere GPU Architecture Support . 1.3. Verifying Ampere Compatibility for Existing Applications  The first step towards making a CUDA application compatible with the NVIDIA Ampere GPU architecture is to check if the application binary already contains compatible GPU code (at least the PTX).
The following sections explain how to accomplish this for an already built CUDA application. 1.3.1. Applications Built Using CUDA Toolkit 10.2 or Earlier  CUDA applications built using CUDA Toolkit versions 2.1 through 10.2 are compatible with NVIDIA Ampere architecture based GPUs as long as they are built to include PTX versions of their kernels.
This can be tested by forcing the PTX to JIT-compile at application load time with following the steps: Download and install the latest driver from https: www.nvidia.com/drivers .
This means the application is not compatible with the NVIDIA Ampere GPU architecture and needs to be rebuilt for compatibility.
On the other hand, if the application works properly with this environment variable set, then the application is compatible with the NVIDIA Ampere GPU architecture.
Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable after testing is done. 1.3.2. Applications Built Using CUDA Toolkit 11.0  CUDA applications built using CUDA Toolkit 11.0 are compatible with the NVIDIA Ampere GPU architecture as long as they are built to include kernels in native cubin (compute capability 8.0) or PTX form or both.
1.4. Building Applications with the NVIDIA Ampere GPU Architecture Support  Depending on the version of the CUDA Toolkit used for building the application, it can be built to include PTX and/or native cubin for the NVIDIA Ampere GPU architecture.
Although it is enough to just include PTX, including native cubin also has the following advantages: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX.
All kernels which do not have native cubins are JIT-compiled from PTX, including kernels from all the libraries linked to the application, even if those kernels are never launched by the application.
Especially when using large libraries, this JIT compilation can take a significant amount of time.
The CUDA driver caches the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a user, but it is time best avoided whenever possible.
PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled cubins may be faster or of greater accuracy. 1.4.1. Building Applications Using CUDA Toolkit 10.x or Earlier  The nvcc compiler included with versions 10.x (10.0, 10.1 and 10.2) of the CUDA Toolkit can generate cubins native to the Volta and Turing architectures (compute capability 7.x).
When using CUDA Toolkit 10.x, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below.
Windows nvcc.exe -ccbin "C:\vs2010\VC\bin" -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT" -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 --compile -o "Release\mykernel.cu.obj" "mykernel.cu" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 -O2 -o mykernel.o -c mykernel.cu Alternatively, the simplified nvcc command-line option -arch=sm_XX can be used.
It is a shorthand equivalent to the following more explicit -gencode= command-line options used above.
-arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target binary by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly.
For CUDA toolkits prior to 10.0, one or more of the -gencode options will need to be removed according to the architectures supported by the specific toolkit version (for example, CUDA toolkit 9.x supports architectures up to _60 and _61).
The final -gencode to generate PTX would also need to be update – for further information and examples see the documentation for the specific CUDA toolkit version.
The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version.
The code= clause specifies the back-end compilation target and can either be cubin or PTX or both.
Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. 1.4.2. Building Applications Using CUDA Toolkit 11.0  With versions 11.0 of the CUDA Toolkit, nvcc can generate cubin native to the NVIDIA Ampere GPU architecture (compute capability 8.0).
When using CUDA Toolkit 11.0, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below.
If the developer made assumptions about warp-synchronicity 2 , this feature can alter the set of threads participating in the executed code compared to previous architectures.
Please see Compute Capability 7.0 in the Programming Guide for details and corrective actions.
To aid migration to the NVIDIA Ampere GPU architecture, developers can opt-in to the Pascal scheduling model with the following combination of compiler options.
nvcc -gencode=arch=compute_60,code=sm_80 ... 2. Revision History  Version 1.0 Initial public release.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
1 Just-in-time compilation 2 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2020-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Hopper Architecture Compatibility v12.5 | PDF | Archive Hopper Compatibility Guide for CUDA Applications The guide to building CUDA applications for Hopper GPUs 1.
About this Document  This application note, Hopper Architecture Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on the NVIDIA ® Hopper architecture based GPUs.
This document provides guidance to developers who are familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Hopper architecture. 1.2. Application Compatibility on Hopper Architecture  A CUDA application binary (with one or more GPU kernels) can contain the compiled GPU code in two forms, binary cubin objects and forward-compatible PTX assembly for each kernel.
A cubin generated for a certain compute capability is supported to run on any GPU with the same major revision and same or higher minor revision of compute capability.
For example, a cubin generated for compute capability 8.0 is supported to run on a GPU with compute capability 8.6, however a cubin generated for compute capability 8.6 is not supported to run on a GPU with compute capability 8.0, and a cubin generated with compute capability 8.x is not supported to run on a GPU with compute capability 9.0.
At the application load time, PTX is compiled to cubin and the cubin is used for kernel execution.
Meaning PTX is supported to run on any GPU with compute capability higher than the compute capability assumed for generation of that PTX.
For example, PTX code generated for compute capability 8.x is supported to run on compute capability 8.x or any higher revision (major or minor), including compute capability 9.0.
Therefore although it is optional, it is recommended that all applications should include PTX of the kernels to ensure forward-compatibility.
To read more about cubin and PTX compatibilities see Compilation with NVCC from the CUDA C++ Programming Guide .
When a CUDA application launches a kernel on a GPU, the CUDA Runtime determines the compute capability of the GPU in the system and uses this information to find the best matching cubin or PTX version of the kernel.
If a cubin compatible with that GPU is present in the binary, the cubin is used as-is for execution.
Otherwise, the CUDA Runtime first generates compatible cubin by JIT-compiling 1 the PTX and then the cubin is used for the execution.
Application binaries that include PTX version of kernels, should work as-is on the Hopper GPUs.
However application binaries which do not include PTX (only include cubins), need to be rebuilt to run on the Hopper GPUs.
To know more about building compatible applications read Building Applications with Hopper Architecture Support Application binaries that include PTX version of kernels with architecture conditional features using sm_90a or compute_90a in order to take full advantage of Hopper GPU architecture, are not forward or backward compatible. 1.3. Verifying Hopper Compatibility for Existing Applications  The first step towards making a CUDA application compatible with Hopper architecture is to check if the application binary already contains compatible GPU code (at least the PTX).
The following sections explain how to accomplish this for an already built CUDA application. 1.3.1. Applications Built Using CUDA Toolkit 11.7 or Earlier  CUDA applications built using CUDA Toolkit versions 2.1 through 11.7 are compatible with Hopper GPUs as long as they are built to include PTX versions of their kernels.
This can be tested by forcing the PTX to JIT-compile at application load time with following the steps: Download and install the latest driver from https: www.nvidia.com/drivers .
This means the application is not Hopper architecture compatible and needs to be rebuilt for compatibility.
On the other hand, if the application works properly with this environment variable set, then the application is Hopper compatible.
Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable after testing is done. 1.3.2. Applications Built Using CUDA Toolkit 11.8  CUDA applications built using CUDA Toolkit 11.8 are compatible with Hopper architecture as long as they are built to include kernels in native cubin (compute capability 9.0) or PTX form or both.
1.4. Building Applications with Hopper Architecture Support  Depending on the version of the CUDA Toolkit used for building the application, it can be built to include PTX and/or native cubin for the Hopper architecture.
Although it is enough to just include PTX, including native cubin also has the following advantages: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX.
All kernels which do not have native cubins are JIT-compiled from PTX, including kernels from all the libraries linked to the application, even if those kernels are never launched by the application 2 .
Especially when using large libraries, this JIT compilation can take a significant amount of time.
The CUDA driver caches the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a user, but it is time best avoided whenever possible.
PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled cubins may be faster or of greater accuracy.
PTX code compiled to target architecture conditional features using sm_90a or compute_90a only runs on devices with compute capability 9.0 and is not backward or forward compatible. 1.4.1. Building Applications Using CUDA Toolkit 11.7 or Earlier  The nvcc compiler included with version 11.7 or earlier (11.0-11.7) of the CUDA Toolkit can generate cubins native to the NVIDIA Ampere GPU architectures (compute capability 8.x).
When using CUDA Toolkit 11.7 or earlier, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below.
Windows nvcc.exe -ccbin "C:\vs2010\VC\bin" -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT" -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 --compile -o "Release\mykernel.cu.obj" "mykernel.cu" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -O2 -o mykernel.o -c mykernel.cu Alternatively, the simplified nvcc command-line option -arch=sm_XX can be used.
It is a shorthand equivalent to the following more explicit -gencode= command-line options used above.
-arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target binary by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly.
For CUDA toolkits prior to 11.0, one or more of the -gencode options need to be removed according to the architectures supported by the specific toolkit version (for example, CUDA toolkit 10.x supports architectures up to sm_72 and sm_75).
For further information and examples see the documentation for the specific CUDA toolkit version.
The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version.
The code= clause specifies the back-end compilation target and can either be cubin or PTX or both.
Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. 1.4.2. Building Applications Using CUDA Toolkit 11.8  With versions 11.8 of the CUDA Toolkit, nvcc can generate cubin native to the Hopper architecture (compute capability 9.0).
When using CUDA Toolkit 11.8, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below.
If the developer made assumptions about warp-synchronicity 3 , this feature can alter the set of threads participating in the executed code compared to previous architectures.
Please see Compute Capability 7.x in the CUDA C++ Programming Guide for details and corrective actions.
To aid migration to the Hopper architecture, developers can opt-in to the Pascal scheduling model with the following combination of compiler options.
nvcc -gencode=arch=compute_60,code=sm_90 ... 2. Revision History  Version 1.0 Initial public release.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
2 Starting with CUDA toolkit 11.8, this default behavior can be changed with environment variable CUDA_MODULE_LOADING.
3 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
NVIDIA Ada GPU Architecture Compatibility v12.5 | PDF | Archive NVIDIA Ada GPU Architecture Compatibility Guide for CUDA Applications The guide to building CUDA applications for NVIDIA Ada GPUs.
About this Document  This application note, NVIDIA Ada GPU Architecture Compatibility Guide for CUDA Applications , is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on the NVIDIA ® Ada Architecture based GPUs.
This document provides guidance to developers who are familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with the NVIDIA Ada GPU architecture. 1.2. Application Compatibility on the NVIDIA Ada GPU Architecture  A CUDA application binary (with one or more GPU kernels) can contain the compiled GPU code in two forms, binary cubin objects and forward-compatible PTX assembly for each kernel.
A cubin generated for a certain compute capability is supported to run on any GPU with the same major revision and same or higher minor revision of compute capability.
For example, a cubin generated for compute capability 8.6 is supported to run on a GPU with compute capability 8.9; however, a cubin generated for compute capability 8.9 is not supported to run on a GPU with compute capability 8.6, and a cubin generated with compute capability 8.x is not supported to run on a GPU with compute capability 9.0.
At the application load time, PTX is compiled to cubin and the cubin is used for kernel execution.
Meaning PTX is supported to run on any GPU with compute capability higher than the compute capability assumed for generation of that PTX.
For example, PTX code generated for compute capability 8.x is supported to run on compute capability 8.x or any higher revision (major or minor), including compute capability 9.x.
Therefore, although it is optional, it is recommended that all applications should include PTX of the kernels to ensure forward-compatibility.
To read more about cubin and PTX compatibilities see Compilation with NVCC from the CUDA C++ Programming Guide .
When a CUDA application launches a kernel on a GPU, the CUDA Runtime determines the compute capability of the GPU in the system and uses this information to find the best matching cubin or PTX version of the kernel.
If a cubin compatible with that GPU is present in the binary, the cubin is used as-is for execution.
Otherwise, the CUDA Runtime first generates compatible cubin by JIT-compiling 1 the PTX and then the cubin is used for the execution.
Application binaries that include PTX version of kernels should work as-is on the NVIDIA Ada architecture based GPUs.
However, application binaries that do not include PTX (only include cubins) need to be rebuilt to run on the NVIDIA Ada architecture based GPUs.
To know more about building compatible applications, read Building Applications with the NVIDIA Ada GPU Architecture Support . 1.3. Compatibility between Ampere and Ada  The NVIDIA Ada architecture is based on Ampere’s Instruction Set Architecture ISA 8.0, extending it with new instructions.
As a consequence, any binary that runs on Ampere will be able to run on Ada (forward compatibility), but an Ada binary will not be able to run on Ampere. 1.4. Verifying Ada Compatibility for Existing Applications  The first step towards making a CUDA application compatible with the NVIDIA Ada GPU architecture is to check if the application binary already contains compatible GPU code (at least the PTX).
The following sections explain how to accomplish this for an already built CUDA application. 1.4.1. Applications Built Using CUDA Toolkit 10.2 or Earlier  CUDA applications built using CUDA Toolkit versions 2.1 through 10.2 are compatible with NVIDIA Ada architecture based GPUs as long as they are built to include PTX versions of their kernels.
This can be tested by forcing the PTX to JIT-compile at application load time with following the steps: Download and install the latest driver from https: www.nvidia.com/drivers .
This means the application is not compatible with the NVIDIA Ada GPU architecture and needs to be rebuilt for compatibility.
On the other hand, if the application works properly with this environment variable set, then the application is compatible with the NVIDIA Ada GPU architecture.
Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable after testing is done. 1.4.2. Applications Built Using CUDA Toolkit 11.0 through 11.7  CUDA applications built using CUDA Toolkit 11.0 through 11.7 are compatible with the NVIDIA Ada GPU architecture as long as they are built to include kernels in Ampere-native cubin (see Compatibility between Ampere and Ada ) or PTX format (see Applications Built Using CUDA Toolkit 10.2 or Earlier ), or both.
1.4.3. Applications Built Using CUDA Toolkit 11.8  CUDA applications built using CUDA Toolkit 11.8 are compatible with the NVIDIA Ada GPU architecture as long as they are built to include kernels in Ampere-native or Ada-native cubin (see Compatibility between Ampere and Ada ), or PTX format (see Applications Built Using CUDA Toolkit 10.2 or Earlier ), or both.
1.5. Building Applications with the NVIDIA Ada GPU Architecture Support  Depending on the version of the CUDA Toolkit used for building the application, it can be built to include PTX and/or native cubin for the NVIDIA Ada GPU architecture.
Although it is sufficient to just include PTX, including native cubin also has the following advantages: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX.
All kernels that do not have native cubins are JIT-compiled from PTX, including kernels from all the libraries linked to the application, even if those kernels are never launched by the application.
Especially when using large libraries, this JIT compilation can take a significant amount of time.
The CUDA driver caches the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a user, but it is time best avoided whenever possible.
PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled cubins may be faster or of greater accuracy. 1.5.1. Building Applications Using CUDA Toolkit 10.x or Earlier  The nvcc compiler included with versions 10.x (10.0, 10.1 and 10.2) of the CUDA Toolkit can generate cubins native to the Volta and Turing architectures (compute capability 7.x).
When using CUDA Toolkit 10.x, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below.
Windows nvcc.exe -ccbin "C:\vs2010\VC\bin" -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT" -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 --compile -o "Release\mykernel.cu.obj" "mykernel.cu" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 -O2 -o mykernel.o -c mykernel.cu Alternatively, the simplified nvcc command-line option -arch=sm_XX can be used.
It is a shorthand equivalent to the following more explicit -gencode= command-line options used above.
-arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target binary by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly.
For CUDA toolkits prior to 10.0, one or more of the -gencode options will need to be removed according to the architectures supported by the specific toolkit version (for example, CUDA toolkit 9.x supports architectures up to _60 and _61).
For further information and examples, see the documentation for the specific CUDA toolkit version.
The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version.
The code= clause specifies the back-end compilation target and can either be cubin or PTX, or both.
Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. 1.5.2. Building Applications Using CUDA Toolkit 11.0 through 11.7  The nvcc compiler included with versions 11.0 through 11.7 of the CUDA Toolkit can generate cubins native to the Ampere architecture (compute capability 8.0 and 8.6).
When using CUDA Toolkit 11.0 through 11.7, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below.
Windows nvcc.exe -ccbin "C:\vs2010\VC\bin" -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT" -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 --compile -o "Release\mykernel.cu.obj" "mykernel.cu" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -O2 -o mykernel.o -c mykernel.cu Alternatively, the simplified nvcc command-line option -arch=sm_XX can be used.
For CUDA toolkits prior to 11.0, one or more of the -gencode options need to be removed according to the architectures supported by the specific toolkit version (for example, CUDA toolkit 10.x supports architectures up to _72 and _75).
The final -gencode to generate PTX also needs to be updated. 1.5.3. Building Applications Using CUDA Toolkit 11.8  With version 11.8 of the CUDA Toolkit, nvcc can generate cubin native to the NVIDIA Ada GPU architecture (compute capability 8.9).
When using CUDA Toolkit 11.8, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below.
If the developer made assumptions about warp-synchronicity 2 , this feature can alter the set of threads participating in the executed code compared to previous architectures.
Please see Compute Capability 7.x in the CUDA C++ Programming Guide for details and corrective actions.
To aid migration to the NVIDIA Ada GPU architecture, developers can opt-in to the Pascal scheduling model with the following combination of compiler options.
nvcc -gencode=arch=compute_60,code=sm_89 ... 2. Revision History  Version 1.0 Initial public release.
1 Just-in-time compilation 2 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization. 3. Notices  3.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Maxwell Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for Maxwell The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Maxwell Architecture.
NVIDIA Maxwell Compute Architecture  Maxwell is NVIDIA’s next-generation architecture for CUDA compute applications.
Maxwell retains and extends the same CUDA programming model as in previous NVIDIA architectures such as Fermi and Kepler, and applications that follow the best practices for those architectures should typically see speedups on the Maxwell architecture without any code changes.
This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging Maxwell architectural features.
1 Maxwell introduces an all-new design for the Streaming Multiprocessor ( SM ) that dramatically improves energy efficiency.
Although the Kepler SMX design was extremely efficient for its generation, through its development, NVIDIA’s GPU architects saw an opportunity for another big leap forward in architectural efficiency; the Maxwell SM is the realization of that vision.
Improvements to control logic partitioning, workload balancing, clock-gating granularity, compiler-based scheduling, number of instructions issued per clock cycle, and many other enhancements allow the Maxwell SM (also called SMM ) to far exceed Kepler SMX efficiency.
The first Maxwell-based GPU is codenamed GM107 and is designed for use in power-limited environments like notebooks and small form factor (SFF) PCs.
GM107 is described in a whitepaper entitled NVIDIA GeForce GTX 750 Ti: Featuring First-Generation Maxwell GPU Technology, Designed for Extreme Performance per Watt .
Second-generation Maxwell GPUs retain the power efficiency of the earlier generation while delivering significantly higher performance.
GM204 is described in a whitepaper entitled NVIDIA GeForce GTX 980: Featuring Maxwell, The Most Advanced GPU Ever Made .
Compute programming features of GM204 are similar to those of GM107, except where explicitly noted in this guide.
For details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures.
Programmers must primarily focus on following those recommendations to achieve the best performance.
The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the Maxwell Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Maxwell.
SMM  The Maxwell Streaming Multiprocessor, SMM, is similar in many respects to the Kepler architecture’s SMX.
The key enhancements of SMM over SMX are geared toward improving efficiency without requiring significant increases in available parallelism per SM from the application. 1.4.1.1. Occupancy  The maximum number of concurrent warps per SMM remains the same as in SMX (i.e., 64), and factors influencing warp occupancy remain similar or improved over SMX: The register file size (64k 32-bit registers) is the same as that of SMX.
As with Kepler, experimentation should be used to determine the optimum balance of register spilling vs.
This should result in an automatic occupancy improvement for kernels with small thread blocks of 64 or fewer threads (shared memory and register file resource requirements permitting).
As such, developers can expect similar or improved occupancy on SMM without changes to their application.
At the same time, warp occupancy requirements (i.e., available parallelism) for maximum device utilization are similar to or less than those of SMX (see Instruction Latencies ). 1.4.1.2. Instruction Scheduling  The number of CUDA Cores per SM has been reduced to a power of two, however with Maxwell’s improved execution efficiency, performance per SM is usually within 10% of Kepler performance, and the improved area efficiency of SMM means CUDA Cores per GPU will be substantially higher vs.
SMM retains the same number of instruction issue slots per clock and reduces arithmetic latencies compared to the Kepler design.
Unlike SMX, however, all SMM core functional units are assigned to a particular scheduler, with no shared units.
Along with the selection of a power-of-two number of CUDA Cores per SM, which simplifies scheduling and reduces stall cycles, this partitioning of SM computational resources in SMM is a major component of the streamlined efficiency of SMM.
The power-of-two number of CUDA Cores per partition simplifies scheduling, as each of SMM’s warp schedulers issue to a dedicated set of CUDA Cores equal to the warp width.
Each warp scheduler still has the flexibility to dual-issue (such as issuing a math operation to a CUDA Core in the same cycle as a memory operation to a load/store unit), but single-issue is now sufficient to fully utilize all CUDA Cores. 1.4.1.3. Instruction Latencies  Another major improvement of SMM is that dependent math latencies have been significantly reduced; a consequence of this is a further reduction of stall cycles, as the available warp-level parallelism (i.e., occupancy) on SMM should be equal to or greater than that of SMX (see Occupancy ), while at the same time each math operation takes less time to complete, improving utilization and throughput.
1.4.1.4. Instruction Throughput  The most significant changes to peak instruction throughputs in SMM are as follows: The change in number of CUDA Cores per SM brings with it a corresponding change in peak single-precision floating point operations per clock per SM.
However, since the number of SMs is typically increased, the result is an increase in aggregate peak throughput; furthermore, the scheduling and latency improvements also discussed above make this peak easier to approach.
The throughput of many integer operations including multiply, logical operations and shift is improved.
In addition, there are now specialized integer instructions that can accelerate pointer arithmetic.
Note As was already the recommended best practice, signed arithmetic should be preferred over unsigned arithmetic wherever possible for best throughput on SMM.
The C language standard places more restrictions on overflow behavior for unsigned math, limiting compiler optimization opportunities. 1.4.2. Memory Throughput  1.4.2.1.
Unified L1/Texture Cache  Maxwell combines the functionality of the L1 and texture caches into a single unit.
As with Kepler, global loads in Maxwell are cached in L2 only, unless using the LDG read-only data cache mechanism introduced in Kepler.
In a manner similar to Kepler GK110B, GM204 retains this behavior by default but also allows applications to opt-in to caching of global loads in its unified L1/Texture cache.
The opt-in mechanism is the same as with GK110B: pass the -Xptxas -dlcm=ca flag to nvcc at compile time.
Local loads also are cached in L2 only, which could increase the cost of register spilling if L1 local load hit rates were high with Kepler.
The balance of occupancy versus spilling should therefore be reevaluated to ensure best performance.
Especially given the improvements to arithmetic latencies, code built for Maxwell may benefit from somewhat lower occupancy (due to increased registers per thread) in exchange for lower spilling.
The unified L1/texture cache acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp.
Two new device attributes were added in CUDA Toolkit 6.0: globalL1CacheSupported and localL1CacheSupported .
Developers who wish to have separately-tuned paths for various architecture generations can use these fields to simplify the path selection process.
If per-thread-block SM resource usage would result in zero occupancy with caching enabled, the CUDA driver will override the caching selection to allow the kernel launch to succeed.
Shared Memory Capacity  With Fermi and Kepler, shared memory and the L1 cache shared the same on-chip storage.
Maxwell, by contrast, provides dedicated space to the shared memory of each SMM, since the functionality of the L1 and texture caches have been merged in SMM.
This increases the shared memory space available per SMM as compared to SMX: GM107 provides 64 KB shared memory per SMM, and GM204 further increases this to 96 KB shared memory per SMM.
This presents several benefits to application developers: Algorithms with significant shared memory capacity requirements (e.g., radix sort) see an automatic 33% to 100% boost in capacity per SM on top of the aggregate boost from higher SM count.
Applications no longer need to select a preference of the L1/shared split for optimal performance.
For purposes of backward compatibility with Fermi and Kepler, applications may optionally continue to specify such a preference, but the preference will be ignored on Maxwell, with the full 64 KB per SMM always going to shared memory.
Note While the per-SM shared memory capacity is increased in SMM, the per-thread-block limit remains 48 KB.
For maximum flexibility on possible future GPUs, NVIDIA recommends that applications use at most 32 KB of shared memory in any one thread block, which would for example allow at least two such thread blocks to fit per SMM. 1.4.3.2. Shared Memory Bandwidth  Kepler SMX introduced an optional 8-byte shared memory banking mode, which had the potential to increase shared memory bandwidth per SM over Fermi for shared memory accesses of 8 or 16 bytes.
However, applications could only benefit from this when storing these larger elements in shared memory (i.e., integers and fp32 values saw no benefit), and only when the developer explicitly opted into the 8-byte bank mode via the API.
To simplify this, Maxwell returns to the Fermi style of shared memory banking, where banks are always four bytes wide.
Aggregate shared memory bandwidth across the chip remains comparable to that of corresponding Kepler chips, given increased SM count.
In this way, all applications using shared memory can now benefit from the higher bandwidth, even when storing only four-byte items into shared memory and without specifying any particular preference via the API. 1.4.3.3. Fast Shared Memory Atomics  Kepler introduced a dramatically higher throughput for atomic operations to global memory as compared to Fermi.
However, atomic operations to shared memory remained essentially unchanged: both architectures implemented shared memory atomics using a lock/update/unlock pattern that could be expensive in the case of high contention for updates to particular locations in shared memory.
Maxwell improves upon this by implementing native shared memory atomic operations for 32-bit integers and native shared memory 32-bit and 64-bit compare-and-swap (CAS), which can be used to implement other atomic functions with reduced overhead compared to the Fermi and Kepler methods.
Note Refer to the CUDA C++ Programming Guide for an example implementation of an fp64 atomicAdd() using atomicCAS() . 1.4.4. Dynamic Parallelism  GK110 introduced a new architectural feature called Dynamic Parallelism, which allows the GPU to create additional work for itself.
A programming model enhancement leveraging this feature was introduced in CUDA 5.0 to enable kernels running on GK110 to launch additional kernels onto the same GPU.
SMM brings Dynamic Parallelism into the mainstream by supporting it across the product line, even in lower-power chips such as GM107.
This will benefit developers, as it means that applications will no longer need special-case algorithm implementations for high-end GPUs that differ from those usable in more power-constrained environments. 2. Revision History  Version 1.0 Initial Public Release Version 1.1 Updated for second-generation Maxwell (compute capability 5.2).
Version 1.2 Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide. 3. Notices  3.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
1 Throughout this guide, Fermi refers to devices of compute capability 2.x, Kepler refers to devices of compute capability 3.x, and Maxwell refers to devices of compute capability 5.x.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Pascal Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for Pascal The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Pascal Architecture.
NVIDIA Pascal Compute Architecture  Pascal retains and extends the same CUDA programming model provided by previous NVIDIA architectures such as Maxwell, and applications that follow the best practices for those architectures should typically see speedups on the Pascal architecture without any code changes.
This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging Pascal architectural features.
2 A detailed overview of the major improvements in GP100 and GP104 over earlier NVIDIA architectures are described in a pair of white papers entitled NVIDIA Tesla P100: The Most Advanced Datacenter Accelerator Ever Built for GP100 and NVIDIA GeForce GTX 1080: Gaming Perfected for GP104.
For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide .
Some of the Pascal features described in this guide are specific to either GP100 or GP104, as noted; if not specified, features apply to both Pascal variants. 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures.
Programmers must primarily focus on following those recommendations to achieve the best performance.
The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the Pascal Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Pascal.
Streaming Multiprocessor  The Pascal Streaming Multiprocessor (SM) is in many respects similar to that of Maxwell.
Pascal further improves the already excellent power efficiency provided by the Maxwell architecture through both an improved 16-nm FinFET manufacturing process and various architectural modifications. 1.4.1.1. Instruction Scheduling  Like Maxwell, Pascal employs a power-of-two number of CUDA Cores per partition.
This simplifies scheduling, since each of the SM’s warp schedulers issue to a dedicated set of CUDA Cores equal to the warp width (32).
Each warp scheduler still has the flexibility to dual-issue (such as issuing a math operation to a CUDA Core in the same cycle as a memory operation to a load/store unit), but single-issue is now sufficient to fully utilize all CUDA Cores.
Like Maxwell, each GP104 SM provides four warp schedulers managing a total of 128 single-precision (FP32) and four double-precision (FP64) cores.
A GP104 processor provides up to 20 SMs, and the similar GP102 design provides up to 30 SMs.
The resulting 2:1 ratio of FP32 to FP64 cores aligns well with GP100’s new datapath configuration, allowing Pascal to process FP64 workloads more efficiently than Kepler GK210, the previous NVIDIA architecture to emphasize FP64 performance. 1.4.1.2. Occupancy  The maximum number of concurrent warps per SM remains the same as in Maxwell (i.e., 64), and other factors influencing warp occupancy remain similar as well: The register file size (64k 32-bit registers) is the same as that of Maxwell.
As with previous architectures, experimentation should be used to determine the optimum balance of register spilling vs.
But each GP100 SM contains fewer CUDA Cores, so the shared memory available per core actually increases on GP100.
The maximum shared memory per block remains limited at 48KB as with prior architectures (see Shared Memory Capacity ).
As such, developers can expect similar occupancy as on Maxwell without changes to their application.
As a result of scheduling improvements relative to Kepler, warp occupancy requirements (i.e., available parallelism) needed for maximum device utilization are generally reduced. 1.4.2. New Arithmetic Primitives  1.4.2.1.
FP16 Arithmetic Support  Pascal provides improved FP16 support for applications, like deep learning, that are tolerant of low floating-point precision.
As with Maxwell, FP16 storage can be used to reduce the required memory footprint and bandwidth compared to FP32 or FP64 storage.
Peak FP16 throughput is attained by using a paired operation to perform two FP16 instructions per core simultaneously.
To be eligible for the paired operation the operands must be stored in a half2 vector type.
GP100, designed with training deep neural networks in mind, provides FP16 throughput up to 2x that of FP32 arithmetic.
However, compensating for reduced FP16 throughput, GP104 provides additional high-throughput INT8 support not available in GP100. 1.4.2.2. INT8 Dot Product  GP104 provides specialized instructions for two-way and four-way integer dot products.
The __dp4a intrinsic computes a dot product of four 8-bit integers with accumulation into a 32-bit integer.
Similarly, __dp2a performs a two-element dot product between two 16-bit integers in one vector, and two 8-bit integers in another with accumulation into a 32-bit integer.
Both instructions offer a throughput equal to that of FP32 arithmetic. 1.4.3. Memory Throughput  1.4.3.1.
This allows much wider interfaces at similar power compared to traditional GDDR technology.
GP100 is linked to up to four stacks of HBM2 and uses two 512-bit memory controllers for each stack.
The effective width of the memory bus is then 4096 bits, a significant increase over the 384 bits in GM200.
Thus, the GP100 equipped Tesla P100 has a peak bandwidth of 732 GB/s with a modest 715 MHz memory clock.
In order to hide DRAM latencies at full HBM2 bandwidth, more memory accesses must be kept in flight compared to GPUs equipped with traditional GDDR5.
Helpfully, the large complement of SMs in GP100 will typically boost the number of concurrent threads (and thus reads-in-flight) compared to previous architectures.
Resource constrained kernels that are limited to low occupancy may benefit from increasing the number of concurrent memory accesses per thread.
The GP100 GPU’s register files, shared memories, L1 and L2 caches, and DRAM are all protected by Single-Error Correct Double-Error Detect (SECDED) ECC code.
When enabling ECC support on a Kepler GK210, the available DRAM would be reduced by 6.25% to allow for the storage of ECC bits.
Fetching ECC bits for each memory transaction also reduced the effective bandwidth by approximately 20% compared to the same GPU with ECC disabled.
HBM2 memories, on the other hand, provide dedicated ECC resources, allowing overhead-free ECC protection. 4 1.4.3.2. Unified L1/Texture Cache  Like Maxwell, Pascal combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp.
In contrast, GP104 follows Maxwell in caching global loads in L2 only, unless using the LDG read-only data cache mechanism.
As with previous architectures, GP104 allows the developer to opt-in to caching all global loads in the unified L1/Texture cache by passing the -Xptxas -dlcm=ca flag to nvcc at compile time.
Kepler serviced loads at a granularity of 128B when L1 caching of global loads was enabled and 32B otherwise.
On Pascal the data access unit is 32B regardless of whether global loads are cached in L1.
So it is no longer necessary to turn off L1 caching in order to reduce wasted global memory transactions associated with uncoalesced accesses.
The balance of occupancy versus spilling should therefore be re-evaluated to ensure best performance.
Two new device attributes were added in CUDA Toolkit 6.0: globalL1CacheSupported and localL1CacheSupported .
Developers who wish to have separately-tuned paths for various architecture generations can use these fields to simplify the path selection process.
If per-thread-block SM resource usage would result in zero occupancy with caching enabled, the CUDA driver will override the caching selection to allow the kernel launch to succeed.
This situation is reported by the profiler. 1.4.4. Atomic Memory Operations  Like Maxwell, Pascal provides native shared memory atomic operations for 32-bit integer arithmetic, along with native 32 or 64-bit compare-and-swap (CAS).
Developers coming from Kepler, where shared memory atomics were implemented in software using a lock/update/unlock sequence, should see a large performance improvement particularly for heavily contended shared-memory atomics.
The atomicAdd() function in CUDA has thus been generalized to support 32 and 64-bit integer and floating-point types.
The rounding mode for all floating-point atomic operations is round-to-nearest-even in Pascal.
For GP100 atomic operations may target the memories of peer GPUs connected through NVLink.
Pascal GPUs provide support system-wide atomic operations targeting migratable allocations 5 If system-wide atomic visibility is desired, operations targeting migratable memory must specify a system scope by using the atomic[Op]_system() intrinsics 6 .
atomicAdd() ) on migratable memory remains valid, but enforces atomic visibility only within the local GPU.
Note Given the potential for incorrect usage of atomic scopes, it is recommended that applications use compute-sanitizer to detect and eliminate errors.
As implemented for Pascal, system-wide atomics are intended to allow developers to experiment with enhanced memory models.
When an atomic targets a migratable address backed by a remote memory space, the local processor page-faults so that the kernel can migrate the appropriate memory page to local memory.
Since the page is now locally resident, subsequent atomics from the same processor will not result in additional page-faults.
However, atomic updates from different processors can incur frequent page-faults. 1.4.5. Shared Memory  1.4.5.1.
Shared Memory Capacity  For Kepler, shared memory and the L1 cache shared the same on-chip storage.
Maxwell and Pascal, by contrast, provide dedicated space to the shared memory of each SM, since the functionality of the L1 and texture caches have been merged.
This increases the shared memory space available per SM as compared to Kepler: GP100 offers 64 KB shared memory per SM, and GP104 provides 96 KB per SM.
This presents several benefits to application developers: Algorithms with significant shared memory capacity requirements (e.g., radix sort) see an automatic 33% to 100% boost in capacity per SM on top of the aggregate boost from higher SM count.
Applications no longer need to select a preference of the L1/shared split for optimal performance.
For maximum flexibility, NVIDIA recommends that applications use at most 32 KB of shared memory in any one thread block.
This would, for example, allow at least two thread blocks to fit per GP100 SM, or 3 thread blocks per GP104 SM. 1.4.5.2. Shared Memory Bandwidth  Kepler provided an optional 8-byte shared memory banking mode, which had the potential to increase shared memory bandwidth per SM for shared memory accesses of 8 or 16 bytes.
However, applications could only benefit from this when storing these larger elements in shared memory (i.e., integers and fp32 values saw no benefit), and only when the developer explicitly opted in to the 8-byte bank mode via the API.
This allows all applications using shared memory to benefit from the higher bandwidth, without specifying any particular preference via the API. 1.4.6. Inter-GPU Communication  1.4.6.1.
NVLink can be used to significantly increase performance for both GPU-to-GPU communication and for GPU access to system memory.
GP100 supports up to four NVLink connections with each connection carrying up to 40 GB/s of bi-directional bandwidth.
Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe.
The cudaDeviceEnablePeerAccess() API call remains necessary to enable direct transfers (over either PCIe or NVLink) between GPUs.
The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs. 1.4.6.2. GPUDirect RDMA Bandwidth  GPUDirect RDMA allows third party devices such as network interface cards (NICs) to directly access GPU memory.
This eliminates unnecessary copy buffers, lowers CPU overhead, and significantly decreases the latency of MPI send/receive messages from/to GPU memory.
Pascal doubles the delivered RDMA bandwidth when reading data from the source GPU memory and writing to the target NIC memory over PCIe. 1.4.7. Compute Preemption  Compute Preemption is a new feature specific to GP100.
Compute Preemption allows compute tasks running on the GPU to be interrupted at instruction-level granularity.
The execution context (registers, shared memory, etc.) are swapped to GPU DRAM so that another application can be swapped in and run.
Compute preemption offers two key advantages for developers: Long-running kernels no longer need to be broken up into small timeslices to avoid an unresponsive graphical user interface or kernel timeouts when a GPU is used simultaneously for compute and graphics.
Interactive kernel debugging on a single-GPU system is now possible. 1.4.8. Unified Memory Improvements  Pascal offers new hardware capabilities to extend Unified Memory (UM) support.
An extended 49-bit virtual addressing space allows Pascal GPUs to address the full 48-bit virtual address space of modern CPUs as well as the memories of all GPUs in the system through a single virtual address space, not limited by the physical memory sizes of any one processor.
Page faulting allows applications to access the same managed memory allocations from both host and device without explicit synchronization.
It also removes the need for the CUDA runtime to pre-synchronize all managed memory allocations before each kernel launch.
Instead, when a kernel accesses a non-resident memory page, it faults, and the page can be migrated to the GPU memory on-demand, or mapped into the GPU address space for access over PCIe/NVLink interfaces.
In cases where the UM heuristics prove suboptimal, further tuning is possible through a set of migration hints that can be added to the source code.
On supporting operating system platforms, any memory allocated with the default OS allocator (for example, malloc or new) can be accessed from both GPU and CPU code using the same pointer.
On such systems, there is no need to explicitly allocate managed memory using cudaMallocManaged() . 2. Revision History  Version 1.0 Initial Public Release Version 1.1 Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
1 Throughout this guide, Kepler refers to devices of compute capability 3.x, Maxwell refers to devices of compute capability 5.x, and Pascal refers to device of compute capability 6.x.
4 As an exception, scattered writes to HBM2 see some overhead from ECC but much less than the overhead with similar access patterns on ECC-protected GDDR5 memory.
5 Migratable, or Unified Memory (UM) , allocations are made with cudaMallocManaged() or, for systems with Heterogeneous Memory Management (HMM) support, malloc() .
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2016-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Volta Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for Volta The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Volta Architecture.
NVIDIA Volta Compute Architecture  Volta is NVIDIA’s latest architecture for CUDA compute applications.
Volta retains and extends the same CUDA programming model provided by previous NVIDIA architectures such as Maxwell and Pascal, and applications that follow the best practices for those architectures should typically see speedups on the Volta architecture without any code changes.
This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging Volta architectural features.
A detailed overview of the major improvements in GV100 over earlier NVIDIA architectures is provided in a white paper entitled NVIDIA Tesla V100 GPU Architecture: The World’s Most Advanced Datacenter GPU .
For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures.
Programmers must primarily focus on following those recommendations to achieve the best performance.
The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the Volta Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Volta.
Streaming Multiprocessor  The Volta Streaming Multiprocessor (SM) provides the following improvements over Pascal. 1.4.1.1. Instruction Scheduling  Each Volta SM includes 4 warp-scheduler units.
Each scheduler handles a static set of warps and issues to a dedicated set of arithmetic instruction units.
Instructions are performed over two cycles, and the schedulers can issue independent instructions every cycle.
Dependent instruction issue latency for core FMA math operations are reduced to four clock cycles, compared to six cycles on Pascal.
As a result, execution latencies of core math operations can be hidden by as few as 4 warps per SM, assuming 4-way instruction-level parallelism ILP per warp.
Many more warps are, of course, recommended to cover the much greater latency of memory transactions and control-flow operations.
GV100 provides up to 84 SMs. 1.4.1.2. Independent Thread Scheduling  The Volta architecture introduces Independent Thread Scheduling among threads in a warp.
This feature enables intra-warp synchronization patterns previously unavailable and simplifies code changes when porting CPU code.
However, Independent Thread Scheduling can also lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity 2 of previous hardware architectures.
When porting existing codes to Volta, the following three code patterns need careful attention.
To avoid data corruption, applications using warp intrinsics ( __shfl* , __any , __all , and __ballot ) should transition to the new, safe, synchronizing counterparts, with the *_sync suffix.
The new warp intrinsics take in a mask of threads that explicitly define which lanes (threads of a warp) must participate in the warp intrinsic.
Applications that assume reads and writes are implicitly visible to other threads in the same warp need to insert the new __syncwarp() warp-wide barrier synchronization instruction between steps where data is exchanged between threads via global or shared memory.
Assumptions that code is executed in lockstep or that reads/writes from separate threads are visible across a warp without synchronization are invalid.
Applications using __syncthreads() or the PTX bar.sync (and their derivatives) in such a way that a barrier will not be reached by some non-exited thread in the thread block must be modified to ensure that all non-exited threads reach the barrier.
The racecheck and synccheck tools provided by compute-sanitizer can help with locating violations. 1.4.1.3. Occupancy  The maximum number of concurrent warps per SM remains the same as in Pascal (i.e., 64), and other factors influencing warp occupancy remain similar as well: The register file size is 64k 32-bit registers per SM.
Shared memory capacity per SM is 96KB, similar to GP104, and a 50% increase compared to GP100.
Overall, developers can expect similar occupancy as on Pascal without changes to their application. 1.4.1.4. Integer Arithmetic  Unlike Pascal GPUs, the GV100 SM includes dedicated FP32 and INT32 cores.
For example, each iteration of a pipelined loop could update addresses and load data for the next iteration while simultaneously processing the current iteration at full FP32 throughput. 1.4.2. Tensor Core Operations  Each Tensor Core performs the following operation: D = AxB + C, where A, B, C, and D are 4x4 matrices.
The matrix multiply inputs A and B are FP16 matrices, while the accumulation matrices C and D may be FP16 or FP32 matrices.
When accumulating in FP32, the FP16 multiply results in a full precision product that is then accumulated using FP32 addition with the other intermediate products for a 4x4x4 matrix multiply.
In practice, Tensor Cores are used to perform much larger 2D or higher dimensional matrix operations, built up from these smaller elements.
The API exposes specialized matrix load, matrix multiply and accumulate, and matrix store operations to efficiently use Tensor Cores from a CUDA-C++ program.
At the CUDA level, the warp-level interface assumes 16x16 size matrices spanning all 32 threads of the warp.
See the CUDA C++ Programming Guide for more information. 1.4.3. Memory Throughput  1.4.3.1.
High Bandwidth Memory  GV100 uses up to eight memory dies per HBM2 stack and four stacks, with a maximum of 32 GB of GPU memory.
A faster and more efficient HBM2 implementation delivers up to 900 GB/s of peak memory bandwidth, compared to 732 GB/s for GP100.
This combination of a new generation HBM2 memory, and a new generation memory controller, in Volta provides 1.5x delivered memory bandwidth, compared to Pascal GP100—and a greater than 95% memory bandwidth efficiency running many workloads.
In order to hide the DRAM latencies at full HBM2 bandwidth more memory accesses must be kept in flight, compared to GPUs equipped with traditional GDDR5.
This is accomplished by the large complement of SMs in GV100, which typically boost the number of concurrent threads, and thus the reads-in-flight, compared to previous architectures.
Resource-constrained kernels that are limited to low occupancy may benefit from increasing the number of concurrent memory accesses per thread. 1.4.3.2. Unified Shared Memory/L1/Texture Cache  In Volta the L1 cache, texture cache, and shared memory are backed by a combined 128 KB data cache.
As in previous architectures, the portion of the cache dedicated to shared memory (known as the carveout ) can be selected at runtime using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout .
A new feature, Volta enables a single thread block to address the full 96 KB of shared memory.
To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit.
Like Pascal, Volta combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp.
Volta increases the maximum capacity of the L1 cache to 128 KB, more than 7x larger than the GP100 L1.
Another benefit of its union with shared memory, the Volta L1 improves in terms of both latency and bandwidth compared to Pascal.
The result is that for many applications Volta narrows the performance gap between explicitly managed shared memory and direct access to device memory.
Also, the cost of register spills is lowered compared to Pascal, and the balance of occupancy versus spilling should be re-evaluated to ensure best performance. 1.4.4. Cooperative Groups  The Volta architecture introduced Independent Thread Scheduling, which enables intra-warp synchronization patterns that were previously not possible.
This is an extension to the CUDA programming model for organizing groups of communicating threads.
Cooperative Groups allows developers to express the granularity at which threads are communicating, helping them to express richer, more efficient parallel decompositions. 1.4.5. Multi-Process Service  The Volta Multi-Process Service is significantly improved compared to previous architecutres, both in terms of performance and robustness.
Intermediary software schedulers, used for MPS with previous architectures, have been replaced by hardware accelerated units within the GPU.
MPS clients now submit tasks directly to the GPU work queues, significantly decreasing submission latency and increasing aggregate throughput.
Volta MPS also provides each client with an isolated address space, 3 and extends Unified Memory support for MPS applications.
Volta MPS also provides control for clients to restrict each client to a fraction of the GPU execution resources.
Developers can use this feature to reduce or eliminate head-of-line blocking where work from one MPS client overwhelms GPU execution resources and prevents other clients from making progress, and thus improve average latency and jitter accross the system. 1.4.6. NVLink Interconnect  NVLink is NVIDIA’s high-speed data interconnect.
NVLink can be used to significantly increase performance for both GPU-to-GPU communication and for GPU access to system memory.
GV100 supports up to six NVLink connections with each connection carrying up to 50 GB/s of bi-directional bandwidth.
Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe.
The cudaDeviceEnablePeerAccess() API call remains necessary to enable direct transfers (over either PCIe or NVLink) between GPUs.
The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs. 2. Revision History  Version 1.0 Initial Public Release Version 1.1 Added Cooperative Groups section.
Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide . 3. Notices  3.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
1 Throughout this guide, Maxwell refers to devices of compute capability 5.x, Pascal refers to device of compute capability 6.x, and Volta refers to devices of compute capability 7.x.
2 The term warp-synchronous refers to code that implicitly assumes threads in the same warp are synchronized at every instruction.
3 As with previous architectures, MPS does not provide fatal fault isolation between clients.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2017-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Turing Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for Turing The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Turing Architecture.
NVIDIA Turing Compute Architecture  Turing is NVIDIA’s latest architecture for CUDA compute applications.
Turing retains and extends the same CUDA programming model provided by previous NVIDIA architectures such as Pascal and Volta, and applications that follow the best practices for those architectures should typically see speedups on the Turing architecture without any code changes.
This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging Turing architectural features.
1 For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures.
Programmers must primarily focus on following those recommendations to achieve the best performance.
The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the Turing Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Turing.
Streaming Multiprocessor  The Turing Streaming Multiprocessor (SM) is based on the same major architecture (7.x) as Volta, and provides similar improvements over Pascal. 1.4.1.1. Instruction Scheduling  Each Turing SM includes 4 warp-scheduler units.
Each scheduler handles a static set of warps and issues to a dedicated set of arithmetic instruction units.
Instructions are performed over two cycles, and the schedulers can issue independent instructions every cycle.
Dependent instruction issue latency for core FMA math operations is four clock cycles, like Volta, compared to six cycles on Pascal.
As a result, execution latencies of core math operations can be hidden by as few as 4 warps per SM, assuming 4-way instruction-level parallelism ILP per warp, or by 16 warps per SM without any instuction-level parallelism.
Like Volta, the Turing SM provides 64 FP32 cores, 64 INT32 cores and 8 improved mixed-precision Tensor Cores.
Turing has a lower double precision throughput than Volta with only 2 FP64 cores. 1.4.1.2. Independent Thread Scheduling  The Turing architecture features the same Independent Thread Scheduling introduced with Volta.
This enables intra-warp synchronization patterns previously unavailable and simplifies code changes when porting CPU code.
However, Independent Thread Scheduling can also lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity 2 of previous hardware architectures.
When porting existing codes to Volta or Turing, the following three code patterns need careful attention.
To avoid data corruption, applications using warp intrinsics ( __shfl* , __any , __all , and __ballot ) should transition to the new, safe, synchronizing counterparts, with the *_sync suffix.
The new warp intrinsics take in a mask of threads that explicitly define which lanes (threads of a warp) must participate in the warp intrinsic.
Applications that assume reads and writes are implicitly visible to other threads in the same warp need to insert the new __syncwarp() warp-wide barrier synchronization instruction between steps where data is exchanged between threads via global or shared memory.
Assumptions that code is executed in lockstep or that reads/writes from separate threads are visible across a warp without synchronization are invalid.
Applications using __syncthreads() or the PTX bar.sync (and their derivatives) in such a way that a barrier will not be reached by some non-exited thread in the thread block must be modified to ensure that all non-exited threads reach the barrier.
The racecheck and synccheck tools provided by compute-sanitizer can help with locating violations. 1.4.1.3. Occupancy  The maximum number of concurrent warps per SM is 32 on Turing (versus 64 on Volta).
Other factors influencing warp occupancy remain otherwise similar: The register file size is 64k 32-bit registers per SM.
Overall, developers can expect similar occupancy as on Pascal or Volta without changes to their application. 1.4.1.4. Integer Arithmetic  Similar to Volta, the Turing SM includes dedicated FP32 and INT32 cores.
For example, each iteration of a pipelined loop could update addresses and load data for the next iteration while simultaneously processing the current iteration at full FP32 throughput. 1.4.2. Tensor Core Operations  Volta introduced Tensor Cores to accelerate matrix multiply operations on mixed precision floating point data.
The API provides specialized matrix load, matrix multiply and accumulate, and matrix store operations, where each warp processes a small matrix fragment, allowing to efficiently use Tensor Cores from a CUDA-C++ program.
In practice, Tensor Cores are used to perform much larger 2D or higher dimensional matrix operations, built up from these smaller matrix fragments.
The Tensor Cores support half precision matrix multiplication, where the matrix multiply inputs A and B are FP16 matrices, while the accumulation matrices C and D may be either FP16 or FP32 matrices.
When accumulating in FP32, the FP16 multiply results in a full precision product that is then accumulated using FP32 addition.
CUDA 10 supports several fragment sizes, 16x16x16, 32x8x16, and 8x32x16 to use the Tensor Cores on Volta or Turing with FP16 inputs.
Any binary compiled for Volta will run on Turing, but Volta binaries using Tensor Cores will only be able to reach half of Turing’s Tensor Core peak performance.
Recompiling the binary specifically for Turing would allow it to reach the peak performance.
Turing’s Tensor Core supports integer matrix multiply operations, which can operate on 8-bit, 4-bit and 1-bit integer inputs, with 32-bit integer accumulation.
When operating on 8-bit inputs, CUDA exposes fragment sizes of 16x16x16, 32x8x16, and 8x32x16.
For sub-byte operations the fragment sizes available are 8x8x32 for 4-bit inputs, or 8x8x128 for 1-bit inputs.
See the CUDA C++ Programming Guide for more information. 1.4.3. Memory Throughput  1.4.3.1.
Unified Shared Memory/L1/Texture Cache  Turing features a unified L1 / Shared Memory cache similar to the one introduced in Volta, but with a smaller size.
The portion of the cache dedicated to shared memory or L1 (known as the carveout ) can be changed at runtime, either automatically by the driver, or manually using the cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout .
Turing supports two carveout configurations, either with 64 KB of shared memory and 32 KB of L1, or with 32 KB of shared memory and 64 KB of L1.
To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit.
Like Pascal and Volta, Turing combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp.
The state-of-the-art L1 cache in Volta and Turing offers lower latency, higher bandwidth, and higher capacity compared to the earlier architectures.
The result is that for many applications Volta and Turing narrow the performance gap between explicitly managed shared memory and direct access to device memory.
Also, the cost of register spills is lowered compared to Pascal, and the balance of occupancy versus spilling should be re-evaluated to ensure best performance. 2. Revision History  Version 1.0 Initial Public Release Version 1.1 Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide .
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
1 Throughout this guide, Kepler refers to devices of compute capability 3.x, Maxwell refers to devices of compute capability 5.x, Pascal refers to devices of compute capability 6.x, Volta refers to devices of compute capability 7.0, and Turing refers to devices of compute capability 7.5.
2 The term warp-synchronous refers to code that implicitly assumes threads in the same warp are synchronized at every instruction.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
NVIDIA Ampere GPU Architecture Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for NVIDIA Ampere GPU Architecture The programming guide for tuning CUDA Applications for GPUs based on the NVIDIA Ampere GPU Architecture.
NVIDIA Ampere GPU Architecture  The NVIDIA Ampere GPU architecture is NVIDIA’s latest architecture for CUDA compute applications.
The NVIDIA Ampere GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as Turing and Volta, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA A100 GPU without any code changes.
This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Ampere GPU architecture’s features.
1 For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures.
Programmers must primarily focus on following those recommendations to achieve the best performance.
The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code.
Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the NVIDIA Ampere GPU Architecture Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with the NVIDIA Ampere GPU Architecture.
Streaming Multiprocessor  The NVIDIA Ampere GPU architecture’s Streaming Multiprocessor (SM) provides the following improvements over Volta and Turing. 1.4.1.1. Occupancy  The maximum number of concurrent warps per SM remains the same as in Volta (i.e., 64) for compute capability 8.0, while for compute capability 8.6 it is 48.
Other factors influencing warp occupancy are: The register file size is 64K 32-bit registers per SM.
The maximum number of thread blocks per SM is 32 for devices of compute capability 8.0 (i.e., A100 GPUs) and 16 for GPUs with compute capability 8.6.
For devices of compute capability 8.0 (i.e., A100 GPUs) shared memory capacity per SM is 164 KB, a 71% increase compared to V100’s capacity of 96 KB.
For devices of compute capability 8.0 (i.e., A100 GPUs) the maximum shared memory per thread block is 163 KB.
Overall, developers can expect similar occupancy as on Volta without changes to their application. 1.4.1.2. Asynchronous Data Copy from Global Memory to Shared Memory  The NVIDIA Ampere GPU architecture adds hardware acceleration for copying data from global memory to shared memory.
These copy instructions are asynchronous, with respect to computation and allow users to explicitly control overlap of compute with data movement from global memory into the SM.
These instructions also avoid using extra registers for memory copies and can also bypass the L1 cache.
For more information please refer to the section on Async Copy in the CUDA C++ Programming Guide . 1.4.1.3. Hardware Acceleration for Split Arrive/Wait Barrier  The NVIDIA Ampere GPU architecture adds hardware acceleration for a split arrive/wait barrier in shared memory.
These barriers can be used to implement fine grained thread controls, producer-consumer computation pipeline and divergence code patterns in CUDA.
For more information on the Arrive/Wait Barriers refer to the Arrive/Wait Barrier section in the CUDA C++ Programming Guide . 1.4.1.4. Warp level support for Reduction Operations  The NVIDIA Ampere GPU architecture adds native support for warp wide reduction operations for 32-bit signed and unsigned integer operands.
The warp wide reduction operations support arithmetic add , min , and max operations on 32-bit signed and unsigned integers and bitwise and , or and xor operations on 32-bit unsigned integers.
For more details on the new warp wide reduction operations refer to Warp Reduce Functions in the CUDA C++ Programming Guide . 1.4.1.5. Improved Tensor Core Operations  The NVIDIA Ampere GPU architecture includes new Third Generation Tensor Cores that are more powerful than the Tensor Cores used in Volta and Turing SMs.
The new Tensor Cores use a larger base matrix size and add powerful new math modes including: Support for FP64 Tensor Core, using new DMMA instructions.
TF32 is a new 19-bit Tensor Core format that can be easily integrated into programs for more accurate DL training than 16-bit HMMA formats.
Support for bitwise AND along with bitwise XOR which was introduced in Turing, through BMMA instructions.
The following table presents the evolution of matrix instruction sizes and supported data types for Tensor Cores across different GPU architecture generations.
While a binary compiled for 8.0 will run as is on 8.6, it is recommended to compile explicitly for 8.6 to benefit from the increased FP32 throughput. 1.4.2. Memory System  1.4.2.1.
Increased Memory Capacity and High Bandwidth Memory  The NVIDIA A100 GPU increases the HBM2 memory capacity from 32 GB in V100 GPU to 40 GB in A100 GPU.
Along with the increased memory capacity, the bandwidth is increased by 72%, from 900 GB/s on Volta V100 to 1550 GB/s on A100. 1.4.2.2. Increased L2 capacity and L2 Residency Controls  The NVIDIA Ampere GPU architecture increases the capacity of the L2 cache to 40 MB in Tesla A100, which is 7x larger than Tesla V100.
Along with the increased capacity, the bandwidth of the L2 cache to the SMs is also increased.
The NVIDIA Ampere GPU architecture allows CUDA users to control the persistence of data in L2 cache.
For more information on the persistence of data in L2 cache, refer to the section on managing L2 cache in the CUDA C++ Programming Guide . 1.4.2.3. Unified Shared Memory/L1/Texture Cache  The NVIDIA A100 GPU based on compute capability 8.0 increases the maximum capacity of the combined L1 cache, texture cache and shared memory to 192 KB, 50% larger than the L1 cache in NVIDIA V100 GPU.
In the NVIDIA Ampere GPU architecture, the portion of the L1 cache dedicated to shared memory (known as the carveout ) can be selected at runtime as in previous architectures such as Volta, using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout .
The NVIDIA A100 GPU supports shared memory capacity of 0, 8, 16, 32, 64, 100, 132 or 164 KB per SM.
GPUs with compute capability 8.6 support shared memory capacity of 0, 8, 16, 32, 64 or 100 KB per SM.
Hence, the A100 GPU enables a single thread block to address up to 163 KB of shared memory and GPUs with compute capability 8.6 can address up to 99 KB of shared memory in a single thread block.
To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit.
Like Volta, the NVIDIA Ampere GPU architecture combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp.
Another benefit of its union with shared memory, similar to Volta L1 is improvement in terms of both latency and bandwidth. 1.4.3. Third Generation NVLink  The third generation of NVIDIA’s high-speed NVLink interconnect is implemented in A100 GPUs, which significantly enhances multi-GPU scalability, performance, and reliability with more links per GPU, much faster communication bandwidth, and improved error-detection and recovery features.
The third generation NVLink has the same bi-directional data rate of 50 GB/s per link, but uses half the number of signal pairs to achieve this bandwidth.
Therefore, the total number of links available is increased to twelve in A100, versus six in V100, yielding 600 GB/s bidirectional bandwidth versus 300 GB/s for V100.
Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe.
The cudaDeviceEnablePeerAccess() API call remains necessary to enable direct transfers (over either PCIe or NVLink) between GPUs.
The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs.
In the NVIDIA Ampere GPU architecture remote NVLINK accesses go through a Link TLB on the remote GPU.
Applications with remote random accesses may want to constrain the remotely accessed region to 64 GB for each peer GPU. 2. Revision History  Version 1.1 Initial Public Release Added support for compute capability 8.6 3.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
1 Throughout this guide, Kepler refers to devices of compute capability 3.x, Maxwell refers to devices of compute capability 5.x, Pascal refers to device of compute capability 6.x, Volta refers to devices of compute capability 7.0, Turing refers to devices of compute capability 7.5, and NVIDIA Ampere GPU Architecture refers to devices of compute capability 8.x Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2020-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
NVIDIA Hopper Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for Hopper GPU Architecture The programming guide for tuning CUDA Applications for GPUs based on the Hopper GPU Architecture.
NVIDIA Hopper GPU Architecture  The NVIDIA® Hopper GPU architecture is NVIDIA’s latest architecture for CUDA® compute applications.
The NVIDIA Hopper GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere GPU architecture and NVIDIA Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA H100 GPU without any code changes.
This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Hopper GPU architecture’s features.
1 For further details on the programming features discussed in this guide, refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures.
Programmers must primarily focus on following those recommendations to achieve the best performance.
The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code.
Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the Hopper Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with NVIDIA Hopper.
Streaming Multiprocessor  The NVIDIA Hopper Streaming Multiprocessor (SM) provides the following improvements over Turing and NVIDIA Ampere GPU architectures. 1.4.1.1. Occupancy  The maximum number of concurrent warps per SM remains the same as in NVIDIA Ampere GPU architecture (that is, 64), and other factors influencing warp occupancy are: The register file size is 64K 32-bit registers per SM.
The maximum number of thread blocks per SM is 32 for devices of compute capability 9.0 (that is, H100 GPUs).
For devices of compute capability 9.0 (H100 GPUs), shared memory capacity per SM is 228 KB, a 39% increase compared to A100’s capacity of 164 KB.
For devices of compute capability 9.0 (H100 GPUs), the maximum shared memory per thread block is 227 KB.
For applications using Thread Block Clusters, it is always recommended to compute the occupancy using cudaOccupancyMaxActiveClusters and launch cluster-based kernels accordingly.
Overall, developers can expect similar occupancy as on NVIDIA Ampere GPU architecture GPUs without changes to their application. 1.4.1.2. Tensor Memory Accelerator  The Hopper architecture builds on top of the asynchronous copies introduced by NVIDIA Ampere GPU architecture and provides a more sophisticated asynchronous copy engine: the Tensor Memory Accelerator (TMA).
TMA allows applications to transfer 1D and up to 5D tensors between global memory and shared memory, in both directions, as well as between the shared memory regions of different SMs in the same cluster (refer to Thread Block Clusters ).
Additionally, for writes from shared memory to global memory, it allows specifying element wise reduction operations such as add/min/max as well as bitwise and/or for most common data types.
This has several advantages: Avoids using registers for moving data between the different memory spaces.
Avoids using SM instructions for moving data: a single thread can issue large data movement instructions to the TMA unit.
The whole block can then continue working on other instructions while the data is in flight and only wait for the data to be consumed when actually necessary.
Enables users to write warp specialized codes, where specific warps specialize on data movement between the different memory spaces while other warps only work on local data within the SM.
This feature will be exposed through cuda::memcpy_async along with the cuda::barrier and cuda::pipeline for synchronizing data movement. 1.4.1.3. Thread Block Clusters  NVIDIA Hopper Architecture adds a new optional level of hierarchy, Thread Block Clusters, that allows for further possibilities when parallelizing applications.
A thread block can read from, write to, and perform atomics in shared memory of other thread blocks within its cluster.
As demonstrated in the CUDA C++ Programming Guide , there are applications that cannot fit required data within shared memory and must use global memory instead.
This can benefit applications that need to communicate data between SMs by utilizing the combined bandwidth of both distributed shared memory and L2.
In order to achieve best performance for accesses to Distributed Shared Memory, access patterns to those described in the CUDA C++ Best Practices Guide for Global Memory should be used.
Specifically, accesses to Distributed Shared Memory should be coalesced and aligned to 32-byte segments, if possible.
Access patterns with non-unit stride should be avoided if possible, which can be achieved by using local shared memory, similar to what is shown in the CUDA C++ Best Practices Guide for Shared Memory .
The maximum portable cluster size supported is 8; however, NVIDIA Hopper H100 GPU allows for a nonportable cluster size of 16 by opting in.
Launching a kernel with a nonportable cluster size requires setting the cudaFuncAttributeNonPortableClusterSizeAllowed function attribute.
Using larger cluster sizes may reduce the maximum number of active blocks across the GPU (refer to Occupancy ). 1.4.1.4. Improved FP32 Throughput  Devices of compute capability 9.0 have 2x more FP32 operations per cycle per SM than devices of compute capability 8.0.
1.4.1.5. Dynamic Programming Instructions  The NVIDIA Hopper architecture adds support for new instructions to accelerate dynamic programming algorithms, such as the Smith-Waterman algorithm for sequence alignment in bioinformatics, and algorithms in graph theory, game theory, ML, and finance problems.
The new instructions permit computation of max and min values among three operands, max and min operations yielding predicates, combined add operation with max or min, operating on signed and unsigned 32-bit int and 16-bit short2 types, and half2.
All DPX instructions with 16-bit short types DPX instructions enable 128 operations per cycle per SM. 1.4.2. Memory System  1.4.2.1.
High-Bandwidth Memory HBM3 Subsystem  The NVIDIA H100 GPU has support for HBM3 and HBM2e memory, with capacity up to 80 GB.
GPUs HBM3 memory system supports up to 3 TB/s memory bandwidth, a 93% increase over the 1.55 TB/s on A100-40GB. 1.4.2.2. Increased L2 Capacity  The NVIDIA Hopper architecture increases the L2 cache capacity from 40 MB in the A100 GPU to 50 MB in the H100 GPU.
Along with the increased capacity, the bandwidth of the L2 cache to the SMs is also increased.
The NVIDIA Hopper architecture allows CUDA users to control the persistence of data in L2 cache similar to the NVIDIA Ampere GPU Architecture.
For more information on the persistence of data in L2 cache, refer to the section on managing L2 cache in the CUDA C++ Programming Guide . 1.4.2.3. Inline Compression  The NVIDIA Hopper architecture allows CUDA compute kernels to benefit from the new inline compression (ILC).
This feature can be applied to individual memory allocation, and the compressor automatically chooses between several possible compression algorithms, or none if there is no suitable pattern.
In case compression can be used, this feature allows accessing global memory at significantly higher bandwidth than global memory bandwidth, since only compressed data needs to be transferred between global memory and SMs.
However, the feature does not allow for reducing memory footprint: since compression is automatic, even if compression is active, the memory region will use the same footprint as if there was no compression.
This is because underlying data may be changed by the user application and may not be compressible during the entire duration of the application.
See the CUDA C++ Programming Guide section on compressible memory : CUmemGenericAllocationHandle allocationHandle ; CUmemAllocationProp prop = {}; memset ( prop , 0 , sizeof ( CUmemAllocationProp )); prop -> type = CU_MEM_ALLOCATION_TYPE_PINNED ; prop -> location .
compressionType = CU_MEM_ALLOCATION_COMP_GENERIC ; cuMemCreate ( & allocationHandle , size , & prop , 0 ); One can check whether compressible memory is available on the given device with: cuDeviceGetAttribute ( & compressionAvailable , CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED , currentDevice ) Note that this example code does not handle errors and compiling this code requires linking against the CUDA library ( libcuda.so ). 1.4.2.4. Unified Shared Memory/L1/Texture Cache  The NVIDIA H100 GPU based on compute capability 9.0 increases the maximum capacity of the combined L1 cache, texture cache, and shared memory to 256 KB, from 192 KB in NVIDIA Ampere Architecture, an increase of 33%.
In the NVIDIA Hopper GPU architecture, the portion of the L1 cache dedicated to shared memory (known as the carveout) can be selected at runtime as in previous architectures such as NVIDIA Ampere Architecture and NVIDIA Volta, using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout .
The NVIDIA H100 GPU supports shared memory capacities of 0, 8, 16, 32, 64, 100, 132, 164, 196 and 228 KB per SM.
Hence, the H100 GPU enables a single thread block to address up to 227 KB of shared memory.
To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit.
Like the NVIDIA Ampere Architecture and NVIDIA Volta GPU architectures, the NVIDIA Hopper GPU architecture combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp before delivery of that data to the warp.
Another benefit of its union with shared memory, similar to previous architectures, is improvement in terms of both latency and bandwidth. 1.4.3. Fourth-Generation NVLink  The fourth generation of NVIDIA’s high-speed NVLink interconnect is implemented in H100 GPUs, which significantly enhances multi-GPU scalability, performance, and reliability with more links per GPU, much faster communication bandwidth, and improved error-detection and recovery features.
The total number of links available is increased to 18 in H100, compared to 12 in A100, yielding 900 GB/s bidirectional bandwidth compared to 600 GB/s for A100.
Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe.
The cudaDeviceEnablePeerAccess() API call remains necessary to enable direct transfers (over either PCIe or NVLink) between GPUs.
The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs. 2. Revision History  Version 1.0 Initial Public Release Added support for compute capability 9.0 1 Throughout this guide, NVIDIA Volta refers to devices of compute capability 7.0, NVIDIA Turing refers to devices of compute capability 7.5, NVIDIA Ampere GPU Architecture refers to devices of compute capability 8.x, and NVIDIA Hopper refers to devices of compute capability 9.0.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
NVIDIA Ada GPU Architecture Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for NVIDIA Ada GPU Architecture The programming guide for tuning CUDA Applications for GPUs based on the NVIDIA Ada GPU Architecture.
NVIDIA Ada GPU Architecture  The NVIDIA ® Ada GPU architecture is NVIDIA’s latest architecture for CUDA ® compute applications.
The NVIDIA Ada GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere and Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA Ada architecture without any code changes.
This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Ada GPU architecture’s features.
1 For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures.
Programmers must primarily focus on following those recommendations to achieve the best performance.
The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code.
Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the NVIDIA Ada GPU Architecture Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with the NVIDIA Ada GPU Architecture.
Streaming Multiprocessor  The NVIDIA Ada GPU architecture’s Streaming Multiprocessor (SM) provides the following improvements over Turing and NVIDIA Ampere GPU architectures. 1.4.1.1. Occupancy  The maximum number of concurrent warps per SM is 48, remaining the same compared to compute capability 8.6 GPUs, and other factors influencing warp occupancy are: The register file size is 64K 32-bit registers per SM.
Overall, developers can expect similar occupancy as on compute capability 8.6 GPUs without changes to their application. 1.4.1.2. Improved Tensor Core Operations  The NVIDIA Ada GPU architecture includes new Ada Fourth Generation Tensor Cores featuring the Hopper FP8 Transformer Engine.
1.4.1.3. Improved FP32 throughput  Devices of compute capability 8.9 have 2x more FP32 operations per cycle per SM than devices of compute capability 8.0.
While a binary compiled for 8.0 will run as-is on 8.9, it is recommended to compile explicitly for 8.9 to benefit from the increased FP32 throughput. 1.4.2. Memory System  1.4.2.1.
Increased L2 capacity  The NVIDIA Ada GPU architecture increases the capacity of the L2 cache to 98304 KB in AD102, 16x larger than GA102.
The NVIDIA Ada GPU architecture allows CUDA users to control the persistence of data in the L2 cache.
For more information on the persistence of data in the L2 cache, refer to the section on managing the L2 cache in the CUDA C++ Programming Guide . 1.4.2.2. Unified Shared Memory/L1/Texture Cache  NVIDIA Ada architecture features a unified L1 cache, texture cache, and shared memory similar to that of the NVIDIA Ampere architecture.
In the NVIDIA Ada GPU architecture, the portion of the L1 cache dedicated to shared memory (known as the carveout ) can be selected at runtime as in previous architectures, such as NVIDIA Ampere, using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout .
The NVIDIA Ada GPU architecture supports shared memory capacity of 0, 8, 16, 32, 64 or 100 KB per SM.
Hence, GPUs with compute capability 8.9 can address up to 99 KB of shared memory in a single thread block.
To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit.
Like the NVIDIA Ampere and NVIDIA Volta GPU architectures, the NVIDIA Ada GPU architecture combines the functionality of the L1 and texture caches into a unified L1/Texture cache that acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp.
Another benefit of its union with shared memory, similar to previous architectures, is improvement in terms of both latency and bandwidth. 2. Revision History  Version 1.0 Initial Public Release Added support for compute capability 8.9 1 Throughout this guide, Volta refers to devices of compute capability 7.0, Turing refers to devices of compute capability 7.5, NVIDIA Ampere GPU Architecture refers to devices of compute capability 8.0 and 8.6, NVIDIA Ada refers to devices of compute capability 8.9.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Data Movement and Conversion Instructions: multimem.ld_reduce, multimem.st, multimem.red 9.7.8.14.
Data Movement and Conversion Instructions: cp.async.wait_group / cp.async.wait_all 9.7.8.24.6.
Parallel Synchronization and Communication Instructions: mbarrier.complete_tx 9.7.12.15.13.
Parallel Synchronization and Communication Instructions: mbarrier.arrive_drop 9.7.12.15.15.
Parallel Synchronization and Communication Instructions: cp.async.mbarrier.arrive 9.7.12.15.16.
Parallel Synchronization and Communication Instructions: mbarrier.test_wait/mbarrier.try_wait 9.7.12.15.17.
Parallel Synchronization and Communication Instructions: mbarrier.pending_count 9.7.12.15.18.
Matrix multiply-accumulate operation using mma.sp instruction with sparse matrix A 9.7.13.5.1.
Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using wgmma.mma_async instruction 9.7.14.5.1.
Asynchronous Warpgroup Level Multiply-and-Accumulate Operation using wgmma.mma_async.sp instruction 9.7.14.6.1.
Matrix fragments for warpgroup-level multiply-accumulate operation with sparse matrix A 9.7.14.6.2.1.
Special Registers: %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_ 10.30.
Introduction v8.5 | PDF | Archive Parallel Thread Execution ISA Version 8.5 The programming guide to using PTX (Parallel Thread Execution) and ISA (Instruction Set Architecture).
Introduction  This document describes PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA).
PTX exposes the GPU as a data-parallel computing device . 1.1. Scalable Data-Parallel Computing using GPUs  Driven by the insatiable market demand for real-time, high-definition 3D graphics, the programmable GPU has evolved into a highly parallel, multithreaded, many-core processor with tremendous computational horsepower and very high memory bandwidth.
The GPU is especially well-suited to address problems that can be expressed as data-parallel computations - the same program is executed on many data elements in parallel - with high arithmetic intensity - the ratio of arithmetic operations to memory operations.
Because the same program is executed for each data element, there is a lower requirement for sophisticated flow control; and because it is executed on many data elements and has high arithmetic intensity, the memory access latency can be hidden with calculations instead of big data caches.
Many applications that process large data sets can use a data-parallel programming model to speed up the computations.
Similarly, image and media processing applications such as post-processing of rendered images, video encoding and decoding, image scaling, stereo vision, and pattern recognition can map image blocks and pixels to parallel processing threads.
In fact, many algorithms outside the field of image rendering and processing are accelerated by data-parallel processing, from general signal processing or physics simulation to computational finance or computational biology.
The PTX-to-GPU translator and driver enable NVIDIA GPUs to be used as programmable parallel computers. 1.2. Goals of PTX  PTX provides a stable programming model and instruction set for general purpose parallel programming.
It is designed to be efficient on NVIDIA GPUs supporting the computation features defined by the NVIDIA Tesla architecture.
High level language compilers for languages such as CUDA and C/C++ generate PTX instructions, which are optimized for and translated to native target-architecture instructions.
The goals for PTX include the following: Provide a stable ISA that spans multiple GPU generations.
Provide a common source-level ISA for optimizing code generators and translators, which map PTX to specific target machines.
Provide a scalable programming model that spans GPU sizes from a single unit to many parallel units. 1.3. PTX ISA Version 8.5  PTX ISA version 8.5 introduces the following new features: Adds support for mma.sp::ordered_metadata instruction.
1.4. Document Structure  The information in this document is organized into the following Chapters: Programming Model outlines the programming model.
State Spaces, Types, and Variables describes state spaces, types, and variable declarations.
Abstracting the ABI describes the function and call syntax, calling convention, and PTX support for abstracting the Application Binary Interface (ABI) .
http: ieeexplore.ieee.org/servlet/opac?punumber=4610933 The OpenCL Specification, Version: 1.1, Document Revision: 44, June 1, 2011.
https: docs.nvidia.com/cuda/cuda-c-programming-guide/index.html CUDA Dynamic Parallelism Programming Guide.
https: docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-dynamic-parallelism CUDA Atomicity Requirements.
https: nvidia.github.io/cccl/libcudacxx/extended_api/memory_model.html#atomicity PTX Writers Guide to Interoperability.
A Highly Multithreaded Coprocessor  The GPU is a compute device capable of executing a very large number of threads in parallel.
It operates as a coprocessor to the main CPU, or host: In other words, data-parallel, compute-intensive portions of applications running on the host are off-loaded onto the device.
More precisely, a portion of an application that is executed many times, but independently on different data, can be isolated into a kernel function that is executed on the GPU as many different threads.
To that effect, such a function is compiled to the PTX instruction set and the resulting kernel is translated at install time to the target GPU instruction set. 2.2. Thread Hierarchy  The batch of threads that executes a kernel is organized as a grid.
A grid consists of either cooperative thread arrays or clusters of cooperative thread arrays as described in this section and illustrated in Figure 1 and Figure 2 .
Cooperative thread arrays (CTAs) implement CUDA thread blocks and clusters implement CUDA thread block clusters. 2.2.1. Cooperative Thread Arrays  The Parallel Thread Execution (PTX) programming model is explicitly parallel: a PTX program specifies the execution of a given thread of a parallel thread array.
A cooperative thread array , or CTA, is an array of threads that execute a kernel concurrently or in parallel.
To coordinate the communication of the threads within the CTA, one can specify synchronization points where threads wait until all threads in the CTA have arrived.
Programs use a data parallel decomposition to partition inputs, work, and results across the threads of the CTA.
Each CTA thread uses its thread identifier to determine its assigned role, assign specific input and output positions, compute addresses, and select work to perform.
The thread identifier is a three-element vector tid , (with elements tid.x , tid.y , and tid.z ) that specifies the thread’s position within a 1D, 2D, or 3D CTA.
Each thread identifier component ranges from zero up to the number of thread ids in that CTA dimension.
Each CTA has a 1D, 2D, or 3D shape specified by a three-element vector ntid (with elements ntid.x , ntid.y , and ntid.z ).
Threads within a CTA execute in SIMT (single-instruction, multiple-thread) fashion in groups called warps .
A warp is a maximal subset of threads from a single CTA, such that the threads execute the same instructions at the same time.
Some applications may be able to maximize performance with knowledge of the warp size, so PTX includes a run-time immediate constant, WARP_SZ , which may be used in any instruction where an immediate operand is allowed. 2.2.2. Cluster of Cooperative Thread Arrays  Cluster is a group of CTAs that run concurrently or in parallel and can synchronize and communicate with each other via shared memory.
The executing CTA has to make sure that the shared memory of the peer CTA exists before communicating with it via shared memory and the peer CTA hasn’t exited before completing the shared memory operation.
Threads within the different CTAs in a cluster can synchronize and communicate with each other via shared memory.
Each CTA in the cluster also has a unique CTA identifier ( cluster_ctarank ) across all dimensions.
The total number of CTAs across all the dimensions in the cluster is specified by cluster_nctarank .
Threads may read and use these values through predefined, read-only special registers %cluster_ctaid , %cluster_nctaid , %cluster_ctarank , %cluster_nctarank .
If the user specifies the cluster dimensions at launch time then it will be treated as explicit cluster launch, otherwise it will be treated as implicit cluster launch with default dimension 1x1x1.
PTX provides read-only special register %is_explicit_cluster to differentiate between explicit and implicit cluster launch. 2.2.3. Grid of Clusters  There is a maximum number of threads that a CTA can contain and a maximum number of CTAs that a cluster can contain.
However, clusters with CTAs that execute the same kernel can be batched together into a grid of clusters, so that the total number of threads that can be launched in a single kernel invocation is very large.
This comes at the expense of reduced thread communication and synchronization, because threads in different clusters cannot communicate and synchronize with each other.
Threads may read and use these values through predefined, read-only special registers %tid , %ntid , %clusterid , %nclusterid , and %gridid .
Thread may use and read these values through predefined, read-only special registers %ctaid and %nctaid .
Each kernel is executed as a batch of threads organized as a grid of clusters consisting of CTAs where cluster is optional level and is applicable only for target architectures sm_90 and higher.
Figure 1 shows a grid consisting of CTAs and Figure 2 shows a grid consisting of clusters.
Grids may be launched with dependencies between one another - a grid may be a dependent grid and/or a prerequisite grid.
To understand how grid dependencies may be defined, refer to the section on CUDA Graphs in the Cuda Programming Guide .
Figure 1 Grid with CTAs  Figure 2 Grid with clusters  A cluster is a set of cooperative thread arrays (CTAs) where a CTA is a set of concurrent threads that execute the same kernel program.
A grid is a set of clusters consisting of CTAs that execute independently. 2.3. Memory Hierarchy  PTX threads may access data from multiple state spaces during their execution as illustrated by Figure 3 where cluster level is introduced from target architecture sm_90 onwards.
Each thread block (CTA) has a shared memory visible to all threads of the block and to all active blocks in the cluster and with the same lifetime as the block.
There are additional state spaces accessible by all threads: the constant, param, texture, and surface state spaces.
The global, constant, param, texture, and surface state spaces are optimized for different memory usages.
For example, texture memory offers different addressing modes as well as data filtering for specific data formats.
Note that texture and surface memory is cached, and within the same kernel call, the cache is not kept coherent with respect to global memory writes and surface memory writes, so any texture fetch or surface read to an address that has been written to via a global or a surface write in the same kernel call returns undefined data.
In other words, a thread can safely read some texture or surface memory location only if this memory location has been updated by a previous kernel call or memory copy, but not if it has been previously updated by the same thread or another thread from the same kernel call.
The global, constant, and texture state spaces are persistent across kernel launches by the same application.
Both the host and the device maintain their own local memory, referred to as host memory and device memory , respectively.
The device memory may be mapped and read or written by the host, or, for more efficient transfer, copied from the host memory through optimized API calls that utilize the device’s high-performance Direct Memory Access (DMA) engine.
A Set of SIMT Multiprocessors  The NVIDIA GPU architecture is built around a scalable array of multithreaded Streaming Multiprocessors (SMs) .
When a host program invokes a kernel grid, the blocks of the grid are enumerated and distributed to multiprocessors with available execution capacity.
A multiprocessor consists of multiple Scalar Processor (SP) cores, a multithreaded instruction unit, and on-chip shared memory.
The multiprocessor creates, manages, and executes concurrent threads in hardware with zero scheduling overhead.
Fast barrier synchronization together with lightweight thread creation and zero-overhead thread scheduling efficiently support very fine-grained parallelism, allowing, for example, a low granularity decomposition of problems by assigning one thread to each data element (such as a pixel in an image, a voxel in a volume, a cell in a grid-based computation).
To manage hundreds of threads running several different programs, the multiprocessor employs an architecture we call SIMT (single-instruction, multiple-thread) .
The multiprocessor maps each thread to one scalar processor core, and each scalar thread executes independently with its own instruction address and register state.
The multiprocessor SIMT unit creates, manages, schedules, and executes threads in groups of parallel threads called warps .
(This term originates from weaving, the first parallel thread technology.) Individual threads composing a SIMT warp start together at the same program address but are otherwise free to branch and execute independently.
When a multiprocessor is given one or more thread blocks to execute, it splits them into warps that get scheduled by the SIMT unit.
The way a block is split into warps is always the same; each warp contains threads of consecutive, increasing thread IDs with the first warp containing thread 0.
At every instruction issue time, the SIMT unit selects a warp that is ready to execute and issues the next instruction to the active threads of the warp.
A warp executes one common instruction at a time, so full efficiency is realized when all threads of a warp agree on their execution path.
If threads of a warp diverge via a data-dependent conditional branch, the warp serially executes each branch path taken, disabling threads that are not on that path, and when all paths complete, the threads converge back to the same execution path.
Branch divergence occurs only within a warp; different warps execute independently regardless of whether they are executing common or disjointed code paths.
SIMT architecture is akin to SIMD (Single Instruction, Multiple Data) vector organizations in that a single instruction controls multiple processing elements.
A key difference is that SIMD vector organizations expose the SIMD width to the software, whereas SIMT instructions specify the execution and branching behavior of a single thread.
In contrast with SIMD vector machines, SIMT enables programmers to write thread-level parallel code for independent, scalar threads, as well as data-parallel code for coordinated threads.
For the purposes of correctness, the programmer can essentially ignore the SIMT behavior; however, substantial performance improvements can be realized by taking care that the code seldom requires threads in a warp to diverge.
In practice, this is analogous to the role of cache lines in traditional code: Cache line size can be safely ignored when designing for correctness but must be considered in the code structure when designing for peak performance.
Vector architectures, on the other hand, require the software to coalesce loads into vectors and manage divergence manually.
How many blocks a multiprocessor can process at once depends on how many registers per thread and how much shared memory per block are required for a given kernel since the multiprocessor’s registers and shared memory are split among all the threads of the batch of blocks.
If there are not enough registers or shared memory available per multiprocessor to process at least one block, the kernel will fail to launch.
Figure 4 Hardware Model  A set of SIMT multiprocessors with on-chip shared memory. 3.2. Independent Thread Scheduling  On architectures prior to Volta, warps used a single program counter shared amongst all 32 threads in the warp together with an active mask specifying the active threads of the warp.
As a result, threads from the same warp in divergent regions or different states of execution cannot signal each other or exchange data, and algorithms requiring fine-grained sharing of data guarded by locks or mutexes can easily lead to deadlock, depending on which warp the contending threads come from.
Starting with the Volta architecture, Independent Thread Scheduling allows full concurrency between threads, regardless of warp.
With Independent Thread Scheduling , the GPU maintains execution state per thread, including a program counter and call stack, and can yield execution at a per-thread granularity, either to make better use of execution resources or to allow one thread to wait for data to be produced by another.
A schedule optimizer determines how to group active threads from the same warp together into SIMT units.
This retains the high throughput of SIMT execution as in prior NVIDIA GPUs, but with much more flexibility: threads can now diverge and reconverge at sub-warp granularity.
Independent Thread Scheduling can lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity of previous hardware architectures.
In particular, any warp-synchronous code (such as synchronization-free, intra-warp reductions) should be revisited to ensure compatibility with Volta and beyond.
The local and global memory spaces are read-write regions of device memory. 4. Syntax  PTX programs are a collection of text source modules (files).
PTX source modules have an assembly-language style syntax with instruction operation codes and operands.
The ptxas optimizing backend compiler optimizes and assembles PTX source modules to produce corresponding binary object files. 4.1. Source Format  Source modules are ASCII text.
All whitespace characters are equivalent; whitespace is ignored except for its use in separating tokens in the language.
The following are common preprocessor directives: #include , #define , #if , #ifdef , #else , #endif , #line , #file C: A Reference Manual by Harbison and Steele provides a good description of the C preprocessor.
Each PTX module must begin with a .version directive specifying the PTX language version, followed by a .target directive specifying the target architecture assumed.
See PTX Module Directives for a more information on these directives. 4.2. Comments  Comments in PTX follow C/C++ syntax, using non-nested /* and */ for comments that may span multiple lines, and using   to begin a comment that extends up to the next newline character, which terminates the current line.
Comments cannot occur within character constants, string literals, or within other comments.
Comments in PTX are treated as whitespace. 4.3. Statements  A PTX statement is either a directive or an instruction.
Examples .reg .b32 r1, r2; .global .f32 array[N]; start: mov.b32 r1, %tid.x; shl.b32 r1, r1, 2;   shift thread id by 2 bits ld.global.b32 r2, array[r1];   thread[tid] gets array[tid] add.f32 r2, r2, 0.5;   add 1/2 4.3.1.
Directive Statements  Directive keywords begin with a dot, so no conflict is possible with user-defined identifiers.
The directives in PTX are listed in Table 1 and described in State Spaces, Types, and Variables and Directives .
Table 1 PTX Directives  .address_size .explicitcluster .maxnreg .section .alias .extern .maxntid .shared .align .file .minnctapersm .sreg .branchtargets .func .noreturn .target .callprototype .global .param .tex .calltargets .loc .pragma .version .common .local .reg .visible .const .maxclusterrank .reqnctapercluster .weak .entry .maxnctapersm .reqntid 4.3.2.
Instruction Statements  Instructions are formed from an instruction opcode followed by a comma-separated list of zero or more operands, and terminated with a semicolon.
Operands may be register variables, constant expressions, address expressions, or label names.
The guard predicate follows the optional label and precedes the opcode, and is written as @p , where p is a predicate register.
Table 2 Reserved Instruction Keywords  abs discard min shf vadd activemask div mma shfl vadd2 add dp2a mov shl vadd4 addc dp4a movmatrix shr vavrg2 alloca elect mul sin vavrg4 and ex2 mul24 slct vmad applypriority exit multimem sqrt vmax atom fence nanosleep st vmax2 bar fma neg stackrestore vmax4 barrier fns not stacksave vmin bfe getctarank or stmatrix vmin2 bfi griddepcontrol pmevent sub vmin4 bfind isspacep popc subc vote bmsk istypep prefetch suld vset bra ld prefetchu suq vset2 brev ldmatrix prmt sured vset4 brkpt ldu rcp sust vshl brx lg2 red szext vshr call lop3 redux tanh vsub clz mad rem testp vsub2 cnot mad24 ret tex vsub4 copysign madc rsqrt tld4 wgmma cos mapa sad trap wmma cp match selp txq xor createpolicy max set vabsdiff cvt mbarrier setmaxnreg vabsdiff2 cvta membar setp vabsdiff4 4.4.
Identifiers  User-defined identifiers follow extended C++ rules: they either start with a letter followed by zero or more letters, digits, underscore, or dollar characters; or they start with an underscore, dollar, or percentage character followed by one or more letters, digits, underscore, or dollar characters: followsym: [a-zA-Z0-9_$] identifier: [a-zA-Z]{followsym}* | {[_$%]{followsym}+ PTX does not specify a maximum length for identifiers and suggests that all implementations support a minimum length of at least 1024 characters.
Many high-level languages such as C and C++ follow similar rules for identifier names, except that the percentage sign is not allowed.
The percentage sign can be used to avoid name conflicts, e.g., between user-defined variable names and compiler-generated names.
PTX predefines one constant and a small number of special registers that begin with the percentage sign, listed in Table 3 .
Table 3 Predefined Identifiers  %clock %laneid %lanemask_gt %pm0, ..., %pm7 %clock64 %lanemask_eq %nctaid %smid %ctaid %lanemask_le %ntid %tid %envreg %lanemask_lt %nsmid %warpid %gridid %lanemask_ge %nwarpid WARP_SZ 4.5.
For predicate-type data and instructions, integer constants are allowed and are interpreted as in C, i.e., zero values are False and non-zero values are True . 4.5.1. Integer Constants  Integer constants are 64-bits in size and are either signed or unsigned, i.e., every integer constant has type .s64 or .u64 .
The signed/unsigned nature of an integer constant is needed to correctly evaluate constant expressions containing operations such as division and ordered comparisons, where the behavior of the operation depends on the operand types.
When used in an instruction or data initialization, each integer constant is converted to the appropriate size based on the data or instruction type at its use.
Integer literals may be followed immediately by the letter U to indicate that the literal is unsigned.
hexadecimal literal: 0[xX]{hexdigit}+U? octal literal: 0{octal digit}+U? binary literal: 0[bB]{bit}+U? decimal literal {nonzero-digit}{digit}*U? Integer literals are non-negative and have a type determined by their magnitude and optional type suffix as follows: literals are signed ( .s64 ) unless the value cannot be fully represented in .s64 or the unsigned suffix is specified, in which case the literal is unsigned ( .u64 ).
The predefined integer constant WARP_SZ specifies the number of threads per warp for the target platform; to date, all target architectures have a WARP_SZ value of 32. 4.5.2. Floating-Point Constants  Floating-point constants are represented as 64-bit double-precision values, and all floating-point constant expressions are evaluated using 64-bit double precision arithmetic.
The only exception is the 32-bit hex notation for expressing an exact single-precision floating-point value; such values retain their exact 32-bit single-precision value and may not be used in constant expressions.
Each 64-bit floating-point constant is converted to the appropriate floating-point size based on the data or instruction type at its use.
Floating-point literals may be written with an optional decimal point and an optional signed exponent.
Unlike C and C++, there is no suffix letter to specify size; literals are always represented in 64-bit double-precision format.
PTX includes a second representation of floating-point constants for specifying the exact machine representation using a hexadecimal constant.
To specify IEEE 754 double-precision floating point values, the constant begins with 0d or 0D followed by 16 hex digits.
To specify IEEE 754 single-precision floating point values, the constant begins with 0f or 0F followed by 8 hex digits.
0[fF]{hexdigit}{8}   single-precision floating point 0[dD]{hexdigit}{16}   double-precision floating point Example mov.f32 $f3, 0F3f800000;   1.0 4.5.3.
For predicate-type data initializers and instruction operands, integer constants are interpreted as in C, i.e., zero values are False and non-zero values are True . 4.5.4. Constant Expressions  In PTX, constant expressions are formed using operators as in C and are evaluated using rules similar to those in C, but simplified by restricting types and sizes, removing most casts, and defining full semantics to eliminate cases where expression evaluation in C is implementation dependent.
Constant expressions are formed from constant literals, unary plus and minus, basic arithmetic operators (addition, subtraction, multiplication, division), comparison operators, the conditional ternary operator ( ? : ), and parentheses.
Integer constant expressions also allow unary logical negation ( ! ), bitwise complement ( ~ ), remainder ( % ), shift operators ( > ), bit-type operators ( & , | , and ^ ), and logical operators ( && , || ).
Operator precedence is highest for unary operators and decreases with each line in the chart.
Operators on the same line have the same precedence and are evaluated right-to-left for unary operators and left-to-right for binary operators.
Table 4 Operator Precedence  Kind Operator Symbols Operator Names Associates Primary () parenthesis n/a Unary +- ! ~ plus, minus, negation, complement right (.s64) (.u64) casts right Binary */ % multiplication, division, remainder left +- addition, subtraction >> = ordered comparisons == != equal, not equal & bitwise AND ^ bitwise XOR | bitwise OR && logical AND || logical OR Ternary ? : conditional right 4.5.5.
Integer Constant Expression Evaluation  Integer constant expressions are evaluated at compile time according to a set of rules that determine the type (signed .s64 versus unsigned .u64 ) of each sub-expression.
These rules are based on the rules in C, but they’ve been simplified to apply only to 64-bit integers, and behavior is fully defined in all cases (specifically, for remainder and shift operators).
Literals are signed unless unsigned is needed to prevent overflow, or unless the literal uses a U suffix.
Unary bitwise complement ( ~ ) interprets the source operand as unsigned and produces an unsigned result.
This normalization is known as the usual arithmetic conversions and simply converts both operands to unsigned type if either operand is unsigned.
Addition, subtraction, multiplication, and division perform the usual arithmetic conversions and produce a result with the same type as the converted operands.
That is, the operands and result are unsigned if either source operand is unsigned, and is otherwise signed.
Note that this differs from C, which allows a negative divisor but defines the behavior to be implementation dependent.
Left and right shift interpret the second operand as unsigned and produce a result with the same type as the first operand.
Note that the behavior of right-shift is determined by the type of the first operand: right shift of a signed value is arithmetic and preserves the sign, and right shift of an unsigned value is logical and shifts in a zero bit.
AND ( & ), OR ( | ), and XOR ( ^ ) perform the usual arithmetic conversions and produce a result with the same type as the converted operands.
Ordered comparisons ( , >= ) perform the usual arithmetic conversions on source operands and produce a signed result.
Casting of expressions to signed or unsigned is supported using ( .s64 ) and ( .u64 ) casts.
For the conditional operator ( ? : ) , the first operand must be an integer, and the second and third operands are either both integers or both floating-point.
The usual arithmetic conversions are performed on the second and third operands, and the result type is the same as the converted type. 4.5.6. Summary of Constant Expression Evaluation Rules  Table 5 contains a summary of the constant expression evaluation rules.
Table 5 Constant Expression Evaluation Rules  Kind Operator Operand Types Operand Interpretation Result Type Primary () any type same as source same as source constant literal n/a n/a .u64 , .s64 , or .f64 Unary +- any type same as source same as source ! integer zero or non-zero .s64 ~ integer .u64 .u64 Cast (.u64) integer .u64 .u64 (.s64) integer .s64 .s64 Binary +- * / .f64 .f64 .f64 integer use usual conversions converted type = .f64 .f64 .s64 integer use usual conversions .s64 == != .f64 .f64 .s64 integer use usual conversions .s64 % integer .u64 .s64 >> =10 bits).
They are supported as source or destination formats by certain instructions. 5.2.4. Packed Data Types  Certain PTX instructions operate on two sets of inputs in parallel, and produce two outputs.
In this section we describe the packed data types supported in PTX. 5.2.4.1. Packed Floating Point Data Types  PTX supports the following four variants of packed floating point data types: .f16x2 packed type containing two .f16 floating point values.
.bf16x2 , .e4m3x2 and .e5m2x2 cannot be used as fundamental types - they are supported as instruction types on certain instructions.
A register variable containing .e4m3x2 or .e5m2x2 data must be declared with .b16 type. 5.2.4.2. Packed Integer Data Types  PTX supports two variants of packed integer data types: .u16x2 and .s16x2 .
They are supported as instruction types on certain instructions. 5.3. Texture Sampler and Surface Types  PTX includes built-in opaque types for defining texture, sampler, and surface descriptor variables.
These types have named fields similar to structures, but all information about layout, field ordering, base address, and overall size is hidden to a PTX program, hence the term opaque .
The use of these opaque types is limited to: Variable definition within global (module) scope and in kernel entry parameter lists.
Static initialization of module-scope variables using comma-delimited static assignment expressions for the named members of the type.
Referencing textures, samplers, or surfaces via texture and surface load/store instructions ( tex , suld , sust , sured ).
The resulting pointer may be stored to and loaded from memory, passed as a parameter to functions, and de-referenced by texture and surface load, store, and query instructions, but the pointer cannot otherwise be treated as an address, i.e., accessing the pointer with ld and st instructions, or performing pointer arithmetic will result in undefined results.
Opaque variables may not appear in initializers, e.g., to initialize a pointer to an opaque variable.
Note Indirect access to textures and surfaces using pointers to opaque variables is supported beginning with PTX ISA version 3.1 and requires target sm_20 or later.
In the unified mode, texture and sampler information is accessed through a single .texref handle.
In the independent mode , texture and sampler information each have their own handle, allowing them to be defined separately and combined at the site of usage in the program.
In independent mode, the fields of the .texref type that describe sampler properties are ignored, since these properties are defined by .samplerref variables.
Table 9 and Table 10 list the named members of each type for unified and independent texture modes.
These members and their values have precise mappings to methods and values defined in the texture HW class as well as exposed values via the API.
Table 9 Opaque Type Fields in Unified Texture Mode  Member .texref values .surfref values width in elements height in elements depth in elements channel_data_type enum type corresponding to source language API channel_order enum type corresponding to source language API normalized_coords 0 , 1 N/A filter_mode nearest , linear N/A addr_mode_0 , addr_mode_1 , addr_mode_2 wrap , mirror , clamp_ogl , clamp_to_edge , clamp_to_border N/A array_size as number of textures in a texture array as number of surfaces in a surface array num_mipmap_levels as number of levels in a mipmapped texture N/A num_samples as number of samples in a multi-sample texture N/A memory_layout N/A 1 for linear memory layout; 0 otherwise 5.3.1.
Texture and Surface Properties  Fields width , height , and depth specify the size of the texture or surface in number of elements in each dimension.
The channel_data_type and channel_order fields specify these properties of the texture or surface using enumeration types corresponding to the source language API.
For example, see Channel Data Type and Channel Order Fields for the OpenCL enumeration types currently supported in PTX. 5.3.2. Sampler Properties  The normalized_coords field indicates whether the texture or surface uses normalized coordinates in the range [0.0, 1.0) instead of unnormalized coordinates in the range [0, N).
If no value is specified, the default is set by the runtime system based on the source language.
The filter_mode field specifies how the values returned by texture reads are computed based on the input texture coordinates.
The addr_mode_{0,1,2} fields define the addressing mode in each dimension, which determine how out-of-range coordinates are handled.
One additional sampler property, force_unnormalized_coords , is available in independent texture mode.
The force_unnormalized_coords field is a property of .samplerref variables that allows the sampler to override the texture header normalized_coords property.
When True , the texture header setting is overridden and unnormalized coordinates are used; when False , the texture header setting is used.
The force_unnormalized_coords property is used in compiling OpenCL; in OpenCL, the property of normalized coordinates is carried in sampler headers.
To compile OpenCL to PTX, texture headers are always initialized with normalized_coords set to True, and the OpenCL sampler-based normalized_coords flag maps (negated) to the PTX-level force_unnormalized_coords flag.
Variables using these types may be declared at module scope or within kernel entry parameter lists.
Example .global .texref my_texture_name; .global .samplerref my_sampler_name; .global .surfref my_surface_name; When declared at module scope, the types may be initialized using a list of static expressions assigning values to the named members.
Example .global .texref tex1; .global .samplerref tsamp1 = { addr_mode_0 = clamp_to_border, filter_mode = nearest }; 5.3.3.
Channel Data Type and Channel Order Fields  The channel_data_type and channel_order fields have enumeration types corresponding to the source language API.
Table 12 and Table 11 show the enumeration values defined in OpenCL version 1.0 for channel data type and channel order.
Table 11 OpenCL 1.0 Channel Data Type Definition  CL_SNORM_INT8 0x10D0 CL_SNORM_INT16 0x10D1 CL_UNORM_INT8 0x10D2 CL_UNORM_INT16 0x10D3 CL_UNORM_SHORT_565 0x10D4 CL_UNORM_SHORT_555 0x10D5 CL_UNORM_INT_101010 0x10D6 CL_SIGNED_INT8 0x10D7 CL_SIGNED_INT16 0x10D8 CL_SIGNED_INT32 0x10D9 CL_UNSIGNED_INT8 0x10DA CL_UNSIGNED_INT16 0x10DB CL_UNSIGNED_INT32 0x10DC CL_HALF_FLOAT 0x10DD CL_FLOAT 0x10DE Table 12 OpenCL 1.0 Channel Order Definition  CL_R 0x10B0 CL_A 0x10B1 CL_RG 0x10B2 CL_RA 0x10B3 CL_RGB 0x10B4 CL_RGBA 0x10B5 CL_BGRA 0x10B6 CL_ARGB 0x10B7 CL_INTENSITY 0x10B8 CL_LUMINANCE 0x10B9 5.4.
Variables  In PTX, a variable declaration describes both the variable’s type and its state space.
In addition to fundamental types, PTX supports types for simple aggregate objects such as vectors and arrays. 5.4.1. Variable Declarations  All storage for data is specified with variable declarations.
A variable declaration names the space in which the variable resides, its type and size, its name, an optional array size, an optional initializer, and an optional fixed address for the variable.
Examples .global .u32 loc; .reg .s32 i; .const .f32 bias[] = {-1.0, 1.0}; .global .u8 bg[4] = {0, 0, 0, 0}; .reg .v4 .f32 accel; .reg .pred p, q, r; 5.4.2.
Vectors of length 2 and 4 of any non-predicate fundamental type can be declared by prefixing the type with .v2 or .v4 .
Three-element vectors may be handled by using a .v4 vector, where the fourth element provides padding.
Examples .global .v4 .f32 V;   a length-4 vector of floats .shared .v2 .u16 uv;   a length-2 vector of unsigned ints .global .v4 .b8 v;   a length-4 vector of bytes By default, vector variables are aligned to a multiple of their overall size (vector length times base-type size), to enable vector load and store instructions which require addresses aligned to a multiple of the access size. 5.4.3. Array Declarations  Array declarations are provided to allow the programmer to reserve space.
To declare an array, the variable name is followed with dimensional declarations similar to fixed-size array declarations in C.
Examples .local .u16 kernel[19][19]; .shared .u8 mailbox[128]; The size of the array specifies how many elements should be reserved.
For the declaration of array kernel above, 19*19 = 361 halfwords are reserved, for a total of 722 bytes.
The size of the first array dimension is determined by the number of elements in the array initializer.
Examples .global .u32 index[] = { 0, 1, 2, 3, 4, 5, 6, 7 }; .global .s32 offset[][2] = { {-1, 0}, {0, -1}, {1, 0}, {0, 1} }; Array index has eight elements, and array offset is a 4x2 array. 5.4.4. Initializers  Declared variables may specify an initial value using a syntax similar to C/C++, where the variable name is followed by an equals sign and the initial value or values for the variable.
A scalar takes a single value, while vectors and arrays take nested lists of values inside of curly braces (the nesting matches the dimensionality of the declaration).
As in C, array initializers may be incomplete, i.e., the number of initializer elements may be less than the extent of the corresponding array dimension, with remaining array locations initialized to the default value for the specified array type.
Examples .const .f32 vals[8] = { 0.33, 0.25, 0.125 }; .global .s32 x[3][2] = { {1,2}, {3} }; is equivalent to .const .f32 vals[8] = { 0.33, 0.25, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0 }; .global .s32 x[3][2] = { {1,2}, {3,0}, {0,0} }; Currently, variable initialization is supported only for constant and global state spaces.
Variables in constant and global state spaces with no explicit initializer are initialized to zero by default.
Variable names appearing in initializers represent the address of the variable; this can be used to statically initialize a pointer to a variable.
Initializers may also contain var+offset expressions, where offset is a byte offset added to the address of var .
By default, the resulting address is the offset in the variable’s state space (as is the case when taking the address of a variable with a mov instruction).
An operator, generic() , is provided to create a generic address for variables used in initializers.
Starting PTX ISA version 7.1, an operator mask() is provided, where mask is an integer immediate.
The only allowed expressions in the mask() operator are integer constant expression and symbol expression representing address of variable.
The mask() operator extracts n consecutive bits from the expression used in initializers and inserts these bits at the lowest position of the initialized variable.
The number n and the starting position of the bits to be extracted is specified by the integer immediate mask .
PTX ISA version 7.1 only supports extracting a single byte starting at byte boundary from the address of the variable.
PTX ISA version 7.3 supports Integer constant expression as an operand in the mask() operator.
Supported values for mask are: 0xFF, 0xFF00, 0XFF0000, 0xFF000000, 0xFF00000000, 0xFF0000000000, 0xFF000000000000, 0xFF00000000000000.
Legacy PTX code is treated as having an implicit generic() operator for each global variable used in an initializer.
PTX 3.1 code should either include explicit generic() operators in initializers, use cvta.global to form generic addresses at runtime, or load from the non-generic address using ld.global .
Device function names appearing in initializers represent the address of the first instruction in the function; this can be used to initialize a table of function pointers to be used with indirect calls.
to initialize a table of kernel function pointers, to be used with CUDA Dynamic Parallelism to launch kernels from GPU.
Variables that hold addresses of variables or functions should be of type .u8 or .u32 or .u64 .
Examples .global .s32 n = 10; .global .f32 blur_kernel[][3] = {{.05,.1,.05},{.1,.4,.1},{.05,.1,.05}}; .global .u32 foo[] = { 2, 3, 5, 7, 9, 11 }; .global .u64 ptr = generic(foo);   generic address of foo[0] .global .u64 ptr = generic(foo)+8;   generic address of foo[2] 5.4.5.
Alignment  Byte alignment of storage for all addressable variables can be specified in the variable declaration.
Alignment is specified using an optional .align byte-count specifier immediately following the state-space specifier.
For arrays, alignment specifies the address alignment for the starting address of the entire array, not for individual elements.
The default alignment for scalar and array variables is to a multiple of the base-type size.
.const .align 4 .b8 bar[8] = {0,0,0,0,2,0,0,0}; Note that all PTX instructions that access memory require that the address be aligned to a multiple of the access size.
For example, the access size of ld.v4.b32 is 16 bytes, while the access size of atom.f16x2 is 4 bytes. 5.4.6. Parameterized Variable Names  Since PTX supports virtual registers, it is quite common for a compiler frontend to generate a large number of register names.
Rather than require explicit declaration of every name, PTX supports a syntax for creating a set of variables having a common prefix string appended with integer suffixes.
For example, suppose a program uses a large number, say one hundred, of .b32 variables, named %r0 , %r1 , …, %r99 .
These 100 register variables can be declared as follows: .reg .b32 %r;   declare %r0, %r1, ..., %r99 This shorthand syntax may be used with any of the fundamental types and with any state space, and may be preceded by an alignment specifier.
Array variables cannot be declared this way, nor are initializers permitted. 5.4.7. Variable Attributes  Variables may be declared with an optional .attribute directive which allows specifying special attributes of variables.
Variable and Function Attribute Directive: .attribute describes the .attribute directive. 5.4.8. Variable and Function Attribute Directive: .attribute  .attribute Variable and function attributes Description Used to specify special attributes of a variable or a function.
.managed .managed attribute specifies that variable will be allocated at a location in unified virtual memory environment where host and other devices in the system can reference the variable directly.
.unified .unified attribute specifies that function has the same memory address on the host and on other devices in the system.
Integer constants uuid1 and uuid2 respectively specify upper and lower 64 bits of the unique identifier associated with the function or the variable.
This attribute can only be used on device functions or on variables in the .global state space.
Variables with .unified attribute are read-only and must be loaded by specifying .unified qualifier on the address operand of ld instruction, otherwise the behavior is undefined.
Examples .global .attribute(.managed) .s32 g; .global .attribute(.managed) .u64 x; .global .attribute(.unified(19,95)) .f32 f; .func .attribute(.unified(0xAB, 0xCD)) bar() { ... } 5.5. Tensors  A tensor is a multi-dimensional matrix structure in the memory.
Tensor is defined by the following properties: Dimensionality Dimension sizes across each dimension Individual element types Tensor stride across each dimension PTX supports instructions which can operate on the tensor data.
PTX Tensor instructions include: Copying data between global and shared memories Reducing the destination tensor data with the source.
The Tensor data can be operated on by various wmma.mma , mma and wgmma.mma_async instructions.
PTX Tensor instructions treat the tensor data in the global memory as a multi-dimensional structure and treat the data in the shared memory as a linear data. 5.5.1. Tensor Dimension, size and format  Tensors can have dimensions: 1D, 2D, 3D, 4D or 5D.
The elements can have one the following types: Bit-sized type: .b32 , .b64 Integer: .u8 , .u16 , .u32 , .s32 , .u64 , .s64 Floating point and alternate floating point: .f16 , .bf16 , .tf32 , .f32 , .f64 (rounded to nearest even).
Tensor can have padding at the end in each of the dimensions to provide alignment for the data in the subsequent dimensions.
Tensor stride can be used to specify the amount of padding in each dimension. 5.5.2. Tensor Access Modes  Tensor data can be accessed in two modes: Tiled mode: In tiled mode, the source multi-dimensional tensor layout is preserved at the destination.
Im2col mode: In im2col mode, the elements in the Bounding Box of the source tensor are rearranged into columns at the destination.
Refer here for more details. 5.5.3. Tiled Mode  This section talks about how Tensor and Tensor access work in tiled mode.
Bounding Box has the following access properties: Bounding Box dimension sizes Out of boundary access mode Traversal strides The tensor-coordinates, specified in the PTX tensor instructions, specify the starting offset of the bounding box.
Starting offset of the bounding box along with the rest of the bounding box information together are used to determine the elements which are to be accessed. 5.5.3.2. Traversal-Stride  While the Bounding Box is iterating the tensor across a dimension, the traversal stride specifies the exact number of elements to be skipped.
Figure 5 illustrates tensor, tensor size, tensor stride, Bounding Box size and traversal stride.
Out of Boundary Access  PTX Tensor operation can detect and handle the case when the Bounding Box crosses the tensor boundary in any dimension.
There are 2 modes: Zero fill mode: Elements in the Bounding Box which fall outside of the tensor boundary are set to 0.
OOB-NaN fill mode: Elements in the Bounding Box which fall outside of the tensor boundary are set to a special NaN called OOB-NaN .
In this mode, the tensor data is treated as a batch of images with the following properties: N : number of images in the batch D, H, W : size of a 3D image (depth, height and width) C: channels per image element The above properties are associated with 3D, 4D and 5D tensors as follows: Dimension N/D/H/W/C applicability 3D NWC 4D NHWC 5D NDHWC 5.5.4.1.
Boundaries along other dimensions are specified by Pixels-per-Column and Channels-per-Pixel parameters as described below.
The following properties describe how to access of the elements in im2col mode: Bounding-Box Lower-Corner Bounding-Box Upper-Corner Pixels-per-Column Channels-per-Pixel Bounding-box Lower-Corner and Bounding-box Upper-Corner specify the two opposite corners of the Bounding Box in the DHW space.
Bounding-box Lower-Corner specifies the corner with the smallest coordinate and Bounding-box Upper-Corner specifies the corner with the largest coordinate.
Bounding-box Upper- and Lower-Corners are 16-bit signed values whose limits varies across the dimensions and are as shown below: 3D 4D 5D Upper- / Lower- Corner sizes [-2 15 , 2 15 -1] [-2 7 , 2 7 -1] [-2 4 , 2 4 -1] Figure 7 and Figure 8 show the Upper-Corners and Lower-Corners.
Figure 7 im2col mode bounding box example 1  Figure 8 im2col mode bounding box example 2  The Bounding-box Upper- and Lower- Corners specify only the boundaries and not the number of elements to be accessed.
The tensor coordinates, specified in the PTX tensor instructions, behaves differently in different dimensions: Across N and C dimensions: specify the starting offsets along the dimension, similar to the tiled mode.
Across DHW dimensions: specify the location of the convolution filter base in the tensor space.
The im2col offsets, specified in the PTX tensor instructions in im2col mode, are added to the filter base coordinates to determine the starting location in the tensor space from where the elements are accessed.
The size of the im2col offsets varies across the dimensions and their valid ranges are as shown below: 3D 4D 5D im2col offsets range [0, 2 16 -1] [0, 2 8 -1] [0, 2 5 -1] Following are some examples of the im2col mode accesses: Example 1 ( Figure 9 ): Tensor Size [ 0 ] = 64 Tensor Size [ 1 ] = 9 Tensor Size [ 2 ] = 14 Tensor Size [ 3 ] = 64 Pixels - per - Column = 64 channels - per - pixel = 8 Bounding - Box Lower - Corner W = -1 Bounding - Box Lower - Corner H = -1 Bounding - Box Upper - Corner W = -1 Bounding - Box Upper - Corner H = -1.
tensor coordinates = ( 7 , 7 , 4 , 0 ) im2col offsets : ( 0 , 0 ) Figure 9 im2col mode example 1  Example 2 ( Figure 10 ): Tensor Size [ 0 ] = 64 Tensor Size [ 1 ] = 9 Tensor Size [ 2 ] = 14 Tensor Size [ 3 ] = 64 Pixels - per - Column = 64 channels - per - pixel = 8 Bounding - Box Lower - Corner W = 0 Bounding - Box Lower - Corner H = 0 Bounding - Box Upper - Corner W = -2 Bounding - Box Upper - Corner H = -2 tensor coordinates = ( 7 , 7 , 4 , 0 ) im2col offsets : ( 2 , 2 ) Figure 10 im2col mode example 2  5.5.4.2.
Traversal Stride  The traversal stride, in im2col mode, does not impact the total number of elements (or pixels) being accessed unlike the tiled mode.
The number of elements traversed along the D, H and W dimensions is strided by the traversal stride for that dimension.
The following example with Figure 11 illustrates accesse with traversal-strides: Tensor Size [ 0 ] = 64 Tensor Size [ 1 ] = 8 Tensor Size [ 2 ] = 14 Tensor Size [ 3 ] = 64 Traversal Stride = 2 Pixels - per - Column = 32 channels - per - pixel = 16 Bounding - Box Lower - Corner W = -1 Bounding - Box Lower - Corner H = -1 Bounding - Box Upper - Corner W = -1 Bounding - Box Upper - Corner H = -1.
Tensor coordinates in the instruction = ( 7 , 7 , 5 , 0 ) Im2col offsets in the instruction : ( 1 , 1 ) Figure 11 im2col mode traversal stride example  5.5.4.3.
Out of Boundary Access  In im2col mode, when the number of requested pixels in NDHW space specified by Pixels-per-Column exceeds the number of available pixels in the image batch then out-of-bounds access is performed.
Similar to tiled mode, zero fill or OOB-NaN fill can be performed based on the Fill-Mode specified. 5.5.5. Interleave layout  Tensor can be interleaved and the following interleave layouts are supported: No interleave (NDHWC) 8 byte interleave (NC/8DHWC8) : C8 utilizes 16 bytes in memory assuming 2B per channel.
16 byte interleave (NC/16HWC16) : C16 utilizes 32 bytes in memory assuming 4B per channel.
The C information is organized in slices where sequential C elements are grouped in 16 byte or 32 byte quantities.
If the total number of channels is not a multiple of the number of channels per slice, then the last slice must be padded with zeros to make it complete 16B or 32B slice.
Interleaved layouts are supported only for the dimensionalities : 3D, 4D and 5D. 5.5.6. Swizzling Modes  The layout of the data in the shared memory can be different to that of global memory, for access performance reasons.
The following describes various swizzling modes: No swizzle mode: There is no swizzling in this mode and the destination data layout is exactly similar to the source data layout.
0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 … Pattern repeats … 32 byte swizzle mode: The following table, where each elements (numbered cell) is 16 byte and the starting address is 256 bytes aligned, shows the pattern of the destination data layout: 0 1 2 3 4 5 6 7 1 0 3 2 5 4 7 6 … Pattern repeats … An example of the 32 byte swizzle mode for NC/(32B)HWC(32B) tensor of 1x2x10x10xC16 dimension, with the innermost dimension holding slice of 16 channels with 2 byte/channel, is shown in Figure 12 .
Figure 12 32-byte swizzle mode example  Figure 13 shows the two fragments of the tensor : one for C/(32B) = 0 and another for C/(32B) = 1.
Figure 13 32-byte swizzle mode fragments  Figure 14 shows the destination data layout with 32 byte swizzling.
Figure 14 32-byte swizzle mode destination data layout  64 byte swizzle mode: The following table, where each elements (numbered cell) is 16 byte and the starting address is 512 bytes aligned, shows the pattern of the destination data layout: 0 1 2 3 4 5 6 7 1 0 3 2 5 4 7 6 2 3 0 1 6 7 4 5 3 2 1 0 7 6 5 4 … Pattern repeats … An example of the 64 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes / channel and 32 channels, is shown in Figure 15 .
Figure 16 64-byte swizzle mode source data layout  Figure 17 shows the destination data layout with 64 byte swizzling.
Figure 17 64-byte swizzle mode destination data layout  128 byte swizzle mode: The following table, where each elements (numbered cell) is 16 byte and the starting address is 1024 bytes aligned, shows the pattern of the destination data layout: 0 1 2 3 4 5 6 7 1 0 3 2 5 4 7 6 2 3 0 1 6 7 4 5 3 2 1 0 7 6 5 4 4 5 6 7 0 1 2 3 5 4 7 6 1 0 3 2 6 7 4 5 2 3 0 1 … Pattern repeats … An example of the 128 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes / channel and 64 channels, is shown in Figure 18 .
Figure 19 128-byte swizzle mode source data layout  Figure 20 shows the destination data layout with 128 byte swizzling.
Tensor-map  The tensor-map is a 128-byte opaque object either in .const space or .param (kernel function parameter) space or .global space which describes the tensor properties and the access properties of the tensor data described in previous sections.
Operand Type Information  All operands in instructions have a known type from their declarations.
Each operand type must be compatible with the type determined by the instruction template and instruction type.
Operands having type different from but compatible with the instruction type are silently cast to the instruction type. 6.2. Source Operands  The source operands are denoted in the instruction descriptions by the names a , b , and c .
PTX describes a load-store machine, so operands for ALU instructions must all be in variables declared in the .reg register state space.
The cvt (convert) instruction takes a variety of operand types and sizes, as its job is to convert from nearly any data type to any other data type (and size).
Most instructions have an optional predicate guard that controls conditional execution, and a few instructions have additional predicate source operands.
Predicate operands are denoted by the names p , q , r , s . 6.3. Destination Operands  PTX instructions that produce a single result store the result in the field denoted by d (for destination) in the instruction descriptions.
The result operand is a scalar or vector variable in the register state space. 6.4. Using Addresses, Arrays, and Vectors  Using scalar variables as operands is straightforward.
The interesting capabilities begin with addresses, arrays, and vectors. 6.4.1. Addresses as Operands  All the memory instructions take an address operand that specifies the memory location being accessed.
[reg+immOff] a sum of register reg containing a byte address plus a constant integer byte offset (signed, 32-bit).
[var+immOff] a sum of address of addressable variable var containing a byte address plus a constant integer byte offset (signed, 32-bit).
For example, among other things, the access may proceed by silently masking off low-order address bits to achieve proper rounding, or the instruction may fault.
Addresses are zero-extended to the specified width as needed, and truncated if the register width exceeds the state space address width for the target architecture.
All addresses and address computations are byte-based; there is no support for C-style pointer arithmetic.
Load and store operations move data between registers and locations in addressable state spaces.
The syntax is similar to that used in many assembly languages, where scalar variables are simply named and addresses are de-referenced by enclosing the address expression in square brackets.
Address expressions include variable names, address registers, address register plus byte offset, and immediate address expressions which evaluate at compile-time to a constant address.
Here are a few examples: .shared .u16 x; .reg .u16 r0; .global .v4 .f32 V; .reg .v4 .f32 W; .const .s32 tbl[256]; .reg .b32 p; .reg .s32 q; ld.shared.u16 r0,[x]; ld.global.v4.f32 W, [V]; ld.const.s32 q, [tbl+12]; mov.u32 p, tbl; 6.4.1.1.
Generic Addressing  If a memory instruction does not specify a state space, the operation is performed using generic addressing.
The state spaces .const , Kernel Function Parameters ( .param ), .local and .shared are modeled as windows within the generic address space.
Each window is defined by a window base and a window size that is equal to the size of the corresponding state space.
A generic address maps to global memory unless it falls within the window for const , local , or shared memory.
Within each window, a generic address maps to an address in the underlying state space by subtracting the window base from the generic address. 6.4.2. Arrays as Operands  Arrays of all types can be declared, and the identifier becomes an address constant in the space where the array is declared.
Array elements can be accessed using an explicitly calculated byte address, or by indexing into the array using square-bracket notation.
The expression within square brackets is either a constant integer, a register variable, or a simple register with constant offset expression, where the offset is a constant expression that is either added or subtracted from a register variable.
If more complicated indexing is desired, it must be written as an address calculation prior to use.
Examples are: ld.global.u32 s, a[0]; ld.global.u32 s, a[N-1]; mov.u32 s, a[1];   move address of a[1] into s 6.4.3.
Vectors as Operands  Vector operands are supported by a limited subset of instructions, which include mov , ld , st , atom , red and tex .
Vector elements can be extracted from the vector with the suffixes .x , .y , .z and .w , as well as the typical color fields .r , .g , .b and .a .
.reg .v4 .f32 V; .reg .f32 a, b, c, d; mov.v4.f32 {a,b,c,d}, V; Vector loads and stores can be used to implement wide loads and stores, which may improve memory performance.
The registers in the load/store operations can be a vector, or a brace-enclosed list of similarly typed scalars.
Here are examples: ld.global.v4.f32 {a,b,c,d}, [addr+16]; ld.global.v2.u32 V2, [addr+8]; Elements in a brace-enclosed vector, say {Ra, Rb, Rc, Rd}, correspond to extracted elements as follows: Ra = V.x = V.r Rb = V.y = V.g Rc = V.z = V.b Rd = V.w = V.a 6.4.4.
Labels and Function Names as Operands  Labels and function names can be used only in bra / brx.idx and call instructions respectively.
Function names can be used in mov instruction to get the address of the function into a register, for use in an indirect call.
Beginning in PTX ISA version 3.1, the mov instruction may be used to take the address of kernel functions, to be passed to a system call that initiates a kernel launch from the GPU.
This feature is part of the support for CUDA Dynamic Parallelism. 6.5. Type Conversion  All operands to all arithmetic, logic, and data movement instruction must be of the same type and size, except for operations where changing the size and/or type is part of the definition of the instruction.
Operands of different sizes or types must be converted prior to the operation. 6.5.1. Scalar Conversions  Table 13 shows what precision and format the cvt instruction uses given operands of differing types.
For example, if a cvt.s32.u16 instruction is given a u16 source operand and s32 as a destination operand, the u16 is zero-extended to s32 .
Conversions to floating-point that are beyond the range of floating-point numbers are represented with the maximum floating-point value (IEEE 754 Inf for f32 and f64 , and ~131,000 for f16 ).
1 If the destination register is wider than the destination format, the result is extended to the destination register width after chopping.
For example, cvt.s16.u32 targeting a 32-bit register first chops to 16-bit, then sign-extends to 32-bit. 6.5.2. Rounding Modifiers  Conversion instructions may specify a rounding modifier.
In PTX, there are four integer rounding modifiers and four floating-point rounding modifiers.
Table 14 Floating-Point Rounding Modifiers  Modifier Description .rn mantissa LSB rounds to nearest even .rna mantissa LSB rounds to nearest, ties away from zero .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Table 15 Integer Rounding Modifiers  Modifier Description .rni round to nearest integer, choosing even integer if source is equidistant between two integers.
.rzi round to nearest integer in the direction of zero .rmi round to nearest integer in direction of negative infinity .rpi round to nearest integer in direction of positive infinity 6.6.
The first is to have multiple threads of execution so that the hardware can issue a memory operation and then switch to other execution.
Another way to hide latency is to issue the load instructions as early as possible, as execution is not blocked until the desired result is used in a subsequent (in time) instruction.
Table 16 Cost Estimates for Accessing State-Spaces  Space Time Notes Register 0 Shared 0 Constant 0 Amortized cost is low, first access is high Local > 100 clocks Parameter 0 Immediate 0 Global > 100 clocks Texture > 100 clocks Surface > 100 clocks 7.
Abstracting the ABI  Rather than expose details of a particular calling convention, stack layout, and Application Binary Interface (ABI), PTX provides a slightly higher-level abstraction and supports multiple ABI implementations.
In this section, we describe the features of PTX needed to achieve this hiding of the ABI.
These include syntax for function definitions, function calls, parameter passing, support for variadic functions ( varargs ), and memory allocated on the stack ( alloca ).
Refer to PTX Writers Guide to Interoperability for details on generating PTX compliant with Application Binary Interface (ABI) for the CUDA ® architecture. 7.1. Function Declarations and Definitions  In PTX, functions are declared and defined using the .func directive.
A function declaration specifies an optional list of return parameters, the function name, and an optional list of input parameters; together these specify the function’s interface, or prototype.
The simplest function has no parameters or return values, and is represented in PTX as follows: .func foo { ...
Here, execution of the call instruction transfers control to foo , implicitly saving the return address.
Execution of the ret instruction within foo transfers control to the instruction following the call.
Scalar and vector base-type input and return parameters may be represented simply as register variables.
At the call, arguments may be register variables or constants, and return values may be placed directly into register variables.
The arguments and return variables at the call must have type and size that match the callee’s corresponding formal parameters.
Example .func (.reg .u32 %res) inc_ptr ( .reg .u32 %ptr, .reg .u32 %inc ) { add.u32 %res, %ptr, %inc; ret; } ...
Subword scalar objects in the source language should be promoted to 32-bit registers in PTX, or use .param state space byte arrays described next.
Objects such as C structures and unions are flattened into registers or byte arrays in PTX and are represented using .param space memory.
For example, consider the following C structure, passed by value to a function: struct { double dbl; char c[4]; }; In PTX, this structure will be flattened into a byte array.
Since memory accesses are required to be aligned to a multiple of the access size, the structure in this example will be a 12 byte array with 8 byte alignment so that accesses to the .f64 field are aligned.
The .param state space is used to pass the structure by value: Example .func (.reg .s32 out) bar (.reg .s32 x, .param .align 8 .b8 y[12]) { .reg .f64 f1; .reg .b32 c1, c2, c3, c4; ...
ld.param.f64 f1, [y+0]; ld.param.b8 c1, [y+8]; ld.param.b8 c2, [y+9]; ld.param.b8 c3, [y+10]; ld.param.b8 c4, [y+11]; ... ... computation using x,f1,c1,c2,c3,c4; } { .param .b8 .align 8 py[12]; ...
st.param.b64 [py+ 0], %rd; st.param.b8 [py+ 8], %rc1; st.param.b8 [py+ 9], %rc2; st.param.b8 [py+10], %rc1; st.param.b8 [py+11], %rc2;   scalar args in .reg space, byte array in .param space call (%out), bar, (%x, py); ...
First, a .param variable y is used in function definition bar to represent a formal parameter.
Second, a .param variable py is declared in the body of the calling function and used to set up the structure being passed to bar.
The following is a conceptual way to think about the .param state space use in device functions.
For a caller, The .param state space is used to set values that will be passed to a called function and/or to receive return values from a called function.
Typically, a .param byte array is used to collect together fields of a structure being passed by value.
For a callee, The .param state space is used to receive parameter values and/or pass return values back to the caller.
In the case of .param space formal parameters that are byte arrays, the argument must also be a .param space byte array with matching type, size, and alignment.
In the case of .param space formal parameters that are base-type scalar or vector variables, the corresponding argument may be either a .param or .reg space variable with matching type and size, or a constant that can be represented in the type of the formal parameter.
In the case of .reg space formal parameters, the corresponding argument may be either a .param or .reg space variable of matching type and size, or a constant that can be represented in the type of the formal parameter.
In the case of .reg space formal parameters, the register must be at least 32-bits in size.
All st.param instructions used for passing arguments to function call must immediately precede the corresponding call instruction and ld.param instruction used for collecting return value must immediately follow the call instruction without any control flow alteration.
This enables compiler optimization and ensures that the .param variable does not consume extra space in the caller’s frame beyond that needed by the ABI.
The .param variable simply allows a mapping to be made at the call site between data that may be in multiple locations (e.g., structure being manipulated by caller is located in registers and memory) to something that can be passed as a parameter or return value to the callee.
The .reg state space can be used to receive and return base-type scalar and vector values, including sub-word size objects when compiling in non-ABI mode.
Note that the choice of .reg or .param state space for parameter passing has no impact on whether the parameter is ultimately passed in physical registers or on the stack.
The mapping of parameters to physical registers and stack locations depends on the ABI definition and the order, size, and alignment of parameters. 7.1.1. Changes from PTX ISA Version 1.x  In PTX ISA version 1.x, formal parameters were restricted to .reg state space, and there was no support for array parameters.
Objects such as C structures were flattened and passed or returned using multiple registers.
Beginning with PTX ISA version 2.0, formal parameters may be in either .reg or .param state space, and .param space parameters support arrays.
For targets sm_20 or higher, PTX restricts functions to a single return value, and a .param byte array should be used to return objects that do not fit into a register.
PTX ISA versions prior to 3.0 permitted variables in .reg and .local state spaces to be defined at module scope.
When compiling to use the ABI, PTX ISA version 3.0 and later disallows module-scoped .reg and .local variables and restricts their use to within function scope.
When compiling without use of the ABI, module-scoped .reg and .local variables are supported as before.
When compiling legacy PTX code (ISA versions prior to 3.0) containing module-scoped .reg or .local variables, the compiler silently disables use of the ABI. 7.2. Variadic Functions  Note Support for variadic functions which was unimplemented has been removed from the spec.
PTX version 6.0 supports passing unsized array parameter to a function which can be used to implement variadic functions.
Alloca  PTX provides alloca instruction for allocating storage at runtime on the per-thread local memory stack.
The allocated stack memory can be accessed with ld.local and st.local instructions using the pointer returned by alloca .
In order to facilitate deallocation of memory allocated with alloca , PTX provides two additional instructions: stacksave which allows reading the value of stack pointer in a local variable, and stackrestore which can restore the stack pointer with the saved value.
alloca , stacksave , and stackrestore instructions are described in Stack Manipulation Instructions .
Preview Feature: Stack manipulation instructions alloca , stacksave and stackrestore are preview features in PTX ISA version 7.3.
All details are subject to change with no guarantees of backward compatibility on future PTX ISA versions or SM architectures. 8. Memory Consistency Model  In multi-threaded executions, the side-effects of memory operations performed by each thread become visible to other threads in a partial and non-identical order.
This means that any two operations may appear to happen in no order, or in different orders, to different threads.
The axioms introduced by the memory consistency model specify exactly which contradictions are forbidden between the orders observed by different threads.
In the absence of any constraint, each read operation returns the value committed by some write operation to the same memory location, including the initial write to that memory location.
The memory consistency model effectively constrains the set of such candidate writes from which a read operation can return a value. 8.1. Scope and applicability of the model  The constraints specified under this model apply to PTX programs with any PTX ISA version number, running on sm_70 or later architectures.
The memory consistency model does not apply to texture (including ld.global.nc ) and surface accesses. 8.1.1. Limitations on atomicity at system scope  When communicating with the host CPU, certain strong operations with system scope may not be performed atomically on some systems.
For more details on atomicity guarantees to host memory, see the CUDA Atomicity Requirements . 8.2. Memory operations  The fundamental storage unit in the PTX memory model is a byte, consisting of 8 bits.
Every byte in a PTX state space has a unique address relative to all threads that have access to the same state space.
The address operand contains a virtual address that gets converted to a physical address during memory access.
The physical address and the size of the data type together define a physical memory location, which is the range of bytes starting from the physical address and extending up to the size of the data type in bytes.
The memory consistency model specification uses the terms “address” or “memory address” to indicate a virtual address, and the term “memory location” to indicate a physical memory location.
Each PTX memory instruction also specifies the operation — either a read, a write or an atomic read-modify-write — to be performed on all the bytes in the corresponding memory location. 8.2.1. Overlap  Two memory locations are said to overlap when the starting address of one location is within the range of bytes constituting the other location.
Two memory operations are said to overlap when they specify the same virtual address and the corresponding memory locations overlap.
The overlap is said to be complete when both memory locations are identical, and it is said to be partial otherwise. 8.2.2. Aliases  Two distinct virtual addresses are said to be aliases if they map to the same memory location.
8.2.3. Multimem Addresses  A multimem address is a virtual address which points to multiple distinct memory locations across devices.
That is, the behavior of accessing a multimem address in any other memory operation is undefined. 8.2.4. Memory Operations on Vector Data Types  The memory consistency model relates operations executed on memory locations with scalar data types, which have a maximum size and alignment of 64 bits.
Memory operations with a vector data type are modelled as a set of equivalent memory operations with a scalar data type, executed in an unspecified order on the elements in the vector. 8.2.5. Memory Operations on Packed Data Types  A packed data type consists of two values of the same scalar data type, as described in Packed Data Types .
A memory operation on a packed data type is modelled as a pair of equivalent memory operations on the scalar data type, executed in an unspecified order on each element of the packed data. 8.2.6. Initialization  Each byte in memory is initialized by a hypothetical write W0 executed before starting any thread in the program.
If the byte is included in a program variable, and that variable has an initial value, then W0 writes the corresponding initial value for that byte; else W0 is assumed to have written an unknown but constant value to the byte. 8.3. State spaces  The relations defined in the memory consistency model are independent of state spaces.
In particular, causality order closes over all memory operations across all the state spaces.
But the side-effect of a memory operation in one state space can be observed directly only by operations that also have access to the same state space.
This further constrains the synchronizing effect of a memory operation in addition to scope.
For example, the synchronizing effect of the PTX instruction ld.relaxed.shared.sys is identical to that of ld.relaxed.shared.cluster , since no thread outside the same cluster can execute an operation that accesses the same memory location. 8.4. Operation types  For simplicity, the rest of the document refers to the following operation types, instead of mentioning specific instructions that give rise to them.
Table 17 Operation Types  Operation Type Instruction/Operation atomic operation atom or red instruction.
read operation All variants of ld instruction and atom instruction (but not red instruction).
write operation All variants of st instruction, and atomic operations if they result in a write.
strong operation A memory fence operation, or a memory operation with a .relaxed , .acquire , .release , .acq_rel , .volatile , or .mmio qualifier.
synchronizing operation A barrier instruction, fence operation, release operation or acquire operation. 8.4.1. mmio Operation  An mmio operation is a memory operation with .mmio qualifier specified.
It is usually performed on a memory location which is mapped to the control registers of peer I/O devices.
It can also be used for communication between threads but has poor performance relative to non- mmio operations.
The semantic meaning of mmio operations cannot be defined precisely as it is defined by the underlying I/O device.
For formal specification of semantics of mmio operation from Memory Consistency Model perspective, it is equivalent to the semantics of a strong operation.
But it follows a few implementation-specific properties, if it meets the CUDA atomicity requirements at the specified scope: Writes are always performed and are never combined within the scope specified.
Reads are always performed, and are not forwarded, prefetched, combined, or allowed to hit any cache within the scope specified.
In such cases the amount of data loaded is implementation specific and varies between 32 and 128 bytes in size. 8.5. Scope  Each strong operation must specify a scope , which is the set of threads that may interact directly with that operation and establish any of the relations described in the memory consistency model.
There are four scopes: Table 18 Scopes  Scope Description .cta The set of all threads executing in the same CTA as the current thread.
.gpu The set of all threads in the current program executing on the same compute device as the current thread.
This also includes other kernel grids invoked by the host program on the same compute device.
.sys The set of all threads in the current program, including all kernel grids invoked by the host program on all compute devices, and all threads constituting the host program itself.
Note that the warp is not a scope ; the CTA is the smallest collection of threads that qualifies as a scope in the memory consistency model. 8.6. Proxies  A memory proxy , or a proxy is an abstract label applied to a method of memory access.
When two memory operations use distinct methods of memory access, they are said to be different proxies .
Other operations such as textures and surfaces all use distinct methods of memory access, also distinct from the generic method.
Although virtual aliases use the generic method of memory access, since using distinct virtual addresses behaves as if using different proxies , they require a proxy fence to establish memory ordering. 8.7. Morally strong operations  Two operations are said to be morally strong relative to each other if they satisfy all of the following conditions: The operations are related in program order (i.e, they are both executed by the same thread), or each operation is strong and specifies a scope that includes the thread executing the other operation.
Most (but not all) of the axioms in the memory consistency model depend on relations between morally strong operations. 8.7.1. Conflict and Data-races  Two overlapping memory operations are said to conflict when at least one of them is a write .
Two conflicting memory operations are said to be in a data-race if they are not related in causality order and they are not morally strong . 8.7.2. Limitations on Mixed-size Data-races  A data-race between operations that overlap completely is called a uniform-size data-race , while a data-race between operations that overlap partially is called a mixed-size data-race .
The axioms in the memory consistency model do not apply if a PTX program contains one or more mixed-size data-races .
But these axioms are sufficient to describe the behavior of a PTX program with only uniform-size data-races .
Atomicity of mixed-size RMW operations In any program with or without mixed-size data-races , the following property holds for every pair of overlapping atomic operations A1 and A2 such that each specifies a scope that includes the other: Either the read-modify-write operation specified by A1 is performed completely before A2 is initiated, or vice versa.
This property holds irrespective of whether the two operations A1 and A2 overlap partially or completely. 8.8. Release and Acquire Patterns  Some sequences of instructions give rise to patterns that participate in memory synchronization as described later.
The release pattern makes prior operations from the current thread 1 visible to some operations from other threads.
The acquire pattern makes some operations from other threads visible to later operations from the current thread.
A release pattern on a location M consists of one of the following: A release operation on M E.g.
: st.release [M]; or atom.acq_rel [M]; or mbarrier.arrive.release [M]; Or a release operation on M followed by a strong write on M in program order E.g.
: st.release [M] ; st.relaxed [M]; Or a memory fence followed by a strong write on M in program order E.g.
: fence; st.relaxed [M]; Any memory synchronization established by a release pattern only affects operations occurring in program order before the first instruction in that pattern.
An acquire pattern on a location M consists of one of the following: An acquire operation on M E.g.
: ld.acquire [M]; or atom.acq_rel [M]; or mbarrier.test_wait.acquire [M]; Or a strong read on M followed by an acquire operation on M in program order E.g.
: ld.relaxed [M]; ld.acquire [M]; Or a strong read on M followed by a memory fence in program order E.g.
: ld.relaxed [M]; fence; Any memory synchronization established by an acquire pattern only affects operations occurring in program order after the last instruction in that pattern.
1 For both release and acquire patterns, this effect is further extended to operations in other threads through the transitive nature of causality order . 8.9. Ordering of memory operations  The sequence of operations performed by each thread is captured as program order while memory synchronization across threads is captured as causality order .
The visibility of the side-effects of memory operations to other memory operations is captured as communication order .
The memory consistency model defines contradictions that are disallowed between communication order on the one hand, and causality order and program order on the other. 8.9.1. Program Order  The program order relates all operations performed by a thread to the order in which a sequential processor will execute instructions in the corresponding PTX source.
It is a transitive relation that forms a total order over the operations performed by the thread, but does not relate operations from different threads. 8.9.1.1. Asynchronous Operations  Some PTX instructions (all variants of cp.async , cp.async.bulk , cp.reduce.async.bulk , wgmma.mma_async ) perform operations that are asynchronous to the thread that executed the instruction.
These asynchronous operations are ordered after prior instructions in the same thread (except in the case of wgmma.mma_async ), but they are not part of the program order for that thread.
Instead, they provide weaker ordering guarantees as documented in the instruction description.
For example, the loads and stores performed as part of a cp.async are ordered with respect to each other, but not to those of any other cp.async instructions initiated by the same thread, nor any other instruction subsequently issued by the thread with the exception of cp.async.commit_group or cp.async.mbarrier.arrive .
The asynchronous mbarrier arrive-on operation performed by a cp.async.mbarrier.arrive instruction is ordered with respect to the memory operations performed by all prior cp.async operations initiated by the same thread, but not to those of any other instruction issued by the thread.
The implicit mbarrier complete-tx operation that is part of all variants of cp.async.bulk and cp.reduce.async.bulk instructions is ordered only with respect to the memory operations performed by the same asynchronous instruction, and in particular it does not transitively establish ordering with respect to prior instructions from the issuing thread. 8.9.2. Observation Order  Observation order relates a write W to a read R through an optional sequence of atomic read-modify-write operations.
A write W precedes a read R in observation order if: R and W are morally strong and R reads the value written by W, or For some atomic operation Z, W precedes Z and Z precedes R in observation order . 8.9.3. Fence-SC Order  The Fence-SC order is an acyclic partial order, determined at runtime, that relates every pair of morally strong fence.sc operations.
8.9.4. Memory synchronization  Synchronizing operations performed by different threads synchronize with each other at runtime as described here.
A fence.sc operation X synchronizes with a fence.sc operation Y if X precedes Y in the Fence-SC order.
A bar{.cta}.sync or bar{.cta}.red or bar{.cta}.arrive operation synchronizes with a bar{.cta}.sync or bar{.cta}.red operation executed on the same barrier.
A release pattern X synchronizes with an acquire pattern Y, if a write operation in X precedes a read operation in Y in observation order , and the first operation in X and the last operation in Y are morally strong .
Completion of a task enqueued in a CUDA stream synchronizes with the start of the following task in the same stream, if any.
For purposes of the above, recording or waiting on a CUDA event in a stream, or causing a cross-stream barrier to be inserted due to cudaStreamLegacy , enqueues tasks in the associated streams even if there are no direct side effects.
An event record task synchronizes with matching event wait tasks, and a barrier arrival task synchronizes with matching barrier wait tasks.
Completion of the last task queued to a stream, if any, synchronizes with return from cudaStreamSynchronize .
Completion of the most recently queued matching event record task, if any, synchronizes with return from cudaEventSynchronize .
Synchronizing a CUDA device or context behaves as if synchronizing all streams in the context, including ones that have been destroyed.
Returning cudaSuccess from an API to query a CUDA handle, such as a stream or event, behaves the same as return from the matching synchronization API.
In addition to establishing a synchronizes relation, the CUDA API synchronization mechanisms above also participate in proxy-preserved base causality order . 8.9.5. Causality Order  Causality order captures how memory operations become visible across threads through synchronizing operations.
The axiom “Causality” uses this order to constrain the set of write operations from which a read operation may read a value.
Relations in the causality order primarily consist of relations in Base causality order 1 , which is a transitive order, determined at runtime.
Base causality order An operation X precedes an operation Y in base causality order if: X precedes Y in program order , or X synchronizes with Y, or For some operation Z, X precedes Z in program order and Z precedes Y in base causality order , or X precedes Z in base causality order and Z precedes Y in program order , or X precedes Z in base causality order and Z precedes Y in base causality order .
Proxy-preserved base causality order A memory operation X precedes a memory operation Y in proxy-preserved base causality order if X precedes Y in base causality order , and: X and Y are performed to the same address, using the generic proxy , or X and Y are performed to the same address, using the same proxy , and by the same thread block, or X and Y are aliases and there is an alias proxy fence along the base causality path from X to Y.
Causality order Causality order combines base causality order with some non-transitive relations as follows: An operation X precedes an operation Y in causality order if: X precedes Y in proxy-preserved base causality order , or For some operation Z, X precedes Z in observation order, and Z precedes Y in proxy-preserved base causality order .
1 The transitivity of base causality order accounts for the “cumulativity” of synchronizing operations. 8.9.6. Coherence Order  There exists a partial transitive order that relates overlapping write operations, determined at runtime, called the coherence order 1 .
Two overlapping write operations are related in coherence order if they are morally strong or if they are related in causality order .
Two overlapping writes are unrelated in coherence order if they are in a data-race , which gives rise to the partial nature of coherence order .
1 Coherence order cannot be observed directly since it consists entirely of write operations.
It may be observed indirectly by its use in constraining the set of candidate writes that a read operation may read from. 8.9.7. Communication Order  The communication order is a non-transitive order, determined at runtime, that relates write operations to other overlapping memory operations.
A write W precedes an overlapping read R in communication order if R returns the value of any byte that was written by W.
A write W precedes a write W’ in communication order if W precedes W’ in coherence order .
A read R precedes an overlapping write W in communication order if, for any byte accessed by both R and W, R returns the value written by a write W’ that precedes W in coherence order .
Communication order captures the visibility of memory operations — when a memory operation X1 precedes a memory operation X2 in communication order , X1 is said to be visible to X2. 8.10. Axioms  8.10.1.
Coherence  If a write W precedes an overlapping write W’ in causality order , then W must precede W’ in coherence order . 8.10.2. Fence-SC  Fence-SC order cannot contradict causality order .
For a pair of morally strong fence.sc operations F1 and F2, if F1 precedes F2 in causality order , then F1 must precede F2 in Fence-SC order. 8.10.3. Atomicity  Single-Copy Atomicity Conflicting morally strong operations are performed with single-copy atomicity .
When a read R and a write W are morally strong , then the following two communications cannot both exist in the same execution, for the set of bytes accessed by both R and W: R reads any byte from W.
Atomicity of read-modify-write (RMW) operations When an atomic operation A and a write W overlap and are morally strong , then the following two communications cannot both exist in the same execution, for the set of bytes accessed by both A and W: A reads any byte from a write W’ that precedes W in coherence order .
u32 % r0 , [ x ]; FINAL STATE : x == 2 Atomicity is guaranteed when the operations are morally strong .
u32 % r0 , [ x ]; FINAL STATE : x == 1 OR x == 2 Atomicity is not guaranteed if the operations are not morally strong . 8.10.4. No Thin Air  Values may not appear “out of thin air”: an execution cannot speculatively produce a value in such a way that the speculation becomes self-satisfying through chains of instruction dependencies and inter-thread communication.
This matches both programmer intuition and hardware reality, but is necessary to state explicitly when performing formal analysis.
u32 [ x ], % r1 ; FINAL STATE : x == 0 AND y == 0 The litmus test known as “LB” (Load Buffering) checks such forbidden values that may arise out of thin air.
Two threads T1 and T2 each read from a first variable and copy the observed result into a second variable, with the first and second variable exchanged between the threads.
If A1 reads from B2 and A2 reads from B1, then values passing through the memory operations in this example form a cycle: A1->B1->A2->B2->A1.
If any of the memory operations in this example were to speculatively associate a different value with the corresponding memory location, then such a speculation would become self-fulfilling, and hence forbidden. 8.10.5. Sequential Consistency Per Location  Within any set of overlapping memory operations that are pairwise morally strong , communication order cannot contradict program order , i.e., a concatenation of program order between overlapping operations and morally strong relations in communication order cannot result in a cycle.
This ensures that each program slice of overlapping pairwise morally strong operations is strictly sequentially-consistent .
u32 % r1 , [ x ]; IF % r0 == 1 THEN % r1 == 1 The litmus test “CoRR” (Coherent Read-Read), demonstrates one consequence of this guarantee.
A thread T1 executes a write W1 on a location x, and a thread T2 executes two (or an infinite sequence of) reads R1 and R2 on the same location x.
If R2 observed the initial value of x instead, then this would form a sequence of morally-strong relations R2->W1->R1 in communication order that contradicts the program order R1->R2 in thread T2.
Hence R2 cannot read the initial value of x in such an execution. 8.10.6. Causality  Relations in communication order cannot contradict causality order .
This constrains the set of candidate write operations that a read operation may read from: If a read R precedes an overlapping write W in causality order , then R cannot read from W.
If a write W precedes an overlapping read R in causality order , then for any byte accessed by both R and W, R cannot read from any write W’ that precedes W in coherence order .
u32 % r1 , [ data ]; IF % r0 == 1 THEN % r1 == 1 The litmus test known as “MP” (Message Passing) represents the essence of typical synchronization algorithms.
A vast majority of useful programs can be reduced to sequenced applications of this pattern.
Thread T1 first writes to a data variable and then to a flag variable while a second thread T2 first reads from the flag variable and then from the data variable.
The operations on the flag are morally strong and the memory operations in each thread are separated by a fence , and these fences are morally strong .
If R1 observes W2, then the release pattern “F1; W2” synchronizes with the acquire pattern “R1; F2”.
Then axiom causality guarantees that R2 cannot read from any write that precedes W1 in coherence order .
u32 % r1 , [ data_alias_2 ]; % r1 == 1 Virtual aliases require an alias proxy fence along the synchronization path.
Litmus Test: Store Buffering The litmus test known as “SB” (Store Buffering) demonstrates the sequential consistency enforced by the fence.sc .
A thread T1 writes to a first variable, and then reads the value of a second variable, while a second thread T2 writes to the second variable and then reads the value of the first variable.
u32 % r1 , [ x ]; % r0 == 1 OR % r1 == 1 In any execution, either F1 precedes F2 in Fence-SC order, or vice versa.
Axiom causality ensures that R2 cannot read from any write that precedes W1 in coherence order .
If each fence.sc in this example were replaced by a fence.acq_rel instruction, then this outcome is not guaranteed.
There may be an execution where the write from each thread remains unobserved from the other thread, i.e., an execution is possible, where both R1 and R2 return the initial value “0” for variables y and x respectively. 9. Instruction Set  9.1.
Format and Semantics of Instruction Descriptions  This section describes each PTX instruction.
In addition to the name and the format of the instruction, the semantics are described, followed by some examples that attempt to show several possible instantiations of the instruction. 9.2. PTX Instructions  PTX instructions generally have from zero to four operands, plus an optional guard predicate appearing after an @ symbol to the left of the opcode : @p opcode; @p opcode a; @p opcode d, a; @p opcode d, a, b; @p opcode d, a, b, c; For instructions that create a result value, the d operand is the destination operand, while a , b , and c are source operands.
Floating Point Comparisons  The ordered floating-point comparisons are eq , ne , lt , le , gt , and ge .
Table 20 Floating-Point Comparison Operators  Meaning Floating-Point Operator a == b && !isNaN(a) && !isNaN(b) eq a != b && !isNaN(a) && !isNaN(b) ne a b && !isNaN(a) && !isNaN(b) gt a >= b && !isNaN(a) && !isNaN(b) ge To aid comparison operations in the presence of NaN values, unordered floating-point comparisons are provided: equ , neu , ltu , leu , gtu , and geu .
If both operands are numeric values (not NaN ), then the comparison has the same result as its ordered counterpart.
Table 21 Floating-Point Comparison Operators Accepting NaN  Meaning Floating-Point Operator a == b || isNaN(a) || isNaN(b) equ a != b || isNaN(a) || isNaN(b) neu a b || isNaN(a) || isNaN(b) gtu a >= b || isNaN(a) || isNaN(b) geu To test for NaN values, two operators num ( numeric ) and nan ( isNaN ) are provided.
num returns True if both operands are numeric values (not NaN ), and nan returns True if either operand is NaN .
Table 22 Floating-Point Comparison Operators Testing for NaN  Meaning Floating-Point Operator !isNaN(a) && !isNaN(b) num isNaN(a) || isNaN(b) nan 9.3.2.
Manipulating Predicates  Predicate values may be computed and manipulated using the following instructions: and , or , xor , not , and mov .
There is no direct conversion between predicates and integer values, and no direct way to load or store predicate register values.
However, setp can be used to generate a predicate from an integer, and the predicate-based select ( selp ) instruction can be used to generate an integer value based on the value of a predicate; for example: selp.u32 %r1,1,0,%p;   convert predicate to 32-bit value 9.4.
Type Information for Instructions and Operands  Typed instructions must have a type-size modifier.
For example, the add instruction requires type and size information to properly perform the addition operation (signed, unsigned, float, different sizes), and this information must be specified as a suffix to the opcode.
Example .reg .u16 d, a, b; add.u16 d, a, b;   perform a 16-bit unsigned add Some instructions require multiple type-size modifiers, most notably the data conversion instruction cvt .
It requires separate type-size modifiers for the result and source, and these are placed in the same order as the operands.
For example: .reg .u16 a; .reg .f32 d; cvt.f32.u16 d, a;   convert 16-bit unsigned to 32-bit float In general, an operand’s type must agree with the corresponding instruction-type modifier.
The rules for operand and instruction type conformance are as follows: Bit-size types agree with any type of the same size.
Signed and unsigned integer types agree provided they have the same size, and integer operands are silently cast to the instruction type if needed.
For example, an unsigned integer operand used in a signed integer instruction will be treated as a signed integer by the instruction.
Floating-point types agree only if they have the same size; i.e., they must match exactly.
Table 23 Type Checking Rules  Operand Type .bX .sX .uX .fX Instruction Type .bX okay okay okay okay .sX okay okay okay invalid .uX okay okay okay invalid .fX okay invalid invalid okay Note Some operands have their type and size defined independently from the instruction type-size.
For example, the shift amount operand for left and right shift instructions always has type .u32 , while the remaining operands have their type and size determined by the instruction type.
Operand Size Exceeding Instruction-Type Size  For convenience, ld , st , and cvt instructions permit source and destination data operands to be wider than the instruction-type size, so that narrow values may be loaded, stored, and converted using regular-width registers.
For example, 8-bit or 16-bit values may be held directly in 32-bit or 64-bit registers when being loaded, stored, or converted to other types and sizes.
The operand type checking rules are relaxed for bit-size and integer (signed and unsigned) instruction types; floating-point instruction types still require that the operand type-size matches exactly, unless the operand is of bit-size type.
When a source operand has a size that exceeds the instruction-type size, the source data is truncated (chopped) to the appropriate number of bits specified by the instruction type-size.
Note that some combinations may still be invalid for a particular instruction; for example, the cvt instruction does not support .bX instruction types, so those rows are invalid for cvt .
The data are truncated (“chopped”) to the instruction-type size and interpreted according to the instruction type.
Integer source registers may be used with any appropriately-sized bit-size or integer instruction type.
The data are truncated to the instruction-type size and interpreted according to the instruction type.
Floating-point source registers can only be used with bit-size or floating-point instruction types.
When a destination operand has a size that exceeds the instruction-type size, the destination data is zero- or sign-extended to the size of the destination register.
If the corresponding instruction type is signed integer, the data is sign-extended; otherwise, the data is zero-extended.
Destination register size must be of equal or greater size than the instruction-type size.
The data are sign-extended to the destination register width for signed integer instruction types, and are zero-extended to the destination register width otherwise.
Integer destination registers may be used with any appropriately-sized bit-size or integer instruction type.
The data are sign-extended to the destination register width for signed integer instruction types, and are zero-extended to the destination register width for bit-size an d unsigned integer instruction types.
Floating-point destination registers can only be used with bit-size or floating-point instruction types.
When used with a narrower bit-size instruction type, the data are zero-extended. 9.5. Divergence of Threads in Control Constructs  Threads in a CTA execute together, at least in appearance, until they come to a conditional control construct such as a conditional branch, conditional function call, or conditional return.
If all of the threads act in unison and follow a single control flow path, the threads are called uniform .
A CTA with divergent threads may have lower performance than a CTA with uniformly executing threads, so it is important to have divergent threads re-converge as soon as possible.
All control constructs are assumed to be divergent points unless the control-flow instruction is marked as uniform, using the .uni suffix.
For divergent control flow, the optimizing code generator automatically determines points of re-convergence.
Therefore, a compiler or code author targeting PTX can ignore the issue of divergent threads, but has the opportunity to improve performance by marking branch points as uniform when the compiler or author can guarantee that the branch point is non-divergent. 9.6. Semantics  The goal of the semantic description of an instruction is to describe the results in all cases in as simple language as possible.
The semantics are described using C, until C is not expressive enough. 9.6.1. Machine-Specific Semantics of 16-bit Code  A PTX program may execute on a GPU with either a 16-bit or a 32-bit data path.
When executing on a 32-bit data path, 16-bit registers in PTX are mapped to 32-bit physical registers, and 16-bit computations are promoted to 32-bit computations.
This can lead to computational differences between code run on a 16-bit machine versus the same code run on a 32-bit machine, since the promoted computation may have bits in the high-order half-word of registers that are not present in 16-bit physical registers.
These extra precision bits can become visible at the application level, for example, by a right-shift instruction.
At the PTX language level, one solution would be to define semantics for 16-bit code that is consistent with execution on a 16-bit data path.
This approach introduces a performance penalty for 16-bit code executing on a 32-bit data path, since the translated code would require many additional masking instructions to suppress extra precision bits in the high-order half-word of 32-bit registers.
Rather than introduce a performance penalty for 16-bit code running on 32-bit GPUs, the semantics of 16-bit instructions in PTX is machine-specific.
A compiler or programmer may chose to enforce portable, machine-independent 16-bit semantics by adding explicit conversions to 16-bit values at appropriate points in the program to guarantee portability of the code.
However, for many performance-critical applications, this is not desirable, and for many applications the difference in execution is preferable to limiting performance. 9.7. Instructions  All PTX instructions may be predicated.
In the following descriptions, the optional guard predicate is omitted from the syntax. 9.7.1. Integer Arithmetic Instructions  Integer arithmetic instructions operate on the integer types in register and constant immediate forms.
The integer arithmetic instructions are: add sub mul mad mul24 mad24 sad div rem abs neg min max popc clz bfind fns brev bfe bfi bmsk szext dp4a dp2a 9.7.1.1.
Syntax add.type d, a, b; add{.sat}.s32 d, a, b;   .sat applies only to .s32 .type = { .u16, .u32, .u64, .s16, .s32, .s64, .u16x2, .s16x2 }; Description Performs addition and writes the resulting value into a destination register.
For .u16x2 , .s16x2 instruction types, forms input vectors by half word values from source operands.
Half-word operands are then added in parallel to produce .u16x2 , .s16x2 result in destination.
Semantics if (type == u16x2 || type == s16x2) { iA[0] = a[0:15]; iA[1] = a[16:31]; iB[0] = b[0:15]; iB[1] = b[16:31]; for (i = 0; i ;   for .hi variant d = t;   for .lo variant Notes The type of the operation represents the types of the a and b operands.
If .hi or .lo is specified, then d is the same size as a and b , and either the upper or lower half of the result is written to the destination register.
If .wide is specified, then d is twice as wide as a and b to receive the full result of the multiplication.
Examples mul.wide.s16 fa,fxs,fys;   16*16 bits yields 32 bits mul.lo.s16 fa,fxs,fys;   16*16 bits, save only the low 16 bits mul.wide.s32 z,x,y;   32*32 bits, creates 64 bit result 9.7.1.4.
Integer Arithmetic Instructions: mad  mad Multiply two values, optionally extract the high or low half of the intermediate result, and add a third value.
Syntax mad.mode.type d, a, b, c; mad.hi.sat.s32 d, a, b, c; .mode = { .hi, .lo, .wide }; .type = { .u16, .u32, .u64, .s16, .s32, .s64 }; Description Multiplies two values, optionally extracts the high or low half of the intermediate result, and adds a third value.
Semantics t = a * b; n = bitwidth of type; d = t + c;   for .wide d = t + c;   for .hi variant d = t + c;   for .lo variant Notes The type of the operation represents the types of the a and b operands.
If .hi or .lo is specified, then d and c are the same size as a and b , and either the upper or lower half of the result is written to the destination register.
If .wide is specified, then d and c are twice as wide as a and b to receive the result of the multiplication.
Saturation modifier: .sat limits result to MININT..MAXINT (no overflow) for the size of the operation.
Syntax mul24.mode.type d, a, b; .mode = { .hi, .lo }; .type = { .u32, .s32 }; Description Compute the product of two 24-bit integer values held in 32-bit source registers, and return either the high or low 32-bits of the 48-bit result.
Semantics t = a * b; d = t;   for .hi variant d = t;   for .lo variant Notes Integer multiplication yields a result that is twice the size of the input operands, i.e., 48-bits.
Examples mul24.lo.s32 d,a,b;   low 32-bits of 24x24-bit signed multiply. 9.7.1.6. Integer Arithmetic Instructions: mad24  mad24 Multiply two 24-bit integer values and add a third value.
Syntax mad24.mode.type d, a, b, c; mad24.hi.sat.s32 d, a, b, c; .mode = { .hi, .lo }; .type = { .u32, .s32 }; Description Compute the product of two 24-bit integer values held in 32-bit source registers, and add a third, 32-bit value to either the high or low 32-bits of the 48-bit result.
Semantics t = a * b; d = t + c;   for .hi variant d = t + c;   for .lo variant Notes Integer multiplication yields a result that is twice the size of the input operands, i.e., 48-bits.
mad24.hi performs a 24x24-bit multiply and adds the high 32 bits of the 48-bit result to a third value.
mad24.lo performs a 24x24-bit multiply and adds the low 32 bits of the 48-bit result to a third value.
Saturation modifier: .sat limits result of 32-bit signed addition to MININT..MAXINT (no overflow).
Examples mad24.lo.s32 d,a,b,c;   low 32-bits of 24x24-bit signed multiply. 9.7.1.7. Integer Arithmetic Instructions: sad  sad Sum of absolute differences.
Syntax sad.type d, a, b, c; .type = { .u16, .u32, .u64, .s16, .s32, .s64 }; Description Adds the absolute value of a-b to c and writes the resulting value into d .
Semantics d = c + ((a iB[i]) ? iA[i] : iB[i]; } } else { d = (a > b) ? a : b;   Integer (signed and unsigned) } Notes Signed and unsigned differ.
Syntax popc.type d, a; .type = { .b32, .b64 }; Description Count the number of one bits in a and place the resulting population count in 32-bit destination register d .
Semantics .u32 d = 0; while (a != 0) { if (a & 0x1) d++; a = a >> 1; } PTX ISA Notes Introduced in PTX ISA version 2.0.
Syntax clz.type d, a; .type = { .b32, .b64 }; Description Count the number of leading zeros in a starting with the most-significant bit and place the result in 32-bit destination register d .
Semantics .u32 d = 0; if (.type == .b32) { max = 32; mask = 0x80000000; } else { max = 64; mask = 0x8000000000000000; } while (d =0; i--) { if (a & (1 0) ? 1 : -1; while ((pos >= 0) && (pos = 32 && .mode == .clamp) ? true : false; mask = too_large ? 0 : (~0) > sign_pos) & 1; } d = (a & ~mask) | (sign_bit ? mask | 0); PTX ISA Notes Introduced in PTX ISA version 7.6.
Examples szext.clamp.s32 rd, ra, rb; szext.wrap.u32 rd, 0xffffffff, 0;   Result is 0. 9.7.1.22. Integer Arithmetic Instructions: bmsk  bmsk Bit Field Mask.
Syntax bmsk.mode.b32 d, a, b; .mode = { .clamp, .wrap }; Description Generates a 32-bit mask starting from the bit position specified in operand a , and of the width specified in operand b .
The resulting bitmask is 0 in the following cases: When the value of a is 32 or higher and .mode is .clamp .
When either the specified value of b or the wrapped value of b (when .mode is specified as .wrap ) is 0.
Semantics a1 = a & 0x1f; mask0 = (~0) = 32 ? true : false; bit-position-overflow = false; bit-width-overflow = false; if (.mode == .clamp) { if (a >= 32) { bit-position-overflow = true; mask0 = 0; } if (b >= 32) { bit-width-overflow = true; } } if (sum-overflow || bit-position-overflow || bit-width-overflow) { mask1 = 0; } else if (b1 == 0) { mask1 = ~0; } d = mask0 & ~mask1; Notes The bitmask width specified by operand b is limited to range 0..32 in .clamp mode and to range 0..31 in .wrap mode.
Examples bmsk.clamp.b32 rd, ra, rb; bmsk.wrap.b32 rd, 1, 2;   Creates a bitmask of 0x00000006. 9.7.1.23. Integer Arithmetic Instructions: dp4a  dp4a Four-way byte dot product-accumulate.
Syntax dp4a.atype.btype d, a, b, c; .atype = .btype = { .u32, .s32 }; Description Four-way byte dot product which is accumulated in 32-bit result.
Operand a and b are 32-bit inputs which hold 4 byte inputs in packed form for dot product.
Semantics d = c;   Extract 4 bytes from a 32bit input and sign or zero extend   based on input type.
Va = extractAndSignOrZeroExt_4(a, .atype); Vb = extractAndSignOrZeroExt_4(b, .btype); for (i = 0; i + c;   for .hi variant d = t + c;   for .lo variant carry-out from addition is written to CC.CF Notes Generally used in combination with madc and addc to implement extended-precision multi-word multiplication.
Extended-Precision Arithmetic Instructions: madc  madc Multiply two values, extract high or low half of result, and add a third value with carry-in and optional carry-out.
Syntax madc{.hi,.lo}{.cc}.type d, a, b, c; .type = { .u32, .s32, .u64, .s64 }; Description Multiplies two values, extracts either the high or low part of the result, and adds a third value along with carry-in.
Writes the result to the destination register and optionally writes the carry-out from the addition into the condition code register.
Semantics t = a * b; d = t + c + CC.CF;   for .hi variant d = t + c + CC.CF;   for .lo variant if .cc specified, carry-out from addition is written to CC.CF Notes Generally used in combination with mad.cc and addc to implement extended-precision multi-word multiplication.
Examples   extended-precision multiply: [r3,r2,r1,r0] = [r5,r4] * [r7,r6] mul.lo.u32 r0,r4,r6;   r0=(r4*r6).
[63:32]+carry-in,   may carry-out addc.u32 r3,0,0;   r3 = carry-in, no carry-out mad.lo.cc.u32 r2,r5,r7,r2;   r2+=(r5*r7).
Floating-Point Instructions  Floating-point instructions operate on .f32 and .f64 register operands and constant immediate values.
The floating-point instructions are: testp copysign add sub mul fma mad div abs neg min max rcp sqrt rsqrt sin cos lg2 ex2 tanh Instructions that support rounding modifiers are IEEE-754 compliant.
Single-precision instructions support subnormal inputs and results by default for sm_20 and subsequent targets, and flush subnormal inputs and results to sign-preserving zero for sm_1x targets.
The optional .ftz modifier on single-precision instructions provides backward compatibility with sm_1x targets by flushing subnormal inputs and results to sign-preserving zero regardless of the target architecture.
Single-precision add , sub , mul , and mad support saturation of results to the range [0.0, 1.0], with NaN s being flushed to positive zero.
NaN payloads are supported for double-precision instructions (except for rcp.approx.ftz.f64 and rsqrt.approx.ftz.f64 , which maps input NaN s to a canonical NaN ).
Note that future implementations may support NaN payloads for single-precision instructions, so PTX programs should not rely on the specific single-precision NaN s being generated.
Table 26 Summary of Floating-Point Instructions  Instruction .rn .rz .rm .rp .ftz .sat Notes {add,sub,mul}.rnd.f32 x x x x x x If no rounding modifier is specified, default is .rn and instructions may be folded into a multiply-add.
{add,sub,mul}.rnd.f64 x x x x n/a n/a If no rounding modifier is specified, default is .rn and instructions may be folded into a multiply-add.
{div,rcp,sqrt}.approx.f32 n/a n/a n/a n/a x n/a n/a rcp.approx.ftz.f64 n/a n/a n/a n/a x n/a .target sm_20 or higher {div,rcp,sqrt}.rnd.f32 x x x x x n/a .target sm_20 or higher {div,rcp,sqrt}.rnd.f64 x x x x n/a n/a .target sm_20 or higher {abs,neg,min,max}.f32 n/a n/a n/a n/a x n/a {abs,neg,min,max}.f64 n/a n/a n/a n/a n/a n/a rsqrt.approx.f32 n/a n/a n/a n/a x n/a rsqrt.approx.f64 n/a n/a n/a n/a n/a n/a rsqrt.approx.ftz.f64 n/a n/a n/a n/a x n/a .target sm_20 or higher {sin,cos,lg2,ex2}.approx.f32 n/a n/a n/a n/a x n/a tanh.approx.f32 n/a n/a n/a n/a n/a n/a .target sm_75 or higher 9.7.3.1.
Syntax testp.op.type p, a;   result is .pred .op = { .finite, .infinite, .number, .notanumber, .normal, .subnormal }; .type = { .f32, .f64 }; Description testp tests common properties of floating-point numbers and returns a predicate value of 1 if True and 0 if False .
testp.finite True if the input is not infinite or NaN testp.infinite True if the input is positive or negative infinity testp.number True if the input is not NaN testp.notanumber True if the input is NaN testp.normal True if the input is a normal number (not NaN , not infinity) testp.subnormal True if the input is a subnormal number (not NaN , not infinity) As a special case, positive and negative zero are considered normal numbers.
Syntax copysign.type d, a, b; .type = { .f32, .f64 }; Description Copy sign bit of a into value of b , and return the result as d .
Syntax add{.rnd}{.ftz}{.sat}.f32 d, a, b; add{.rnd}.f64 d, a, b; .rnd = { .rn, .rz, .rm, .rp }; Description Performs addition and writes the resulting value into a destination register.
Semantics d = a + b; Notes Rounding modifiers: .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity The default value of rounding modifier is .rn .
Note that an add instruction with an explicit rounding modifier is treated conservatively by the code optimizer.
An add instruction with no rounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code optimizer.
In particular, mul / add sequences with no rounding modifiers may be optimized to use fused-multiply-add instructions on the target device.
Rounding modifiers have the following target requirements: .rn , .rz available for all targets .rm , .rp for add.f64 , requires sm_13 or higher.
Syntax sub{.rnd}{.ftz}{.sat}.f32 d, a, b; sub{.rnd}.f64 d, a, b; .rnd = { .rn, .rz, .rm, .rp }; Description Performs subtraction and writes the resulting value into a destination register.
Semantics d = a - b; Notes Rounding modifiers: .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity The default value of rounding modifier is .rn .
Note that a sub instruction with an explicit rounding modifier is treated conservatively by the code optimizer.
A sub instruction with no rounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code optimizer.
In particular, mul / sub sequences with no rounding modifiers may be optimized to use fused-multiply-add instructions on the target device.
Rounding modifiers have the following target requirements: .rn , .rz available for all targets .rm , .rp for sub.f64 , requires sm_13 or higher.
Syntax mul{.rnd}{.ftz}{.sat}.f32 d, a, b; mul{.rnd}.f64 d, a, b; .rnd = { .rn, .rz, .rm, .rp }; Description Compute the product of two values.
Semantics d = a * b; Notes For floating-point multiplication, all operands must be the same size.
Rounding modifiers: .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity The default value of rounding modifier is .rn .
Note that a mul instruction with an explicit rounding modifier is treated conservatively by the code optimizer.
A mul instruction with no rounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code optimizer.
In particular, mul/add and mul/sub sequences with no rounding modifiers may be optimized to use fused-multiply-add instructions on the target device.
Rounding modifiers have the following target requirements: .rn , .rz available for all targets .rm , .rp for mul.f64 , requires sm_13 or higher.
Syntax fma.rnd{.ftz}{.sat}.f32 d, a, b, c; fma.rnd.f64 d, a, b, c; .rnd = { .rn, .rz, .rm, .rp }; Description Performs a fused multiply-add with no loss of precision in the intermediate product and addition.
Semantics d = a*b + c; Notes fma.f32 computes the product of a and b to infinite precision and then adds c to this product, again in infinite precision.
The resulting value is then rounded to single precision using the rounding mode specified by .rnd .
fma.f64 computes the product of a and b to infinite precision and then adds c to this product, again in infinite precision.
The resulting value is then rounded to double precision using the rounding mode specified by .rnd .
Rounding modifiers (no default): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported.
Syntax mad{.ftz}{.sat}.f32 d, a, b, c;   .target sm_1x mad.rnd{.ftz}{.sat}.f32 d, a, b, c;   .target sm_20 mad.rnd.f64 d, a, b, c;   .target sm_13 and higher .rnd = { .rn, .rz, .rm, .rp }; Description Multiplies two values and adds a third, and then writes the resulting value into a destination register.
Semantics d = a*b + c; Notes For .target sm_20 and higher: mad.f32 computes the product of a and b to infinite precision and then adds c to this product, again in infinite precision.
mad.f64 computes the product of a and b to infinite precision and then adds c to this product, again in infinite precision.
For .target sm_1x : mad.f32 computes the product of a and b at double precision, and then the mantissa is truncated to 23 bits, but the exponent is preserved.
Note that this is different from computing the product with mul , where the mantissa can be rounded and the exponent will be clamped.
The exception for mad.f32 is when c = +/-0.0 , mad.f32 is identical to the result computed using separate mul and add instructions.
When JIT-compiled for SM 2.0 devices, mad.f32 is implemented as a fused multiply-add (i.e., fma.rn.ftz.f32 ).
In this case, mad.f32 can produce slightly different numeric results and backward compatibility is not guaranteed in this case.
In PTX ISA versions 2.0 and later, a rounding modifier is required for mad.f32 for sm_20 and higher targets.
However for PTX ISA version 3.0 and earlier, ptxas does not enforce this requirement and mad.f32 silently defaults to mad.rn.f32 .
For PTX ISA version 3.1, ptxas generates a warning and defaults to mad.rn.f32 , and in subsequent releases ptxas will enforce the requirement for PTX ISA version 3.2 and later.
Rounding modifiers have the following target requirements: .rn , .rz , .rm , .rp for mad.f64 , requires sm_13 or higher.
Syntax div.approx{.ftz}.f32 d, a, b;   fast, approximate divide div.full{.ftz}.f32 d, a, b;   full-range approximate divide div.rnd{.ftz}.f32 d, a, b;   IEEE 754 compliant rounding div.rnd.f64 d, a, b;   IEEE 754 compliant rounding .rnd = { .rn, .rz, .rm, .rp }; Description Divides a by b , stores result in d .
Semantics d = a / b; Notes Fast, approximate single-precision divides: div.approx.f32 implements a fast approximation to divide, computed as d = a * (1/b) .
Examples @p min.ftz.f32 z,z,x; min.f64 a,b,c;   fp32 min with .NaN min.NaN.f32 f0,f1,f2;   fp32 min with .xorsign.abs min.xorsign.abs.f32 Rd, Ra, Rb; 9.7.3.12.
Syntax max{.ftz}{.NaN}{.xorsign.abs}.f32 d, a, b; max.f64 d, a, b; Description Store the maximum of a and b in d .
If .NaN modifier is specified, the result is canonical NaN if either of the inputs is NaN .
If .abs modifier is specified, the magnitude of destination operand d is the maximum of absolute values of both the input arguments.
If .xorsign modifier is specified, the sign bit of destination d is equal to the XOR of the sign bits of both the inputs.
Modifiers .abs and .xorsign must be specified together and .xorsign considers the sign bit of both inputs before applying .abs operation.
Semantics if (.xorsign) { xorsign = getSignBit(a) ^ getSignBit(b); if (.abs) { a = |a|; b = |b|; } } if (isNaN(a) && isNaN(b)) d = NaN; else if (.NaN && (isNaN(a) || isNaN(b))) d = NaN; else if (isNaN(a)) d = b; else if (isNaN(b)) d = a; else d = (a > b) ? a : b; if (.xorsign && !isNaN(d)) { setSignBit(d, xorsign); } Notes Subnormal numbers: sm_20+ By default, subnormal numbers are supported.
Examples max.ftz.f32 f0,f1,f2; max.f64 a,b,c;   fp32 max with .NaN max.NaN.f32 f0,f1,f2;   fp32 max with .xorsign.abs max.xorsign.abs.f32 Rd, Ra, Rb; 9.7.3.13.
Syntax rcp.approx{.ftz}.f32 d, a;   fast, approximate reciprocal rcp.rnd{.ftz}.f32 d, a;   IEEE 754 compliant rounding rcp.rnd.f64 d, a;   IEEE 754 compliant rounding .rnd = { .rn, .rz, .rm, .rp }; Description Compute 1/a , store result in d .
Semantics d = 1 / a; Notes Fast, approximate single-precision reciprocal: rcp.approx.f32 implements a fast approximation to reciprocal.
Input Result -Inf -0.0 -subnormal -Inf -0.0 -Inf +0.0 +Inf +subnormal +Inf +Inf +0.0 NaN NaN Reciprocal with IEEE 754 compliant rounding: Rounding modifiers (no default): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported.
rcp.rn.f64 and explicit modifiers .approx and .ftz were introduced in PTX ISA version 1.4.
For PTX ISA versions 1.0 through 1.3, rcp.f32 defaults to rcp.approx.ftz.f32 , and rcp.f64 defaults to rcp.rn.f64 .
Floating Point Instructions: rcp.approx.ftz.f64  rcp.approx.ftz.f64 Compute a fast, gross approximation to the reciprocal of a value.
Syntax rcp.approx.ftz.f64 d, a; Description Compute a fast, gross approximation to the reciprocal as follows: extract the most-significant 32 bits of .f64 operand a in 1.11.20 IEEE floating-point format (i.e., ignore the least-significant 32 bits of a ), compute an approximate .f64 reciprocal of this value using the most-significant 20 bits of the mantissa of operand a , place the resulting 32-bits in 1.11.20 IEEE floating-point format in the most-significant 32-bits of destination d ,and zero the least significant 32 mantissa bits of .f64 destination d .
Semantics tmp = a[63:32];   upper word of a, 1.11.20 format d[63:32] = 1.0 / tmp; d[31:0] = 0x00000000; Notes rcp.approx.ftz.f64 implements a fast, gross approximation to reciprocal.
Input a[63:32] Result d[63:32] -Inf -0.0 -subnormal -Inf -0.0 -Inf +0.0 +Inf +subnormal +Inf +Inf +0.0 NaN NaN Input NaN s map to a canonical NaN with encoding 0x7fffffff00000000 .
Syntax sqrt.approx{.ftz}.f32 d, a;   fast, approximate square root sqrt.rnd{.ftz}.f32 d, a;   IEEE 754 compliant rounding sqrt.rnd.f64 d, a;   IEEE 754 compliant rounding .rnd = { .rn, .rz, .rm, .rp }; Description Compute sqrt( a ) and store the result in d .
Semantics d = sqrt(a); Notes sqrt.approx.f32 implements a fast approximation to square root.
Input Result -Inf NaN -normal NaN -subnormal -0.0 -0.0 -0.0 +0.0 +0.0 +subnormal +0.0 +Inf +Inf NaN NaN Square root with IEEE 754 compliant rounding: Rounding modifiers (no default): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported.
sqrt.rn.f64 and explicit modifiers .approx and .ftz were introduced in PTX ISA version 1.4.
For PTX ISA versions 1.0 through 1.3, sqrt.f32 defaults to sqrt.approx.ftz.f32 , and sqrt.f64 defaults to sqrt.rn.f64 .
Floating Point Instructions: rsqrt  rsqrt Take the reciprocal of the square root of a value.
Syntax rsqrt.approx{.ftz}.f32 d, a; rsqrt.approx.f64 d, a; Description Compute 1/sqrt(a) and store the result in d .
Semantics d = 1/sqrt(a); Notes rsqrt.approx implements an approximation to the reciprocal square root.
Input Result -Inf NaN -normal NaN -subnormal -Inf -0.0 -Inf +0.0 +Inf +subnormal +Inf +Inf +0.0 NaN NaN The maximum absolute error for rsqrt.f32 is 2 -22.4 over the range 1.0-4.0.
For PTX ISA versions 1.0 through 1.3, rsqrt.f32 defaults to rsqrt.approx.ftz.f32 , and rsqrt.f64 defaults to rsqrt.approx.f64 .
Floating Point Instructions: rsqrt.approx.ftz.f64  rsqrt.approx.ftz.f64 Compute an approximation of the square root reciprocal of a value.
Syntax rsqrt.approx.ftz.f64 d, a; Description Compute a double-precision ( .f64 ) approximation of the square root reciprocal of a value.
The least significant 32 bits of the double-precision ( .f64 ) destination d are all zeros.
Semantics tmp = a[63:32];   upper word of a, 1.11.20 format d[63:32] = 1.0 / sqrt(tmp); d[31:0] = 0x00000000; Notes rsqrt.approx.ftz.f64 implements a fast approximation of the square root reciprocal of a value.
Input Result -Inf NaN -subnormal -Inf -0.0 -Inf +0.0 +Inf +subnormal +Inf +Inf +0.0 NaN NaN Input NaN s map to a canonical NaN with encoding 0x7fffffff00000000 .
Input Result -Inf NaN -subnormal -0.0 -0.0 -0.0 +0.0 +0.0 +subnormal +0.0 +Inf NaN NaN NaN The maximum absolute error is 2 -20.9 in quadrant 00.
Syntax cos.approx{.ftz}.f32 d, a; Description Find the cosine of the angle a (in radians).
Input Result -Inf NaN -subnormal +1.0 -0.0 +1.0 +0.0 +1.0 +subnormal +1.0 +Inf NaN NaN NaN The maximum absolute error is 2 -20.9 in quadrant 00.
Semantics d = log(a) / log(2); Notes lg2.approx.f32 implements a fast approximation to log 2 (a).
Input Result -Inf NaN -subnormal -Inf -0.0 -Inf +0.0 -Inf +subnormal -Inf +Inf +Inf NaN NaN The maximum absolute error is 2 -22.6 for mantissa.
Input Result -Inf +0.0 -subnormal +1.0 -0.0 +1.0 +0.0 +1.0 +subnormal +1.0 +Inf +Inf NaN NaN The maximum absolute error is 2 -22.5 for fraction in the primary range.
Floating Point Instructions: tanh  tanh Find the hyperbolic tangent of a value (in radians) Syntax tanh.approx.f32 d, a; Description Take hyperbolic tangent value of a .
Semantics d = tanh(a); Notes tanh.approx.f32 implements a fast approximation to FP32 hyperbolic-tangent.
Results of tanh for various corner-case inputs are as follows: Input Result -Inf -1.0 -subnormal Same as input -0.0 -0.0 +0.0 +0.0 +subnormal Same as input +Inf 1.0 NaN NaN The subnormal numbers are supported.
Note The subnormal inputs gets passed through to the output since the value of tanh(x) for small values of x is approximately the same as x .
Half Precision Floating-Point Instructions  Half precision floating-point instructions operate on .f16 and .f16x2 register operands.
The half precision floating-point instructions are: add sub mul fma neg abs min max tanh ex2 Half-precision add , sub , mul , and fma support saturation of results to the range [0.0, 1.0], with NaN s being flushed to positive zero.
Half-precision instructions return an unspecified NaN . 9.7.4.1. Half Precision Floating Point Instructions: add  add Add two values.
Syntax add{.rnd}{.ftz}{.sat}.f16 d, a, b; add{.rnd}{.ftz}{.sat}.f16x2 d, a, b; add{.rnd}.bf16 d, a, b; add{.rnd}.bf16x2 d, a, b; .rnd = { .rn }; Description Performs addition and writes the resulting value into a destination register.
For .f16x2 and .bf16x2 instruction type, forms input vectors by half word values from source operands.
Half-word operands are then added in parallel to produce .f16x2 or .bf16x2 result in destination.
Semantics if (type == f16 || type == bf16) { d = a + b; } else if (type == f16x2 || type == bf16x2) { fA[0] = a[0:15]; fA[1] = a[16:31]; fB[0] = b[0:15]; fB[1] = b[16:31]; for (i = 0; i -0.0.
Examples min.ftz.f16 h0,h1,h2; min.f16x2 b0,b1,b2;   SIMD fp16 min with .NaN min.NaN.f16x2 b0,b1,b2; min.bf16 h0, h1, h2;   SIMD bf16 min with NaN min.NaN.bf16x2 b0, b1, b2;   scalar bf16 min with xorsign.abs min.xorsign.abs.bf16 Rd, Ra, Rb 9.7.4.8.
Syntax max{.ftz}{.NaN}{.xorsign.abs}.f16 d, a, b; max{.ftz}{.NaN}{.xorsign.abs}.f16x2 d, a, b; max{.NaN}{.xorsign.abs}.bf16 d, a, b; max{.NaN}{.xorsign.abs}.bf16x2 d, a, b; Description Store the maximum of a and b in d .
For .f16x2 and .bf16x2 instruction types, input vectors are formed with half-word values from source operands.
Half-word operands are then processed in parallel to store .f16x2 or .bf16x2 result in destination.
Semantics if (type == f16 || type == bf16) { if (.xorsign) { xorsign = getSignBit(a) ^ getSignBit(b); if (.abs) { a = |a|; b = |b|; } } if (isNaN(a) && isNaN(b)) d = NaN; if (.NaN && (isNaN(a) || isNaN(b))) d = NaN; else if (isNaN(a)) d = b; else if (isNaN(b)) d = a; else d = (a > b) ? a : b; if (.xorsign && !isNaN(d)) { setSignBit(d, xorsign); } } else if (type == f16x2 || type == bf16x2) { fA[0] = a[0:15]; fA[1] = a[16:31]; fB[0] = b[0:15]; fB[1] = b[16:31]; for (i = 0; i fB[i]) ? fA[i] : fB[i]; if (.xorsign && !isNaN(fA[i])) { setSignBit(d[i], xorsign); } } } Notes Subnormal numbers: By default, subnormal numbers are supported.
Examples max.ftz.f16 h0,h1,h2; max.f16x2 b0,b1,b2;   SIMD fp16 max with NaN max.NaN.f16x2 b0,b1,b2;   scalar f16 max with xorsign.abs max.xorsign.abs.f16 Rd, Ra, Rb; max.bf16 h0, h1, h2;   scalar bf16 max and NaN max.NaN.bf16x2 b0, b1, b2;   SIMD bf16 max with xorsign.abs max.xorsign.abs.bf16x2 Rd, Ra, Rb; 9.7.4.9.
Half Precision Floating Point Instructions: tanh  tanh Find the hyperbolic tangent of a value (in radians) Syntax tanh.approx.type d, a; .type = {.f16, .f16x2, .bf16, .bf16x2} Description Take hyperbolic tangent value of a .
For .f16x2 or .bf16x2 instruction type, each of the half-word operands are operated in parallel and the results are packed appropriately into a .f16x2 or .bf16x2 .
Semantics if (.type == .f16 || .type == .bf16) { d = tanh(a) } else if (.type == .f16x2 || .type == .bf16x2) { fA[0] = a[0:15]; fA[1] = a[16:31]; d[0] = tanh(fA[0]) d[1] = tanh(fA[1]) } Notes tanh.approx.
{f16, f16x2, bf16, bf16x2} implements an approximate hyperbolic tangent in the target format.
Results of tanh for various corner-case inputs are as follows: Input Result -Inf -1.0 -0.0 -0.0 +0.0 +0.0 +Inf 1.0 NaN NaN The maximum absolute error for .f16 type is 2-10.987.
Examples tanh.approx.f16 h1, h0; tanh.approx.f16x2 hd1, hd0; tanh.approx.bf16 b1, b0; tanh.approx.bf16x2 hb1, hb0; 9.7.4.10.
Syntax ex2.approx.atype d, a; ex2.approx.ftz.btype d, a; .atype = { .f16, .f16x2} .btype = { .bf16, .bf16x2} Description Raise 2 to the power a .
Semantics if (.type == .f16 || .type == .bf16) { d = 2 ^ a } else if (.type == .f16x2 || .type == .bf16x2) { fA[0] = a[0:15]; fA[1] = a[16:31]; d[0] = 2 ^ fA[0] d[1] = 2 ^ fA[1] } Notes ex2.approx.
Results of ex2.approx.ftz.bf16 for various corner-case inputs are as follows: Input Result -Inf +0.0 -subnormal +1.0 -0.0 +1.0 +0.0 +1.0 +subnormal +1.0 +Inf +Inf NaN NaN Results of ex2.approx.f16 for various corner-case inputs are as follows: Input Result -Inf +0.0 -0.0 +1.0 +0.0 +1.0 +Inf +Inf NaN NaN The maximum relative error for .f16 type is 2-9.9.
Examples ex2.approx.f16 h1, h0; ex2.approx.f16x2 hd1, hd0; ex2.approx.ftz.bf16 b1, b2; ex2.approx.ftz.bf16x2 hb1, hb2; 9.7.5.
Comparison and Selection Instructions  The comparison select instructions are: set setp selp slct As with single-precision floating-point instructions, the set , setp , and slct instructions support subnormal numbers for sm_20 and higher targets and flush single-precision subnormal inputs to sign-preserving zero for sm_1x targets.
The optional .ftz modifier provides backward compatibility with sm_1x targets by flushing subnormal inputs and results to sign-preserving zero regardless of the target architecture. 9.7.5.1. Comparison and Selection Instructions: set  set Compare two numeric values with a relational operator, and optionally combine this result with a predicate value by applying a Boolean operator.
Syntax set.CmpOp{.ftz}.dtype.stype d, a, b; set.CmpOp.BoolOp{.ftz}.dtype.stype d, a, b, {! }c; .CmpOp = { eq, ne, lt, le, gt, ge, lo, ls, hi, hs, equ, neu, ltu, leu, gtu, geu, num, nan }; .BoolOp = { and, or, xor }; .dtype = { .u32, .s32, .f32 }; .stype = { .b16, .b32, .b64, .u16, .u32, .u64, .s16, .s32, .s64, .f32, .f64 }; Description Compares two numeric values and optionally combines the result with another predicate value by applying a Boolean operator.
If this result is True , 1.0f is written for floating-point destination types, and 0xffffffff is written for integer destination types.
Operand d has type .dtype ; operands a and b have type .stype ; operand c has type .pred .
Semantics t = (a CmpOp b) ? 1 : 0; if (isFloat(dtype)) d = BoolOp(t, c) ? 1.0f : 0x00000000; else d = BoolOp(t, c) ? 0xffffffff : 0x00000000; Integer Notes The signed and unsigned comparison operators are eq , ne , lt , le , gt , ge .
For unsigned values, the comparison operators lo , ls , hi , and hs for lower, lower-or-same, higher, and higher-or-same may be used instead of lt , le , gt , ge , respectively.
To aid comparison operations in the presence of NaN values, unordered versions are included: equ , neu , ltu , leu , gtu , geu .
If both operands are numeric values (not NaN ), then these comparisons have the same result as their ordered counterparts.
Comparison and Selection Instructions: setp  setp Compare two numeric values with a relational operator, and (optionally) combine this result with a predicate value by applying a Boolean operator.
Syntax setp.CmpOp{.ftz}.type p[|q], a, b; setp.CmpOp.BoolOp{.ftz}.type p[|q], a, b, {! }c; .CmpOp = { eq, ne, lt, le, gt, ge, lo, ls, hi, hs, equ, neu, ltu, leu, gtu, geu, num, nan }; .BoolOp = { and, or, xor }; .type = { .b16, .b32, .b64, .u16, .u32, .u64, .s16, .s32, .s64, .f32, .f64 }; Description Compares two values and combines the result with another predicate value by applying a Boolean operator.
A related value computed using the complement of the compare result is written to the second destination operand.
1 : 0; p = BoolOp(t, c); q = BoolOp(!t, c); Integer Notes The signed and unsigned comparison operators are eq , ne , lt , le , gt , ge .
Comparison and Selection Instructions: selp  selp Select between source operands, based on the value of the predicate source operand.
Syntax selp.type d, a, b, c; .type = { .b16, .b32, .b64, .u16, .u32, .u64, .s16, .s32, .s64, .f32, .f64 }; Description Conditional selection.
Comparison and Selection Instructions: slct  slct Select one source operand, based on the sign of the third operand.
Syntax slct.dtype.s32 d, a, b, c; slct{.ftz}.dtype.f32 d, a, b, c; .dtype = { .b16, .b32, .b64, .u16, .u32, .u64, .s16, .s32, .s64, .f32, .f64 }; Description Conditional selection.
Operands d , a , and b are treated as a bitsize type of the same width as the first instruction type; operand c must match the second instruction type ( .s32 or .f32 ).
Semantics d = (c >= 0) ? a : b; Floating Point Notes For .f32 comparisons, negative zero equals zero.
slct.ftz.dtype.f32 flushes subnormal values of operand c to sign-preserving zero, and operand a is selected.
sm_1x slct.dtype.f32 flushes subnormal values of operand c to sign-preserving zero, and operand a is selected.
Half Precision Comparison Instructions  The comparison instructions are: set setp 9.7.6.1.
Half Precision Comparison Instructions: set  set Compare two numeric values with a relational operator, and optionally combine this result with a predicate value by applying a Boolean operator.
Result of this computation is written in destination register in the following way: If result is True , 0xffffffff is written for destination types .u32 / .s32 .
1.0 in target precision floating point format is written for destination type .f16 , .bf16 .
0.0 in target precision floating point format is written for destination type .f16 , .bf16 .
If the source type is .f16x2 or .bf16x2 then result of individual operations are packed in the 32-bit destination operand.
Semantics if (stype == .f16x2 || stype == .bf16x2) { fA[0] = a[0:15]; fA[1] = a[16:31]; fB[0] = b[0:15]; fB[1] = b[16:31]; t[0] = (fA[0] CmpOp fB[0]) ? 1 : 0; t[1] = (fA[1] CmpOp fB[1]) ? 1 : 0; if (dtype == .f16x2 || stype == .bf16x2) { for (i = 0; i > (32-n)); case shf.r:   extract 32 lsbs u32 d = (b > n); } Notes Use funnel shift for multi-word shift operations and for rotate operations.
The shift amount is limited to the range 0..32 in clamp mode and 0..31 in wrap mode, so shifting multi-word values by distances greater than 32 requires first moving 32-bit words, then using shf to shift the remaining 0..31 distance.
To shift data sizes greater than 64 bits to the right, use repeated shf.r instructions applied to adjacent words, operating from least-significant word towards most-significant word.
To shift data sizes greater than 64 bits to the left, use repeated shf.l instructions applied to adjacent words, operating from most-significant word towards least-significant word.
Use funnel shift to perform 32-bit left or right rotate by supplying the same value for source arguments a and b .
Example shf.l.clamp.b32 r3,r1,r0,16;   128-bit left shift; n > n shf.r.clamp.b32 r4,r0,r1,n; shf.r.clamp.b32 r5,r1,r2,n; shf.r.clamp.b32 r6,r2,r3,n; shr.s32 r7,r3,n;   result is sign-extended shf.r.clamp.b32 r1,r0,r0,n;   rotate right by n; n > b; Notes Shift amounts greater than the register width N are clamped to N .
The sizes of the destination and first source operand must match, but not necessarily the type.
Data Movement and Conversion Instructions  These instructions copy data from place to place, and from state space to state space, possibly converting it from one format to another.
The isspacep instruction is provided to query whether a generic address falls within a particular state space window.
The cvta instruction converts addresses between generic and const , global , local , or shared state spaces.
The Data Movement and Conversion Instructions are: mov shfl.sync prmt ld ldu st st.async multimem.ld_reduce , multimem.st , multimem.red prefetch , prefetchu isspacep cvta cvt cvt.pack cp.async cp.async.commit_group cp.async.wait_group , cp.async.wait_all cp.async.bulk cp.reduce.async.bulk cp.async.bulk.prefetch cp.async.bulk.tensor cp.reduce.async.bulk.tensor cp.async.bulk.prefetch.tensor cp.async.bulk.commit_group cp.async.bulk.wait_group tensormap.replace 9.7.8.1.
Cache Operators  PTX ISA version 2.0 introduced optional cache operators on load and store instructions.
The use of a cache operator on an ld or st instruction does not change the memory consistency behavior of the program.
Table 27 Cache Operators for Memory Load Instructions  Operator Meaning .ca Cache at all levels, likely to be accessed again.
The default load instruction cache operation is ld.ca, which allocates cache lines in all levels (L1 and L2) with normal eviction policy.
Global data is coherent at the L2 level, but multiple L1 caches are not coherent for global data.
If one thread stores to global memory via one L1 cache, and a second thread loads that address via a second L1 cache with ld.ca , the second thread may get stale L1 cache data, rather than the data stored by the first thread.
The driver must invalidate global L1 cache lines between dependent grids of parallel threads.
Stores by the first grid program are then correctly fetched by the second grid program issuing default ld.ca loads cached in L1.
Use ld.cg to cache loads only globally, bypassing the L1 cache, and cache only in the L2 cache.
The ld.cs load cached streaming operation allocates global lines with evict-first policy in L1 and L2 to limit cache pollution by temporary streaming data that may be accessed once or twice.
The compiler/programmer may use ld.lu when restoring spilled registers and popping function stack frames to avoid needless write-backs of lines that will not be used again.
The ld.lu instruction performs a load cached streaming operation ( ld.cs ) on global addresses.
The ld.cv load operation applied to a global System Memory address invalidates (discards) a matching L2 line and re-fetches the line on each new load.
Table 28 Cache Operators for Memory Store Instructions  Operator Meaning .wb Cache write-back all coherent levels.
The default store instruction cache operation is st.wb , which writes back cache lines of coherent cache levels with normal eviction policy.
If one thread stores to global memory, bypassing its L1 cache, and a second thread in a different SM later loads from that address via a different L1 cache with ld.ca , the second thread may get a hit on stale L1 cache data, rather than get the data from L2 or memory stored by the first thread.
The driver must invalidate global L1 cache lines between dependent grids of thread arrays.
Stores by the first grid program are then correctly missed in L1 and fetched by the second grid program issuing default ld.ca loads.
Use st.cg to cache global store data only globally, bypassing the L1 cache, and cache only in the L2 cache.
The st.cs store cached-streaming operation allocates cache lines with evict-first policy to limit cache pollution by streaming output data.
The st.wt store write-through operation applied to a global System Memory address writes through the L2 cache. 9.7.8.2. Cache Eviction Priority Hints  PTX ISA version 7.4 adds optional cache eviction priority hints on load and store instructions.
It is supported for .global state space and generic addresses where the address points to .global state space.
Table 29 Cache Eviction Priority Hints for Memory Load and Store Instructions  Cache Eviction Priority Meaning evict_normal Cache data with normal eviction priority.
evict_first Data cached with this priority will be first in the eviction priority order and will likely be evicted when cache eviction is required.
evict_last Data cached with this priority will be last in the eviction priority order and will likely be evicted only after other data with evict_normal or evict_first eviction priotity is already evicted.
no_allocate Do not allocate data to cache. 9.7.8.3. Data Movement and Conversion Instructions: mov  mov Set a register variable with the value of a register variable or an immediate value.
Syntax mov.type d, a; mov.type d, sreg; mov.type d, avar;   get address of variable mov.type d, avar+imm;   get address of variable with offset mov.u32 d, fname;   get address of device function mov.u64 d, fname;   get address of device function mov.u32 d, kernel;   get address of entry function mov.u64 d, kernel;   get address of entry function .type = { .pred, .b16, .b32, .b64, .u16, .u32, .u64, .s16, .s32, .s64, .f32, .f64 }; Description Write register d with the value of a .
Operand a may be a register, special register, variable with optional offset in an addressable memory space, or function name.
For variables declared in .const , .global , .local , and .shared state spaces, mov places the non-generic address of the variable (i.e., the address of the variable in its state space) into the destination register.
The generic address of a variable in const , global , local , or shared state space may be generated by first taking the address within the state space with mov and then converting it to a generic address using the cvta instruction; alternately, the generic address of a variable declared in const , global , local , or shared state space may be taken directly using the cvta instruction.
Note that if the address of a device function parameter is moved to a register, the parameter will be copied onto the stack and the address will be in the local state space.
Semantics d = a; d = sreg; d = &avar;   address is non-generic; i.e., within the variable's declared state space d = &avar+imm; Notes Although only predicate and bit-size types are required, we include the arithmetic types for the programmer’s convenience: their use enhances program readability and allows additional type checking.
When moving address of a kernel or a device function, only .u32 or .u64 instruction types are allowed.
Kernel function addresses should only be used in the context of CUDA Dynamic Parallelism system calls.
Examples mov.f32 d,a; mov.u16 u,v; mov.f32 k,0.1; mov.u32 ptr, A;   move address of A into ptr mov.u32 ptr, A[5];   move address of A[5] into ptr mov.u32 ptr, A+20;   move address with offset into ptr mov.u32 addr, myFunc;   get address of device function 'myFunc' mov.u64 kptr, main;   get address of entry function 'main' 9.7.8.4.
Data Movement and Conversion Instructions: mov  mov Move vector-to-scalar (pack) or scalar-to-vector (unpack).
Syntax mov.type d, a; .type = { .b16, .b32, .b64, .b128 }; Description Write scalar register d with the packed value of vector register a , or write vector register d with the unpacked values from scalar register a .
When destination operand d is a vector register, the sink symbol '_' may be used for one or more elements provided that at least one element is a scalar register.
For bit-size types, mov may be used to pack vector elements into a scalar register or unpack sub-fields of a scalar register into a vector.
Both the overall size of the vector and the size of the scalar must match the size of the instruction type.
Semantics   pack two 8-bit elements into .b16 d = a.x | (a.y = maxLane); break; case .down: j = lane + bval; pval = (j = maxLane); break; case .down: j = lane + bval; pval = (j > 0) & 0xf; ctl[1] = (c >> 4) & 0xf; ctl[2] = (c >> 8) & 0xf; ctl[3] = (c >> 12) & 0xf; } else { ctl[0] = ctl[1] = ctl[2] = ctl[3] = (c >> 0) & 0x3; } tmp[07:00] = ReadByte( mode, ctl[0], tmp64 ); tmp[15:08] = ReadByte( mode, ctl[1], tmp64 ); tmp[23:16] = ReadByte( mode, ctl[2], tmp64 ); tmp[31:24] = ReadByte( mode, ctl[3], tmp64 ); PTX ISA Notes Introduced in PTX ISA version 2.0.
Data Movement and Conversion Instructions: ld  ld Load a register variable from an addressable state space variable.
If no sub-qualifier is specified with .shared state space, then ::cta is assumed by default.
Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands If no sub-qualifier is specified with .param state space, then : ::func is assumed when access is inside a device function.
Otherwise, when accessing device function parameters or any other .param variables from entry function ::func is assumed by default.
For ld.param::entry instruction, operand a must be a kernel parameter address, otherwise behavior is undefined.
For ld.param::func instruction, operand a must be a device function parameter address, otherwise behavior is undefined.
Instruction ld.param{::func} used for reading value returned from device function call cannot be predicated.
See Parameter State Space and Function Declarations and Definitions for descriptions of the proper use of ld.param .
The .relaxed and .acquire qualifiers indicate memory synchronization as described in the Memory Consistency Model .
The .scope qualifier indicates the set of threads with which an ld.relaxed or ld.acquire instruction can directly synchronize 1 .
The effects of this instruction become visible to other threads only when synchronization is established by other means.
An ld.volatile operation is always performed and it will not be reordered with respect to other volatile operations to the same memory location.
The qualifiers .volatile , .relaxed and .acquire may be used only with .global and .shared spaces and with generic addressing, where the address points to .global or .shared space.
The qualifier .mmio may be used only with .global space and with generic addressing, where the address points to .global space.
The optional qualifier .unified must be specified on operand a if a is the address of a variable declared with .unified attribute as described in Variable and Function Attribute Directive: .attribute .
The qualifier .level::eviction_priority specifies the eviction policy that will be used during memory access.
The .level::prefetch_size qualifier is a hint to fetch additional data of the specified size into the respective cache level.The sub-qualifier prefetch_size can be set to either of 64B , 128B , 256B thereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes respectively.
The qualifier .level::prefetch_size may only be used with .global state space and with generic addressing where the address points to .global state space.
If the generic address does not fall within the address window of the global memory, then the prefetching behavior is undefined.
When the optional argument cache-policy is specified, the qualifier .level::cache_hint is required.
The 64-bit operand cache-policy specifies the cache eviction policy that may be used during the memory access.
The qualifiers .unified and .level::cache_hint are only supported for .global state space and for generic addressing where the address points to the .global state space.
It is treated as a performance hint only, and does not change the memory consistency behavior of the program.
1 This synchronization is further extended to other threads through the transitive nature of causality order , as described in the memory consistency model.
Semantics d = a;   named variable a d = *(&a+immOff)   variable-plus-offset d = *a;   register d = *(a+immOff);   register-plus-offset d = *(immAddr);   immediate address Notes Destination d must be in the .reg state space.
The value loaded is sign-extended to the destination register width for signed integers, and is zero-extended to the destination register width for unsigned and bit-size types.
.f16 data may be loaded using ld.b16 , and then converted to .f32 or .f64 using cvt or can be used in half precision floating point instructions.
.f16x2 data may be loaded using ld.b32 and then used in half precision floating point instructions.
Support for scope qualifier, .relaxed , .acquire , .weak qualifiers introduced in PTX ISA version 6.0.
Support for .level::eviction_priority , .level::prefetch_size and .level::cache_hint qualifiers introduced in PTX ISA version 7.4.
Support for ::entry and ::func sub-qualifiers on .param space introduced in PTX ISA version 8.3.
Support for scope qualifier, .relaxed , .acquire , .weak qualifiers require sm_70 or higher.
Data Movement and Conversion Instructions: ld.global.nc  ld.global.nc Load a register variable from global state space via non-coherent cache.
Note On some architectures, the texture cache is larger, has higher bandwidth, and longer latency than the global memory cache.
For applications with sufficient parallelism to cover the longer latency, ld.global.nc should offer better performance than ld.global on such architectures.
Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands The qualifier .level::eviction_priority specifies the eviction policy that will be used during memory access.
Examples ld.global.nc.f32 d, [a]; ld.gloal.nc.L1::evict_last.u32 d, [a]; createpolicy.fractional.L2::evict_last.b64 cache-policy, 0.5; ld.global.nc.L2::cache_hint.f32 d, [a], cache-policy; ld.global.nc.L2::64B.b32 d, [a];   Prefetch 64B to L2 ld.global.nc.L2::256B.f64 d, [a];   Prefetch 256B to L2 ld.global.nc.b128 d, [a]; 9.7.8.10.
Data Movement and Conversion Instructions: ldu  ldu Load read-only data from an address that is common across threads in the warp.
Syntax ldu{.ss}.type d, [a];   load from address ldu{.ss}.vec.type d, [a];   vec load from address .ss = { .global };   state space .vec = { .v2, .v4 }; .type = { .b8, .b16, .b32, .b64, .b128, .u8, .u16, .u32, .u64, .s8, .s16, .s32, .s64, .f32, .f64 }; Description Load read-only data into register variable d from the location specified by the source address operand a in the global state space, where the address is guaranteed to be the same across all threads in the warp.
Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands Semantics d = a;   named variable a d = *(&a+immOff)   variable-plus-offset d = *a;   register d = *(a+immOff);   register-plus-offset d = *(immAddr);   immediate address Notes Destination d must be in the .reg state space.
.f16 data may be loaded using ldu.b16 , and then converted to .f32 or .f64 using cvt or can be used in half precision floating point instructions.
.f16x2 data may be loaded using ldu.b32 and then used in half precision floating point instructions.
Examples ldu.global.f32 d,[a]; ldu.global.b32 d,[p+4]; ldu.global.v4.f32 Q,[p]; ldu.global.b128 d,[a]; 9.7.8.11.
Data Movement and Conversion Instructions: st  st Store data to an addressable state space variable.
Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands If .param is specified without any sub-qualifiers then it defaults to .param::func .
Instruction st.param{::func} used for passing arguments to device function cannot be predicated.
See Parameter State Space and Function Declarations and Definitions for descriptions of the proper use of st.param .
The qualifiers .relaxed and .release indicate memory synchronization as described in the Memory Consistency Model .
The .scope qualifier indicates the set of threads with which an st.relaxed or st.release instruction can directly synchronize 1 .
An st.volatile operation is always performed and it will not be reordered with respect to other volatile operations to the same memory location.
The qualifiers .volatile , .relaxed and .release may be used only with .global and .shared spaces and with generic addressing, where the address points to .global or .shared space.
The qualifier .level::cache_hint is only supported for .global state space and for generic addressing where the address points to the .global state space.
Semantics d = a;   named variable d *(&a+immOffset) = b;   variable-plus-offset *a = b;   register *(a+immOffset) = b;   register-plus-offset *(immAddr) = b;   immediate address Notes Operand b must be in the .reg state space.
Support for scope qualifier, .relaxed , .release , .weak qualifiers introduced in PTX ISA version 6.0.
Support for .level::eviction_priority and .level::cache_hint qualifiers introduced in PTX ISA version 7.4.
Support for scope qualifier, .relaxed , .release , .weak qualifiers require sm_70 or higher.
Examples st.global.f32 [a],b; st.local.b32 [q+4],a; st.global.v4.s32 [p],Q; st.local.b32 [q+-8],a;   negative offset st.local.s32 [100],r7;   immediate address cvt.f16.f32 %r,%r;   %r is 32-bit register st.b16 [fs],%r;   store lower st.global.relaxed.sys.u32 [gbl], %r0; st.shared.release.cta.u32 [sh], %r1; st.global.relaxed.cluster.u32 [gbl], %r2; st.shared::cta.release.cta.u32 [sh + 4], %r1; st.shared::cluster.u32 [sh + 8], %r1; st.global.mmio.relaxed.sys.u32 [gbl], %r1; st.global.L1::no_allocate.f32 [p], a; createpolicy.fractional.L2::evict_last.b64 cache-policy, 0.25; st.global.L2::cache_hint.b32 [a], b, cache-policy; st.param::func.b64 [param1], %rp1; st.global.b128 [a], b;   128-bit store 9.7.8.12.
Data Movement and Conversion Instructions: st.async  st.async Asynchronous store operation on shared memory.
Syntax st.async{.weak}{.ss}{.completion_mechanism}{.vec}.type [a], b, [mbar]; .ss = { .shared::cluster }; .type = { .b32, .b64, .u32, .u64, .s32, .s64, .f32, .f64 }; .vec = { .v2, .v4 }; .completion_mechanism = { .mbarrier::complete_tx::bytes }; Description st.async is a non-blocking instruction which initiates an asynchronous store operation that stores the value specified by source operand b to the destination memory location specified by operand a .
The modifier .completion_mechanism specifies that upon completion of the asynchronous operation, complete-tx operation, with completeCount argument equal to amount of data stored in bytes, will be performed on the mbarrier object specified by the operand mbar .
Operand a represents destination address and must be a register or of the form register + immOff as described in Addresses as Operands .
The shared memory addresses of destination operand a and the mbarrier object mbar , must meet all of the following conditions: They belong to the same CTA.
They are different to the CTA of the executing thread but must be within the same cluster.
The state space of the address {.ss} , if specified, is applicable to both operands a and mbar .
If the generic addresses specified do not fall within the address window of .shared::cluster state space, then the behaviour is undefined.
The store operation in st.async is treated as a weak memory operation and the complete_tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model .
Examples st.async.shared::cluster.mbarrier::complete_tx::bytes.u32 [addr], b, [mbar_addr] 9.7.8.13.
Data Movement and Conversion Instructions: multimem.ld_reduce, multimem.st, multimem.red  The multimem.
* operations operate on multimem addresses and accesses all of the multiple memory locations which the multimem address points to.
Accessing a multimem address with ld , st or any other memory operations results in undefined behavior.
multimem.ld_reduce, multimem.st, multimem.red Perform memory operations on the multimem address.
Instruction multimem.st performs a store operation of the input operand b to all the memory locations pointed to by the multimem address a .
Instruction multimem.red performs a reduction operation on all the memory locations pointed to by the multimem address a , with operand b .
Instruction multimem.ld_reduce performs reduction on the values loaded from all the memory locations that the multimem address points to.
In contrast, the multimem.red perform reduction on all the memory locations that the multimem address points to.
Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands .
If the address specified by a does not fall within the address window of .global state space then the behavior is undefined.
For floating-point type multi- operations, the size of the specified type along with .vec must equal either 32-bits or 64-bits or 128-bits.
The following table describes the valid combinations of .op and base type: op Base type .add .u32 , .u64 , .s32 .f16 , .f16x2 , .bf16 , .bf16x2 .f32 , .f64 .and , .or , .xor .b32 , .b64 .min , .max .u32 , .s32 , .u64 , .s644 .f16 , .f16x2 , .bf16 , .bf16x2 For multimem.ld_reduce , the default precision of the intermediate accumulation is same as the specified type.
Optionally for .f16 , .f16x2 , .bf16 and .bf16x2 types, .acc::f32 can be specified to change the precision of the intermediate accumulation to .f32 .
Optional qualifiers .ldsem , .stsem and .redsem specify the memory synchronizing effect of the multimem.ld_reduce , multimem.st and multimem.red respectively, as described in Memory Consistency Model .
If explicit semantics qualifiers are not specified, then multimem.ld_reduce and multimem.st default to .weak and multimem.red defaults to .relaxed .
The optional .scope qualifier specifies the set of threads that can directly observe the memory synchronizing effect of this operation, as described in Memory Consistency Model .
If the .scope qualifier is not specified for multimem.red then .sys scope is assumed by default.
Examples multimem.ld_reduce.and.b32 val1_b32, [addr1]; multimem.ld_reduce.acquire.gpu.global.add.u32 val2_u32, [addr2]; multimem.st.relaxed.gpu.b32 [addr3], val3_b32; multimem.st.release.cta.global.u32 [addr4], val4_u32; multimem.red.relaxed.gpu.max.f64 [addr5], val5_f64; multimem.red.release.cta.global.add.v4.f32 [addr6], {val6, val7, val8, val9}; multimem.ld_reduce.add.acc::f32.v2.f16x2 {val_10, val_11}, [addr7]; 9.7.8.14.
Data Movement and Conversion Instructions: prefetch, prefetchu  prefetch, prefetchu Prefetch line containing a generic address at a specified level of memory hierarchy, in specified state space.
Syntax prefetch{.space}.level [a];   prefetch to data cache prefetch.global.level::eviction_priority [a];   prefetch to data cache prefetchu.L1 [a];   prefetch to uniform cache prefetch{.tensormap_space}.tensormap [a];   prefetch the tensormap .space = { .global, .local }; .level = { .L1, .L2 }; .level::eviction_priority = { .L2::evict_last, .L2::evict_normal }; .tensormap_space = { .const, .param }; Description The prefetch instruction brings the cache line containing the specified address in global or local memory state space into the specified cache level.
If the .tensormap qualifier is specified then the prefetch instruction brings the cache line containing the specified address in the .const or .param memory state space for subsequent use by the cp.async.bulk.tensor instruction.
Optionally, the eviction priority to be applied on the prefetched cache line can be specified by the modifier .level::eviction_priority .
Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands The prefetchu instruction brings the cache line containing the specified generic address into the specified uniform cache level.
A prefetch into the uniform cache requires a generic address, and no operation occurs if the address maps to a const , local , or shared memory location.
Examples prefetch.global.L1 [ptr]; prefetch.global.L2::evict_last [ptr]; prefetchu.L1 [addr]; prefetch.global.tensormap [ptr]; 9.7.8.15.
Data Movement and Conversion Instructions: applypriority  applypriority Apply the cache eviction priority to the specified address in the specified cache level.
Syntax applypriority{.global}.level::eviction_priority [a], size; .level::eviction_priority = { .L2::evict_normal }; Description The applypriority instruction applies the cache eviction priority specified by the .level::eviction_priority qualifier to the address range [a..a+size) in the specified cache level.
If the specified address does not fall within the address window of .global state space then the behavior is undefined.
The operand size is an integer constant that specifies the amount of data, in bytes, in the specified cache level on which the priority is to be applied.
If the data pointed to by address a is not already present in the specified cache level, then the data will be prefetched before applying the specified priority.
Data Movement and Conversion Instructions: discard  discard Invalidate the data in cache at the specified address and cache level.
Syntax discard{.global}.level [a], size; .level = { .L2 }; Description The discard instruction invalidates the data at the address range [a ..
a + (size - 1)] in the cache level specified by the .level qualifier without writing back the data in the cache to the memory.
The operand size is an integer constant that specifies the amount of data, in bytes, in the cache level specified by the .level qualifier to be discarded.
Data Movement and Conversion Instructions: createpolicy  createpolicy Create a cache eviction policy for the specified cache level.
Syntax   Range-based policy createpolicy.range{.global}.level::primary_priority{.level::secondary_priority}.b64 cache-policy, [a], primary-size, total-size;   Fraction-based policy createpolicy.fractional.level::primary_priority{.level::secondary_priority}.b64 cache-policy{, fraction};   Converting the access property from CUDA APIs createpolicy.cvt.L2.b64 cache-policy, access-property; .level::primary_priority = { .L2::evict_last, .L2::evict_normal, .L2::evict_first, .L2::evict_unchanged }; .level::secondary_priority = { .L2::evict_first, .L2::evict_unchanged }; Description The createpolicy instruction creates a cache eviction policy for the specified cache level in an opaque 64-bit register specified by the destination operand cache-policy .
The cache eviction policy specifies how cache eviction priorities are applied to global memory addresses used in memory operations with .level::cache_hint qualifier.
There are two types of cache eviction policies: Range-based policy The cache eviction policy created using createpolicy.range specifies the cache eviction behaviors for the following three address ranges: [a ..
When a range-based cache eviction policy is used in a memory operation with .level::cache_hint qualifier, the eviction priorities are applied as follows: If the memory address falls in the primary range, the eviction priority specified by .L2::primary_priority is applied.
If the memory address falls in any of the secondary ranges, the eviction priority specified by .L2::secondary_priority is applied.
If the memory address does not fall in either of the above ranges, then the applied eviction priority is unspecified.
The 32-bit operand total-size specifies the combined size, in bytes, of the address range including primary and secondary ranges.
Fraction-based policy A memory operation with .level::cache_hint qualifier can use the fraction-based cache eviction policy to request the cache eviction priority specified by .L2:primary_priority to be applied to a fraction of cache accesses specified by the 32-bit floating point operand fraction .
The remainder of the cache accesses get the eviction priority specified by .L2::secondary_priority .
This implies that in a memory operation that uses a fraction-based cache policy, the memory access has a probability specified by the operand fraction of getting the cache eviction priority specified by .L2::primary_priority .
The access property created using the CUDA APIs can be converted into cache eviction policy by the instruction createpolicy.cvt .
Examples createpolicy.fractional.L2::evict_last.b64 policy, 1.0; createpolicy.fractional.L2::evict_last.L2::evict_unchanged.b64 policy, 0.5; createpolicy.range.L2::evict_last.L2::evict_first.b64 policy, [ptr], 0x100000, 0x200000;   access-prop is created by CUDA APIs.
Data Movement and Conversion Instructions: isspacep  isspacep Query whether a generic address falls within a specified state space window.
Syntax isspacep.space p, a;   result is .pred .space = { const, .global, .local, .shared{::cta, ::cluster}, .param{::entry} }; Description Write predicate register p with 1 if generic address a falls within the specified state space window and with 0 otherwise.
isspacep.param{::entry} returns 1 if the generic address falls within the window of Kernel Function Parameters , otherwise returns 0 .
isspacep.global returns 1 for Kernel Function Parameters as .param window is contained within the .global window.
Note ispacep.shared::cluster will return 1 for every shared memory address that is accessible to the threads in the cluster, whereas ispacep.shared::cta will return 1 only if the address is of a variable declared in the executing CTA.
Examples isspacep.const iscnst, cptr; isspacep.global isglbl, gptr; isspacep.local islcl, lptr; isspacep.shared isshrd, sptr; isspacep.param::entry isparam, pptr; isspacep.shared::cta isshrdcta, sptr; isspacep.shared::cluster ishrdany sptr; 9.7.8.19.
Data Movement and Conversion Instructions: cvta  cvta Convert address from .const , Kernel Function Parameters ( .param ), .global , .local , or .shared state space to generic, or vice-versa.
Take the generic address of a variable declared in .const , Kernel Function Parameters ( .param ), .global , .local , or .shared state space.
Syntax   convert const, global, local, or shared address to generic address cvta.space.size p, a;   source address in register a cvta.space.size p, var;   get generic address of var cvta.space.size p, var+imm;   generic address of var+offset   convert generic address to const, global, local, or shared address cvta.to.space.size p, a; .space = { .const, .global, .local, .shared{::cta, ::cluster}, .param{::entry} }; .size = { .u32, .u64 }; Description Convert a const , Kernel Function Parameters ( .param ), global , local , or shared address to a generic address, or vice-versa.
For variables declared in .const , Kernel Function Parameters ( .param ), .global , .local , or .shared state space, the generic address of the variable may be taken using cvta .
The source is either a register or a variable defined in const , Kernel Function Parameters ( .param ), global , local , or shared memory with an optional offset.
When converting a generic address into a const , Kernel Function Parameters ( .param ), global , local , or shared address, the resulting address is undefined in cases where the generic address does not fall within the address window of the specified state space.
For cvta with .shared state space, the address must belong to the space specified by ::cta or ::cluster sub-qualifier, otherwise the behavior is undefined.
Note: The current implementation does not allow generic pointers to const space variables in programs that contain pointers to constant buffers passed as kernel parameters.
Examples cvta.const.u32 ptr,cvar; cvta.local.u32 ptr,lptr; cvta.shared::cta.u32 p,As+4; cvta.shared::cluster.u32 ptr, As; cvta.to.global.u32 p,gptr; cvta.param.u64 ptr,pvar; cvta.to.param::entry.u64 epptr, ptr; 9.7.8.20.
Data Movement and Conversion Instructions: cvt  cvt Convert a value from one type to another.
Syntax cvt{.irnd}{.ftz}{.sat}.dtype.atype d, a;   integer rounding cvt{.frnd}{.ftz}{.sat}.dtype.atype d, a;   fp rounding cvt.frnd2{.relu}{.satfinite}.f16.f32 d, a; cvt.frnd2{.relu}{.satfinite}.f16x2.f32 d, a, b; cvt.frnd2{.relu}{.satfinite}.bf16.f32 d, a; cvt.frnd2{.relu}{.satfinite}.bf16x2.f32 d, a, b; cvt.rna{.satfinite}.tf32.f32 d, a; cvt.frnd2{.relu}.tf32.f32 d, a; cvt.rn.satfinite{.relu}.f8x2type.f32 d, a, b; cvt.rn.satfinite{.relu}.f8x2type.f16x2 d, a; cvt.rn.
{.relu}.f16x2.f8x2type d, a; .irnd = { .rni, .rzi, .rmi, .rpi }; .frnd = { .rn, .rz, .rm, .rp }; .frnd2 = { .rn, .rz }; .dtype = .atype = { .u8, .u16, .u32, .u64, .s8, .s16, .s32, .s64, .bf16, .f16, .f32, .f64 }; .f8x2type = { .e4m3x2, .e5m2x2 }; Description Convert between different types and sizes.
For .f16x2 and .bf16x2 instruction type, two inputs a and b of .f32 type are converted into .f16 or .bf16 type and the converted values are packed in the destination register d , such that the value converted from input a is stored in the upper half of d and the value converted from input b is stored in the lower half of d For .f16x2 instruction type, destination operand d has .f16x2 or .b32 type.
When converting to .e4m3x2 / .e5m2x2 data formats, the destination operand d has .b16 type.
When converting two .f32 inputs to .e4m3x2 / .e5m2x2 , each input is converted to the specified format, and the converted values are packed in the destination operand d such that the value converted from input a is stored in the upper 8 bits of d and the value converted from input b is stored in the lower 8 bits of d .
When converting an .f16x2 input to .e4m3x2 / .e5m2x2 , each .f16 input from operand a is converted to the specified format.
The converted values are packed in the destination operand d such that the value converted from the upper 16 bits of input a is stored in the upper 8 bits of d and the value converted from the lower 16 bits of input a is stored in the lower 8 bits of d .
The converted values are packed in the destination operand d such that the value converted from the upper 8 bits of a is stored in the upper 16 bits of d and the value converted from the lower 8 bits of a is stored in the lower 16 bits of d .
Rounding modifier is mandatory in all of the following cases: float-to-float conversions, when destination type is smaller than source type All float-to-int conversions All int-to-float conversions All conversions involving .f16x2 , .e4m3x2, .e5m2x2, .bf16x2 and .tf32 instruction types.
.satfinite modifier is only supported for conversions involving the following types: .e4m3x2 and .e5m2x2 destination types.
.tf32 as destination type with rounding mode specified as round to nearest, ties away from zero.
Semantics if (/* inst type is .f16x2 or .bf16x2 */) { d[31:16] = convert(a); d[15:0] = convert(b); } else { d = convert(a); } Integer Notes Integer rounding is required for float-to-integer conversions, and for same-size float-to-float conversions where the value is rounded to an integer.
Integer rounding modifiers: .rni round to nearest integer, choosing even integer if source is equidistant between two integers .rzi round to nearest integer in the direction of zero .rmi round to nearest integer in direction of negative infinity .rpi round to nearest integer in direction of positive infinity In float-to-integer conversion, NaN inputs are converted to 0.
For cvt.ftz.dtype.f32 float-to-integer conversions and cvt.ftz.f32.f32 float-to-float conversions with integer rounding, subnormal inputs are flushed to sign-preserving zero.
Modifier .ftz can only be specified when either .dtype or .atype is .f32 and applies only to single precision ( .f32 ) inputs and results.
sm_1x For cvt.ftz.dtype.f32 float-to-integer conversions and cvt.ftz.f32.f32 float-to-float conversions with integer rounding, subnormal inputs are flushed to sign-preserving zero.
Note: In PTX ISA versions 1.4 and earlier, the cvt instruction did not flush single-precision subnormal inputs or results to zero if the destination type size was 64-bits.
Saturation modifier: .sat For integer destination types, .sat limits the result to MININT..MAXINT for the size of the operation.
The saturation modifier is allowed only in cases where the destination type’s value range is not a superset of the source type’s value range; i.e., the .sat modifier is illegal in cases where saturation is not possible based on the source and destination types.
For float-to-integer conversions, the result is clamped to the destination range by default; i.e, .sat is redundant.
Floating Point Notes Floating-point rounding is required for float-to-float conversions that result in loss of precision, and for integer-to-float conversions.
Floating-point rounding modifiers: .rn mantissa LSB rounds to nearest even .rna mantissa LSB rounds to nearest, ties away from zero .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity A floating-point value may be rounded to an integral value using the integer rounding modifiers (see Integer Notes).
Modifier .ftz may be specified to flush single-precision subnormal inputs and results to sign-preserving zero.
Note: In PTX ISA versions 1.4 and earlier, the cvt instruction did not flush single-precision subnormal inputs or results to zero if either source or destination type was .f64 .
Specifically, if the PTX ISA version is 1.4 or earlier, single-precision subnormal inputs and results are flushed to sign-preserving zero only for cvt.f32.f16 , cvt.f16.f32 , and cvt.f32.f32 instructions.
Saturation modifier: .sat : For floating-point destination types, .sat limits the result to the range [0.0, 1.0].
.relu : For .f16 , .f16x2 , .bf16 , .bf16x2 , .e4m3x2 , .e5m2x2 and .tf32 destination types, .relu clamps the result to 0 if negative.
.satfinite : For .f16 , .f16x2 , .bf16 , .bf16x2 , .e4m3x2 , .e5m2x2 and .tf32 destination formats, if the input value is NaN , then the result is NaN in the specified destination format.
If the absolute value of input (ignoring sign) is greater than MAX_NORM of the specified destination format, then the result is sign-preserved MAX_NORM of the destination format.
Notes A source register wider than the specified type may be used, except when the source operand has .bf16 or .bf16x2 format.
See Operand Size Exceeding Instruction-Type Size for a description of these relaxed type-checking rules.
A destination register wider than the specified type may be used, except when the destination operand has .bf16 , .bf16x2 or .tf32 format.
The result of conversion is sign-extended to the destination register width for signed integers, and is zero-extended to the destination register width for unsigned, bit-size, and floating-point types.
.relu modifier and { .f16x2 , .bf16 , .bf16x2 , .tf32 } destination formats introduced in PTX ISA version 7.0.
.relu modifier and { .f16x2 , .bf16 , .bf16x2 , .tf32 } destination formats require sm_80 or higher.
Data Movement and Conversion Instructions: cvt.pack  cvt.pack Convert two integer values from one integer type to another and pack the results.
Syntax cvt.pack.sat.convertType.abType d, a, b; .convertType = { .u16, .s16 } .abType = { .s32 } cvt.pack.sat.convertType.abType.cType d, a, b, c; .convertType = { .u2, .s2, .u4, .s4, .u8, .s8 } .abType = { .s32 } .cType = { .b32 } Description Convert two 32-bit integers a and b into specified type and pack the results into d .
Source operands a and b are integers of type .abType and the source operand c is an integer of type .cType .
The inputs a and b are converted to values of type specified by .convertType with saturation and the results after conversion are packed into lower bits of d .
Data Movement and Conversion Instructions: cp.reduce.async.bulk  cp.reduce.async.bulk Initiates an asynchronous reduction operation.
The size of the source and the destination array must be the same and is specified by the operand size .
Each data element in the destination array is reduced inline with the corresponding data element in the source array with the reduction operation specified by the modifier .redOp .
The type of each data element in the source and the destination array is specified by the modifier .type .
The source address operand srcMem is located in the state space specified by .src and the destination address operand dstMem is located in the state specified by the .dst .
The 32-bit operand size specifies the amount of memory to be copied from the source location and used in the reduction operation, in terms of number of bytes.
The memory range [dstMem, dstMem + size - 1] must not overflow the destination memory space and the memory range [srcMem, srcMem + size - 1] must not overflow the source memory space.
The operations supported by .redOp are classified as follows: The bit-size operations are .and , .or , and .xor .
The .inc and .dec operations return a result in the range [0..x] where x is the value at the source state space.
The current implementation of cp.reduce.async.bulk.add.f32 flushes subnormal inputs and results to sign-preserving zero.
The cp.reduce.async.bulk.add.f16 and cp.reduce.async.bulk.add.bf16 operations require .noftz qualifier.
The following table describes the valid combinations of .redOp and element type: .dst .redOp Element type .shared::cluster .add .u32 , .s32 , .u64 .min , .max .u32 , .s32 .inc , .dec .u32 .and , .or , .xor .b32 .global .add .u32 , .s32 , .u64 , .f32 , .f64 , .f16 , .bf16 .min , .max .u32 , .s32 , .u64 , .s64 , .f16 , .bf16 .inc , .dec .u32 .and , .or , .xor .b32 , .b64 The modifier .completion_mechanism specifies the completion mechanism that is supported on the instruction variant.
The completion mechanisms that are supported for different variants are summarized in the following table: Completion mechanism .dst .src Description .mbarrier::...
.shared::cluster .global mbarrier based completion mechanism .shared::cluster .shared::cta .bulk_group .global .shared::cta Bulk async-group based completion mechanism The modifier .mbarrier::complete_tx::bytes specifies that the cp.reduce.async.bulk variant uses mbarrier based completion mechanism.
The complete-tx operation, with completeCount argument equal to amount of data copied in bytes, will be performed on the mbarrier object specified by the operand mbar .
The modifier .bulk_group specifies that the cp.reduce.async.bulk variant uses bulk async-group based completion mechanism.
The qualifier .level::cache_hint is only supported when at least one of the .src or .dst statespaces is .global state space.
Each reduction operation performed by the cp.reduce.async.bulk has individually .relaxed.gpu memory ordering semantics.
The load operations in cp.reduce.async.bulk are treated as weak memory operation and the complete-tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model .
Examples cp.reduce.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes.add.u64 [dstMem], [srcMem], size, [mbar]; cp.reduce.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes.min.s32 [dstMem], [srcMem], size, [mbar]; cp.reduce.async.bulk.global.shared::cta.bulk_group.min.f16 [dstMem], [srcMem], size; cp.reduce.async.bulk.global.shared::cta.bulk_group.L2::cache_hint.xor.s32 [dstMem], [srcMem], size, policy; cp.reduce.async.bulk.global.shared::cta.bulk_group.add.noftz.f16 [dstMem], [srcMem], size; 9.7.8.24.8.
Data Movement and Conversion Instructions: cp.async.bulk.prefetch  cp.async.bulk.prefetch Provides a hint to the system to initiate the asynchronous prefetch of data to the cache.
Syntax cp.async.bulk.prefetch.L2.src{.level::cache_hint} [srcMem], size {, cache-policy} .src = { .global } .level::cache_hint = { .L2::cache_hint } Description cp.async.bulk.prefetch is a non-blocking instruction which may initiate an asynchronous prefetch of data from the location specified by source address operand srcMem , in .src statespace, to the L2 cache.
The 32-bit operand size specifies the amount of memory to be prefetched in terms of number of bytes.
Examples cp.async.bulk.prefetch.L2.global [srcMem], size; cp.async.bulk.prefetch.L2.global.L2::cache_hint [srcMem], size, policy; 9.7.8.24.9.
Data Movement and Conversion Instructions: cp.async.bulk.tensor  cp.async.bulk.tensor Initiates an asynchronous copy operation on the tensor data from one state space to another.
The operand dstMem specifies the location in the .dst state space into which the tensor data has to be copied and srcMem specifies the location in the .src state space from which the tensor data has to be copied.
The operand tensorMap is the generic address of the opaque tensor-map object which resides either in .param space or .const space or .global space.
The operand tensorMap specifies the properties of the tensor copy operation, as described in Tensor-map .
The vector operand tensorCoords specifies the starting coordinates in the tensor data in the global memory from or to which the copy operation has to be performed.
The number of tensor coordinates in the vector argument tensorCoords should be equal to the dimension specified by the modifier .dim .
The modifier .completion_mechanism specifies the completion mechanism that is supported on the instruction variant.
The completion mechanisms that are supported for different variants are summarized in the following table: Completion mechanism .dst .src Description .mbarrier::...
.shared::cluster .global mbarrier based completion mechanism .bulk_group .global .shared::cta Bulk async-group based completion mechanism The modifier .mbarrier::complete_tx::bytes specifies that the cp.async.bulk.tensor variant uses mbarrier based completion mechanism.
The modifier .bulk_group specifies that the cp.async.bulk.tensor variant uses bulk async-group based completion mechanism.
The qualifier .load_mode specifies how the data in the source location is copied into the destination location.
In .tile mode, the multi-dimensional layout of the source tensor is preserved at the destination.
In .im2col mode, some dimensions of the source tensors are unrolled in a single dimensional column at the destination.
The length of the vector operand im2colOffsets is two less than the number of dimension .dim of the tensor operation.
The modifier .im2col_no_offs is the same as .im2col mode except there is no im2colOffsets vector involved.
The optional modifier .multicast::cluster allows copying of data from global memory to shared memory of multiple CTAs in the cluster.
Operand ctaMask specifies the destination CTAs in the cluster such that each bit position in the 16-bit ctaMask operand corresponds to the %ctaid of the destination CTA.
The source data is multicast to the same offset as dstMem in the shared memory of each destination CTA.
The mbarrier signal is also multicast to the same offset as mbar in the shared memory of the destination CTA.
The copy operation in cp.async.bulk.tensor is treated as a weak memory operation and the complete-tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model .
Notes .multicast::cluster qualifier is optimized for target architecture sm_90a and may have substantially reduced performance on other targets and hence .multicast::cluster is advised to be used with .target sm_90a .
Examples .reg .b16 ctaMask; .reg .u16 i2cOffW, i2cOffH, i2cOffD; .reg .b64 l2CachePolicy; cp.async.bulk.tensor.1d.shared::cluster.global.tile [sMem0], [tensorMap0, {tc0}], [mbar0]; @p cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster [sMem1], [tensorMap1, {tc0, tc1}], [mbar2], ctaMask; @p cp.async.bulk.tensor.5d.shared::cluster.global.im2col.mbarrier::complete_tx::bytes [sMem2], [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], [mbar2], {i2cOffW, i2cOffH, i2cOffD}; @p cp.async.bulk.tensor.3d.im2col.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint [sMem3], [tensorMap3, {tc0, tc1, tc2}], [mbar3], {i2cOffW}, policy; @p cp.async.bulk.tensor.1d.global.shared::cta.bulk_group [tensorMap3, {tc0}], [sMem3]; 9.7.8.24.10.
Data Movement and Conversion Instructions: cp.reduce.async.bulk.tensor  cp.reduce.async.bulk.tensor Initiates an asynchronous reduction operation on the tensor data.
Syntax   shared::cta -> global: cp.reduce.async.bulk.tensor.dim.dst.src.redOp{.load_mode}.completion_mechanism{.level::cache_hint} [tensorMap, tensorCoords], [srcMem] {,cache-policy} .dst = { .global } .src = { .shared::cta } .dim = { .1d, .2d, .3d, .4d, .5d } .completion_mechanism = { .bulk_group } .load_mode = { .tile, .im2col_no_offs } .redOp = { .add, .min, .max, .inc, .dec, .and, .or, .xor} Description cp.reduce.async.bulk.tensor is a non-blocking instruction which initiates an asynchronous reduction operation of tensor data in the .dst state space with tensor data in the .src state space.
The operand srcMem specifies the location of the tensor data in the .src state space using which the reduction operation has to be performed.
Each element of the tensor data in the .dst state space is reduced inline with the corresponding element from the tensor data in the .src state space.
The type of each tensor data element in the source and the destination tensor is specified in Tensor-map .
The vector operand tensorCoords specifies the starting coordinates of the tensor data in the global memory on which the reduce operation is to be performed.
The following table describes the valid combinations of .redOp and element type: .redOp Element type .add .u32 , .s32 , .u64 , .f32 , .f16 , .bf16 .min , .max .u32 , .s32 , .u64 , .s64 , .f16 , .bf16 .inc , .dec .u32 .and , .or , .xor .b32 , .b64 The modifier .completion_mechanism specifies the completion mechanism that is supported on the instruction variant.
Value .bulk_group of the modifier .completion_mechanism specifies that cp.reduce.async.bulk.tensor instruction uses bulk async-group based completion mechanism.
In .im2col_no_offs mode, some dimensions of the source tensors are unrolled in a single dimensional column at the destination.
Each reduction operation performed by cp.reduce.async.bulk.tensor has individually .relaxed.gpu memory ordering semantics.
The load operations in cp.reduce.async.bulk.tensor are treated as weak memory operations and the complete-tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model .
Examples cp.reduce.async.bulk.tensor.1d.global.shared::cta.add.tile.bulk_group [tensorMap0, {tc0}], [sMem0]; cp.reduce.async.bulk.tensor.2d.global.shared::cta.and.bulk_group.L2::cache_hint [tensorMap1, {tc0, tc1}], [sMem1] , policy; cp.reduce.async.bulk.tensor.3d.global.shared::cta.xor.im2col.bulk_group [tensorMap2, {tc0, tc1, tc2}], [sMem2] 9.7.8.24.11.
Data Movement and Conversion Instructions: cp.async.bulk.prefetch.tensor  cp.async.bulk.prefetch.tensor Provides a hint to the system to initiate the asynchronous prefetch of tensor data to the cache.
Syntax   global -> shared::cluster: cp.async.bulk.prefetch.tensor.dim.L2.src{.load_mode}{.level::cache_hint} [tensorMap, tensorCoords] {, im2colOffsets } {, cache-policy} .src = { .global } .dim = { .1d, .2d, .3d, .4d, .5d } .load_mode = { .tile, .im2col } .level::cache_hint = { .L2::cache_hint } Description cp.async.bulk.prefetch.tensor is a non-blocking instruction which may initiate an asynchronous prefetch of tensor data from the location in .src statespace to the L2 cache.
cp.async.bulk.prefetch.tensor is treated as a weak memory operation in the Memory Consistency Model .
Examples .reg .b16 ctaMask; .reg .u16 i2cOffW, i2cOffH, i2cOffD; .reg .b64 l2CachePolicy; cp.async.bulk.prefetch.tensor.1d.L2.global.tile [tensorMap0, {tc0}]; @p cp.async.bulk.prefetch.tensor.2d.L2.global [tensorMap1, {tc0, tc1}]; @p cp.async.bulk.prefetch.tensor.5d.L2.global.im2col [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], {i2cOffW, i2cOffH, i2cOffD}; @p cp.async.bulk.prefetch.tensor.3d.L2.global.im2col.L2::cache_hint [tensorMap3, {tc0, tc1, tc2}], {i2cOffW}, policy; 9.7.8.24.12.
Data Movement and Conversion Instructions: cp.async.bulk.commit_group  cp.async.bulk.commit_group Commits all prior initiated but uncommitted cp.async.bulk instructions into a cp.async.bulk-group .
Syntax cp.async.bulk.commit_group; Description cp.async.bulk.commit_group instruction creates a new per-thread bulk async-group and batches all prior cp{.reduce}.async.bulk.
{.prefetch}{.tensor} instructions satisfying the following conditions into the new bulk async-group : The prior cp{.reduce}.async.bulk.
{.prefetch}{.tensor} instructions use bulk_group based completion mechanism, and They are initiated by the executing thread but not committed to any bulk async-group .
{.prefetch}{.tensor} instructions then cp.async.bulk.commit_group results in an empty bulk async-group .
Data Movement and Conversion Instructions: cp.async.bulk.wait_group  cp.async.bulk.wait_group Wait for completion of bulk async-groups .
Syntax cp.async.bulk.wait_group{.read} N; Description cp.async.bulk.wait_group instruction will cause the executing thread to wait until only N or fewer of the most recent bulk async-groups are pending and all the prior bulk async-groups committed by the executing threads are complete.
For example, when N is 0, the executing thread waits on all the prior bulk async-groups to complete.
By default, cp.async.bulk.wait_group instruction will cause the executing thread to wait till all the bulk async operations in the specified bulk async-group have completed all of the following: Reading from the source locations.
The optional .read modifier indicates that the waiting has to be done until all the bulk async operations in the specified bulk async-group have completed reading from their source locations.
Data Movement and Conversion Instructions: tensormap.replace  tensormap.replace Modifies the field of a tensor-map object.
Syntax tensormap.replace.mode.field1{.ss}.b1024.type [addr], new_val; tensormap.replace.mode.field2{.ss}.b1024.type [addr], ord, new_val; tensormap.replace.mode.field3{.ss}.b1024.type [addr], new_val; .mode = { .tile } .field1 = { .global_address, .rank } .field2 = { .box_dim, .global_dim, .global_stride, .element_stride } .field3 = { .elemtype, .interleave_layout, .swizzle_mode, .fill_mode } .ss = { .global, .shared::cta } .type = { .b32, .b64 } Description The tensormap.replace instruction replaces the field, specified by .field qualifier, of the tensor-map object at the location specified by the address operand addr with a new value.
Qualifier .mode specifies the mode of the tensor-map object located at the address operand addr .
The immediate integer operand ord specifies the ordinal of the field across the rank of the tensor which needs to be replaced in the tensor-map object.
For field .rank , the operand new_val must be ones less than the desired tensor rank as this field uses zero-based numbering.
When .field3 is specified, the operand new_val must be an immediate and the Table 30 shows the mapping of the operand new_val across various fields.
Table 30 Tensormap new_val validity  new_val .field3 .elemtype .interleave_layout .swizzle_mode .fill_mode 0 .u8 No interleave No swizzling Zero fill 1 .u16 16B interleave 32B swizzling OOB-NaN fill 2 .u32 32B interleave 64B swizzling x 3 .s32 x 128B swizzling x 4 .u64 x x x 5 .s64 x x x 6 .f16 x x x 7 .f32 x x x 8 .f32.ftz x x x 9 .f64 x x x 10 .bf16 x x x 11 .tf32 x x x 12 .tf32.ftz x x x If no state space is specified then Generic Addressing is used.
If the address specified by addr does not fall within the address window of .global or .shared::cta state space then the behavior is undefined.
tensormap.replace is treated as a weak memory operation, on the entire 1024-bit opaque tensor-map object, in the Memory Consistency Model .
Examples tensormap.replace.tile.global_address.shared::cta.b1024.b64 [sMem], new_val; 9.7.9.
Texture Instructions  This section describes PTX instructions for accessing textures and samplers.
PTX supports the following operations on texture and sampler descriptors: Static initialization of texture and sampler descriptors.
Ability to query fields within texture and sampler descriptors. 9.7.9.1. Texturing Modes  For working with textures and samplers, PTX has two modes of operation.
The advantage of unified mode is that it allows 256 samplers per kernel (128 for architectures prior to sm_3x ), with the restriction that they correspond 1-to-1 with the 256 possible textures per kernel (128 for architectures prior to sm_3x ).
The advantage of independent mode is that textures and samplers can be mixed and matched, but the number of samplers is greatly restricted to 32 per kernel (16 for architectures prior to sm_3x ).
Table 31 summarizes the number of textures, samplers and surfaces available in different texturing modes.
Table 31 Texture, sampler and surface limits  Texturing mode Resource sm_1x , sm_2x sm_3x+ Unified mode Textures 128 256 Samplers 128 256 Surfaces 8 16 Independent mode Textures 128 256 Samplers 16 32 Surfaces 8 16 The texturing mode is selected using .target options texmode_unified and texmode_independent .
Example : calculate an element’s power contribution as element’s power/total number of elements.
.target texmode_independent .global .samplerref tsamp1 = { addr_mode_0 = clamp_to_border, filter_mode = nearest }; ...
.entry compute_power ( .param .texref tex1 ) { txq.width.b32 r6, [tex1];   get tex1's width txq.height.b32 r5, [tex1];   get tex1's height tex.2d.v4.f32.f32 {r1,r2,r3,r4}, [tex1, tsamp1, {f1,f2}]; mul.u32 r5, r5, r6; add.f32 r1, r1, r2; add.f32 r3, r3, r4; add.f32 r1, r1, r3; cvt.f32.u32 r5, r5; div.f32 r1, r1, r5; } 9.7.9.2.
Mipmaps  A mipmap is a sequence of textures, each of which is a progressively lower resolution representation of the same image.
The height and width of each image, or level of detail (LOD), in the mipmap is a power of two smaller than the previous level.
Mipmaps are used in graphics applications to improve rendering speed and reduce aliasing artifacts.
For example, a high-resolution mipmap image is used for objects that are close to the user; lower-resolution images are used as the object appears farther away.
Mipmap filtering modes are provided when switching between two levels of detail (LODs) in order to avoid abrupt changes in visual fidelity.
Example: If the texture has a basic size of 256 by 256 pixels, then the associated mipmap set may contain a series of eight images, each one-fourth the total area of the previous one: 128×128 pixels, 64×64, 32×32, 16×16, 8×8, 4×4, 2×2, 1×1 (a single pixel).
If, for example, a scene is rendering this texture in a space of 40×40 pixels, then either a scaled up version of the 32×32 (without trilinear interpolation) or an interpolation of the 64×64 and the 32×32 mipmaps (with trilinear interpolation) would be used.
The total number of LODs in a complete mipmap pyramid is calculated through the following equation: numLODs = 1 + floor(log2(max(w, h, d))) The finest LOD is called the base level and is the 0th level.
Each successively smaller mipmap level has half the {width, height, depth} of the previous level, but if this half value is a fractional value, it’s rounded down to the next largest integer.
Essentially, the size of a mipmap level can be specified as: max(1, floor(w_b / 2^i)) x max(1, floor(h_b / 2^i)) x max(1, floor(d_b / 2^i)) where i is the ith level beyond the 0th level (the base level).
PTX support for mipmaps The PTX tex instruction supports three modes for specifying the LOD: base , level , and grad ient.
In gradmode, two floating-point vector arguments provide partials (e.g., {ds/dx, dt/dx} and {ds/dy, dt/dy} for a 2d texture), which the tex instruction uses to compute the LOD.
The instruction loads data from the texture named by operand a at coordinates given by operand c into destination d .
Operand c is a scalar or singleton tuple for 1d textures; is a two-element vector for 2d textures; and is a four-element vector for 3d textures, where the fourth element is ignored.
The optional destination predicate p is set to True if data from texture at specified coordinates is resident in memory, False otherwise.
Memory residency of Texture Data at specified coordinates is dependent on execution environment setup using Driver API calls, prior to kernel launch.
Refer to Driver API documentation for more details including any system/implementation specific behavior.
Operand e is a singleton tuple for 1d textures; is a two element vector 2d textures; and is four-element vector for 3d textures, where the fourth element is ignored.
When using depth compare operand, the elements in texture coordinate vector c have .f32 type.
A texture base address is assumed to be aligned to a 16 byte boundary, and the address given by the coordinate vector must be naturally aligned to a multiple of the access size.
If an address is not properly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently masking off low-order address bits to achieve proper rounding, or the instruction may fault.
The instruction first selects a texture from the texture array named by operand a using the index given by the first element of the array coordinate vector c .
The instruction then loads data from the selected texture at coordinates given by the remaining elements of operand c into destination d .
Operand c is a bit-size type vector or tuple containing an index into the array of textures followed by coordinates within the selected texture, as follows: For 1d texture arrays, operand c has type .v2.b32 .
The first element is interpreted as an unsigned integer index ( .u32 ) into the texture array, and the second element is interpreted as a 1d texture coordinate of type .ctype .
The first element is interpreted as an unsigned integer index ( .u32 ) into the texture array, and the next two elements are interpreted as 2d texture coordinates of type .ctype .
Operand e is a singleton tuple for 1d texture arrays; and is a two element vector 2d texture arrays.
When using depth compare operand, the coordinates in texture coordinate vector c have .f32 type.
The texture array index is a 32-bit unsigned integer, and texture coordinate elements are 32-bit signed integer or floating point values.
The instruction loads data from the cubemap texture named by operand a at coordinates given by operand c into destination d .
Cubemap textures are special two-dimensional layered textures consisting of six layers that represent the faces of a cube.
When accessing a cubemap, the texture coordinate vector c has type .v4.f32 , and comprises three floating-point coordinates ( s , t , r ) and a fourth padding argument which is ignored.
The ( s , t , r ) coordinates can be thought of as a direction vector emanating from the center of the cube.
Of the three coordinates ( s , t , r ), the coordinate of the largest magnitude (the major axis) selects the cube face.
Then, the other two coordinates (the minor axes) are divided by the absolute value of the major axis to produce a new ( s , t ) coordinate pair to lookup into the selected cube face.
operand f is .f32 scalar value that specifies depth compare value for cubemap depth textures.
The instruction first selects a cubemap texture from the cubemap array named by operand a using the index given by the first element of the array coordinate vector c .
The instruction then loads data from the selected cubemap texture at coordinates given by the remaining elements of operand c into destination d .
Cubemap array textures consist of an array of cubemaps, i.e., the total number of layers is a multiple of six.
The first element is interpreted as an unsigned integer index ( .u32 ) into the cubemap array, and the remaining three elements are interpreted as floating-point cubemap coordinates ( s , t , r ), used to lookup in the selected cubemap as described above.
Operand f is .f32 scalar value that specifies depth compare value for cubemap depth textures.
The instruction loads data from the texture named by operand a from sample number given by first element of the operand c , at coordinates given by remaining elements of operand c into destination d .
The first element in operand c is interpreted as unsigned integer sample number ( .u32 ), and the next two elements are interpreted as signed integer ( .s32 ) 2d texture coordinates.
The instruction first selects a multi-sample texture from the multi-sample texture array named by operand a using the index given by the first element of the array coordinate vector c .
The instruction then loads data from the selected multi-sample texture from sample number given by second element of the operand c , at coordinates given by remaining elements of operand c into destination d .
When accessing a multi-sample texture array, texture coordinate vector c has type .v4.b32 .
The first element in operand c is interpreted as unsigned integer sampler number, the second element is interpreted as unsigned integer index ( .u32 ) into the multi-sample texture array and the next two elements are interpreted as signed integer ( .s32 ) 2d texture coordinates.
.level (lod explicit) Requires an additional 32-bit scalar argument, lod , which contains the LOD to fetch from.
.grad (lod gradient) Requires two .f32 vectors, dPdx and dPdy , that specify the partials.
The vectors are singletons for 1d and a1d textures; are two-element vectors for 2d and a2d textures; and are four-element vectors for 3d, cube and acube textures, where the fourth element is ignored for 3d and cube geometries.
Indirect texture access Beginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target architecture sm_20 or higher.
In indirect access, operand a is a .u64 register holding the address of a .texref variable.
Notes For compatibility with prior versions of PTX, the square brackets are not required and .v4 coordinate vectors are allowed for any geometry, with the extra elements being ignored.
Extension using opaque .texref and .samplerref types and independent mode texturing introduced in PTX ISA version 1.5.
Texture Instructions: tld4  tld4 Perform a texture fetch of the 4-texel bilerp footprint.
Syntax tld4.comp.2d.v4.dtype.f32 d[|p], [a, c] {, e} {, f}; tld4.comp.geom.v4.dtype.f32 d[|p], [a, b, c] {, e} {, f};   explicit sampler .comp = { .r, .g, .b, .a }; .geom = { .2d, .a2d, .cube, .acube }; .dtype = { .u32, .s32, .f32 }; Description Texture fetch of the 4-texel bilerp footprint using a texture coordinate vector.
The instruction loads the bilerp footprint from the texture named by operand a at coordinates given by operand c into vector destination d .
The four texel samples are placed into destination vector d in counter-clockwise order starting at lower left.
tld4.2d For 2D textures, operand c specifies coordinates as a two-element, 32-bit floating-point vector.
The first element in operand c is interpreted as an unsigned integer index ( .u32 ) into the texture array, and the next two elements are interpreted as 32-bit floating point coordinates of 2d texture.
tld4.cube For cubemap textures, operand c specifies four-element vector which comprises three floating-point coordinates (s, t, r) and a fourth padding argument which is ignored.
The (s, t, r) coordinates can be thought of as a direction vector emanating from the center of the cube.
Of the three coordinates (s, t, r), the coordinate of the largest magnitude (the major axis) selects the cube face.
Then, the other two coordinates (the minor axes) are divided by the absolute value of the major axis to produce a new (s, t) coordinate pair to lookup into the selected cube face.
The first element in operand c is interpreted as an unsigned integer index ( .u32 ) into the cubemap texture array, and the remaining three elements are interpreted as floating-point cubemap coordinates (s, t, r), used to lookup in the selected cubemap.
Examples  Example of unified mode texturing tld4.r.2d.v4.s32.f32 {r1,r2,r3,r4}, [tex_a,{f1,f2}];   Example of independent mode texturing tld4.r.2d.v4.u32.f32 {u1,u2,u3,u4}, [tex_a,smpl_x,{f1,f2}];   Example of unified mode texturing using offset tld4.r.2d.v4.s32.f32 {r1,r2,r3,r4}, [tex_a,{f1,f2}], {r5, r6};   Example of unified mode texturing using compare tld4.r.2d.v4.f32.f32 {f1,f2,f3,f4}, [tex_a,{f5,f6}], f7;   Example of optional destination predicate tld4.r.2d.v4.f32.f32 {f1,f2,f3,f4}|p, [tex_a,{f5,f6}], f7; 9.7.9.5.
Syntax txq.tquery.b32 d, [a];   texture attributes txq.level.tlquery.b32 d, [a], lod;   texture attributes txq.squery.b32 d, [a];   sampler attributes .tquery = { .width, .height, .depth, .channel_data_type, .channel_order, .normalized_coords, .array_size, .num_mipmap_levels, .num_samples}; .tlquery = { .width, .height, .depth }; .squery = { .force_unnormalized_coords, .filter_mode, .addr_mode_0, addr_mode_1, addr_mode_2 }; Description Query an attribute of a texture or sampler.
Query Returns .width .height .depth value in elements .channel_data_type Unsigned integer corresponding to source language’s channel data type enumeration.
If the source language combines channel data type and channel order into a single enumeration type, that value is returned for both channel_data_type and channel_order queries.
.channel_order Unsigned integer corresponding to source language’s channel order enumeration.
Overrides the normalized_coords field of a .texref variable used with a .samplerref in a tex instruction.
.filter_mode Integer from enum { nearest, linear } .addr_mode_0 .addr_mode_1 .addr_mode_2 Integer from enum { wrap, mirror, clamp_ogl, clamp_to_edge, clamp_to_border } .array_size For a texture array, number of textures in array, 0 otherwise.
.num_mipmap_levels For a mipmapped texture, number of levels of details (LOD), 0 otherwise.
In unified mode, sampler attributes are also accessed via a .texref argument, and in independent mode sampler attributes are accessed via a separate .samplerref argument.
txq.level txq.level requires an additional 32bit integer argument, lod , which specifies LOD and queries requested attribute for the specified LOD.
.array_size , .num_mipmap_levels , .num_samples samples queries were added in PTX ISA version 4.1.
Examples txq.width.b32 %r1, [tex_A]; txq.filter_mode.b32 %r1, [tex_A];   unified mode txq.addr_mode_0.b32 %r1, [smpl_B];   independent mode txq.level.width.b32 %r1, [tex_A], %r_lod; 9.7.9.6.
Texture Instructions: istypep  istypep Query whether a register points to an opaque variable of a specified type.
Syntax istypep.type p, a;   result is .pred .type = { .texref, .samplerref, .surfref }; Description Write predicate register p with 1 if register a points to an opaque variable of the specified type, and with 0 otherwise.
Examples istypep.texref istex, tptr; istypep.samplerref issampler, sptr; istypep.surfref issurface, surfptr; 9.7.10.
PTX supports the following operations on surface descriptors: Static initialization of surface descriptors.
Syntax suld.b.geom{.cop}.vec.dtype.clamp d, [a, b];   unformatted .geom = { .1d, .2d, .3d, .a1d, .a2d }; .cop = { .ca, .cg, .cs, .cv };   cache operation .vec = { none, .v2, .v4 }; .dtype = { .b8 , .b16, .b32, .b64 }; .clamp = { .trap, .clamp, .zero }; Description suld.b.
The instruction loads data from the surface named by operand a at coordinates given by operand b into destination d .
Operand b is a scalar or singleton tuple for 1d surfaces; is a two-element vector for 2d surfaces; and is a four-element vector for 3d surfaces, where the fourth element is ignored.
The lowest dimension coordinate represents a byte offset into the surface and is not scaled, and the size of the data transfer matches the size of destination operand d .
The instruction first selects a surface layer from the surface array named by operand a using the index given by the first element of the array coordinate vector b .
The instruction then loads data from the selected surface at coordinates given by the remaining elements of operand b into destination d .
Operand b is a bit-size type vector or tuple containing an index into the array of surfaces followed by coordinates within the selected surface, as follows: For 1d surface arrays, operand b has type .v2.b32 .
The first element is interpreted as an unsigned integer index ( .u32 ) into the surface array, and the second element is interpreted as a 1d surface coordinate of type .s32 .
The first element is interpreted as an unsigned integer index ( .u32 ) into the surface array, and the next two elements are interpreted as 2d surface coordinates of type .s32 .
A surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the coordinate vector must be naturally aligned to a multiple of the access size.
The .clamp field specifies how to handle out-of-bounds addresses: .trap causes an execution trap on out-of-bounds addresses .clamp loads data at the nearest surface location (sized appropriately) .zero loads zero for out-of-bounds addresses Indirect surface access Beginning with PTX ISA version 3.1, indirect surface access is supported for target architecture sm_20 or higher.
In indirect access, operand a is a .u64 register holding the address of a .surfref variable.
Examples suld.b.1d.v4.b32.trap {s1,s2,s3,s4}, [surf_B, {x}]; suld.b.3d.v2.b64.trap {r1,r2}, [surf_A, {x,y,z,w}]; suld.b.a1d.v2.b32 {r0,r1}, [surf_C, {idx,x}]; suld.b.a2d.b32 r0, [surf_D, {idx,x,y,z}];   z ignored 9.7.10.2.
{a1d,a2d}{.cop}.vec.ctype.clamp [a, b], c;   unformatted .cop = { .wb, .cg, .cs, .wt };   cache operation .vec = { none, .v2, .v4 }; .ctype = { .b8 , .b16, .b32, .b64 }; .clamp = { .trap, .clamp, .zero }; Description sust.
The instruction stores data from operand c to the surface named by operand a at coordinates given by operand b .
The lowest dimension coordinate represents a byte offset into the surface and is not scaled.
The source vector elements are interpreted left-to-right as R , G , B , and A surface components.
Surface sample components that do not occur in the source vector will be written with an unpredictable value.
The source data interpretation is based on the surface sample format as follows: If the surface format contains UNORM , SNORM , or FLOAT data, then .f32 is assumed; if the surface format contains UINT data, then .u32 is assumed; if the surface format contains SINT data, then .s32 is assumed.
{a1d,a2d} Surface layer selection, followed by an unformatted store to the selected surface.
The instruction then stores the data in operand c to the selected surface at coordinates given by the remaining elements of operand b .
The .clamp field specifies how to handle out-of-bounds addresses: .trap causes an execution trap on out-of-bounds addresses .clamp stores data at the nearest surface location (sized appropriately) .zero drops stores to out-of-bounds addresses Indirect surface access Beginning with PTX ISA version 3.1, indirect surface access is supported for target architecture sm_20 or higher.
sust.p , additional clamp modifiers, and cache operations introduced in PTX ISA version 2.0.
Examples sust.p.1d.v4.b32.trap [surf_B, {x}], {f1,f2,f3,f4}; sust.b.3d.v2.b64.trap [surf_A, {x,y,z,w}], {r1,r2}; sust.b.a1d.v2.b64 [surf_C, {idx,x}], {r1,r2}; sust.b.a2d.b32 [surf_D, {idx,x,y,z}], r0;   z ignored 9.7.10.3.
Syntax sured.b.op.geom.ctype.clamp [a,b],c;   byte addressing sured.p.op.geom.ctype.clamp [a,b],c;   sample addressing .op = { .add, .min, .max, .and, .or }; .geom = { .1d, .2d, .3d }; .ctype = { .u32, .u64, .s32, .b32, .s64 };   for sured.b .ctype = { .b32, .b64 };   for sured.p .clamp = { .trap, .clamp, .zero }; Description Reduction to surface memory using a surface coordinate vector.
The instruction performs a reduction operation with data from operand c to the surface named by operand a at coordinates given by operand b .
Operation add applies to .u32 , .u64 , and .s32 types; min and max apply to .u32 , .s32 , .u64 and .s64 types; operations and and or apply to .b32 type.
For type .b32 , the data is interpreted as .u32 or .s32 based on the surface sample format as follows: if the surface format contains UINT data, then .u32 is assumed; if the surface format contains SINT data, then .s32 is assumed.
For type .b64 , if the surface format contains UINT data, then .u64 is assumed; if the surface format contains SINT data, then .s64 is assumed.
Examples sured.b.add.2d.u32.trap [surf_A, {x,y}], r1; sured.p.min.1d.u32.trap [surf_B, {x}], r1; sured.b.max.1d.u64.trap [surf_C, {x}], r1; sured.p.min.1d.b64.trap [surf_D, {x}], r1; 9.7.10.4.
Syntax suq.query.b32 d, [a]; .query = { .width, .height, .depth, .channel_data_type, .channel_order, .array_size, .memory_layout }; Description Query an attribute of a surface.
.memory_layout 1 for surface with linear memory layout; 0 otherwise Indirect surface access Beginning with PTX ISA version 3.1, indirect surface access is supported for target architecture sm_20 or higher.
Control Flow Instructions  The following PTX instructions and syntax are for controlling execution in a PTX program: {} @ bra call ret exit 9.7.11.1.
Syntax { instructionList } Description The curly braces create a group of instructions, used primarily for defining a function body.
The curly braces also provide a mechanism for determining the scope of a variable: any variable declared within a scope is not available outside the scope.
Syntax @{! }p instruction; Description Execute an instruction or instruction block for threads that have the guard predicate True .
Examples setp.eq.f32 p,y,0;   is y zero? @!p div.f32 ratio,x,y   avoid division by zero @q bra L23;   conditional branch 9.7.11.3.
Syntax @p bra{.uni} tgt;   tgt is a label bra{.uni} tgt;   unconditional branch Description Continue execution at the target.
all active threads in a warp that are currently executing this instruction have identical values for the guard predicate and branch target.
Unimplemented indirect branch introduced in PTX ISA version 2.1 has been removed from the spec.
Examples bra.uni L_exit;   uniform unconditional jump @q bra L23;   conditional branch 9.7.11.4.
Control Flow Instructions: brx.idx  brx.idx Branch to a label indexed from a list of potential branch targets.
Syntax @p brx.idx{.uni} index, tlist; brx.idx{.uni} index, tlist; Description Index into a list of possible destination labels, and continue execution from the chosen label.
all active threads in a warp that are currently executing this instruction have identical values for the guard predicate and the index argument.
Behaviour is undefined if the value of index is greater than or equal to the length of tlist .
The .branchtargets directive must be defined in the local function scope before it is used.
Semantics if (p) { if (index = s) ? 0 : r+1; dec(r, s) = (r==0 || r > s) ? s : r-1; exch(r, s) = s; cas(r,s,t) = (r == s) ? t : r; Notes Simple reductions may be specified by using the bit bucket destination operand _ .
Per-element atomicity of atom.f16x2 clarified in PTX ISA version 6.3, with retrospective effect from PTX ISA version 6.2.
Parallel Synchronization and Communication Instructions: red  red Reduction operations on global and shared memory.
vec_16_bit.half_word_type [a], b{, cache-policy}; red{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_32_bit.packed_type [a], b {, cache-policy}; .sem = { .relaxed, .release }; .scope = { .cta, .cluster, .gpu, .sys }; .op = { .add, .min, .max }; .half_word_type = { .f16, .bf16 }; .packed_type = { .f16x2,.bf16x2 }; .vec_16_bit = { .v2, .v4, .v8 } .vec_32_bit = { .v2, .v4 }; .level::cache_hint = { .L2::cache_hint } Description Performs a reduction operation with operand b and the value in location a , and stores the result of the specified operation at location a , overwriting the original value.
red with scalar type may be used only with .global and .shared spaces and with generic addressing, where the address points to .global or .shared space.
red with vector type may be used only with .global space and with generic addressing where the address points to .global space.
For red with vector type, operand b is brace-enclosed vector expressions, size of which is equal to the size of vector qualifier.
The optional .sem qualifier specifies a memory synchronizing effect as described in the Memory Consistency Model .
The optional .scope qualifier specifies the set of threads that can directly observe the memory synchronizing effect of this operation, as described in the Memory Consistency Model .
For red with vector type, the supported combinations of vector qualifier, types and reduction operations supported on these combinations are depicted in following table: Vector qualifier Types .f16 / bf16 .f16x2 / bf16x2 .f32 .v2 .add , .min , .max .add , .min , .max .add .v4 .add , .min , .max .add , .min , .max .add .v8 .add , .min , .max Not supported Not Supported Two atomic operations { atom or red } are performed atomically with respect to each other only if each operation specifies a scope that includes the other.
When this condition is not met, each operation observes the other operation being performed as if it were split into a read followed by a dependent write.
red instruction on packed type or vector type, accesses adjacent scalar elements in memory.
In such case, the atomicity is guaranteed separately for each of the individual scalar elements; the entire red is not guaranteed to be atomic as a single access.
For sm_6x and earlier architectures, red operations on .shared state space do not guarantee atomicity with respect to normal store instructions to the same address.
It is the programmer’s responsibility to guarantee correctness of programs that use shared memory reduction instructions, e.g., by inserting barriers between normal stores and reduction operations to a common address, or by using atom.exch to store to locations accessed by other reduction operations.
Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands The bit-size operations are .and , .or , and .xor .
Current implementation of red.add.f32 on global memory flushes subnormal inputs and results to sign-preserving zero; whereas red.add.f32 on shared memory supports subnormal inputs and results and doesn’t flush them to zero.
red.add.f16 , red.add.f16x2 , red.add.bf16 and red.add.bf16x2 operation requires the .noftz qualifier; it preserves subnormal inputs and results, and does not flush them to zero.
Semantics *a = operation(*a, b); where inc(r, s) = (r >= s) ? s : r-1; PTX ISA Notes Introduced in PTX ISA version 1.2.
Per-element atomicity of red.f16x2 clarified in PTX ISA version 6.3, with retrospective effect from PTX ISA version 6.2 Support for .level::cache_hint qualifier introduced in PTX ISA version 7.4.
Parallel Synchronization and Communication Instructions: red.async  red.async Asynchronous reduction operation on shared memory.
The shared memory addresses of destination operand a and the mbarrier object mbar , must meet all of the following conditions: They Belong to the same CTA.
With .shared::cluster , if the addresses specified do not fall within the address window of .shared::cluster state space, then the behaviour is undefined.
The reduce operation in red.async is treated as a relaxed memory operation and the complete_tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model .
Examples red.async.relaxed.cluster.shared::cluster.mbarrier::complete_tx::bytes.min.u32 [addr], b, [mbar_addr]; 9.7.12.8.
Parallel Synchronization and Communication Instructions: vote (deprecated)  vote (deprecated) Vote across thread group.
Syntax vote.mode.pred d, {! }a; vote.ballot.b32 d, {! }a;   'ballot' form, returns bitmask .mode = { .all, .any, .uni }; Deprecation Note The vote instruction without a .sync qualifier is deprecated in PTX ISA version 6.0.
Support for this instruction with .target lower than sm_70 may be removed in a future PTX ISA version.
Removal Note Support for vote instruction without a .sync qualifier is removed in PTX ISA version 6.4 for .target sm_70 or higher.
Description Performs a reduction of the source predicate across all active threads in a warp.
The reduction modes are: .all True if source predicate is True for all active threads in warp.
In the ballot form, vote.ballot.b32 simply copies the predicate from each thread in a warp into the corresponding bit position of destination register d , where the bit position corresponds to the thread’s lane id.
An inactive thread in warp will contribute a 0 for its entry when participating in vote.ballot.b32 .
Release Notes Note that vote applies to threads in a single warp, not across an entire CTA.
Examples vote.all.pred p,q; vote.uni.pred p,q; vote.ballot.b32 r1,p;   get 'ballot' across warp 9.7.12.9.
Parallel Synchronization and Communication Instructions: vote.sync  vote.sync Vote across thread group.
Syntax vote.sync.mode.pred d, {! }a, membermask; vote.sync.ballot.b32 d, {! }a, membermask;   'ballot' form, returns bitmask .mode = { .all, .any, .uni }; Description vote.sync will cause executing thread to wait until all non-exited threads corresponding to membermask have executed vote.sync with the same qualifiers and same membermask value before resuming execution.
Operand membermask specifies a 32-bit integer which is a mask indicating threads participating in this instruction where the bit position corresponds to thread’s laneid .
In the mode form, vote.sync performs a reduction of the source predicate across all non-exited threads in membermask .
The destination operand d is a predicate register and its value is the same across all threads in membermask .
The reduction modes are: .all True if source predicate is True for all non-exited threads in membermask .
.uni True if source predicate has the same value in all non-exited threads in membermask .
In this form, vote.sync.ballot.b32 simply copies the predicate from each thread in membermask into the corresponding bit position of destination register d , where the bit position corresponds to the thread’s lane id.
A thread not specified in membermask will contribute a 0 for its entry in vote.sync.ballot.b32 .
Note For .target sm_6x or below, all threads in membermask must execute the same vote.sync instruction in convergence, and only threads belonging to some membermask can be active when the vote.sync instruction is executed.
Examples vote.sync.all.pred p,q,0xffffffff; vote.sync.ballot.b32 r1,p,0xffffffff;   get 'ballot' across warp 9.7.12.10.
Parallel Synchronization and Communication Instructions: match.sync  match.sync Broadcast and compare a value across threads in warp.
Syntax match.any.sync.type d, a, membermask; match.all.sync.type d[|p], a, membermask; .type = { .b32, .b64 }; Description match.sync will cause executing thread to wait until all non-exited threads from membermask have executed match.sync with the same qualifiers and same membermask value before resuming execution.
Operand membermask specifies a 32-bit integer which is a mask indicating threads participating in this instruction where the bit position corresponds to thread’s laneid.
match.sync performs broadcast and compare of operand a across all non-exited threads in membermask and sets destination d and optional predicate p based on mode.
The matching operation modes are: .all d is set to mask corresponding to non-exited threads in membermask if all non-exited threads in membermask have same value of operand a ; otherwise d is set to 0.
Optionally predicate p is set to true if all non-exited threads in membermask have same value of operand a ; otherwise p is set to false.
.any d is set to mask of non-exited threads in membermask that have same value of operand a .
The behavior of match.sync is undefined if the executing thread is not in the membermask .
Release Notes Note that match.sync applies to threads in a single warp, not across an entire CTA.
Parallel Synchronization and Communication Instructions: activemask  activemask Queries the active threads within a warp.
Syntax activemask.b32 d; Description activemask queries predicated-on active threads from the executing warp and sets the destination d with 32-bit integer mask where bit position in the mask corresponds to the thread’s laneid .
An active thread will contribute 1 for its entry in the result and exited or inactive or predicated-off thread will contribute 0 for its entry in the result.
Parallel Synchronization and Communication Instructions: redux.sync  redux.sync Perform reduction operation on the data from each predicated active thread in the thread group.
Syntax redux.sync.op.type dst, src, membermask; .op = {.add, .min, .max} .type = {.u32, .s32} redux.sync.op.b32 dst, src, membermask; .op = {.and, .or, .xor} Description redux.sync will cause the executing thread to wait until all non-exited threads corresponding to membermask have executed redux.sync with the same qualifiers and same membermask value before resuming execution.
redux.sync performs a reduction operation .op of the 32 bit source register src across all non-exited threads in the membermask .
Reduction operation can be one of the bitwise operation in .and , .or , .xor or arithmetic operation in .add , .min , .max .
The behavior of redux.sync is undefined if the executing thread is not in the membermask .
Release Notes Note that redux.sync applies to threads in a single warp, not across an entire CTA.
Examples .reg .b32 dst, src, init, mask; redux.sync.add.s32 dst, src, 0xff; redux.sync.xor.b32 dst, src, mask; 9.7.12.13.
Parallel Synchronization and Communication Instructions: griddepcontrol  griddepcontrol Control execution of dependent grids.
Syntax griddepcontrol.action; .action = { .launch_dependents, .wait } Description The griddepcontrol instruction allows the dependent grids and prerequisite grids as defined by the runtime, to control execution in the following way: .launch_dependents modifier signals that specific dependents the runtime system designated to react to this instruction can be scheduled as soon as all other CTAs in the grid issue the same instruction or have completed.
There is no guarantee that the dependent will launch before the completion of the current grid.
Repeated invocations of this instruction by threads in the current CTA will have no additional side effects past that of the first invocation.
.wait modifier causes the executing thread to wait until all prerequisite grids in flight have completed and all the memory operations from the prerequisite grids are performed and made visible to the current grid.
Note If the prerequisite grid is using griddepcontrol.launch_dependents , then the dependent grid must use griddepcontrol.wait to ensure correct functional execution.
Parallel Synchronization and Communication Instructions: elect.sync  elect.sync Elect a leader thread from a set of threads.
Syntax elect.sync d|p, membermask; Description elect.sync elects one predicated active leader thread from among a set of threads specified by membermask .
The predicate destination p is set to True for the leader thread, and False for all other threads.
Operand membermask specifies a 32-bit integer indicating the set of threads from which a leader is to be elected.
The mandatory .sync qualifier indicates that elect causes the executing thread to wait until all threads in the membermask execute the elect instruction before resuming execution.
Parallel Synchronization and Communication Instructions: mbarrier  mbarrier is a barrier created in shared memory that supports : Synchronizing any subset of threads within a CTA One-way synchronization of threads across CTAs of a cluster.
As noted in mbarrier support with shared memory , threads can perform only arrive operations but not *_wait on an mbarrier located in shared::cluster space.
Waiting for completion of asynchronous memory operations initiated by a thread and making them visible to other threads.
An mbarrier object is an opaque object in memory which can be initialized and invalidated using : mbarrier.init mbarrier.inval Operations supported on mbarrier object s are : mbarrier.expect_tx mbarrier.complete_tx mbarrier.arrive mbarrier.arrive_drop mbarrier.test_wait mbarrier.try_wait mbarrier.pending_count cp.async.mbarrier.arrive Performing any mbarrier operation except mbarrier.init on an uninitialized mbarrier object results in undefined behavior.
Unlike bar{.cta} / barrier{.cta} instructions which can access a limited number of barriers per CTA, mbarrier objects are used defined and are only limited by the total shared memory size available.
mbarrier operations enable threads to perform useful work after the arrival at the mbarrier and before waiting for the mbarrier to complete. 9.7.12.15.1. Size and alignment of mbarrier object  An mbarrier object is an opaque object with the following type and alignment requirements : Type Alignment (bytes) Memory space .b64 8 .shared 9.7.12.15.2.
Contents of the mbarrier object  An opaque mbarrier object keeps track of the following information : Current phase of the mbarrier object Count of pending arrivals for the current phase of the mbarrier object Count of expected arrivals for the next phase of the mbarrier object Count of pending asynchronous memory operations (or transactions) tracked by the current phase of the mbarrier object .
An mbarrier object progresses through a sequence of phases where each phase is defined by threads performing an expected number of arrive-on operations.
The valid range of each of the counts is as shown below: Count name Minimum value Maximum value Expected arrival count 1 2 20 - 1 Pending arrival count 0 2 20 - 1 tx-count -(2 20 - 1) 2 20 - 1 9.7.12.15.3.
An mbarrier object must be invalidated to repurpose its memory. 9.7.12.15.4. Phase of the mbarrier object  The phase of an mbarrier object is the number of times the mbarrier object has been used to synchronize threads and cp.async operations.
In each phase {0, 1, 2, …}, threads perform in program order : arrive-on operations to complete the current phase and test_wait / try_wait operations to check for the completion of the current phase.
An mbarrier object is automatically reinitialized upon completion of the current phase for immediate use in the next phase.
For each phase of the mbarrier object, at least one test_wait or try_wait operation must be performed which returns True for waitComplete before an arrive-on operation in the subsequent phase. 9.7.12.15.5. Tracking asynchronous operations by the mbarrier object  Starting with the Hopper architecture ( sm_9x ), mbarrier object supports a new count, called tx-count , which is used for tracking the completion of asynchronous memory operations or transactions.
tx-count tracks the number of asynchronous transactions, in units specified by the asynchronous memory operation, that are outstanding and yet to be complete.
The tx-count of an mbarrier object must be set to the total amount of asynchronous memory operations, in units as specified by the asynchronous operations, to be tracked by the current phase.
Upon completion of each of the asynchronous operations, the complete-tx operation will be performed on the mbarrier object and thus progress the mbarrier towards the completion of the current phase. 9.7.12.15.5.1. expect-tx operation  The expect-tx operation, with an expectCount argument, increases the tx-count of an mbarrier object by the value specified by expectCount .
This makes the current phase of the mbarrier object to expect and track the completion of additional asynchronous transactions. 9.7.12.15.5.2. complete-tx operation  The complete-tx operation, with an completeCount argument, on an mbarrier object consists of the following: mbarrier signaling Signals the completion of asynchronous transactions that were tracked by the current phase.
mbarrier potentially completing the current phase If the current phase has been completed then the mbarrier transitions to the next phase.
Refer to Phase Completion of the mbarrier object for details on phase completion requirements and phase transition process. 9.7.12.15.6. Phase Completion of the mbarrier object  The requirements for completion of the current phase are described below.
Upon completion of the current phase, the phase transitions to the subsequent phase as described below.
Current phase completion requirements An mbarrier object completes the current phase when all of the following conditions are met: The count of the pending arrivals has reached zero.
Phase transition When an mbarrier object completes the current phase, the following actions are performed atomically: The mbarrier object transitions to the next phase.
The pending arrival count is reinitialized to the expected arrival count. 9.7.12.15.7. Arrive-on operation on mbarrier object  An arrive-on operation, with an optional count argument, on an mbarrier object consists of the following 2 steps : mbarrier signalling: Signals the arrival of the executing thread OR completion of the cp.async instruction which signals the arrive-on operation initiated by the executing thread on the mbarrier object .
mbarrier potentially completing the current phase: If the current phase has been completed then the mbarrier transitions to the next phase. 9.7.12.15.8. mbarrier support with shared memory  The following table summarizes the support of various mbarrier operations on mbarrier objects located at different shared memory locations: mbarrier operations .shared::cta .shared::cluster mbarrier.arrive Supported Supported, cannot return result mbarrier.expect_tx Supported Supported mbarrier.complete_tx Supported Supported Other mbarrier operations Supported Not supported 9.7.12.15.9.
Parallel Synchronization and Communication Instructions: mbarrier.init  mbarrier.init Initialize the mbarrier object .
Syntax mbarrier.init{.shared{::cta}}.b64 [addr], count; Description mbarrier.init initializes the mbarrier object at the location specified by the address operand addr with the unsigned 32-bit integer count .
The value of operand count must be in the range as specified in Contents of the mbarrier object .
If the address specified by addr does not fall within the address window of .shared::cta state space then the behavior is undefined.
Examples .shared .b64 shMem, shMem2; .reg .b64 addr; .reg .b32 %r1; cvta.shared.u64 addr, shMem2; mbarrier.init.b64 [addr], %r1; bar.cta.sync 0;   ...
other mbarrier operations on addr mbarrier.init.shared::cta.b64 [shMem], 12; bar.sync 0;   ...
Parallel Synchronization and Communication Instructions: mbarrier.inval  mbarrier.inval Invalidates the mbarrier object .
Syntax mbarrier.inval{.shared{::cta}}.b64 [addr]; Description mbarrier.inval invalidates the mbarrier object at the location specified by the address operand addr .
An mbarrier object must be invalidated before using its memory location for any other purpose.
Performing any mbarrier operation except mbarrier.init on an invalidated mbarrier object results in undefined behaviour.
Examples .shared .b64 shmem; .reg .b64 addr; .reg .b32 %r1; .reg .pred t0;   Example 1 : bar.sync 0; @t0 mbarrier.init.b64 [addr], %r1;   ...
other mbarrier operations on addr bar.sync 0; @t0 mbarrier.inval.b64 [addr];   Example 2 : bar.cta.sync 0; mbarrier.init.shared.b64 [shmem], 12;   ...
other mbarrier operations on shmem bar.cta.sync 0; @t0 mbarrier.inval.shared.b64 [shmem];   shmem can be reused here for unrelated use : bar.cta.sync 0; st.shared.b64 [shmem], ...;   shmem can be re-initialized as mbarrier object : bar.cta.sync 0; @t0 mbarrier.init.shared.b64 [shmem], 24;   ...
other mbarrier operations on shmem bar.cta.sync 0; @t0 mbarrier.inval.shared::cta.b64 [shmem]; 9.7.12.15.11.
Parallel Synchronization and Communication Instructions: mbarrier.expect_tx  mbarrier.expect_tx Perfoms expect-tx operation on the mbarrier object .
Syntax mbarrier.expect_tx{.sem}{.scope}{.space}.b64 [addr], txCount; .sem = { .relaxed } .scope = { .cta, .cluster } .space = { .shared{::cta}, .shared::cluster } Description A thread executing mbarrier.expect_tx performs an expect-tx operation on the mbarrier object at the location specified by the address operand addr .
The 32-bit unsigned integer operand txCount specifies the expectCount argument to the expect-tx operation.
If the address specified by addr does not fall within the address window of .shared::cta or .shared::cluster state space then the behavior is undefined.
This operation does not provide any memory ordering semantics and thus is a relaxed operation.
Examples mbarrier.expect_tx.b64 [addr], 32; mbarrier.expect_tx.relaxed.cta.shared.b64 [mbarObj1], 512; mbarrier.expect_tx.relaxed.cta.shared.b64 [mbarObj2], 512; 9.7.12.15.12.
Parallel Synchronization and Communication Instructions: mbarrier.complete_tx  mbarrier.complete_tx Perfoms complete-tx operation on the mbarrier object .
Syntax mbarrier.complete_tx{.sem}{.scope}{.space}.b64 [addr], txCount; .sem = { .relaxed } .scope = { .cta, .cluster } .space = { .shared{::cta}, .shared::cluster } Description A thread executing mbarrier.complete_tx performs a complete-tx operation on the mbarrier object at the location specified by the address operand addr .
The 32-bit unsigned integer operand txCount specifies the completeCount argument to the complete-tx operation.
mbarrier.complete_tx does not involve any asynchronous memory operations and only simulates the completion of an asynchronous memory operation and its side effect of signaling to the mbarrier object .
Examples mbarrier.complete_tx.b64 [addr], 32; mbarrier.complete_tx.shared.b64 [mbarObj1], 512; mbarrier.complete_tx.relaxed.cta.b64 [addr2], 32; 9.7.12.15.13.
Parallel Synchronization and Communication Instructions: mbarrier.arrive  mbarrier.arrive Performs arrive-on operation on the mbarrier object .
Syntax mbarrier.arrive{.sem}{.scope}{.shared{::cta}}.b64 state, [addr]{, count}; mbarrier.arrive{.sem}{.scope}{.shared::cluster}.b64 _, [addr] {,count} mbarrier.arrive.expect_tx{.sem}{.scope}{.shared{::cta}}.b64 state, [addr], txCount; mbarrier.arrive.expect_tx{.sem}{.scope}{.shared::cluster}.b64 _, [addr], txCount; mbarrier.arrive.noComplete{.sem}{.cta}{.shared{::cta}}.b64 state, [addr], count; .sem = { .release } .scope = { .cta, .cluster } Description A thread executing mbarrier.arrive performs an arrive-on operation on the mbarrier object at the location specified by the address operand addr .
The 32-bit unsigned integer operand count specifies the count argument to the arrive-on operation.
The optional qualifier .expect_tx specifies that an expect-tx operation is performed prior to the arrive-on operation.
When both qualifiers .arrive and .expect_tx are specified, then the count argument of the arrive-on operation is assumed to be 1.
A mbarrier.arrive operation with .noComplete qualifier must not cause the mbarrier to complete its current phase, otherwise the behavior is undefined.
The value of the operand count must be in the range as specified in Contents of the mbarrier object .
Note: for sm_8x , when the argument count is specified, the modifier .noComplete is required.
mbarrier.arrive operation on an mbarrier object located in .shared::cta returns an opaque 64-bit register capturing the phase of the mbarrier object prior to the arrive-on operation in the destination operand state.
mbarrier.arrive operation on an mbarrier object located in .shared::cluster but not in .shared::cta cannot return a value.
The optional .scope qualifier indicates the set of threads that directly observe the memory synchronizing effect of this operation, as described in the Memory Consistency Model .
Support for sink symbol ‘_’ as the destination operand is introduced in PTX ISA version 7.1.
Support for count argument without the modifier .noComplete introduced in PTX ISA version 7.8.
Support for .scope and .sem qualifiers introduced in PTX ISA version 8.0 Target ISA Notes Requires sm_80 or higher.
Examples .reg .b32 cnt, remoteAddr32, remoteCTAId, addr32; .reg .b64 %r, addr, remoteAddr64; .shared .b64 shMem, shMem2; cvta.shared.u64 addr, shMem2; mov.b32 addr32, shMem2; mapa.shared::cluster.u32 remoteAddr32, addr32, remoteCTAId; mapa.u64 remoteAddr64, addr, remoteCTAId; cvta.shared.u64 addr, shMem2; mbarrier.arrive.shared.b64 %r0, [shMem]; mbarrier.arrive.shared::cta.b64 %r0, [shMem2]; mbarrier.arrive.release.cta.shared::cluster.b64 _, [remoteAddr32]; mbarrier.arrive.release.cluster.b64 _, [remoteAddr64], cnt; mbarrier.arrive.expect_tx.release.cluster.b64 _, [remoteAddr64], tx_count; mbarrier.arrive.noComplete.b64 %r1, [addr], 2; mbarrier.arrive.b64 %r2, [addr], cnt; 9.7.12.15.14.
Parallel Synchronization and Communication Instructions: mbarrier.arrive_drop  mbarrier.arrive_drop Decrements the expected count of the mbarrier object and performs arrive-on operation .
Syntax mbarrier.arrive_drop{.sem}{.scope}{.shared{::cta}}.b64 state, [addr]{, count}; mbarrier.arrive_drop{.sem}{.scope}{.shared::cluster}.b64 _, [addr] {,count}; mbarrier.arrive_drop.expect_tx{.shared{::cta}}{.sem}{.scope}.b64 state, [addr], tx_count; mbarrier.arrive_drop.expect_tx{.shared::cluster}{.sem}{.scope}.b64 _, [addr], tx_count; mbarrier.arrive_drop.noComplete{.sem}{.cta}{.shared{::cta}}.b64 state, [addr], count; .sem = { .release } .scope = { .cta, .cluster } Description A thread executing mbarrier.arrive_drop on the mbarrier object at the location specified by the address operand addr performs the following steps: Decrements the expected arrival count of the mbarrier object by the value specified by the 32-bit integer operand count .
The decrement done in the expected arrivals count of the mbarrier object will be for all the subsequent phases of the mbarrier object .
When both qualifiers .arrive and .expect_tx are specified, then the count argument of the arrive-on operation is assumed to be 1.
mbarrier.arrive_drop operation forms the release pattern as described in the Memory Consistency Model and synchronizes with the acquire patterns.
The optional .scope qualifier indicates the set of threads that an mbarrier.arrive_drop instruction can directly synchronize.
A mbarrier.arrive_drop with .noComplete qualifier must not complete the mbarrier, otherwise the behavior is undefined.
A thread that wants to either exit or opt out of participating in the arrive-on operation can use mbarrier.arrive_drop to drop itself from the mbarrier .
mbarrier.arrive_drop operation on an mbarrier object located in .shared::cta returns an opaque 64-bit register capturing the phase of the mbarrier object prior to the arrive-on operation in the destination operand state .
mbarrier.arrive_drop operation on an mbarrier object located in .shared::cluster but not in .shared::cta cannot return a value.
Examples .reg .b32 cnt; .reg .b64 %r1; .shared .b64 shMem;   Example 1 @p mbarrier.arrive_drop.shared.b64 _, [shMem]; @p exit; @p2 mbarrier.arrive_drop.noComplete.shared.b64 _, [shMem], %a; @p2 exit; ..
@!p mbarrier.arrive.shared.b64 %r1, [shMem]; @!p mbarrier.test_wait.shared.b64 q, [shMem], %r1;   Example 2 mbarrier.arrive_drop.shared::cluster.b64 _, [addr]; mbarrier.arrive_drop.shared::cta.release.cluster.b64 _, [addr], cnt;   Example 3 mbarrier.arrive_drop.expect_tx.shared::cta.release.cta.b64 state, [addr], tx_count; 9.7.12.15.15.
Parallel Synchronization and Communication Instructions: cp.async.mbarrier.arrive  cp.async.mbarrier.arrive Makes the mbarrier object track all prior cp.async operations initiated by the executing thread.
Syntax cp.async.mbarrier.arrive{.noinc}{.shared{::cta}}.b64 [addr]; Description Causes an arrive-on operation to be triggered by the system on the mbarrier object upon the completion of all prior cp.async operations initiated by the executing thread.
When .noinc modifier is not specified, the pending count of the mbarrier object is incremented by 1 prior to the asynchronous arrive-on operation .
This results in a zero-net change for the pending count from the asynchronous arrive-on operation during the current phase.
The pending count of the mbarrier object after the increment should not exceed the limit as mentioned in Contents of the mbarrier object .
When the .noinc modifier is specified, the increment to the pending count of the mbarrier object is not performed.
Hence the decrement of the pending count done by the asynchronous arrive-on operation must be accounted for in the initialization of the mbarrier object .
cp.async.ca.shared.global [shard1], [gbl1], 4; cp.async.cg.shared.global [shard2], [gbl2], 16; .
mbarrier.arrive.shared.b64 state, [shMem]; waitLoop: mbarrier.test_wait.shared.b64 p, [shMem], state; @!p bra waitLoop;   Example 2: with .noinc   Tracks arrive-on from mbarrier.arrive and cp.async.mbarrier.arrive.
All threads participating in the mbarrier perform cp.async mov.b32 copyOperationCnt, threadCount;   3 arrive-on operations will be triggered per-thread mul.lo.u32 copyArrivalCnt, copyOperationCnt, 3; add.u32 totalCount, threadCount, copyArrivalCnt; mbarrier.init.shared.b64 [shMem], totalCount; .
cp.async.ca.shared.global [shard1], [gbl1], 4; cp.async.cg.shared.global [shard2], [gbl2], 16; ...
Presence of .noinc requires mbarrier initalization to have accounted for arrive-on from cp.async cp.async.mbarrier.arrive.noinc.shared.b64 [shMem];   1st instance .
cp.async.ca.shared.global [shard3], [gbl3], 4; cp.async.ca.shared.global [shard4], [gbl4], 16; cp.async.mbarrier.arrive.noinc.shared::cta.b64 [shMem];   2nd instance .
cp.async.ca.shared.global [shard5], [gbl5], 4; cp.async.cg.shared.global [shard6], [gbl6], 16; cp.async.mbarrier.arrive.noinc.shared.b64 [shMem];   3rd and last instance .
mbarrier.arrive.shared.b64 state, [shMem]; waitLoop: mbarrier.test_wait.shared.b64 p, [shMem], state; @!p bra waitLoop; 9.7.12.15.16.
Parallel Synchronization and Communication Instructions: mbarrier.test_wait/mbarrier.try_wait  mbarrier.test_wait/mbarrier.try_wait Checks whether the mbarrier object has completed the phase.
Syntax mbarrier.test_wait{.sem}{.scope}{.shared{::cta}}.b64 waitComplete, [addr], state; mbarrier.test_wait.parity{.sem}{.scope}{.shared{::cta}}.b64 waitComplete, [addr], phaseParity; mbarrier.try_wait{.sem}{.scope}{.shared{::cta}}.b64 waitComplete, [addr], state {, suspendTimeHint}; mbarrier.try_wait.parity{.sem}{.scope}{.shared{::cta}}.b64 waitComplete, [addr], phaseParity {, suspendTimeHint}; .sem = { .acquire } .scope = { .cta, .cluster } Description The test_wait and try_wait operations test for the completion of the current or the immediately preceding phase of an mbarrier object at the location specified by the operand addr .
mbarrier.test_wait is a non-blocking instruction which tests for the completion of the phase.
mbarrier.try_wait is a potentially blocking instruction which tests for the completion of the phase.
Suspended thread resumes execution when the specified phase completes OR before the phase completes following a system-dependent time limit.
The optional 32-bit unsigned integer operand suspendTimeHint specifies the time limit, in nanoseconds, that may be used for the time limit instead of the system-dependent limit.
mbarrier.test_wait and mbarrier.try_wait test for completion of the phase : Specified by the operand state , which was returned by an mbarrier.arrive instruction on the same mbarrier object during the current or the immediately preceding phase.
Or Indicated by the operand phaseParity , which is the integer parity of either the current phase or the immediately preceding phase of the mbarrier object .
The .parity variant of the instructions test for the completion of the phase indicated by the operand phaseParity , which is the integer parity of either the current phase or the immediately preceding phase of the mbarrier object .
Note: the use of the .parity variants of the instructions requires tracking the phase of an mbarrier object throughout its lifetime.
The test_wait and try_wait operations are valid only for : the current incomplete phase, for which waitComplete returns False .
When mbarrier.test_wait and mbarrier.try_wait operations return True , they form the acquire pattern as described in the Memory Consistency Model .
The optional .scope qualifier indicates the set of threads that the mbarrier.test_wait and mbarrier.try_wait instructions can directly synchronize.
The following ordering of memory operations hold for the executing thread when mbarrier.test_wait or mbarrier.try_wait returns True : All memory accesses (except async operations ) requested prior, in program order, to mbarrier.arrive during the completed phase by the participating threads of the CTA are performed and are visible to the executing thread.
All cp.async operations requested prior, in program order, to cp.async.mbarrier.arrive during the completed phase by the participating threads of the CTA are performed and made visible to the executing thread.
All cp.async.bulk asynchronous operations using the same mbarrier object requested prior, in program order, to mbarrier.arrive during the completed phase by the participating threads of the CTA are performed and made visible to the executing thread.
All memory accesses requested after the mbarrier.test_wait or mbarrier.try_wait , in program order, are not performed and not visible to memory accesses performed prior to mbarrier.arrive , in program order, by other threads participating in the mbarrier .
There is no ordering and visibility guarantee for memory accesses requested by the thread after mbarrier.arrive and prior to mbarrier.test_wait , in program order.
Support for .scope and .sem qualifiers introduced in PTX ISA version 8.0 Target ISA Notes mbarrier.test_wait requires sm_80 or higher.
Examples   Example 1a, thread synchronization with test_wait: .reg .b64 %r1; .shared .b64 shMem; mbarrier.init.shared.b64 [shMem], N;   N threads participating in the mbarrier. ... mbarrier.arrive.shared.b64 %r1, [shMem];   N threads executing mbarrier.arrive   computation not requiring mbarrier synchronization...
waitLoop: mbarrier.test_wait.shared.b64 complete, [shMem], %r1; @!complete nanosleep.u32 20; @!complete bra waitLoop;   Example 1b, thread synchronization with try_wait : .reg .b64 %r1; .shared .b64 shMem; mbarrier.init.shared.b64 [shMem], N;   N threads participating in the mbarrier. ... mbarrier.arrive.shared.b64 %r1, [shMem];   N threads executing mbarrier.arrive   computation not requiring mbarrier synchronization...
waitLoop: mbarrier.try_wait.shared.b64 complete, [shMem], %r1; @!complete bra waitLoop;   Example 2, thread synchronization using phase parity : .reg .b32 i, parArg; .reg .b64 %r1; .shared .b64 shMem; mov.b32 i, 0; mbarrier.init.shared.b64 [shMem], N;   N threads participating in the mbarrier. ... loopStart :   One phase per loop iteration ...
and.b32 parArg, i, 1; waitLoop: mbarrier.test_wait.parity.shared.b64 complete, [shMem], parArg; @!complete nanosleep.u32 20; @!complete bra waitLoop; ...
add.u32 i, i, 1; setp.lt.u32 p, i, IterMax; @p bra loopStart;   Example 3, Asynchronous copy completion waiting : .reg .b64 state; .shared .b64 shMem2; .shared .b64 shard1, shard2; .global .b64 gbl1, gbl2; mbarrier.init.shared.b64 [shMem2], threadCount; ...
cp.async.ca.shared.global [shard1], [gbl1], 4; cp.async.cg.shared.global [shard2], [gbl2], 16;   Absence of .noinc accounts for arrive-on from prior cp.async operation cp.async.mbarrier.arrive.shared.b64 [shMem2]; ...
mbarrier.arrive.shared.b64 state, [shMem2]; waitLoop: mbarrier.test_wait.shared::cta.b64 p, [shMem2], state; @!p bra waitLoop;   Example 4, Synchronizing the CTA0 threads with cluster threads .reg .b64 %r1, addr, remAddr; .shared .b64 shMem; cvta.shared.u64 addr, shMem; mapa.u64 remAddr, addr, 0;   CTA0’s shMem instance   One thread from CTA0 executing the below initialization operation @p0 mbarrier.init.shared::cta.b64 [shMem], N;   N = no of cluster threads barrier.cluster.arrive; barrier.cluster.wait;   Entire cluster executing the below arrive operation mbarrier.arrive.release.cluster.b64 _, [remAddr];   computation not requiring mbarrier synchronization ...
Only CTA0 threads executing the below wait operation waitLoop: mbarrier.try_wait.parity.acquire.cluser.shared::cta.b64 complete, [shMem], 0; @!complete bra waitLoop; 9.7.12.15.17.
Parallel Synchronization and Communication Instructions: mbarrier.pending_count  mbarrier.pending_count Query the pending arrival count from the opaque mbarrier state.
Syntax mbarrier.pending_count.b64 count, state; Description The pending count can be queried from the opaque mbarrier state using mbarrier.pending_count .
The state operand is a 64-bit register that must be the result of a prior mbarrier.arrive.noComplete or mbarrier.arrive_drop.noComplete instruction.
The destination register count is a 32-bit unsigned integer representing the pending count of the mbarrier object prior to the arrive-on operation from which the state register was obtained.
Examples .reg .b32 %r1; .reg .b64 state; .shared .b64 shMem; mbarrier.arrive.noComplete.b64 state, [shMem], 1; mbarrier.pending_count.b64 %r1, state; 9.7.12.15.18.
Parallel Synchronization and Communication Instructions: tensormap.cp_fenceproxy  tensormap.cp_fenceproxy A fused copy and fence operation.
Syntax tensormap.cp_fenceproxy.cp_qualifiers.fence_qualifiers.sync.aligned [dst], [src], size; .cp_qualifiers = { .global.shared::cta } .fence_qualifiers = { .to_proxy::from_proxy.release.scope } .to_proxy::from_proxy = { .tensormap::generic } .scope = { .cta, .cluster, .gpu , .sys } Description The tensormap.cp_fence instructions perform the following operations in order : Copies data of size specified by the size argument, in bytes, from the location specified by the address operand src in shared memory to the location specified by the address operand dst in the global memory, in the generic proxy.
Establishes a uni-directional proxy release pattern on the ordering from the copy operation to the subsequent access performed in the tensormap proxy on the address dst .
The operands src and dst specify non-generic addresses in shared::cta and global state space respectively.
The optional .scope qualifier specifies the set of threads that can directly observe the proxy synchronizing effect of this operation, as described in Memory Consistency Model .
The mandatory .sync qualifier indicates that tensormap.cp_fenceproxy causes the executing thread to wait until all threads in the warp execute the same tensormap.cp_fenceproxy instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warp must execute the same tensormap.cp_fenceproxy instruction.
In conditionally executed code, an aligned tensormap.cp_fenceproxy instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined.
Examples   Example: manipulate a tensor-map object and then consume it in cp.async.bulk.tensor .reg .b64 new_addr; .global .align 128 .b8 gbl[128]; .shared .align 128 .b8 sMem[128]; cp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes [sMem], [gMem], 128, [mbar]; ...
try_wait_loop: mbarrier.try_wait.shared.b64 p, [mbar], state; @!p bra try_wait loop; tensormap.replace.tile.global_address.shared.b1024.b64 [sMem], new_addr; tensormap.cp_fenceproxy.global.shared::cta.proxy.tensormap::generic.release.gpu .sync.aligned [gbl], [sMem], 128; fence.proxy.tensormap::generic.acquire.gpu [gbl], 128; cp.async.bulk.tensor.1d.shared::cluster.global.tile [addr0], [gbl, {tc0}], [mbar0]; 9.7.13.
Warp Level Matrix Multiply-Accumulate Instructions  The matrix multiply and accumulate operation has the following form: D = A * B + C where D and C are called accumulators and may refer to the same matrix.
PTX provides two ways to perform matrix multiply-and-accumulate computation: Using wmma instructions: This warp-level computation is performed collectively by all threads in the warp as follows: Load matrices A, B and C from memory into registers using the wmma.load operation.
When the operation completes, the destination registers in each thread hold a fragment of the loaded matrix.
Perform the matrix multiply and accumulate operation using the wmma.mma operation on the loaded matrices.
When the operation completes, the destination registers in each thread hold a fragment of the result matrix returned by the wmma.mma operation.
Alternately, result matrix D can also be used as argument C for a subsequent wmma.mma operation.
The wmma.load and wmma.store instructions implicitly handle the organization of matrix elements when loading the input matrices from memory for the wmma.mma operation and when storing the result back to memory.
Using mma instruction: Similar to wmma , mma also requires computation to be performed collectively by all threads in the warp however distribution of matrix elements across different threads in warp needs to be done explicitly before invoking the mma operation.
The sparse variant can be used when A is a structured sparse matrix as described in Sparse matrix storage . 9.7.13.1. Matrix Shape  The matrix multiply and accumulate operations support a limited set of shapes for the operand matrices A, B and C.
The shapes of all three matrix operands are collectively described by the tuple MxNxK , where A is an MxK matrix, B is a KxN matrix, while C and D are MxN matrices.
Matrix Data-types  The matrix multiply and accumulate operation is supported separately on integer, floating-point, sub-byte integer and single bit data-types.
For floating-point matrix multiply and accumulate operation, different matrix operands may have different precision, as described later.
Data-type Multiplicands (A or B) Accumulators (C or D) Integer .u8 , .s8 .s32 Floating Point .f16 .f16 , .f32 Alternate floating Point .bf16 .f32 Alternate floating Point .tf32 .f32 Alternate floating Point .e4m3 or .e5m2 .f32 Floating Point .f64 .f64 Sub-byte integer both .u4 or both .s4 .s32 Single-bit integer .b1 .s32 9.7.13.3.
Matrix multiply-accumulate operation using wmma instructions  This section describes warp level wmma.load, wmma.mma and wmma.store instructions and the organization of various matrices invovled in these instruction. 9.7.13.3.1. Matrix Fragments for WMMA  Each thread in the warp holds a fragment of the matrix.
The distribution of fragments loaded by the threads in a warp is unspecified and is target architecture dependent, and hence the identity of the fragment within the matrix is also unspecified and is target architecture dependent.
The fragment returned by a wmma operation can be used as an operand for another wmma operation if the shape, layout and element type of the underlying matrix matches.
Since fragment layout is architecture dependent, using the fragment returned by a wmma operation in one function as an operand for a wmma operation in a different function may not work as expected if the two functions are linked together but were compiled for different link-compatible SM architectures.
Note passing wmma fragment to a function having .weak linkage is unsafe since at link time references to such function may get resolved to a function in different compilation module.
Integer fragments Multiplicands (A or B): Data-type Shape Matrix Fragment .u8 or .s8 .m16n16k16 A A vector expression of two .b32 registers, with each register containing four elements from the matrix.
B A vector expression of two .b32 registers, with each register containing four elements from the matrix.
.m8n32k16 A A vector expression containing a single .b32 register containing four elements from the matrix.
B A vector expression of four .b32 registers, with each register containing four elements from the matrix.
.m32n8k16 A A vector expression of four .b32 registers, with each register containing four elements from the matrix.
B A vector expression containing single .b32 register, with each containing four elements from the matrix.
Accumulators (C or D): Data-type Shape Fragment .s32 .m16n16k16 A vector expression of eight .s32 registers.
.m8n32k16 .m32n8k16 Floating point fragments Data-type Matrix Fragment .f16 A or B A vector expression of eight .f16x2 registers.
Floating point fragments for .bf16 data format Multiplicands (A or B): Data-type Shape Matrix Fragment .bf16 .m16n16k16 A A vector expression of four .b32 registers, with each register containing two elements from the matrix.
B .m8n32k16 A A vector expression containing a two .b32 registers, with containing two elements from the matrix.
B A vector expression of eight .b32 registers, with each register containing two elements from the matrix.
.m32n8k16 A A vector expression of eight .b32 registers, with each register containing two elements from the matrix.
B A vector expression containing two .b32 registers, with each containing two elements from the matrix.
Accumulators (C or D): Data-type Matrix Fragment .f32 C or D A vector expression containing eight .f32 registers.
Floating point fragments for .tf32 data format Multiplicands (A or B): Data-type Shape Matrix Fragment .tf32 .m16n16k8 A A vector expression of four .b32 registers.
Accumulators (C or D): Data-type Shape Matrix Fragment .f32 .m16n16k8 C or D A vector expression containing eight .f32 registers.
Double precision floating point fragments Multiplicands (A or B): Data-type Shape Matrix Fragment .f64 .m8n8k4 A or B A vector expression of single .f64 register.
Accumulators (C or D): Data-type Shape Matrix Fragment .f64 .m8n8k4 C or D A vector expression containing single .f64 register.
Sub-byte integer and single-bit fragments Multiplicands (A or B): Data-type Shape Fragment .u4 or .s4 .m8n8k32 A vector expression containing a single .b32 register, containing eight elements from the matrix.
.b1 .m8n8k128 A vector expression containing a single .b32 register, containing 32 elements from the matrix.
Accumulators (C or D): Data-type Shape Fragment .s32 .m8n8k32 A vector expression of two .s32 registers.
Manipulating fragment contents The contents of a matrix fragment can be manipulated by reading and writing to individual registers in the fragment, provided the following conditions are satisfied: All matrix element in the fragment are operated on uniformly across threads, using the same parameters.
For example, if each register corresponding to a given matrix is multiplied by a uniform constant value, then the resulting matrix is simply the scaled version of the original matrix.
Note that type conversion between .f16 and .f32 accumulator fragments is not supported in either direction.
The result is undefined even if the order of elements in the fragment remains unchanged. 9.7.13.3.2. Matrix Storage for WMMA  Each matrix can be stored in memory with a row-major or column-major layout.
In a row-major format, consecutive elements of each row are stored in contiguous memory locations, and the row is called the leading dimension of the matrix.
In a column-major format, consecutive elements of each column are stored in contiguous memory locations and the column is called the leading dimension of the matrix.
Consecutive instances of the leading dimension (rows or columns) need not be stored contiguously in memory.
The wmma.load and wmma.store operations accept an optional argument stride that specifies the offset from the beginning of each row (or column) to the next, in terms of matrix elements (and not bytes).
For example, the matrix being accessed by a wmma operation may be a submatrix from a larger matrix stored in memory.
This allows the programmer to compose a multiply-and-accumulate operation on matrices that are larger than the shapes supported by the wmma operation.
Address Alignment: The starting address of each instance of the leading dimension (row or column) must be aligned with the size of the corresponding fragment in bytes.
Note that the starting address is determined by the base pointer and the optional stride .
Consider the following instruction as an example: wmma.load.a.sync.aligned.row.m16n16k16.f16 {x0,...,x7}, [p], s; Fragment size in bytes = 32 (eight elements of type .f16x2 ) Actual stride in bytes = 2 * s (since stride is specified in terms of .f16 elements, not bytes) For each row of this matrix to be aligned at fragment size the following must be true: p is a multiple of 32.
Default value for stride: The default value of the stride is the size of the leading dimension of the matrix.
For example, for an MxK matrix, the stride is K for a row-major layout and M for a column-major layout.
In particular, the default strides for the supported matrix shapes are as follows: Shape A (row) A (column) B (row) B (column) Accumulator (row) Accumulator (column) 16x16x16 16 16 16 16 16 16 8x32x16 16 8 32 16 32 8 32x8x16 16 32 8 16 8 32 8x8x32 32 8 8 32 8 8 8x8x128 128 8 8 128 8 8 16x16x8 8 16 16 8 16 16 8x8x4 4 8 8 4 8 8 9.7.13.3.3.
wmma.load operation may be used only with .global and .shared spaces and with generic addressing, where the address points to .global or .shared space.
The mutually exclusive qualifiers .a , .b and .c indicate whether matrix A, B or C is being loaded respectively for the wmma computation.
The destination operand r is a brace-enclosed vector expression that can hold the fragment returned by the load operation, as described in Matrix Fragments for WMMA .
The .shape qualifier indicates the dimensions of all the matrix arguments involved in the intended wmma computation.
The .layout qualifier indicates whether the matrix to be loaded is stored in row-major or column-major format.
stride is an optional 32-bit integer operand that provides an offset in terms of matrix elements between the start of consecutive instances of the leading dimension (rows or columns).
The default value of stride is described in Matrix Storage for WMMA and must be specified if the actual value is larger than the default.
For example, if the matrix is a sub-matrix of a larger matrix, then the value of stride is the leading dimension of the larger matrix.
The required alignment for address p and stride is described in the Matrix Storage for WMMA .
The mandatory .sync qualifier indicates that wmma.load causes the executing thread to wait until all threads in the warp execute the same wmma.load instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warp must execute the same wmma.load instruction.
In conditionally executed code, a wmma.load instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined.
The behavior of wmma.load is undefined if all threads do not use the same qualifiers and the same values of p and stride , or if any thread in the warp has exited.
Double precision and alternate floating point precision wmma introduced in PTX ISA version 7.0.
Modifier .aligned is required from PTX ISA version 6.3 onwards, and considered implicit in PTX ISA versions less than 6.3.
Preview Feature: Sub-byte wmma and single-bit wmma are preview features in PTX ISA version 6.3.
Examples   Load elements from f16 row-major matrix B .reg .b32 x; wmma.load.b.sync.aligned.m16n16k16.row.f16 {x0,x1,x2,x3,x4,x5,x,x7}, [ptr];   Now use {x0, ..., x7} for the actual wmma.mma   Load elements from f32 column-major matrix C and scale the values: .reg .b32 x; wmma.load.c.sync.aligned.m16n16k16.col.f32 {x0,x1,x2,x3,x4,x5,x6,x7}, [ptr]; mul.f32 x0, x0, 0.1;   repeat for all registers x; ...
mul.f32 x7, x7, 0.1;   Now use {x0, ..., x7} for the actual wmma.mma   Load elements from integer matrix A: .reg .b32 x   destination registers x contain four packed .u8 values each wmma.load.a.sync.aligned.m32n8k16.row.u8 {x0,x1,x2,x3}, [ptr];   Load elements from sub-byte integer matrix A: .reg .b32 x0;   destination register x0 contains eight packed .s4 values wmma.load.a.sync.aligned.m8n8k32.row.s4 {x0}, [ptr];   Load elements from .bf16 matrix A: .reg .b32 x; wmma.load.a.sync.aligned.m16n16k16.row.bf16 {x0,x1,x2,x3}, [ptr];   Load elements from .tf32 matrix A: .reg .b32 x; wmma.load.a.sync.aligned.m16n16k8.row.tf32 {x0,x1,x2,x3}, [ptr];   Load elements from .f64 matrix A: .reg .b32 x; wmma.load.a.sync.aligned.m8n8k4.row.f64 {x0}, [ptr]; 9.7.13.3.4.
The source operand r is a brace-enclosed vector expression that matches the shape of the fragment expected by the store operation, as described in Matrix Fragments for WMMA .
It must match the .shape qualifier specified on the wmma.mma instruction that produced the D matrix being stored.
The mandatory .sync qualifier indicates that wmma.store causes the executing thread to wait until all threads in the warp execute the same wmma.store instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warp must execute the same wmma.store instruction.
In conditionally executed code, a wmma.store instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined.
The behavior of wmma.store is undefined if all threads do not use the same qualifiers and the same values of p and stride , or if any thread in the warp has exited.
Examples   Storing f32 elements computed by a wmma.mma .reg .b32 x; wmma.mma.sync.m16n16k16.row.col.f32.f32 {d0, d1, d2, d3, d4, d5, d6, d7}, ...; wmma.store.d.sync.m16n16k16.row.f32 [ptr], {d0, d1, d2, d3, d4, d5, d6, d7};   Store s32 accumulator for m16n16k16 shape: .reg .b32 d; wmma.store.d.sync.aligned.m16n16k16.row.s32 [ptr], {d0, d1, d2, d3, d4, d5, d6, d7};   Store s32 accumulator for m8n8k128 shape: .reg .b32 d wmma.store.d.sync.aligned.m8n8k128.row.s32 [ptr], {d0, d1};   Store f64 accumulator for m8n8k4 shape: .reg .f64 d; wmma.store.d.sync.aligned.m8n8k4.row.f64 [ptr], {d0, d1}; 9.7.13.3.5.
The register arguments a , b , c and d hold unspecified fragments of the corresponding matrices as described in Matrix Fragments for WMMA The qualifiers .dtype , .atype , .btype and .ctype indicate the data-type of the elements in the matrices D, A, B and C respectively.
For wmma.mma without explicit .atype and .btype : .atype and .btype are implicitly set to .f16 .
Also, the values for .atype and .btype must be the same, i.e., either both are .s8 or both are .u8 .
Also, the values for .atype and .btype must be the same; i.e., either both are .s4 , both are .u4 , or both are .b1 .
For single-bit wmma , multiplication is replaced by a sequence of logical operations; specifically, wmma.xor.popc and wmma.and.popc computes the XOR, AND respectively of a 128-bit row of A with a 128-bit column of B, then counts the number of set bits in the result ( popc ).
The qualifiers .alayout and .blayout must match the layout specified on the wmma.load instructions that produce the contents of operands a and b respectively.
Similarly, the qualifiers .atype , .btype and .ctype must match the corresponding qualifiers on the wmma.load instructions that produce the contents of operands a , b and c respectively.
The .shape qualifier must match the .shape qualifier used on the wmma.load instructions that produce the contents of all three input operands a , b and c respectively.
The destination operand d is a brace-enclosed vector expression that matches the .shape of the fragment computed by the wmma.mma instruction.
Saturation at the output: The optional qualifier .satfinite indicates that the final values in the destination register are saturated as follows: The output is clamped to the minimum or maximum 32-bit signed integer value.
Precision and rounding for .f16 floating point operations: Element-wise multiplication of matrix A and B is performed with at least single precision.
When .ctype or .dtype is .f32 , accumulation of the intermediate values is performed with at least single precision.
When both .ctype and .dtype are specified as .f16 , the accumulation is performed with at least half precision.
Precision and rounding for .bf16 , .tf32 floating point operations: Element-wise multiplication of matrix A and B is performed with specified precision.
Rounding modifiers on double precision wmma.mma (default is .rn ): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity The mandatory .sync qualifier indicates that wmma.mma causes the executing thread to wait until all threads in the warp execute the same wmma.mma instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warp must execute the same wmma.mma instruction.
In conditionally executed code, a wmma.mma instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined.
The behavior of wmma.mma is undefined if all threads in the same warp do not use the same qualifiers, or if any thread in the warp has exited.
Support for .satfinite on floating point wmma.mma is deprecated in PTX ISA version 6.4 and is removed from PTX ISA version 6.5.
Matrix multiply-accumulate operation using mma instruction  This section describes warp-level mma , ldmatrix , stmatrix , and movmatrix instructions and the organization of various matrices involved in these instructions. 9.7.13.4.1. Matrix Fragments for mma.m8n8k4 with .f16 floating point type  A warp executing mma.m8n8k4 with .f16 floating point type will compute 4 MMA operations of shape .m8n8k4 .
MMA Computation Threads participating in MMA computation MMA computation 1 Threads with %laneid 0-3 (low group) and 16-19 (high group) MMA computation 2 Threads with %laneid 4-7 (low group) and 20-23 (high group) MMA computation 3 Threads with %laneid 8-11 (low group) and 24-27 (high group) MMA computation 4 Threads with %laneid 12-15 (low group) and 28-31 (high group) For each of the individual MMA computation shown above, each of the required thread holds a fragment of the matrix for performing mma operation as follows: Multiplicand A: .atype Fragment Elements (low to high) .f16 A vector expression containing two .f16x2 registers, with each register containing two .f16 elements from the matrix A.
a0, a1, a2, a3 The layout of the fragments held by different threads is shown below: Fragment layout for Row Major matrix A is shown in Figure 21 .
Figure 21 MMA .m8n8k4 fragment layout for row-major matrix A with .f16 type  The row and column of a matrix fragment can be computed as: row = % laneid % 4 if % laneid > 2 col = % laneid % 4 Multiplicand B: .btype Fragment Elements (low to high) .f64 A vector expression containing a single .f64 register, containing a single .f64 element from the matrix B.
Figure 29 MMA .m8n8k4 fragment layout for matrix B with .f64 type  The row and column of a matrix fragment can be computed as: row = % laneid % 4 col = % laneid >> 2 Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .f64 A vector expression containing of two .f64 registers containing two .f64 elements from the matrix C.
Figure 30 MMA .m8n8k4 fragment layout for accumulator matrix C/D with .f64 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 , 1 } 9.7.13.4.3.
Matrix Fragments for mma.m8n8k16  A warp executing mma.m8n8k16 will compute an MMA operation of shape .m8n8k16 .
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds a fragment of the matrix.
Multiplicand A: .atype Fragment Elements (low to high) .s8 / .u8 A vector expression containing a single .b32 register, containing four .s8 or .u8 elements from the matrix A.
a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 31 .
Figure 31 MMA .m8n8k16 fragment layout for matrix A with .u8 / .s8 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 4 ) + i for ai where i = { 0 ,.., 3 } Multiplicand B: .btype Fragment Elements (low to high) .s8 / .u8 A vector expression containing a single .b32 register, containing four .s8 or .u8 elements from the matrix B.
b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 32 .
Figure 32 MMA .m8n8k16 fragment layout for matrix B with .u8 / .s8 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 4 ) + i for bi where i = { 0 ,.., 3 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing of two .s32 registers.
Figure 33 MMA .m8n8k16 fragment layout for accumulator matrix C/D with .s32 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + i for ci where i = { 0 , 1 } 9.7.13.4.4.
Matrix Fragments for mma.m8n8k32  A warp executing mma.m8n8k32 will compute an MMA operation of shape .m8n8k32 .
Multiplicand A: .atype Fragment Elements (low to high) .s4 / .u4 A vector expression containing a single .b32 register, containing eight .s4 or .u4 elements from the matrix A.
a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 34 .
Figure 34 MMA .m8n8k32 fragment layout for matrix A with .u4 / .s4 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 8 ) + i for ai where i = { 0 ,.., 7 } Multiplicand B: .btype Fragment Elements (low to high) .s4 / .u4 A vector expression containing a single .b32 register, containing eight .s4 or .u4 elements from the matrix B.
b0, b1, b2, b3, b4, b5, b6, b7 The layout of the fragments held by different threads is shown in Figure 35 .
Figure 35 MMA .m8n8k32 fragment layout for matrix B with .u4 / .s4 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 8 ) + i for bi where i = { 0 ,.., 7 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression of two .s32 registers.
c0, c1 The layout of the fragments held by different threads is shown in Figure 36 : Figure 36 MMA .m8n8k32 fragment layout for accumulator matrix C/D with .s32 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + i for ci where i = { 0 , 1 } 9.7.13.4.5.
Matrix Fragments for mma.m8n8k128  A warp executing mma.m8n8k128 will compute an MMA operation of shape .m8n8k128 .
Multiplicand A: .atype Fragment Elements (low to high) .b1 A vector expression containing a single .b32 register, containing thirty two .b1 elements from the matrix A.
a0, a1, … a30, a31 The layout of the fragments held by different threads is shown in Figure 37 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 32 ) + i for ai where i = { 0 ,.., 31 } Multiplicand B: .btype Fragment Elements (low to high) .b1 A vector expression containing a single .b32 register, containing thirty two .b1 elements from the matrix B.
b0, b1, …, b30, b31 The layout of the fragments held by different threads is shown in Figure 38 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 32 ) + i for bi where i = { 0 ,.., 31 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing two .s32 registers, containing two .s32 elements from the matrix C (or D).
Figure 39 MMA .m8n8k128 fragment layout for accumulator matrix C/D with .s32 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + i for ci where i = { 0 , 1 } 9.7.13.4.6.
Matrix Fragments for mma.m16n8k4  A warp executing mma.m16n8k4 will compute an MMA operation of shape .m16n8k4 .
Multiplicand A: .tf32 : .atype Fragment Elements (low to high) .tf32 A vector expression containing two .b32 registers, containing two .tf32 elements from the matrix A.
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 groupID + 8 for a1 col = threadID_in_group .f64 : .atype Fragment Elements (low to high) .f64 A vector expression containing two .f64 registers, containing two .f64 elements from the matrix A.
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 groupID + 8 for a1 col = threadID_in_group Multiplicand B: .tf32 : .btype Fragment Elements (low to high) .tf32 A vector expression of a single .b32 register, containing a single .tf32 element from the matrix B.
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group col = groupID .f64 : .btype Fragment Elements (low to high) .f64 A vector expression of a single .f64 register, containing a single .f64 element from the matrix B.
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group col = groupID Accumulators (C or D): .tf32 : .ctype / .dtype Fragment Elements (low to high) .f32 A vector expression containing four .f32 registers, containing four .f32 elements from the matrix C (or D).
c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 44 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } .f64 : .ctype / .dtype Fragment Elements (low to high) .f64 A vector expression containing four .f64 registers, containing four .f64 elements from the matrix C (or D).
c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 45 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.7.
Matrix Fragments for mma.m16n8k8  A warp executing mma.m16n8k8 will compute an MMA operation of shape .m16n8k8 .
Multiplicand A: .f16 and .bf16 : .atype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing two .f16x2 registers, with each register containing two .f16 / .bf16 elements from the matrix A.
a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 46 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a1 groupID + 8 for a2 and a3 col = threadID_in_group * 2 + ( i & 0x1 ) for ai where i = { 0 ,.., 3 } .tf32 : .atype Fragment Elements (low to high) .tf32 A vector expression containing four .b32 registers, containing four .tf32 elements from the matrix A.
a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 47 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a2 groupID + 8 for a1 and a3 col = threadID_in_group for a0 and a1 threadID_in_group + 4 for a2 and a3 .f64 : .atype Fragment Elements (low to high) .f64 A vector expression containing four .f64 registers, containing four .f64 elements from the matrix A.
a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 48 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a2 groupID + 8 for a1 and a3 col = threadID_in_group for a0 and a1 threadID_in_group + 4 for a2 and a3 Multiplicand B: .f16 and .bf16 : .btype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing a single .f16x2 register, containing two .f16 / .bf16 elements from the matrix B.
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 2 ) + i for bi where i = { 0 , 1 } col = groupID .tf32 : .btype Fragment Elements (low to high) .tf32 A vector expression containing two .b32 registers, containing two .tf32 elements from the matrix B.
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group for b0 threadID_in_group + 4 for b1 col = groupID .f64 : .btype Fragment Elements (low to high) .f64 A vector expression containing two .f64 registers, containing two .f64 elements from the matrix B.
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group for b0 threadID_in_group + 4 for b1 col = groupID Accumulators (C or D): .f16 , .bf16 and .tf32 : .ctype / .dtype Fragment Elements (low to high) .f16 A vector expression containing two .f16x2 registers, with each register containing two .f16 elements from the matrix C (or D).
Figure 52 MMA .m16n8k8 fragment layout for accumulator matrix C/D with .f16x2 / .f32 type.
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } .f64 : .ctype / .dtype Fragment Elements (low to high) .f64 A vector expression of four .f64 registers containing four .f64 elements from the matrix C (or D).
c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 53 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.8.
Matrix Fragments for mma.m16n8k16 with floating point type  A warp executing mma.m16n8k16 floating point types will compute an MMA operation of shape .m16n8k16 .
Multiplicand A: .f16 and .bf16 : .atype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing four .f16x2 registers, with each register containing two .f16 / .bf16 elements from the matrix A.
a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 54 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 4 .f64 : .atype Fragment Elements (low to high) .f64 A vector expression containing eight .f64 registers, with each register containing one .f64 element from the matrix A.
a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 55 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i % 2 = 0 groupID + 8 Otherwise col = ( i * 2 ) + threadID_in_group for ai where i % 2 = 0 ( i * 2 ) - 2 + ( threadID_in_group Otherwise Multiplicand B: .f16 and .bf16 : .btype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing two .f16x2 registers, with each register containing two .f16 / .bf16 elements from the matrix B.
b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 56 .
 where the row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 2 ) + ( i & 0x1 ) for bi where i = 2 col = groupID .f64 : .atype Fragment Elements (low to high) .f64 A vector expression containing four .f64 registers, with each register containing one .f64 element from the matrix B.
b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 57 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group + ( i * 4 ) for bi where i > 2 threadID_in_group = % laneid % 4 row = groupID for ci where i = 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.9.
Matrix Fragments for mma.m16n8k16 with integer type  A warp executing mma.m16n8k16 with .u8 or .s8 integer type will compute an MMA operation of shape .m16n8k16 .
Multiplicand A: .atype Fragment Elements (low to high) .u8 / .s8 A vector expression containing two .b32 registers, with each register containing four .u8 / .s8 elements from the matrix A.
a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 59 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i = 4 col = ( threadID_in_group * 4 ) + ( i & 0x3 ) for ai where i = { 0 ,.., 7 } Multiplicand B: .btype Fragment Elements (low to high) .u8 / .s8 A vector expression containing a single .b32 register, containing four .u8 / .s8 elements from the matrix B.
b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 60 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 4 ) + i for bi where i = { 0 ,.., 3 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing four .s32 elements from the matrix C (or D).
c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 61 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i = 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.10.
Matrix Fragments for mma.m16n8k32  A warp executing mma.m16n8k32 will compute an MMA operation of shape .m16n8k32 .
Multiplicand A: .s4 or .u4 : .atype Fragment Elements (low to high) .s4 / .u4 A vector expression containing two .b32 registers, with each register containing eight .u4 / .s4 elements from the matrix A.
a0, a1, …, a14, a15 The layout of the fragments held by different threads is shown in Figure 62 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i = 8 col = ( threadID_in_group * 8 ) + ( i & 0x7 ) for ai where i = { 0 ,.., 15 } .s8 or .u8 or .e4m3 or .e5m2 : .atype Fragment Elements (low to high) .s8 / .u8 A vector expression containing four .b32 registers, with each register containing four .s8 / .u8 elements from the matrix A.
a0, a1, .., a14, a15 .e4m3 / .e5m2 A vector expression containing four .b32 registers, with each register containing four .e4m3 / .e5m2 elements from the matrix A.
a0, a1, …, a14, a15 The layout of the fragments held by different threads is shown in Figure 63 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 8 Multiplicand B: .s4 or .u4 : .btype Fragment Elements (low to high) .s4 / .u4 A vector expression containing a single .b32 register, containing eight .s4 / .u4 elements from the matrix B.
b0, b1, b2, b3, b4, b5, b6, b7 The layout of the fragments held by different threads is shown in Figure 64 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 8 ) + ( i & 0x7 ) for bi where i = { 0 ,.., 7 } col = groupID .s8 or .u8 or .e4m3 or .e5m2 : .btype Fragment Elements (low to high) .s8 / .u8 A vector expression containing two .b32 registers, with each register containing four .s8 / .u8 elements from the matrix B.
b0, b1, b2, b3, b4, b5, b6, b7 .e4m3 / .e5m2 A vector expression containing two .b32 registers, with each register containing four .e4m3 / .e5m2 elements from the matrix B.
b0, b1, b2, b3, b4, b5, b6, b7 The layout of the fragments held by different threads is shown in Figure 65 and Figure 66 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 4 ) + ( i & 0x3 ) for bi where i = 4 col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing four .s32 elements from the matrix C (or D).
c0, c1, c2, c3 .f32 A vector expression containing four .f32 registers, containing four .f32 elements from the matrix C (or D).
c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 67 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i = 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.11.
Matrix Fragments for mma.m16n8k64  A warp executing mma.m16n8k64 will compute an MMA operation of shape .m16n8k64 .
Multiplicand A: .atype Fragment Elements (low to high) .s4 / .u4 A vector expression containing four .b32 registers, with each register containing eight .s4 / .u4 elements from the matrix A.
a0, a1, …, a30, a31 The layout of the fragments held by different threads is shown in Figure 68 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 16 Multiplicand B: .btype Fragment Elements (low to high) .s4 / .u4 A vector expression containing two .b32 registers, with each register containing eight .s4 / .u4 elements from the matrix B.
b0, b1, …, b14, b15 The layout of the fragments held by different threads is shown in Figure 69 and Figure 70 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 8 ) + ( i & 0x7 ) for bi where i = 8 col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing four .s32 elements from the matrix C (or D).
c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 71 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i = 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.12.
Matrix Fragments for mma.m16n8k128  A warp executing mma.m16n8k128 will compute an MMA operation of shape .m16n8k128 .
Multiplicand A: .atype Fragment Elements (low to high) .b1 A vector expression containing two .b32 registers, with each register containing thirty two .b1 elements from the matrix A.
a0, a1, …, a62, a63 The layout of the fragments held by different threads is shown in Figure 72 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i = 32 col = ( threadID_in_group * 32 ) + ( i & 0x1F ) for ai where i = { 0 , ..., 63 } Multiplicand B: .btype Fragment Elements (low to high) .b1 A vector expression containing a single .b32 register containing thirty two .b1 elements from the matrix B.
b0, b1, … , b30, b31 The layout of the fragments held by different threads is shown in Figure 73 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 32 ) + i for bi where i = { 0 ,..., 31 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing four .s32 elements from the matrix C (or D).
c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 74 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i = 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 , 1 , 2 , 3 } 9.7.13.4.13.
Matrix Fragments for mma.m16n8k256  A warp executing mma.m16n8k256 will compute an MMA operation of shape .m16n8k256 .
Multiplicand A: .atype Fragment Elements (low to high) .b1 A vector expression containing four .b32 registers, with each register containing thirty two .b1 elements from the matrix A.
a0, a1, …, a126, a127 The layout of the fragments held by different threads is shown in Figure 75 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 64 Multiplicand B: .btype Fragment Elements (low to high) .b1 A vector expression containing two .b32 registers, with each register containing thirty two .b1 elements from the matrix B.
b0, b1, …, b62, b63 The layout of the fragments held by different threads is shown in Figure 76 and Figure 77 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 32 ) + ( i & 0x1F ) for bi where i = 32 col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing four .s32 elements from the matrix C (or D).
c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 78 .
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i = 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 , 1 , 2 , 3 } 9.7.13.4.14.
A warp executing mma.sync.m8n8k4 instruction computes 4 matrix multiply and accumulate operations.
Rest of the mma.sync operations compute a single matrix mutliply and accumulate operation per warp.
For single-bit mma.sync , multiplication is replaced by a sequence of logical operations; specifically, mma.xor.popc and mma.and.popc computes the XOR, AND respectively of a k-bit row of A with a k-bit column of B, then counts the number of set bits in the result ( popc ).
Operands a and b represent two multiplicand matrices A and B, while c and d represent the accumulator and destination matrices, distributed across the threads in warp.
The registers in each thread hold a fragment of matrix as described in Matrix multiply-accumulate operation using mma instruction .
The qualifiers .dtype , .atype , .btype and .ctype indicate the data-type of the elements in the matrices D, A, B and C respectively.
Specific shapes have type restrictions : .m8n8k4 : When .ctype is .f32 , .dtype must also be .f32 .
The qualifiers .alayout and .blayout indicate the row-major or column-major layouts of matrices A and B respectively.
Precision and rounding : .f16 floating point operations: Element-wise multiplication of matrix A and B is performed with at least single precision.
.e4m3 and .e5m2 floating point operations : Element-wise multiplication of matrix A and B is performed with specified precision.
.bf16 and .tf32 floating point operations : Element-wise multiplication of matrix A and B is performed with specified precision.
.f64 floating point operations : Precision of the element-wise multiplication and addition operation is identical to that of .f64 precision fused multiply-add.
The .satfinite qualifier indicates that on overflow, the accumulated value is limited to the range MIN_INT32 ..
MAX_INT32 (where the bounds are defined as the minimum negative signed 32-bit integer and the maximum positive signed 32-bit integer respectively).
The mandatory .sync qualifier indicates that mma instruction causes the executing thread to wait until all threads in the warp execute the same mma instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warp must execute the same mma instruction.
In conditionally executed code, a mma instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined.
The behavior of mma instruction is undefined if all threads in the same warp do not use the same qualifiers, or if any thread in the warp has exited.
Notes Programs using double precision floating point mma instruction with shapes .m16n8k4 , .m16n8k8 , and .m16n8k16 require at least 64 registers for compilation.
.f16 floating point type mma operation with .m8n8k4 shape introduced in PTX ISA version 6.4.
.f16 floating point type mma operation with .m16n8k8 shape introduced in PTX ISA version 6.5.
.f64 floating point type mma operation with .m8n8k4 shape introduced in PTX ISA version 7.0.
.f16 floating point type mma operation with .m16n8k16 shape introduced in PTX ISA version 7.0.
.bf16 alternate floating point type mma operation with .m16n8k8 and .m16n8k16 shapes introduced in PTX ISA version 7.0.
.tf32 alternate floating point type mma operation with .m16n8k4 and .m16n8k8 shapes introduced in PTX ISA version 7.0.
.u8/.s8 integer type mma operation with .m16n8k16 and .m16n8k32 shapes introduced in PTX ISA version 7.0.
.u4/.s4 integer type mma operation with .m16n8k32 and .m16n8k64 shapes introduced in PTX ISA version 7.0.
.b1 single-bit integer type mma operation with .m8n8k128 , .m16n8k128 and .m16n8k256 shapes introduced in PTX ISA version 7.0.
.f64 floating point type mma operation with .m16n8k4 , .m16n8k8 , and .m16n8k16 shapes introduced in PTX ISA version 7.8.
Support for .e4m3 and .e5m2 alternate floating point type mma operation introduced in PTX ISA version 8.4.
Note mma.sync.m8n8k4 is optimized for target architecture sm_70 and may have substantially reduced performance on other target architectures.
.bf16 alternate floating point type mma operation with .m16n8k8 and .m16n8k16 shapes requires sm_80 or higher.
.tf32 alternate floating point type mma operation with .m16n8k4 and .m16n8k8 shapes requires sm_80 or higher.
.u8/.s8 integer type mma operation with .m16n8k16 and .m16n8k32 shapes requires sm_80 or higher.
.u4/.s4 integer type mma operation with .m16n8k32 and .m16n8k64 shapes requires sm_80 or higher.
.b1 single-bit integer type mma operation with .m16n8k128 and .m16n8k256 shapes requires sm_80 or higher.
.f64 floating point type mma operation with .m16n8k4 , .m16n8k8 , and .m16n8k16 shapes require sm_90 or higher.
Warp-level matrix load instruction: ldmatrix  ldmatrix Collectively load one or more matrices from shared memory for mma instruction Syntax ldmatrix.sync.aligned.shape.num{.trans}{.ss}.type r, [p]; .shape = {.m8n8}; .num = {.x1, .x2, .x4}; .ss = {.shared{::cta}}; .type = {.b16}; Description Collectively load one or more matrices across all threads in a warp from the location indicated by the address operand p , from .shared state space into destination register r .
If no state space is provided, generic addressing is used, such that the address in p points into .shared space.
If the generic address doesn’t fall in .shared state space, then the behavior is undefined.
The mandatory .sync qualifier indicates that ldmatrix causes the executing thread to wait until all threads in the warp execute the same ldmatrix instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warp must execute the same ldmatrix instruction.
In conditionally executed code, an ldmatrix instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise the behavior is undefined.
The behavior of ldmatrix is undefined if all threads do not use the same qualifiers, or if any thread in the warp has exited.
The destination operand r is a brace-enclosed vector expression consisting of 1, 2, or 4 32-bit registers as per the value of .num .
The eight addresses required for each matrix are provided by eight threads, depending upon the value of .num as shown in the following table.
Addresses addr0–addr7 correspond to the rows of the first matrix, addresses addr8–addr15 correspond to the rows of the second matrix, and so on.
.num Threads 0–7 Threads 8–15 Threads 16–23 Threads 24–31 .x1 addr0–addr7 – – – .x2 addr0–addr7 addr8–addr15 – – .x4 addr0–addr7 addr8–addr15 addr16–addr23 addr24–addr31 Note For .target sm_75 or below, all threads must contain valid addresses.
For .num = .x1 and .num = .x2 , addresses contained in lower threads can be copied to higher threads to achieve the expected behavior.
Each thread in a warp loads fragments of a row, with thread 0 receiving the first fragment in its register r , and so on.
Figure 79 ldmatrix fragment layout  When .num = .x2 , the elements of the second matrix are loaded in the next destination register in each thread as per the layout in above table.
Similarly, when .num = .x4 , elements of the third and fourth matrices are loaded in the subsequent destination registers in each thread.
The ldmatrix instruction is treated as a weak memory operation in the Memory Consistency Model .
Examples   Load a single 8x8 matrix using 64-bit addressing .reg .b64 addr; .reg .b32 d; ldmatrix.sync.aligned.m8n8.x1.shared::cta.b16 {d}, [addr];   Load two 8x8 matrices in column-major format .reg .b64 addr; .reg .b32 d; ldmatrix.sync.aligned.m8n8.x2.trans.shared.b16 {d0, d1}, [addr];   Load four 8x8 matrices .reg .b64 addr; .reg .b32 d; ldmatrix.sync.aligned.m8n8.x4.b16 {d0, d1, d2, d3}, [addr]; 9.7.13.4.16.
Warp-level matrix store instruction: stmatrix  stmatrix Collectively store one or more matrices to shared memory.
Syntax stmatrix.sync.aligned.shape.num{.trans}{.ss}.type [p], r; .shape = {.m8n8}; .num = {.x1, .x2, .x4}; .ss = {.shared{::cta}}; .type = {.b16}; Description Collectively store one or more matrices across all threads in a warp to the location indicated by the address operand p , in .shared state space.
The mandatory .sync qualifier indicates that stmatrix causes the executing thread to wait until all threads in the warp execute the same stmatrix instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warp must execute the same stmatrix instruction.
In conditionally executed code, an stmatrix instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise the behavior is undefined.
The behavior of stmatrix is undefined if all threads do not use the same qualifiers, or if any thread in the warp has exited.
The source operand r is a brace-enclosed vector expression consisting of 1, 2, or 4 32-bit registers as per the value of .num .
.num Threads 0–7 Threads 8–15 Threads 16–23 Threads 24–31 .x1 addr0–addr7 – – – .x2 addr0–addr7 addr8–addr15 – – .x4 addr0–addr7 addr8–addr15 addr16–addr23 addr24–addr31 When storing 8x8 matrices, a group of four consecutive threads stores 16 bytes.
Each thread in a warp stores fragments of a row, with thread 0 storing the first fragment from its register r , and so on.
Figure 80 stmatrix fragment layout  When .num = .x2 , the elements of the second matrix are storedd from the next source register in each thread as per the layout in above table.
Similarly, when .num = .x4 , elements of the third and fourth matrices are stored from the subsequent source registers in each thread.
The stmatrix instruction is treated as a weak memory operation in the Memory Consistency Model .
Examples   Store a single 8x8 matrix using 64-bit addressing .reg .b64 addr; .reg .b32 r; stmatrix.sync.aligned.m8n8.x1.shared.b16 [addr], {r};   Store two 8x8 matrices in column-major format .reg .b64 addr; .reg .b32 r; stmatrix.sync.aligned.m8n8.x2.trans.shared::cta.b16 [addr], {r0, r1};   Store four 8x8 matrices .reg .b64 addr; .reg .b32 r; stmatrix.sync.aligned.m8n8.x4.b16 [addr], {r0, r1, r2, r3}; 9.7.13.4.17.
Warp-level matrix transpose instruction: movmatrix  movmatrix Transpose a matrix in registers across the warp.
Syntax movmatrix.sync.aligned.shape.trans.type d, a; .shape = {.m8n8}; .type = {.b16}; Description Move a row-major matrix across all threads in a warp, reading elements from source a , and writing the transposed elements to destination d .
The mandatory .sync qualifier indicates that movmatrix causes the executing thread to wait until all threads in the warp execute the same movmatrix instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warp must execute the same movmatrix instruction.
In conditionally executed code, a movmatrix instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise the behavior is undefined.
Operands a and d are 32-bit registers containing fragments of the input matrix and the resulting matrix respectively.
The mandatory qualifier .trans indicates that the resulting matrix in d is a transpose of the input matrix specified by a .
Each thread in a warp holds a fragment of a row of the input matrix, with thread 0 holding the first fragment in register a , and so on.
Figure 81 movmatrix source matrix fragment layout  Each thread in a warp holds a fragment of a column of the result matrix, with thread 0 holding the first fragment in register d , and so on.
A group of four threads holds an entire column of the result matrix as shown in Figure 82 .
Figure 82 movmatrix result matrix fragment layout  PTX ISA Notes Introduced in PTX ISA version 7.8.
Matrix multiply-accumulate operation using mma.sp instruction with sparse matrix A  This section describes warp-level mma.sp{::ordered_metadata} instruction with sparse matrix A.
This variant of the mma operation can be used when A is a structured sparse matrix with 50% zeros in each row distributed in a shape-specific granularity.
For an MxNxK sparse mma.sp{::ordered_metadata} operation, the MxK matrix A is packed into MxK/2 elements.
For each K-wide row of matrix A, 50% elements are zeros and the remaining K/2 non-zero elements are packed in the operand representing matrix A.
The mapping of these K/2 elements to the corresponding K-wide row is provided explicitly as metadata. 9.7.13.5.1. Sparse matrix storage  Granularity of sparse matrix A is defined as the ratio of the number of non-zero elements in a sub-chunk of the matrix row to the total number of elements in that sub-chunk where the size of the sub-chunk is shape-specific.
Values 0b0000 , 0b0101 , 0b1010 , 0b1111 are invalid values for metadata and will result in undefined behavior.
In a group of four consecutive threads, one or more threads store the metadata for the whole group depending upon the matrix shape.
Figure 83 shows an example of a 16x16 matrix A represented in sparse format and sparsity selector indicating which thread in a group of four consecutive threads stores the metadata.
Figure 83 Sparse MMA storage example  Granularities for different matrix shapes and data types are described below.
Sparse mma.sp{::ordered_metadata} with half-precision and .bf16 type For the .m16n8k16 and .m16n8k32 mma.sp{::ordered_metadata} operations, matrix A is structured sparse at a granularity of 2:4.
In other words, each chunk of four adjacent elements in a row of matrix A has two zeros and two non-zero elements.
Only the two non-zero elements are stored in the operand representing matrix A and their positions in the four-wide chunk in matrix A are indicated by two 2-bit indices in the metadata operand.
For mma.sp::ordered_metadata , 0b0100 , 0b1000 , 0b1001 , 0b1100 , 0b1101 , 0b1110 are the meaningful values of indices; any other values result in an undefined behavior.
 The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k16 : One thread within a group of four consecutive threads contributes the metadata for the entire group.
m16n8k32 : A thread-pair within a group of four consecutive threads contributes the sparsity metadata.
Hence, the sparsity selector must be either 0 (threads T0, T1) or 1 (threads T2, T3); any other value results in an undefined behavior.
Sparse mma.sp{::ordered_metadata} with .tf32 type When matrix A has .tf32 elements, matrix A is structured sparse at a granularity of 1:2.
In other words, each chunk of two adjacent elements in a row of matrix A has one zero and one non-zero element.
Only the non-zero elements are stored in the operand for matrix A and their positions in a two-wide chunk in matrix A are indicated by the 4-bit index in the metadata.
0b1110 and 0b0100 are the only meaningful index values; any other values result in an undefined behavior.
 The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k8 : One thread within a group of four consecutive threads contributes the metadata for the entire group.
m16n8k16 : A thread-pair within a group of four consecutive threads contributes the sparsity metadata.
Sparse mma.sp{::ordered_metadata} with integer type When matrices A and B have .u8 / .s8 elements, matrix A is structured sparse at a granularity of 2:4.
In other words, each chunk of four adjacent elements in a row of matrix A have two zeroes and two non-zero elements.
Only the two non-zero elements are stored in sparse matrix and their positions in the four-wide chunk are indicated by two 2-bit indices in the metadata.
 when matrices A and B have .u4 / .s4 elements, matrix A is pair-wise structured sparse at a granularity of 4:8.
In other words, each chunk of eight adjacent elements in a row of matrix A has four zeroes and four non-zero values.
Further, the zero and non-zero values are clustered in sub-chunks of two elements each within the eight-wide chunk.
i.e., each two-wide sub-chunk within the eight-wide chunk must be all zeroes or all non-zeros.
Only the four non-zero values are stored in sparse matrix and the positions of the two two-wide sub-chunks with non-zero values in the eight-wide chunk of a row of matrix A are indicated by two 2-bit indices in the metadata.
 The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k32 with .u8 / .s8 type and m16n8k64 with .u4 / .s4 type: A thread-pair within a group of four consecutive threads contributes the sparsity metadata.
m16n8k64 with .u8 / .s8 type and m16n8k128 with .u4 / .s4 type: All threads within a group of four consecutive threads contribute the sparsity metadata.
Sparse mma.sp{::ordered_metadata} with .e4m3 / .e5m2 type When matrices A and B have .e4m3 / .e5m2 elements, matrix A is structured sparse at a granularity of 2:4.
0b0100 , 0b1000 , 0b1001 , 0b1100 , 0b1101 , 0b1110 are the meaningful values of indices; any other values result in an undefined behavior.
 The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k64 : All threads within a group of four consecutive threads contribute the sparsity metadata. 9.7.13.5.2. Matrix fragments for multiply-accumulate operation with sparse matrix A  In this section we describe how the contents of thread registers are associated with fragments of various matrices and the sparsity metadata.
The following conventions are used throughout this section: For matrix A, only the layout of a fragment is described in terms of register vector sizes and their association with the matrix data.
For matrix B, when the combination of matrix dimension and the supported data type is not already covered in Matrix multiply-accumulate operation using mma instruction , a pictorial representation of matrix fragments is provided.
For matrices C and D, since the matrix dimension - data type combination is the same for all supported shapes, and is already covered in Matrix multiply-accumulate operation using mma instruction , the pictorial representations of matrix fragments are not included in this section.
For the metadata operand, pictorial representations of the association between indices of the elements of matrix A and the contents of the metadata operand are included.
Tk: [m..n] present in cell [x][y..z] indicates that bits m through n (with m being higher) in the metadata operand of thread with %laneid=k contains the indices of the non-zero elements from the chunk [x][y]..[x][z] of matrix A. 9.7.13.5.2.1. Matrix Fragments for sparse mma.m16n8k16 with .f16 and .bf16 types  A warp executing sparse mma.m16n8k16 with .f16 / .bf16 floating point type will compute an MMA operation of shape .m16n8k16 .
Multiplicand A: .atype Fragment Elements .f16 / .bf16 A vector expression containing two .b32 registers, with each register containing two non-zero .f16 / .bf16 elements out of 4 consecutive elements from matrix A.
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a1 groupID + 8 for a2 and a3 col = [ firstcol ...
lastcol ]   As per the mapping of non-zero elements   as described in Sparse matrix storage Where firstcol = threadID_in_group * 4 lastcol = firstcol + 3 Matrix fragments for multiplicand B and accumulators C and D are the same as in case of Matrix Fragments for mma.m16n8k16 with floating point type for .f16 / .b16 formats.
Metadata: A .b32 register containing 16 2-bit vectors each storing the index of a non-zero element of a 4-wide chunk of matrix A as shown in Figure 90 .
Figure 90 Sparse MMA .m16n8k16 metadata layout for .f16 / .bf16 type.  9.7.13.5.2.2. Matrix Fragments for sparse mma.m16n8k32 with .f16 and .bf16 types  A warp executing sparse mma.m16n8k32 with .f16 / .bf16 floating point type will compute an MMA operation of shape .m16n8k32 .
Multiplicand A: .atype Fragment Elements .f16 / .bf16 A vector expression containing four .b32 registers, with each register containing two non-zero .f16 / .bf16 elements out of 4 consecutive elements from matrix A.
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 4 lastcol = firstcol + 3 Multiplicand B: .atype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing four .b32 registers, each containing two .f16 / .bf16 elements from matrix B.
b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 92 .
 Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for mma.m16n8k16 with floating point type for .f16 / .b16 formats.
Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing the indices of two non-zero element from a 4-wide chunk of matrix A as shown in Figure 93 .
Figure 93 Sparse MMA .m16n8k32 metadata layout for .f16 / .bf16 type.  9.7.13.5.2.3. Matrix Fragments for sparse mma.m16n8k16 with .tf32 floating point type  A warp executing sparse mma.m16n8k16 with .tf32 floating point type will compute an MMA operation of shape .m16n8k16 .
Multiplicand A: .atype Fragment Elements .tf32 A vector expression containing four .b32 registers, with each register containing one non-zero .tf32 element out of 2 consecutive elements from matrix A.
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a2 groupID + 8 for a1 and a3 col = [ firstcol ...
lastcol ]   As per the mapping of non-zero elements   as described in Sparse matrix storage Where firstcol = threadID_in_group * 2 for a0 and a1 ( threadID_in_group * 2 ) + 8 for a2 and a3 lastcol = firstcol + 1 Multiplicand B: .atype Fragment Elements (low to high) .tf32 A vector expression containing four .b32 registers, each containing four .tf32 elements from matrix B.
b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 95 .
 Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for mma.m16n8k16 with floating point type .
Metadata: A .b32 register containing 8 4-bit vectors each storing the index of a non-zero element of a 2-wide chunk of matrix A as shown in Figure 96 .
Figure 96 Sparse MMA .m16n8k16 metadata layout for .tf32 type.  9.7.13.5.2.4. Matrix Fragments for sparse mma.m16n8k8 with .tf32 floating point type  A warp executing sparse mma.m16n8k8 with .tf32 floating point type will compute an MMA operation of shape .m16n8k8 .
Multiplicand A: .atype Fragment Elements .tf32 A vector expression containing two .b32 registers, each containing one non-zero .tf32 element out of 2 consecutive elements from matrix A.
 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 groupID + 8 for a1 col = [ firstcol ...
lastcol ]   As per the mapping of non-zero elements   as described in Sparse matrix storage Where firstcol = threadID_in_group * 2 lastcol = firstcol + 1 Matrix fragments for multiplicand B and accumulators C and D are the same as in case of Matrix Fragments for mma.m16n8k8 for .tf32 format.
Metadata: A .b32 register containing 8 4-bit vectors each storing the index of a non-zero element of a 2-wide chunk of matrix A as shown in Figure 98 .
Figure 98 Sparse MMA .m16n8k8 metadata layout for .tf32 type.  9.7.13.5.2.5. Matrix Fragments for sparse mma.m16n8k32 with .u8/.s8 integer type  A warp executing sparse mma.m16n8k32 with .u8 / .s8 integer type will compute an MMA operation of shape .m16n8k32 .
Multiplicand A: .atype Fragment Elements .u8 / .s8 A vector expression containing two .b32 registers, with each register containing four non-zero .u8 / .s8 elements out of 8 consecutive elements from matrix A.
 groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 > 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 8 lastcol = firstcol + 7 Multiplicand B: .btype Fragment Elements (low to high) .u8 / .s8 A vector expression containing four .b32 registers, each containing four .u8 / .s8 elements from matrix B.
b0, b1, b2, b3, …, b15 .e4m3 / .e5m2 A vector expression containing four .b32 registers, each containing four .e4m3 / .e5m2 elements from matrix B.
The layout of the fragments held by different threads is shown in Figure 103 , Figure 104 , Figure 105 and Figure 106 .
Figure 103 Sparse MMA .m16n8k64 fragment layout for rows 0–15 of matrix B with .u8 / .s8 / .e4m3 / .e5m2 type.
 Figure 104 Sparse MMA .m16n8k64 fragment layout for rows 16–31 of matrix B with .u8 / .s8 / .e4m3 / .e5m2 type.
 Figure 105 Sparse MMA .m16n8k64 fragment layout for rows 32–47 of matrix B with .u8 / .s8 / .e4m3 / .e5m2 type.
 Figure 106 Sparse MMA .m16n8k64 fragment layout for rows 48–63 of matrix B with .u8 / .s8 / .e4m3 / .e5m2 type.
 Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for mma.m16n8k16 with integer type .
Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing the indices of two non-zero elements from a 4-wide chunk of matrix A as shown in Figure 107 and Figure 108 .
Figure 107 Sparse MMA .m16n8k64 metadata layout for columns 0–31 for .u8 / .s8 / .e4m3 / .e5m2 type.
 Figure 108 Sparse MMA .m16n8k64 metadata layout for columns 32–63 for .u8 / .s8 / .e4m3 / .e5m2 type.  9.7.13.5.2.7. Matrix Fragments for sparse mma.m16n8k64 with .u4/.s4 integer type  A warp executing sparse mma.m16n8k64 with .u4 / .s4 integer type will compute an MMA operation of shape .m16n8k64 .
Multiplicand A: .atype Fragment Elements .u4 / .s4 A vector expression containing two .b32 registers, with each register containing eight non-zero .u4 / .s4 elements out of 16 consecutive elements from matrix A.
 groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 > 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 16 lastcol = firstcol + 15 Multiplicand B: .atype Fragment Elements (low to high) .u4 / .s4 A vector expression containing four .b32 registers, each containing eight .u4 / .s4 elements from matrix B.
b0, b1, b2, b3, …, b31 The layout of the fragments held by different threads is shown in Figure 113 , Figure 114 , Figure 115 , Figure 116 .
Figure 113 Sparse MMA .m16n8k128 fragment layout for rows 0–31 of matrix B with .u4 / .s4 type.
 Figure 114 Sparse MMA .m16n8k128 fragment layout for rows 31–63 of matrix B with .u4 / .s4 type.
 Figure 115 Sparse MMA .m16n8k128 fragment layout for rows 64–95 of matrix B with .u4 / .s4 type.
 Figure 116 Sparse MMA .m16n8k128 fragment layout for rows 96–127 of matrix B with .u4 / .s4 type.
 Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for mma.m16n8k64 .
Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing the indices of four non-zero elements from a 8-wide chunk of matrix A as shown in Figure 117 and Figure 118 .
A warp executing mma.sp.sync/mma.sp::ordered_metadata.sync instruction compute a single matrix mutliply and accumulate operation.
Operands a and b represent two multiplicand matrices A and B, while c and d represent the accumulator and destination matrices, distributed across the threads in warp.
Matrix A is structured sparse as described in Sparse matrix storage Operands e and f represent sparsity metadata and sparsity selector respectively.
Operand e is a 32-bit integer and operand f is a 32-bit integer constant with values in the range 0..3 Instruction mma.sp::ordered_metadata requires the indices in the sparsity metadata to be sorted in an increasing order starting from LSB, otherwise behavior is undefined.
The registers in each thread hold a fragment of matrix as described in Matrix fragments for multiply-accumulate operation with sparse matrix A .
In case of shapes .m16n8k16 and .m16n8k32 , .dtype must be the same as .ctype Precision and rounding : .f16 floating point operations : Element-wise multiplication of matrix A and B is performed with at least single precision.
Integer operations : The integer mma.sp/mma.sp::ordered_metadata operation is performed with .s32 accumulators.
The mandatory .sync qualifier indicates that mma.sp/mma.sp::ordered_metadata instruction causes the executing thread to wait until all threads in the warp execute the same mma.sp/mma.sp::ordered_metadata instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warp must execute the same mma.sp/mma.sp::ordered_metadata instruction.
In conditionally executed code, a mma.sp/mma.sp::ordered_metadata instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined.
The behavior of mma.sp/mma.sp::ordered_metadata instruction is undefined if all threads in the same warp do not use the same qualifiers, or if any thread in the warp has exited.
Notes mma.sp instruction may have substantially reduced performance on some target architectures.
Support for .e4m3 and .e5m2 alternate floating point type mma operation introduced in PTX ISA version 8.4.
Asynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions  The warpgroup level matrix multiply and accumulate operation has either of the following forms, where matrix D is called accumulator: D = A * B + D D = A * B , where the input from accumulator D is disabled.
The wgmma instructions perform warpgroup level matrix multiply-and-accumulate operation by having all threads in a warpgroup collectively perform the following actions: Load matrices A, B and D into registers or into shared memory.
Perform the following fence operations: wgmma.fence operations to indicate that the register/shared-memory across the warpgroup have been written into.
fence.proxy.async operation to make the generic proxy operations visible to the async proxy.
Issue the asynchronous matrix multiply and accumulate operations using the wgmma.mma_async operation on the input matrices.
Create a wgmma-group and commit all the prior outstanding wgmma.mma_async operations into the group, by using wgmma.commit_group operation.
Once the wgmma-group completes, all the wgmma.mma_async operations have been performed and completed. 9.7.14.1. Warpgroup  A warpgroup is a set of four contiguous warps such that the warp-rank of the first warp is a multiple of 4.
warp-rank of a warp is defined as: (%tid.x + %tid.y * %ntid.x + %tid.z * %ntid.x * %ntid.y) / 32 9.7.14.2.
Matrix Shape  The matrix multiply and accumulate operations support a limited set of shapes for the operand matrices A, B and D.
The shapes of all three matrix operands are collectively described by the tuple MxNxK , where A is an MxK matrix, B is a KxN matrix, while D is a MxN matrix.
For integer matrix multiply and accumulate operation, both multiplicand matrices (A and B) must have elements of the same data-type, e.g.
Data-type Multiplicands (A or B) Accumulator (D) Integer both .u8 or both .s8 .s32 Floating Point .f16 .f16 , .f32 Alternate floating Point .bf16 .f32 Alternate floating Point .tf32 .f32 Alternate floating Point .e4m3 , .e5m2 .f16 , .f32 Single-bit integer .b1 .s32 9.7.14.4.
Async Proxy  The wgmma.mma_async operations are performed in the asynchronous proxy (or async proxy).
For the async proxy, fence.proxy.async should be used to synchronize memory between generic proxy and the async proxy.
The completion of a wgmma.mma_async operation is followed by an implicit generic-async proxy fence.
So the result of the asynchronous operation is made visible to the generic proxy as soon as its completion is observed.
wgmma.commit_group and wgmma.wait_group operations must be used to wait for the completion of the wgmma.mma_async instructions. 9.7.14.5. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using wgmma.mma_async instruction  This section describes warpgroup level wgmma.mma_async instruction and the organization of various matrices involved in this instruction.
9.7.14.5.1. Register Fragments and Shared Memory Matrix Layouts  The input matrix A of the warpgroup wide MMA operations can be either in registers or in the shared memory.
This section describes the layouts of register fragments and shared memory expected by the warpgroup MMA instructions.
When the matrices are in shared memory, their starting addresses must be aligned to 16 bytes. 9.7.14.5.1.1. Register Fragments  This section describes the organization of various matrices located in register operands of the wgmma.mma_async instruction.
9.7.14.5.1.1.1. Matrix Fragments for wgmma.mma_async.m64nNk16  A warpgroup executing wgmma.mma_async.m64nNk16 will compute an MMA operation of shape .m64nNk16 where N is a valid n dimension as listed in Matrix Shape .
Elements of the matrix are distributed across the threads in a warpgroup so each thread of the warpgroup holds a fragment of the matrix.
Multiplicand A in registers: .atype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing four .f16x2 registers, with each register containing two .f16 / .bf16 elements from matrix A.
a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 119 .
 Accumulator D: .dtype Fragment Elements (low to high) .f16 A vector expression containing N/4 number of .f16x2 registers, with each register containing two .f16 elements from matrix D.
d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N = 8*i where i = {1, 2, ...
Figure 120 WGMMA .m64nNk16 register fragment layout for accumulator matrix D.  9.7.14.5.1.1.2. Matrix Fragments for wgmma.mma_async.m64nNk8  A warpgroup executing wgmma.mma_async.m64nNk8 will compute an MMA operation of shape .m64nNk8 where N is a valid n dimension as listed in Matrix Shape .
Multiplicand A in registers: .atype Fragment Elements (low to high) .tf32 A vector expression containing four .b32 registers containing four .tf32 elements from matrix A.
a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 121 .
 Accumulator D: .dtype Fragment Elements (low to high) .f32 A vector expression containing N/2 number of .f32 registers.
d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N = 8*i where i = {1, 2, ...
Figure 122 WGMMA .m64nNk8 register fragment layout for accumulator matrix D.  9.7.14.5.1.1.3. Matrix Fragments for wgmma.mma_async.m64nNk32  A warpgroup executing wgmma.mma_async.m64nNk32 will compute an MMA operation of shape .m64nNk32 where N is a valid n dimension as listed in Matrix Shape .
Multiplicand A in registers: .atype Fragment Elements (low to high) .s8 / .u8 A vector expression containing four .b32 registers, with each register containing four .u8 / .s8 elements from matrix A.
a0, a1, a2, a3, … , a14, a15 .e4m3 / .e5m2 A vector expression containing four .b32 registers, with each register containing four .e4m3 / .e5m2 elements from matrix A.
 Accumulator D: .dtype Fragment Elements (low to high) Miscellaneous Information .s32 A vector expression containing N/2 number of .s32 registers.
d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N depends on .dtype, as described in the next column.
N = 8*i where i = {1, 2, 3, 4} = 16*i where i = {3, 4, ..., 15, 16} .f32 A vector expression containing N/2 number of .f32 registers.
, 32} .f16 A vector expression containing N/4 number of .f16x2 registers, with each register containing two .f16 elements from matrix D.
Figure 124 WGMMA .m64nNk32 register fragment layout for accumulator matrix D.  9.7.14.5.1.1.4. Matrix Fragments for wgmma.mma_async.m64nNk256  A warpgroup executing wgmma.mma_async.m64nNk256 will compute an MMA operation of shape .m64nNk256 where N is a valid n dimension as listed in Matrix Shape .
Multiplicand A in registers: .atype Fragment Elements (low to high) .b1 A vector expression containing four .b32 registers, with each register containing thirty two .b1 element from matrix A.
a0, a1, a2, …, a127 The layout of the fragments held by different threads is shown in Figure 125 .
 Accumulator D: .dtype Fragment Elements (low to high) .s32 A vector expression containing N/2 number of .s32 registers.
d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N = 8*i where i = {1, 2, 3, 4} = 16*i where i = {3, 4, ..., 15, 16} The layout of the fragments held by different threads is shown in Figure 126 .
Figure 126 WGMMA .m64nNk256 register fragment layout for accumulator matrix D.  9.7.14.5.1.2. Shared Memory Matrix Layout  Matrices in shared memory are organized into a number of smaller matrices called core matrices.
Matrix A is made up of 8x2 core matrices and Matrix B is made up of 2x(N/8) core matrices.
This section describes the layout of the core matrices for each shape. 9.7.14.5.1.2.1. Shared Memory Layout for wgmma.mma_async.m64nNk16  Core matrices of A and B are as follows: Core matrix Matrix description Matrix size A Each row is made up of eight .f16 / .bf16 elements.
Figure 127 WGMMA .m64nNk16 core matrices for A and B  Layout of core matrices of A is shown in Figure 128 .
Figure 128 WGMMA .m64nNk16 core matrix layout for A  Layout of core matrices of B is shown in Figure 129 .
Shared Memory Layout for wgmma.mma_async.m64nNk8  Core matrices of A and B are as follows: Core matrix Matrix description Matrix size A Each row is made up of four .tf32 elements.
Figure 130 WGMMA .m64nNk8 core matrices for A and B  Layout of core matrices of A is shown in Figure 131 .
Figure 131 WGMMA .m64nNk8 core matrix layout for A  Layout of core matrices of B is shown in Figure 132 .
Shared Memory Layout for wgmma.mma_async.m64nNk32  Core matrices of A and B are as follows: .atype/ .btype Core matrix Matrix description Matrix size .s8 / .u8 A Each row is made up of sixteen .s8 / .u8 elements.
Figure 133 WGMMA .m64nNk32 core matrices for A and B  Layout of core matrices of A is shown in Figure 134 .
Figure 134 WGMMA .m64nNk32 core matrix layout for A  Layout of core matrices of B is shown in Figure 135 .
Shared Memory Layout for wgmma.mma_async.m64nNk256  Core matrices of A and B are as follows: Core matrix Matrix description Matrix size A Each row is made up of 256 .b1 elements.
Figure 136 WGMMA .m64nNk256 core matrices for A and B  Layout of core matrices of A is shown in Figure 137 .
Figure 137 WGMMA .m64nNk256 core matrix layout for A  Layout of core matrices of B is shown in Figure 138 .
Strides  Leading dimension byte offset of matrix A or B is the distance, in bytes, between two adjacent core matrices in the K dimension.
Stride dimension byte offset of matrix A or B is the distance, in bytes, between two adjacent core matrices in the M or N dimension.
Figure 139 and Figure 140 show the leading dimension byte offset and the stride dimension byte offsets for A and B matrices.
Matrix A: Figure 139 WGMMA stride and leading dimension byte offset for matrix A  Matrix B: Figure 140 WGMMA stride and leading dimension byte offset for matrix B  Leading dimension byte offset and stride dimension byte offset must be specified in the matrix descriptor as described in Matrix Descriptor Format . 9.7.14.5.1.2.6. Swizzling Modes  The core matrices can be swizzled in the shared memory by specifying one of the following swizzling modes: No swizzling: All the elements of the entire core matrix are adjacent to each other and there is no swizzling.
Figure 141 illustrates this: Figure 141 WGMMA core matrices with no swizzling  32-Byte swizzling: A group of two adjacent core matrices are swizzled as shown in Figure 142 .
Figure 142 WGMMA core matrices with 32-byte swizzling  64-Byte swizzling: A group of four adjacent core matrices are swizzled as shown in Figure 143 .
Figure 143 WGMMA core matrices with 64-byte swizzling  128-Byte swizzling: A group of eight adjacent core matrices are swizzled as shown in Figure 144 .
Matrix Descriptor Format  Matrix descriptor specifies the properties of the matrix in shared memory that is a multiplicand in the matrix multiply and accumulate operation.
It is a 64-bit value contained in a register with the following layout: Bit-field Size in bits Description 13–0 14 matrix-descriptor-encode(Matrix start address) 29–16 14 matrix-descriptor-encode(Leading dimension byte offset) 45–32 14 matrix-descriptor-encode(Stride dimension byte offset) 51–49 3 Matrix base offset.
63–62 2 Specifies the swizzling mode to be used: 0: No swizzle 1: 128-Byte swizzle 2: 64-Byte swizzle 3: 32-Byte swizzle where matrix-descriptor-encode(x) = (x & 0x3FFFF) >> 0x4 The value of base offset is 0 when the repeating pattern of the specified swizzling mode starts as per the below table: Swizzling mode Starting address of the repeating pattern 128-Byte swizzle 1024-Byte boundary 64-Byte swizzle 512-Byte boundary 32-Byte swizzle 256-Byte boundary Otherwise, the base offset must be a non-zero value, computed using the following formula: base offset = (pattern start addr >> 0x7) & 0x7 9.7.14.5.2.
The operation of the form D = A*B is issued when the input predicate argument scale-d is false.
wgmma.fence instruction must be used to fence the register accesses of wgmma.mma_async instruction from their prior accesses.
wgmma.commit_group and wgmma.wait_group operations must be used to wait for the completion of the asynchronous matrix multiply and accumulate operations before the results are accessed.
Register operand d represents the accumulator matrix as well as the destination matrix, distributed across the participating threads.
Register operand a represents the multiplicand matrix A in register distributed across the participating threads.
The 64-bit register operands a-desc and b-desc are the matrix descriptors which represent the multiplicand matrices A and B in shared memory respectively.
For certain floating point variants, the input matrices A and B can be transposed by specifying the value 1 for the immediate integer arguments imm-trans-a and imm-trans-b respectively.
The transpose operation is only supported for the wgmma.mma_async variants with .f16 / .bf16 types on matrices accessed from shared memory using matrix descriptors.
For the floating point variants of the wgmma.mma_async operation, each element of the input matrices A and B can be negated by specifying the value -1 for operands imm-scale-a and imm-scale-b respectively.
The qualifiers .dtype , .atype and .btype indicate the data type of the elements in matrices D, A and B respectively.
.atype and .btype must be the same for all floating point wgmma.mma_async variants except for the FP8 floating point variants.
The sizes of individual data elements of matrices A and B in alternate floating point variants of the wgmma.mma_async operation are as follows: Matrices A and B have 8-bit data elements when .atype / .btype is .e4m3 / .e5m2 .
Precision and rounding: Floating point operations: Element-wise multiplication of matrix A and B is performed with at least single precision.
When .dtype is .f32 , accumulation of the intermediate values is performed with at least single precision.
.bf16 and .tf32 floating point operations: Element-wise multiplication of matrix A and B is performed with specified precision.
wgmma.mma_async operation involving type .tf32 will truncate lower 13 bits of the 32-bit input data before multiplication is issued.
Integer operations: The integer wgmma.mma_async operation is performed with .s32 accumulators.
The mandatory .sync qualifier indicates that wgmma.mma_async instruction causes the executing thread to wait until all threads in the warp execute the same wgmma.mma_async instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the same wgmma.mma_async instruction.
In conditionally executed code, a wgmma.mma_async instruction should only be used if it is known that all threads in the warpgroup evaluate the condition identically, otherwise behavior is undefined.
Asynchronous Warpgroup Level Multiply-and-Accumulate Operation using wgmma.mma_async.sp instruction  This section describes warp-level wgmma.mma_async.sp instruction with sparse matrix A.
This variant of the wgmma.mma_async operation can be used when A is a structured sparse matrix with 50% zeros in each row distributed in a shape-specific granularity.
For an MxNxK sparse wgmma.mma_async.sp operation, the MxK matrix A is packed into MxK/2 elements. 9.7.14.6.1. For example, in a 64x32 matrix A used in floating point wgmma.mma_async operations, sparsity is expected to be at 2:4 granularity, i.e.
Matrix A and its corresponding input operand to the sparse wgmma is similar to the diagram shown in Figure 83 , with an appropriate matrix size.
Sparse wgmma.mma_async.sp with half-precision and .bf16 type For .f16 and .bf16 types, for all supported 64xNx32 shapes, matrix A is structured sparse at a granularity of 2:4.
Only the two non-zero elements are stored in matrix A and their positions in the four-wide chunk in Matrix A are indicated by two 2-bits indices in the metadata operand.
 The sparsity selector indicates a thread-pair within a group of four consecutive threads which contributes the sparsity metadata.
Sparse wgmma.mma_async.sp with .tf32 type For .tf32 type, for all supported 64xNx16 shapes, matrix A is structured sparse at a granularity of 1:2.
In other words, each chunk of two adjacent elements in a row of matrix A have one zero and one non-zero element.
Only the non-zero element is stored in operand for matrix A and the 4-bit index in the metadata indicates the position of the non-zero element in the two-wide chunk.
0b1110 and 0b0100 are the only meaningful values of the index, the remaining values result in an undefined behavior.
Sparse wgmma.mma_async.sp with .e4m3 and .e5m2 floating point type For .e4m3 and .e5m2 types, for all supported 64xNx64 shapes, matrix A is structured sparse at a granularity of 2:4.
 All threads contribute the sparsity metadata and the sparsity selector must be 0; any other value results in an undefined behavior.
Sparse wgmma.mma_async.sp with integer type For the integer type, for all supported 64xNx64 shapes, matrix A is structured sparse at a granularity of 2:4.
Only the two non-zero elements are stored in matrix A and two 2-bit indices in the metadata indicate the position of these two non-zero elements in the four-wide chunk.
Figure 148 Sparse WGMMA metadata example for .u8 / .s8 type. 9.7.14.6.2. Matrix fragments for warpgroup-level multiply-accumulate operation with sparse matrix A  In this section we describe how the contents of thread registers are associated with fragments of A matrix and the sparsity metadata.
The following table shows the assignment of warps to rows of matrix A: Warp Sparsity information for rows of matrix A %warpid % 4 = 3 48-63 %warpid % 4 = 2 32-47 %warpid % 4 = 1 16-31 %warpid % 4 = 0 0-15 The following conventions are used throughout this section: For matrix A, only the layout of a fragment is described in terms of register vector sizes and their association with the matrix data.
For matrix D, since the matrix dimension - data type combination is the same for all supported shapes, and is already covered in Matrix multiply-accumulate operation using wgmma instruction , the pictorial representations of matrix fragments are not included in this section. 9.7.14.6.2.1. Matrix Fragments for sparse wgmma.mma_async.m64nNk32  A warpgroup executing sparse wgmma.mma_async.m64nNk32 will compute an MMA operation of shape .m64nNk32 where N is a valid n dimension as listed in Matrix shape .
Multiplicand A, from shared memory is documented in Shared Memory Layout for wgmma.mma_async.m64nNk32 .
Multiplicand A, from registers: .atype Fragments Elements .f16 / .bf16 A vector expression containing four .b32 registers, with each register containing two non-zero .f16 / .bf16 elements out of 4 consecutive elements from matrix A.
Non-zero elements: a0, a1, a2, a3, a4, a5, a6, a7 Mapping of the non-zero elements is as described in Sparse matrix storage The layout of the fragments held by different threads is shown in Figure 149 .
 Accumulator D: Matrix fragments for accumulator D are the same as in case of Matrix Fragments for wgmma.m64nNk32 with floating point type for the same .dtype format.
Multiplicand B: Shared memory layout for Matrix B is documented in Shared Memory Layout for wgmma.mma_async.m64nNk32 .
Metadata operand is a .b32 register containing 16 2-bit vectors each storing the index of a non-zero element of a 4-wide chunk of matrix A.
Figure 150 Sparse WGMMA .m64nNk32 metadata layout for .f16 / .bf16 type.  9.7.14.6.2.2. Matrix Fragments for sparse wgmma.mma_async.m64nNk16  A warpgroup executing sparse wgmma.mma_async.m64nNk16 will compute an MMA operation of shape .m64nNk16 where N is a valid n dimension as listed in Matrix shape .
Multiplicand A, from shared memory is documented in Shared Memory Layout for wgmma.mma_async.m64nNk16 .
Multiplicand A, from registers: .atype Fragments Elements .tf32 A vector expression containing four .b32 registers, containing four non-zero .tf32 elements out of eight consecutive elements from matrix A.
Non-zero elements: a0, a1, a2, a3 Mapping of the non-zero elements is as described in Sparse matrix storage The layout of the fragments held by different threads is shown in Figure 151 .
 Accumulator D: Matrix fragments for accumulator D are the same as in case of Matrix Fragments for wgmma.m64nNk8 with floating point type for the same .dtype format.
Multiplicand B: Shared memory layout for Matrix B is documented in Shared Memory Layout for wgmma.mma_async.m64nNk16 .
Metadata operand is a .b32 register containing eight 4-bit vectors each storing the index of a non-zero element of a 2-wide chunk of matrix A.
Figure 152 Sparse WGMMA .m64nNk16 metadata layout for .tf32 type.  9.7.14.6.2.3. Matrix Fragments for sparse wgmma.mma_async.m64nNk64  A warpgroup executing sparse wgmma.mma_async.m64nNk64 will compute an MMA operation of shape .m64nNk64 where N is a valid n dimension as listed in Matrix shape .
Multiplicand A, from shared memory is documented in Shared Memory Layout for wgmma.mma_async.m64nNk64 .
Multiplicand A, from registers: .atype Fragments Elements .e4m3 / .e5m2 A vector expression containing four .b32 registers, with each register containing four non-zero .e4m3 / .e5m2 elements out of eight consecutive elements from matrix A.
Non-zero elements: a0, a1, a2, … , a15 Mapping of the non-zero elements is as described in Sparse matrix storage .s8 / .u8 A vector expression containing four .b32 registers, with each register containing four non-zero .s8 / .u8 elements out of eight consecutive elements from matrix A.
Figure 153 Sparse WGMMA .m64nNk64 fragment layout for matrix A with .e4m3 / .e5m2 / .s8 / .u8 type.
Multiplicand B: Shared memory layout for Matrix B is documented in Shared Memory Layout for wgmma.mma_async.m64nNk64 .
Metadata operand is a .b32 register containing 16 4-bit vectors each storing the indices of two non-zero elements of a 4-wide chunk of matrix A.
Figure 154 shows the mapping of the metadata bits to the elements of columns 0–31 of matrix A.
Figure 154 Sparse WGMMA .m64nNk64 metadata layout for .e4m3 / .e5m2 / .s8 / .u8 type for columns 0–31  Figure 155 shows the mapping of the metadata bits to the elements of columns 32–63 of matrix A.
Figure 155 Sparse WGMMA .m64nNk64 metadata layout for .e4m3 / .e5m2 / .s8 / .u8 type for columns 32–63  9.7.14.6.3.
Matrix A is made up of 8x2 packed core matrices and Matrix B is made up of 4x (N/8) core matrices. 9.7.14.6.3.1. Shared Memory Layout for wgmma.mma_async.sp.m64nNk32  Core matrices of A and B are as follows: Core matrix Matrix Description Matrix size A Each row is made up of sixteen .f16 / .bf16 elements, with two non-zero elements out of four consecutive elements.
Figure 156 Sparse WGMMA .m64nNk32 core matrices for A and B  Layout of core matrices of A is shown in Figure 157 .
Figure 157 Sparse WGMMA .m64nNk32 core matrix layout for A  Layout of core matrices of B is shown in Figure 158 .
Shared Memory Layout for wgmma.mma_async.sp.m64nNk16  Core matrices of A and B are as follows: Core matrix Matrix Description Matrix size A Each row is made up of eight .tf32 elements with a non-zero element out of two consecutive elements.
Figure 159 Sparse WGMMA .m64nNk16 core matrices for A and B  Layout of core matrices of A is shown in Figure 160 .
Figure 160 Sparse WGMMA .m64nNk16 core matrix layout for A  Layout of core matrices of B is shown in Figure 161 .
Shared Memory Layout for wgmma.mma_async.sp.m64nNk64  Core matrices of A and B are as follows: Core matrix Matrix Description Matrix size A Each row is made up of thirty-two .e4m3 / .e5m2 elements, with two non-zero elements out of four consecutive elements.
Figure 162 Sparse WGMMA .m64nNk64 core matrices for A and B  Layout of core matrices of A is shown in Figure 163 .
Figure 163 Sparse WGMMA .m64nNk64 core matrix layout for A  Layout of core matrices of B is shown in Figure 164 .
The matrix A is stored in the packed format Mx(K/2) as described in Matrix multiply-accumulate operation using wgmma.mma_async.sp instruction with sparse matrix A .
Operands sp-meta and sp-sel represent sparsity metadata and sparsity selector respectively.
Operand sp-meta is a 32-bit integer and operand sp-sel is a 32-bit integer constant with values in the range 0..3.
The valid values of sp-meta and sp-sel for each shape is specified in Matrix multiply-accumulate operation using wgmma.mma_async.sp instruction with sparse matrix A and are summarized here : Matrix shape .atype Valid values of sp-meta Valid values of sp-sel .m64nNk16 .tf32 0b1110 , 0b0100 0 (threads T0, T1) or 1 (threads T2, T3) .m64nNk32 .f16 / .bf16 0b00, 0b01, 0b10, 0b11 0 (threads T0, T1) or 1 (threads T2, T3) .m64nNk64 .e4m3 / .e5m2 / .s8 / .u8 0b00, 0b01, 0b10, 0b11 0 (all threads contribute) Matrices A and B are stored in row-major and column-major format respectively.
Examples of integer type wgmma.fence.sync.aligned; wgmma.mma_async.sp.sync.aligned.m64n8k64.s32.u8.u8 {s32d0, s32d1, s32d2, s32d3}, descA, descB, spMeta, 0, scaleD; wgmma.mma_async.sp.sync.aligned.m64n8k64.s32.s8.u8 {s32d0, s32d1, s32d2, s32d3}, descA, descB, spMeta, 0, scaleD; wgmma.commit_group.sync.aligned; wgmma.wait_group.sync.aligned 0; 9.7.14.7.
Asynchronous wgmma Proxy Operations  This section describes warpgroup level wgmma.fence , wgmma.commit_group and wgmma.wait_group instructions. 9.7.14.7.1. Asynchronous Multiply-and-Accumulate Instruction: wgmma.fence  wgmma.fence Enforce an ordering of register accesses between wgmma.mma_async and other operations.
Syntax wgmma.fence.sync.aligned; Description wgmma.fence instruction establishes an ordering between prior accesses to any warpgroup registers and subsequent accesses to the same registers by a wgmma.mma_async instruction.
Only the accumulator register and the input registers containing the fragments of matrix A require this ordering.
The wgmma.fence instruction must be issued by all warps of the warpgroup at the following locations: Before the first wgmma.mma_async operation in a warpgroup.
Between a register access by a thread in the warpgroup and any wgmma.mma_async instruction that accesses the same registers, either as accumulator or input register containing fragments of matrix A, except when these are accumulator register accesses across multiple wgmma.mma_async instructions of the same shape.
An async proxy fence must be used to establish an ordering between prior writes to shared memory matrices and subsequent reads of the same matrices in a wgmma.mma_async instruction.
The mandatory .sync qualifier indicates that wgmma.fence instruction causes the executing thread to wait until all threads in the warp execute the same wgmma.fence instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the same wgmma.fence instruction.
In conditionally executed code, an wgmma.fence instruction should only be used if it is known that all threads in the warpgroup evaluate the condition identically, otherwise the behavior is undefined.
Examples   Example 1, first use example: wgmma.fence.sync.aligned;   Establishes an ordering w.r.t.
prior accesses to the registers s32d wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8 {s32d0, s32d1, s32d2, s32d3}, descA, descB, scaleD; wgmma.commit_group.sync.aligned; wgmma.wait_group.sync.aligned 0;   Example 2, use-case with the input value updated in between: wgmma.fence.sync.aligned; wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8 {s32d0, s32d1, s32d2, s32d3}, descA, descB, scaleD; ...
mov.b32 s32d0, new_val; wgmma.fence.sync.aligned; wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8 {s32d4, s32d5, s32d6, s32d7}, {s32d0, s32d1, s32d2, s32d3}, descB, scaleD; wgmma.commit_group.sync.aligned; wgmma.wait_group.sync.aligned 0; 9.7.14.7.2.
Asynchronous Multiply-and-Accumulate Instruction: wgmma.commit_group  wgmma.commit_group Commits all prior uncommitted wgmma.mma_async operations into a wgmma-group .
Syntax wgmma.commit_group.sync.aligned; Description wgmma.commit_group instruction creates a new wgmma-group per warpgroup and batches all prior wgmma.mma_async instructions initiated by the executing warp but not committed to any wgmma-group into the new wgmma-group.
If there are no uncommitted wgmma.mma_async instructions then wgmma.commit_group results in an empty wgmma-group.
An executing thread can wait for the completion of all wgmma.mma_async operations in a wgmma-group by using wgmma.wait_group .
The mandatory .sync qualifier indicates that wgmma.commit_group instruction causes the executing thread to wait until all threads in the warp execute the same wgmma.commit_group instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the same wgmma.commit_group instruction.
In conditionally executed code, an wgmma.commit_group instruction should only be used if it is known that all threads in the warpgroup evaluate the condition identically, otherwise the behavior is undefined.
Asynchronous Multiply-and-Accumulate Instruction: wgmma.wait_group  wgmma.wait_group Signal the completion of a preceding warpgroup operation.
Syntax wgmma.wait_group.sync.aligned N; Description wgmma.wait_group instruction will cause the executing thread to wait until only N or fewer of the most recent wgmma-groups are pending and all the prior wgmma-groups committed by the executing threads are complete.
For example, when N is 0, the executing thread waits on all the prior wgmma-groups to complete.
Accessing the accumulator register or the input register containing the fragments of matrix A of a wgmma.mma_async instruction without first performing a wgmma.wait_group instruction that waits on a wgmma-group including that wgmma.mma_async instruction is undefined behavior.
The mandatory .sync qualifier indicates that wgmma.wait_group instruction causes the executing thread to wait until all threads in the warp execute the same wgmma.wait_group instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the same wgmma.wait_group instruction.
In conditionally executed code, an wgmma.wait_group instruction should only be used if it is known that all threads in the warpgroup evaluate the condition identically, otherwise the behavior is undefined.
Examples wgmma.fence.sync.aligned; wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8 {s32d0, s32d1, s32d2, s32d3}, descA, descB, scaleD; wgmma.commit_group.sync.aligned; wgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16 {f32d0, f32d1, f32d2, f32d3}, {f16a0, f16a1, f16a2, f16a3}, descB, 1, -1, -1, 1; wgmma.commit_group.sync.aligned; wgmma.wait_group.sync.aligned 0; 9.7.15.
Stack Manipulation Instructions  The stack manipulation instructions can be used to dynamically allocate and deallocate memory on the stack frame of the current function.
Stack Manipulation Instructions: stacksave  stacksave Save the value of stack pointer into a register.
Syntax stacksave.type d; .type = { .u32, .u64 }; Description Copies the current value of stack pointer into the destination register d .
Pointer returned by stacksave can be used in a subsequent stackrestore instruction to restore the stack pointer.
If d is modified prior to use in stackrestore instruction, it may corrupt data in the stack.
Stack Manipulation Instructions: stackrestore  stackrestore Update the stack pointer with a new value.
Syntax stackrestore.type a; .type = { .u32, .u64 }; Description Sets the current stack pointer to source register a .
When stackrestore is used with operand a written by a prior stacksave instruction, it will effectively restore the state of stack as it was before stacksave was executed.
Note that if stackrestore is used with an arbitrary value of a , it may cause corruption of stack pointer.
This implies that the correct use of this feature requires that stackrestore.type a is used after stacksave.type a without redefining the value of a between them.
Syntax alloca.type ptr, size{, immAlign}; .type = { .u32, .u64 }; Description The alloca instruction dynamically allocates memory on the stack frame of the current function and updates the stack pointer accordingly.
The returned pointer ptr points to local memory and can be used in the address operand of ld.local and st.local instructions.
If sufficient memory is unavailable for allocation on the stack, then execution of alloca may result in stack overflow.
In such cases, attempting to access the allocated memory with ptr will result in undefined program behavior.
The memory allocated by alloca is deallocated in the following ways: It is automatically deallocated when the function exits.
It can be explicitly deallocated using stacksave and stackrestore instructions: stacksave can be used to save the value of stack pointer before executing alloca , and stackrestore can be used after alloca to restore stack pointer to the original value which was previously saved with stacksave .
Note that accessing deallocated memory after executing stackrestore results in undefined behavior.
size is an unsigned value which specifies the amount of memory in number of bytes to be allocated on stack.
immAlign is a 32-bit value which specifies the alignment requirement in number of bytes for the memory allocated by alloca .
immAlign is an optional argument with default value being 8 which is the minimum guaranteed alignment.
Semantics alloca.type ptr, size, immAlign: a = max(immAlign, frame_align);   frame_align is the minimum guaranteed alignment   Allocate size bytes of stack memory with alignment a and update the stack pointer.
Since ptr is the lowest address of the memory   allocated by alloca, the memory can be accessed using ptr up to (ptr + size of allocated memory).
Examples .reg .u32 ra, stackptr, ptr, size; stacksave.u32 stackptr;   Save the current stack pointer alloca ptr, size, 8;   Allocate stack memory st.local.u32 [ptr], ra;   Use the allocated stack memory stackrestore.u32 stackptr;   Deallocate memory by restoring the stack pointer 9.7.16.
However, the video instructions may be classified as either scalar or SIMD based on whether their core operation applies to one or multiple values.
The video instructions are: vadd , vadd2 , vadd4 vsub , vsub2 , vsub4 vmad vavrg2 , vavrg4 vabsdiff , vabsdiff2 , vabsdiff4 vmin , vmin2 , vmin4 vmax , vmax2 , vmax4 vshl vshr vset , vset2 , vset4 9.7.16.1.
Scalar Video Instructions  All scalar video instructions operate on 32-bit register operands.
The scalar video instructions are: vadd vsub vabsdiff vmin vmax vshl vshr vmad vset The scalar video instructions execute the following stages: Extract and sign- or zero-extend byte, half-word, or word values from its source operands, to produce signed 33-bit input values.
Optionally perform one of the following: apply a second operation to the intermediate result and a third operand, or truncate the intermediate result to a byte or half-word value and merge into a specified position in the third operand to produce the final result.
The general format of scalar video instructions is as follows:   32-bit scalar operation, with optional secondary operation vop.dtype.atype.btype{.sat} d, a{.asel}, b{.bsel}; vop.dtype.atype.btype{.sat}.secop d, a{.asel}, b{.bsel}, c;   32-bit scalar operation, with optional data merge vop.dtype.atype.btype{.sat} d.dsel, a{.asel}, b{.bsel}, c; .dtype = .atype = .btype = { .u32, .s32 }; .dsel = .asel = .bsel = { .b0, .b1, .b2, .b3, .h0, .h1 }; .secop = { .add, .min, .max }; The source and destination operands are all 32-bit registers.
The type of each operand ( .u32 or .s32 ) is specified in the instruction type; all combinations of dtype , atype , and btype are valid.
Using the atype/btype and asel/bsel specifiers, the input values are extracted and sign- or zero-extended internally to .s33 values.
The intermediate result is optionally clamped to the range of the destination type (signed or unsigned), taking into account the subword destination size in the case of optional data merging.
.s33 optSaturate( .s34 tmp, Bool sat, Bool sign, Modifier dsel ) { if ( !sat ) return tmp; switch ( dsel ) { case .b0, .b1, .b2, .b3: if ( sign ) return CLAMP( tmp, S8_MAX, S8_MIN ); else return CLAMP( tmp, U8_MAX, U8_MIN ); case .h0, .h1: if ( sign ) return CLAMP( tmp, S16_MAX, S16_MIN ); else return CLAMP( tmp, U16_MAX, U16_MIN ); default: if ( sign ) return CLAMP( tmp, S32_MAX, S32_MIN ); else return CLAMP( tmp, U32_MAX, U32_MIN ); } } This intermediate result is then optionally combined with the third source operand using a secondary arithmetic operation or subword data merge, as shown in the following pseudocode.
.s33 optSecOp(Modifier secop, .s33 tmp, .s33 c) { switch ( secop ) { .add: return tmp + c; .min: return MIN(tmp, c); .max return MAX(tmp, c); default: return tmp; } } .s33 optMerge( Modifier dsel, .s33 tmp, .s33 c ) { switch ( dsel ) { case .h0: return ((tmp & 0xffff) | (0xffff0000 & c); case .h1: return ((tmp & 0xffff) 32 ) tb = 32; if ( mode == .wrap ) tb = tb & 0x1f; switch ( vop ){ case vshl: tmp = ta > tb; }   saturate, taking into account destination type and merge operations tmp = optSaturate( tmp, sat, isSigned(dtype), dsel ); d = optSecondaryOp( op2, tmp, c );   optional secondary operation d = optMerge( dsel, tmp, c );   optional merge with c operand PTX ISA Notes Introduced in PTX ISA version 2.0.
Examples vshl.s32.u32.u32.clamp r1, r2, r3; vshr.u32.u32.u32.wrap r1, r2, r3.h1; 9.7.16.1.3.
Syntax   32-bit scalar operation vmad.dtype.atype.btype{.sat}{.scale} d, {-}a{.asel}, {-}b{.bsel}, {-}c; vmad.dtype.atype.btype.po{.sat}{.scale} d, a{.asel}, b{.bsel}, c; .dtype = .atype = .btype = { .u32, .s32 }; .asel = .bsel = { .b0, .b1, .b2, .b3, .h0, .h1 }; .scale = { .shr7, .shr15 }; Description Calculate (a*b) + c , with optional operand negates, plus one mode, and scaling.
Although PTX syntax allows separate negation of the a and b operands, internally this is represented as negation of the product (a*b) .
The intermediate result of (a*b) is unsigned if atype and btype are unsigned and the product (a*b) is not negated; otherwise, the intermediate result is signed.
The final result is optionally saturated to the appropriate 32-bit range based on the type (signed or unsigned) of the final result.
Semantics   extract byte/half-word/word and sign- or zero-extend   based on source operand type ta = partSelectSignExtend( a, atype, asel ); tb = partSelectSignExtend( b, btype, bsel ); signedFinal = isSigned(atype) || isSigned(btype) || (a.negate ^ b.negate) || c.negate; tmp[127:0] = ta * tb; lsb = 0; if ( .po ) { lsb = 1; } else if ( a.negate ^ b.negate ) { tmp = ~tmp; lsb = 1; } else if ( c.negate ) { c = ~c; lsb = 1; } c128[127:0] = (signedFinal) sext32( c ) : zext ( c ); tmp = tmp + c128 + lsb; switch( scale ) { case .shr7: result = (tmp >> 7) & 0xffffffffffffffff; case .shr15: result = (tmp >> 15) & 0xffffffffffffffff; } if ( .sat ) { if (signedFinal) result = CLAMP(result, S32_MAX, S32_MIN); else result = CLAMP(result, U32_MAX, U32_MIN); } PTX ISA Notes Introduced in PTX ISA version 2.0.
Examples vmad.s32.s32.u32.sat r0, r1, r2, -r3; vmad.u32.u32.u32.shr15 r0, r1.h0, r2.h0, r3; 9.7.16.1.4.
Syntax   32-bit scalar operation, with optional secondary operation vset.atype.btype.cmp d, a{.asel}, b{.bsel}; vset.atype.btype.cmp.op2 d, a{.asel}, b{.bsel}, c;   32-bit scalar operation, with optional data merge vset.atype.btype.cmp d.dsel, a{.asel}, b{.bsel}, c; .atype = .btype = { .u32, .s32 }; .cmp = { .eq, .ne, .lt, .le, .gt, .ge }; .dsel = .asel = .bsel = { .b0, .b1, .b2, .b3, .h0, .h1 }; .op2 = { .add, .min, .max }; Description Compare input values using specified comparison, with optional secondary arithmetic operation or subword data merge.
The intermediate result of the comparison is always unsigned, and therefore destination d and operand c are also unsigned.
Semantics   extract byte/half-word/word and sign- or zero-extend   based on source operand type ta = partSelectSignExtend( a, atype, asel ); tb = partSelectSignExtend( b, btype, bsel ); tmp = compare( ta, tb, cmp ) ? 1 : 0; d = optSecondaryOp( op2, tmp, c );   optional secondary operation d = optMerge( dsel, tmp, c );   optional merge with c operand PTX ISA Notes Introduced in PTX ISA version 2.0.
SIMD Video Instructions  The SIMD video instructions operate on pairs of 16-bit values and quads of 8-bit values.
The SIMD video instructions are: vadd2 , vadd4 vsub2 , vsub4 vavrg2 , vavrg4 vabsdiff2 , vabsdiff4 vmin2 , vmin4 vmax2 , vmax4 vset2 , vset4 PTX includes SIMD video instructions for operation on pairs of 16-bit values and quads of 8-bit values.
The SIMD video instructions execute the following stages: Form input vectors by extracting and sign- or zero-extending byte or half-word values from the source operands, to form pairs of signed 17-bit values.
Optionally clamp the result to the appropriate signed or unsigned range, as determinted by the destination type.
Optionally perform one of the following: perform a second SIMD merge operation, or apply a scalar accumulate operation to reduce the intermediate SIMD results to a single scalar.
The general format of dual half-word SIMD video instructions is as follows:   2-way SIMD operation, with second SIMD merge or accumulate vop2.dtype.atype.btype{.sat}{.add} d{.mask}, a{.asel}, b{.bsel}, c; .dtype = .atype = .btype = { .u32, .s32 }; .mask = { .h0, .h1, .h10 }; .asel = .bsel = { .hxy, where x,y are from { 0, 1, 2, 3 } }; The general format of quad byte SIMD video instructions is as follows:   4-way SIMD operation, with second SIMD merge or accumulate vop4.dtype.atype.btype{.sat}{.add} d{.mask}, a{.asel}, b{.bsel}, c; .dtype = .atype = .btype = { .u32, .s32 }; .mask = { .b0, .b1, .b10 .b2, .b20, .b21, .b210, .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 }; .asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 }; The source and destination operands are all 32-bit registers.
The sign of the intermediate result depends on dtype . 9.7.16.2.1. SIMD Video Instructions: vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2  vadd2, vsub2 Integer dual half-word SIMD addition/subtraction.
Syntax   SIMD instruction with secondary SIMD merge operation vop2.dtype.atype.btype{.sat} d{.mask}, a{.asel}, b{.bsel}, c;   SIMD instruction with secondary accumulate operation vop2.dtype.atype.btype.add d{.mask}, a{.asel}, b{.bsel}, c; vop2 = { vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2 }; .dtype = .atype = .btype = { .u32, .s32 }; .mask = { .h0, .h1, .h10 };   defaults to .h10 .asel = .bsel = { .hxy, where x,y are from { 0, 1, 2, 3 } }; .asel defaults to .h10 .bsel defaults to .h32 Description Two-way SIMD parallel arithmetic operation with secondary operation.
Elements of each dual half-word source to the operation are selected from any of the four half-words in the two source operands a and b using the asel and bsel modifiers.
The results are optionally clamped to the appropriate range determined by the destination type (signed or unsigned).
For instructions with a secondary SIMD merge operation: For half-word positions indicated in mask, the selected half-word results are copied into destination d .
For all other positions, the corresponding half-word from source operand c is copied to d .
For instructions with a secondary accumulate operation: For half-word positions indicated in mask, the selected half-word results are added to operand c , producing a result in d .
Syntax (predefined) .sreg .v4 .u32 %tid;   thread id vector .sreg .u32 %tid.x, %tid.y, %tid.z;   thread id components Description A predefined, read-only, per-thread special register initialized with the thread identifier within the CTA.
The %tid special register contains a 1D, 2D, or 3D vector to match the CTA shape; the %tid value in unused dimensions is 0 .
The number of threads in each dimension are specified by the predefined special register %ntid .
It is guaranteed that: 0 ; .reg .v4 .b32 %rx; mov.u32 %r0, %clusterid.x; mov.u32 %r1, %clusterid.z; mov.v4.u32 %rx, %clusterid; 10.13.
Syntax (predefined) .sreg .v4 .u32 %nclusterid; .sreg .u32 %nclusterid.x, %nclusterid.y, %nclusterid.z; Description A predefined, read-only special register initialized with the number of clusters in each grid dimension.
The %nclusterid special register contains a 3D grid shape vector that holds the grid dimensions in terms of clusters.
Examples .reg .b32 %r; .reg .v4 .b32 %rx; mov.u32 %r0, %nclusterid.x; mov.u32 %r1, %nclusterid.z; mov.v4.u32 %rx, %nclusterid; 10.14.
Syntax (predefined) .sreg .v4 .u32 %cluster_ctaid; .sreg .u32 %cluster_ctaid.x, %cluster_ctaid.y, %cluster_ctaid.z; Description A predefined, read-only special register initialized with the CTA identifier in a cluster in each dimension.
The %cluster_ctaid special register contains a 1D, 2D, or 3D vector, depending upon the shape of the cluster.
It is guaranteed that: 0 ; .reg .v4 .b32 %rx; mov.u32 %r0, %cluster_ctaid.x; mov.u32 %r1, %cluster_ctaid.z; mov.v4.u32 %rx, %cluster_ctaid; 10.15.
Special Registers: %cluster_nctaid  %cluster_nctaid Number of CTA identifiers per cluster.
Syntax (predefined) .sreg .v4 .u32 %cluster_nctaid; .sreg .u32 %cluster_nctaid.x, %cluster_nctaid.y, %cluster_nctaid.z; Description A predefined, read-only special register initialized with the number of CTAs in a cluster in each dimension.
The %cluster_nctaid special register contains a 3D grid shape vector that holds the cluster dimensions in terms of CTAs.
Examples .reg .b32 %r; .reg .v4 .b32 %rx; mov.u32 %r0, %cluster_nctaid.x; mov.u32 %r1, %cluster_nctaid.z; mov.v4.u32 %rx, %cluster_nctaid; 10.16.
Special Registers: %cluster_ctarank  %cluster_ctarank CTA identifier in a cluster across all dimensions.
Syntax (predefined) .sreg .u32 %cluster_ctarank; Description A predefined, read-only special register initialized with the CTA rank within a cluster across all dimensions.
It is guaranteed that: 0 ; Description Special registers %pm0..%pm7 are unsigned 32-bit read-only performance monitor counters.
Special Registers: %pm0_64..%pm7_64  %pm0_64..%pm7_64 64 bit Performance monitoring counters.
Syntax (predefined) .sreg .u64 %pm0_64; .sreg .u64 %pm1_64; .sreg .u64 %pm2_64; .sreg .u64 %pm3_64; .sreg .u64 %pm4_64; .sreg .u64 %pm5_64; .sreg .u64 %pm6_64; .sreg .u64 %pm7_64; Description Special registers %pm0_64..%pm7_64 are unsigned 64-bit read-only performance monitor counters.
Syntax (predefined) .sreg .b32 %envreg; Description A set of 32 pre-defined read-only registers used to capture execution environment of PTX program outside of PTX virtual machine.
These registers are initialized by the driver prior to kernel launch and can contain cta-wide or grid-wide values.
Special Registers: %globaltimer, %globaltimer_lo, %globaltimer_hi  %globaltimer, %globaltimer_lo, %globaltimer_hi %globaltimer A predefined, 64-bit global nanosecond timer.
Syntax (predefined) .sreg .u64 %globaltimer; .sreg .u32 %globaltimer_lo, %globaltimer_hi; Description Special registers intended for use by NVIDIA tools.
Special Registers: %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_  %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_ %reserved_smem_offset_begin Start of the reserved shared memory region.
Syntax (predefined) .sreg .b32 %reserved_smem_offset_begin; .sreg .b32 %reserved_smem_offset_end; .sreg .b32 %reserved_smem_offset_cap; .sreg .b32 %reserved_smem_offset_; Description These are predefined, read-only special registers containing information about the shared memory region which is reserved for the NVIDIA system software use.
This region of shared memory is not available to users, and accessing this region from user code results in undefined behavior.
Examples .reg .b32 %reg_begin, %reg_end, %reg_cap, %reg_offset0, %reg_offset1; mov.b32 %reg_begin, %reserved_smem_offset_begin; mov.b32 %reg_end, %reserved_smem_offset_end; mov.b32 %reg_cap, %reserved_smem_offset_cap; mov.b32 %reg_offset0, %reserved_smem_offset_0; mov.b32 %reg_offset1, %reserved_smem_offset_1; 10.30.
Special Registers: %total_smem_size  %total_smem_size Total size of shared memory used by a CTA of a kernel.
Syntax (predefined) .sreg .u32 %total_smem_size; Description A predefined, read-only special register initialized with total size of shared memory allocated (statically and dynamically, excluding the shared memory reserved for the NVIDIA system software use) for the CTA of a kernel at launch time.
Size is returned in multiples of shared memory allocation unit size supported by target architecture.
Allocation unit values are as follows: Target architecture Shared memory allocation unit size sm_2x 128 bytes sm_3x , sm_5x , sm_6x , sm_7x 256 bytes sm_8x , sm_9x 128 bytes PTX ISA Notes Introduced in PTX ISA version 4.1.
Special Registers: %aggr_smem_size  %aggr_smem_size Total size of shared memory used by a CTA of a kernel.
Syntax (predefined) .sreg .u32 %aggr_smem_size; Description A predefined, read-only special register initialized with total aggregated size of shared memory consisting of the size of user shared memory allocated (statically and dynamically) at launch time and the size of shared memory region which is reserved for the NVIDIA system software use.
Special Registers: %dynamic_smem_size  %dynamic_smem_size Size of shared memory allocated dynamically at kernel launch.
Syntax (predefined) .sreg .u32 %dynamic_smem_size; Description Size of shared memory allocated dynamically at kernel launch.
A predefined, read-only special register initialized with size of shared memory allocated dynamically for the CTA of a kernel at launch time.
Special Registers: %current_graph_exec  %current_graph_exec An Identifier for currently executing CUDA device graph.
Syntax (predefined) .sreg .u64 %current_graph_exec; Description A predefined, read-only special register initialized with the identifier referring to the CUDA device graph being currently executed.
PTX Module Directives  The following directives declare the PTX ISA version of the code in the module, the target architecture for which the code was generated, and the size of addresses within the PTX module.
Syntax .version major.minor   major, minor are integers Description Specifies the PTX language version number.
The major number is incremented when there are incompatible changes to the PTX language, such as changes to the syntax or semantics.
The version major number is used by the PTX compiler to ensure correct execution of legacy PTX code.
Semantics Indicates that this module must be compiled with tools that support an equal or greater version number.
Each PTX module must begin with a .version directive, and no other .version directive is allowed anywhere else within the module.
Syntax .target stringlist   comma separated list of target specifiers string = { sm_90a, sm_90,   sm_9x target architectures sm_80, sm_86, sm_87, sm_89,   sm_8x target architectures sm_70, sm_72, sm_75,   sm_7x target architectures sm_60, sm_61, sm_62,   sm_6x target architectures sm_50, sm_52, sm_53,   sm_5x target architectures sm_30, sm_32, sm_35, sm_37,   sm_3x target architectures sm_20,   sm_2x target architectures sm_10, sm_11, sm_12, sm_13,   sm_1x target architectures texmode_unified, texmode_independent,   texturing mode debug,   platform option map_f64_to_f32 };   platform option Description Specifies the set of features in the target architecture for which the current PTX code was generated.
In general, generations of SM architectures follow an onion layer model, where each generation adds new features and retains all features of previous generations.
The onion layer model allows the PTX code generated for a given target to be run on later generation devices.
Target architectures with suffix “ a ”, such as sm_90a , include architecture-accelerated features that are supported on the specified architecture only, hence such targets do not follow the onion layer model.
Architecture-accelerated features can only be used with targets that support these features.
Semantics Each PTX module must begin with a .version directive, immediately followed by a .target directive containing a target architecture and optional platform options.
A .target directive specifies a single target architecture, but subsequent .target directives can be used to change the set of target features allowed during parsing.
A program with multiple .target directives will compile and run only on devices that support all features of the highest-numbered architecture listed in the program.
PTX features are checked against the specified target architecture, and an error is generated if an unsupported feature is used.
The following table summarizes the features in PTX that vary according to target architecture.
sm_72 Adds support for integer multiplicand and accumulator matrices in wmma instructions.
sm_75 Adds support for sub-byte integer and single-bit multiplicant matrices in wmma instructions.
sm_53 Adds support for arithmetic, comparsion and texture instructions for .f16 and .f16x2 types.
The texturing mode is specified for an entire module and cannot be changed within the module.
The .target debug option declares that the PTX file contains DWARF debug information, and subsequent compilation of PTX will retain information needed for source-level debugging.
If the debug option is declared, an error message is generated if no DWARF information is found in the file.
map_f64_to_f32 indicates that all double-precision instructions map to single-precision regardless of the target architecture.
This enables high-level language compilers to compile programs containing type double to target device that do not support double-precision operations.
Note that .f64 storage remains as 64-bits, with only half being used by instructions converted from .f64 to .f32 .
Examples .target sm_10   baseline target architecture .target sm_13   supports double-precision .target sm_20, texmode_independent .target sm_90   baseline target architecture .target sm_90a   PTX using arch accelerated features 11.1.3.
PTX Module Directives: .address_size  .address_size Address size used throughout PTX module.
Syntax .address_size address-size address-size = { 32, 64 }; Description Specifies the address size assumed throughout the module by the PTX code and the binary DWARF information in PTX.
In the presence of separate compilation all modules must specify (or default to) the same address size.
The .address_size directive is optional, but it must immediately follow the .target directive if present within a module.
Examples   example directives .address_size 32   addresses are 32 bit .address_size 64   addresses are 64 bit   example of directive placement within a module .version 2.3 .target sm_20 .address_size 64 ...
.entry foo () { ... } 11.2. Specifying Kernel Entry Points and Functions  The following directives specify kernel entry points and functions.
Kernel and Function Directives: .entry  .entry Kernel entry point and body, with optional parameters.
Syntax .entry kernel-name ( param-list ) kernel-body .entry kernel-name kernel-body Description Defines a kernel entry point name, parameters, and body for the kernel function.
Parameters are passed via .param space memory and are listed within an optional parenthesized parameter list.
Parameters may be referenced by name within the kernel body and loaded into registers using ld.param{::entry} instructions.
In addition to normal parameters, opaque .texref , .samplerref , and .surfref variables may be passed as parameters.
These parameters can only be referenced by name within texture and surface load, store, and query instructions and cannot be accessed via ld.param instructions.
At kernel launch, the kernel dimensions and properties are established and made available via special registers, e.g., %ntid , %nctaid , etc.
PTX ISA Notes For PTX ISA version 1.4 and later, parameter variables are declared in the kernel parameter list.
For PTX ISA versions 1.0 through 1.3, parameter variables are declared in the kernel body.
The maximum memory size supported by PTX for normal (non-opaque type) parameters is 32764 bytes.
The following table shows the allowed parameter size for a PTX ISA version: PTX ISA Version Maximum parameter size (In bytes) PTX ISA version 8.1 and above 32764 PTX ISA version 1.5 and above 4352 PTX ISA version 1.4 and above 256 The CUDA and OpenCL drivers support the following limits for parameter memory: Driver Parameter memory size CUDA 256 bytes for sm_1x , 4096 bytes for sm_2x and higher , 32764 bytes fo sm_70 and higher OpenCL 32764 bytes for sm_70 and higher, 4352 bytes on sm_6x and lower Target ISA Notes Supported on all target architectures.
Examples .entry cta_fft .entry filter ( .param .b32 x, .param .b32 y, .param .b32 z ) { .reg .b32 %r; ld.param.b32 %r1, [x]; ld.param.b32 %r2, [y]; ld.param.b32 %r3, [z]; ...
} .entry prefix_sum ( .param .align 4 .s32 pitch[8000] ) { .reg .s32 %t; ld.param::entry.s32 %t, [pitch]; ... } 11.2.2. Kernel and Function Directives: .func  .func Function definition.
Syntax .func {.attribute(attr-list)} fname {.noreturn} function-body .func {.attribute(attr-list)} fname (param-list) {.noreturn} function-body .func {.attribute(attr-list)} (ret-param) fname (param-list) function-body Description Defines a function, including input and return parameters and optional function body.
An optional .noreturn directive indicates that the function does not return to the caller function.
An optional .attribute directive specifies additional information associated with the function.
See the description of Variable and Function Attribute Directive: .attribute for allowed attributes.
Parameters in register state space may be referenced directly within instructions in the function body.
Parameters in .param space are accessed using ld.param{::func} and st.param{::func} instructions in the body.
The last parameter in the parameter list may be a .param array of type .b8 with no size specified.
It is used to pass an arbitrary number of parameters to the function packed into a single array object.
When calling a function with such an unsized last argument, the last argument may be omitted from the call instruction if no parameter is passed through it.
The result of an access is undefined if no array was passed, or if the access was outside the bounds of the actual array being passed.
The implementation of parameter passing is left to the optimizing translator, which may use a combination of registers and stack locations to pass parameters.
Release Notes For PTX ISA version 1.x code, parameters must be in the register state space, there is no stack, and recursion is illegal.
PTX ISA versions 2.0 and later with target sm_20 or higher allow parameters in the .param state space, implements an ABI with stack, and supports recursion.
PTX ISA versions 2.0 and later with target sm_20 or higher support at most one return value.
Target ISA Notes Functions without unsized array parameter supported on all target architectures.
Examples .func (.reg .b32 rval) foo (.reg .b32 N, .reg .f64 dbl) { .reg .b32 localVar; ...
.func (.param .u32 rval) bar(.param .u32 N, .param .align 4 .b8 numbers[]) { .reg .b32 input0, input1; ld.param.b32 input0, [numbers + 0]; ld.param.b32 input1, [numbers + 4]; ...
.param .u32 N; .param .align 4 .b8 numbers[8]; st.param.u32 [N], 2; st.param.b32 [numbers + 0], 5; st.param.b32 [numbers + 4], 10; call (rval), bar, (N, numbers); ... 11.2.3. Kernel and Function Directives: .alias  .alias Define an alias to existing function symbol.
Syntax .alias fAlias, fAliasee; Description .alias is a module scope directive that defines identifier fAlias to be an alias to function specified by fAliasee .
Identifier fAliasee is a function symbol which must be defined in the same module as .alias declaration.
Program can use either fAlias or fAlisee identifiers to reference function defined with fAliasee .
.param .u32 p; call bar, (p);   call foo through alias } .entry filter ( .param .b32 x, .param .b32 y, .param .b32 z ) { .reg .b32 %r1, %r2, %r3; ld.param.b32 %r1, [x]; ld.param.b32 %r2, [y]; ld.param.b32 %r3, [z]; ... } 11.3. Control Flow Directives  PTX provides directives for specifying potential targets for brx.idx and call instructions.
Control Flow Directives: .branchtargets  .branchtargets Declare a list of potential branch targets.
Syntax Label: .branchtargets list-of-labels ; Description Declares a list of potential branch targets for a subsequent brx.idx , and associates the list with the label at the start of the line.
All control flow labels in the list must occur within the same function as the declaration.
The list of labels may use the compact, shorthand syntax for enumerating a range of labels having a common prefix, similar to the syntax described in Parameterized Variable Names .
ts: .branchtargets N; @p brx.idx %r0, ts; ... 11.3.2. Control Flow Directives: .calltargets  .calltargets Declare a list of potential call targets.
Syntax Label: .calltargets list-of-functions ; Description Declares a list of potential call targets for a subsequent indirect call, and associates the list with the label at the start of the line.
All functions named in the list must be declared prior to the .calltargets directive, and all functions must have the same type signature.
@p call (%f1), %r0, (%x), calltgt; ... 11.3.3. Control Flow Directives: .callprototype  .callprototype Declare a prototype for use in an indirect call.
Syntax   no input or return parameters label: .callprototype _ .noreturn;   input params, no return params label: .callprototype _ (param-list) .noreturn;   no input params,   return params label: .callprototype (ret-param) _ ;   input, return parameters label: .callprototype (ret-param) _ (param-list); Description Defines a prototype with no specific function name, and associates the prototype with a label.
The prototype may then be used in indirect call instructions where there is incomplete knowledge of the possible call targets.
Parameters may have either base types in the register or parameter state spaces, or array types in parameter state space.
Examples Fproto1: .callprototype _ ; Fproto2: .callprototype _ (.param .f32 _); Fproto3: .callprototype (.param .u32 _) _ ; Fproto4: .callprototype (.param .u32 _) _ (.param .f32 _); ...
example of array parameter Fproto5: .callprototype _ (.param .b8 _[12]); Fproto6: .callprototype _ (.param .f32 _) .noreturn; ...
@p call %r0, (%f1), Fproto6; ... 11.4. Performance-Tuning Directives  To provide a mechanism for low-level performance tuning, PTX supports the following directives, which pass information to the backend optimizing compiler.
.maxnreg .maxntid .reqntid .minnctapersm .maxnctapersm (deprecated) .pragma The .maxnreg directive specifies the maximum number of registers to be allocated to a single thread; the .maxntid directive specifies the maximum number of threads in a thread block (CTA); the .reqntid directive specifies the required number of threads in a thread block (CTA); and the .minnctapersm directive specifies a minimum number of thread blocks to be scheduled on a single multiprocessor (SM).
These can be used, for example, to throttle the resource requirements (e.g., registers) to increase total thread count and provide a greater opportunity to hide memory latency.
The .minnctapersm directive can be used together with either the .maxntid or .reqntid directive to trade-off registers-per-thread against multiprocessor utilization without needed to directly specify a maximum number of registers.
This may achieve better performance when compiling PTX for multiple devices having different numbers of registers per SM.
Currently, the .maxnreg , .maxntid , .reqntid , and .minnctapersm directives may be applied per-entry and must appear between an .entry directive and its body.
The directives take precedence over any module-level constraints passed to the optimizing backend.
A warning message is generated if the directives’ constraints are inconsistent or cannot be met for the specified target device.
The directive passes a list of strings to the backend, and the strings have no semantics within the PTX virtual machine model.
The interpretation of .pragma values is determined by the backend implementation and is beyond the scope of the PTX ISA.
Note that .pragma directives may appear at module (file) scope, at entry-scope, or as statements within a kernel or device function body. 11.4.1. Performance-Tuning Directives: .maxnreg  .maxnreg Maximum number of registers that can be allocated per thread.
Syntax .maxnreg n Description Declare the maximum number of registers per thread in a CTA.
The actual number of registers used may be less; for example, the backend may be able to compile to fewer registers, or the maximum number of registers may be further constrained by .maxntid and .maxctapersm .
Performance-Tuning Directives: .maxntid  .maxntid Maximum number of threads in the thread block (CTA).
Syntax .maxntid nx .maxntid nx, ny .maxntid nx, ny, nz Description Declare the maximum number of threads in the thread block (CTA).
This maximum is specified by giving the maximum extent of each dimension of the 1D, 2D, or 3D CTA.
Semantics The maximum number of threads in the thread block, computed as the product of the maximum extent specified for each dimension, is guaranteed not to be exceeded in any invocation of the kernel in which this directive appears.
Exceeding the maximum number of threads results in a runtime error or kernel launch failure.
Note that this directive guarantees that the total number of threads does not exceed the maximum, but does not guarantee that the limit in any particular dimension is not exceeded.
Performance-Tuning Directives: .reqntid  .reqntid Number of threads in the thread block (CTA).
Syntax .reqntid nx .reqntid nx, ny .reqntid nx, ny, nz Description Declare the number of threads in the thread block (CTA) by specifying the extent of each dimension of the 1D, 2D, or 3D CTA.
Semantics The size of each CTA dimension specified in any invocation of the kernel is required to be equal to that specified in this directive.
Specifying a different CTA dimension at launch will result in a runtime error or kernel launch failure.
Performance-Tuning Directives: .minnctapersm  .minnctapersm Minimum number of CTAs per SM.
Syntax .minnctapersm ncta Description Declare the minimum number of CTAs from the kernel’s grid to be mapped to a single multiprocessor (SM).
Notes Optimizations based on .minnctapersm need either .maxntid or .reqntid to be specified as well.
If the total number of threads on a single SM resulting from .minnctapersm and .maxntid / .reqntid exceed maximum number of threads supported by an SM then directive .minnctapersm will be ignored.
In PTX ISA version 2.1 or higher, a warning is generated if .minnctapersm is specified without specifying either .maxntid or .reqntid .
Examples .entry foo .maxntid 256 .minnctapersm 4 { ... } 11.4.5. Performance-Tuning Directives: .maxnctapersm (deprecated)  .maxnctapersm Maximum number of CTAs per SM.
Syntax .maxnctapersm ncta Description Declare the maximum number of CTAs from the kernel’s grid that may be mapped to a single multiprocessor (SM).
Notes Optimizations based on .maxnctapersm generally need .maxntid to be specified as well.
The optimizing backend compiler uses .maxntid and .maxnctapersm to compute an upper-bound on per-thread register usage so that the specified number of CTAs can be mapped to a single multiprocessor.
However, if the number of registers used by the backend is sufficiently lower than this bound, additional CTAs may be mapped to a single multiprocessor.
Examples .entry foo .maxntid 256 .maxnctapersm 4 { ... } 11.4.6. Performance-Tuning Directives: .noreturn  .noreturn Indicate that the function does not return to its caller function.
Syntax .noreturn Description Indicate that the function does not return to its caller function.
Semantics An optional .noreturn directive indicates that the function does not return to caller function.
.noreturn directive can only be specified on device functions and must appear between a .func directive and its body.
If a function with .noreturn directive returns to the caller function at runtime, then the behavior is undefined.
Examples .func foo .noreturn { ... } 11.4.7. Performance-Tuning Directives: .pragma  .pragma Pass directives to PTX backend compiler.
Syntax .pragma list-of-strings ; Description Pass module-scoped, entry-scoped, or statement-level directives to the PTX backend compiler.
Semantics The interpretation of .pragma directive strings is implementation-specific and has no impact on PTX semantics.
See Descriptions of .pragma Strings for descriptions of the pragma strings defined in ptxas .
Examples .pragma "nounroll";   disable unrolling in backend   disable unrolling for current kernel .entry foo .pragma "nounroll"; { ... } 11.5. Debugging Directives  DWARF-format debug information is passed through PTX modules using the following directives: @@DWARF .section .file .loc The .section directive was introduced in PTX ISA version 2.0 and replaces the @@DWARF syntax.
The @@DWARF syntax was deprecated in PTX ISA version 2.0 but is supported for legacy PTX ISA version 1.x code.
Beginning with PTX ISA version 3.0, PTX files containing DWARF debug information should include the .target debug platform option.
This forward declaration directs PTX compilation to retain mappings for source-level debugging. 11.5.1. Debugging Directives: @@dwarf  @@dwarf DWARF-format information.
Syntax @@DWARF dwarf-string dwarf-string may have one of the .byte byte-list   comma-separated hexadecimal byte values .4byte int32-list   comma-separated hexadecimal integers in range [0..2^32-1] .quad int64-list   comma-separated hexadecimal integers in range [0..2^64-1] .4byte label .quad label PTX ISA Notes Introduced in PTX ISA version 1.2.
Examples @@DWARF .section .debug_pubnames, "", @progbits @@DWARF .byte 0x2b, 0x00, 0x00, 0x00, 0x02, 0x00 @@DWARF .4byte .debug_info @@DWARF .4byte 0x000006b5, 0x00000364, 0x61395a5f, 0x5f736f63 @@DWARF .4byte 0x6e69616d, 0x63613031, 0x6150736f, 0x736d6172 @@DWARF .byte 0x00, 0x00, 0x00, 0x00, 0x00 11.5.2.
Examples .section .debug_pubnames { .b32 LpubNames_end0-LpubNames_begin0 LpubNames_begin0: .b8 0x2b, 0x00, 0x00, 0x00, 0x02, 0x00 .b32 .debug_info info_label1: .b32 0x000006b5, 0x00000364, 0x61395a5f, 0x5f736f63 .b32 0x6e69616d, 0x63613031, 0x6150736f, 0x736d6172 .b8 0x00, 0x00, 0x00, 0x00, 0x00 LpubNames_end0: } .section .debug_info { .b32 11430 .b8 2, 0 .b32 .debug_abbrev .b8 8, 1, 108, 103, 101, 110, 102, 101, 58, 32, 69, 68, 71, 32, 52, 46, 49 .b8 0 .b32 3, 37, 176, -99 .b32 info_label1 .b32 .debug_loc+0x4 .b8 -11, 11, 112, 97 .b32 info_label1+12 .b64 -1 .b16 -5, -65535 } 11.5.3.
Syntax .file file_index "filename" {, timestamp, file_size} Description Associates a source filename with an integer index.
.file directive allows optionally specifying an unsigned number representing time of last modification and an unsigned integer representing size in bytes of source file.
The .file directive is allowed only in the outermost scope, i.e., at the same level as kernel and device function declarations.
Examples .file 1 "example.cu" .file 2 "kernel.cu" .file 1 “kernel.cu”, 1339013327, 64118 11.5.4.
Syntax .loc file_index line_number column_position .loc file_index line_number column_position,function_name label {+ immediate }, inlined_at file_index2 line_number2 column_position2 Description Declares the source file location (source file, line number, and column position) to be associated with lexically subsequent PTX instructions.
To indicate PTX instructions that are generated from a function that got inlined, additional attribute .inlined_at can be specified as part of the .loc directive.
.inlined_at attribute specifies source location at which the specified function is inlined.
file_index2 , line_number2 , and column_position2 specify the location at which function is inlined.
Source location specified as part of .inlined_at directive must lexically precede as source location in .loc directive.
Offset is specified as label expression or label + immediate expression where label is defined in .debug_str section.
DWARF section .debug_str contains ASCII null-terminated strings that specify the name of the function that is inlined.
Note that a PTX instruction may have a single associated source location, determined by the nearest lexically preceding .loc directive, or no associated source location if there is no preceding .loc directive.
Examples .loc 2 4237 0 L1:   line 4237, col 0 of file #2,   inherited from mov mov.u32 %r1,%r2;   line 4237, col 0 of file #2 add.u32 %r2,%r1,%r3;   line 4237, col 0 of file #2 ...
L2:   line 4239, col 5 of file #2,   inherited from sub .loc 2 4239 5 sub.u32 %r2,%r1,%r3;   line 4239, col 5 of file #2 .loc 1 21 3 .loc 1 9 3, function_name info_string0, inlined_at 1 21 3 ld.global.u32 %r1, [gg];   Function at line 9 setp.lt.s32 %p1, %r1, 8;   inlined at line 21 .loc 1 27 3 .loc 1 10 5, function_name info_string1, inlined_at 1 27 3 .loc 1 15 3, function_name .debug_str+16, inlined_at 1 10 5 setp.ne.s32 %p2, %r1, 18; @%p2 bra BB2_3; .section .debug_str { info_string0: .b8 95   _ .b8 90   z .b8 51   3 .b8 102   f .b8 111   o .b8 111   o .b8 118   v .b8 0 info_string1: .b8 95   _ .b8 90   z .b8 51   3 .b8 98   b .b8 97   a .b8 114   r .b8 118   v .b8 0 .b8 95   _ .b8 90   z .b8 51   3 .b8 99   c .b8 97   a .b8 114   r .b8 118   v .b8 0 } 11.6.
Syntax .extern identifier Description Declares identifier to be defined external to the current module.
The module defining such identifier must define it as .weak or .visible only once in a single object file.
Extern declaration of symbol may appear multiple times and references to that get resolved against the single definition of that symbol.
Unlike C, where identifiers are globally visible unless declared static, PTX identifiers are visible only within the current module unless declared .visible outside the current.
Weak symbols are similar to globally visible symbols, except during linking, weak symbols are only chosen after globally visible symbols during symbol resolution.
Unlike globally visible symbols, multiple object files may declare the same weak symbol, and references to a symbol get resolved against a weak symbol only if no global symbols have the same name.
Syntax .common identifier Description Declares identifier to be globally visible but “common”.
However multiple object files may declare the same common symbol and they may have different types and sizes and references to a symbol get resolved against a common symbol with the largest size.
Only one object file can initialize a common symbol and that must have the largest size among all other definitions of that common symbol from different object files.
Cluster Dimension Directives  The following directives specify information about clusters: .reqnctapercluster .explicitcluster .maxclusterrank The .reqnctapercluster directive specifies the number of CTAs in the cluster.
The .explicitcluster directive specifies that the kernel should be launched with explicit cluster details.
The cluster dimension directives can be applied only on kernel functions. 11.7.1. Cluster Dimension Directives: .reqnctapercluster  .reqnctapercluster Declare the number of CTAs in the cluster.
Syntax .reqnctapercluster nx .reqnctapercluster nx, ny .reqnctapercluster nx, ny, nz Description Set the number of thread blocks (CTAs) in the cluster by specifying the extent of each dimension of the 1D, 2D, or 3D cluster.
For kernels with .reqnctapercluster directive specified, runtime will use the specified values for configuring the launch if the same are not specified at launch time.
Semantics If cluster dimension is explicitly specified at launch time, it should be equal to the values specified in this directive.
Specifying a different cluster dimension at launch will result in a runtime error or kernel launch failure.
Examples .entry foo .reqnctapercluster 2 { . . } .entry bar .reqnctapercluster 2, 2, 1 { .
.entry ker .reqnctapercluster 3, 2 { . 11.7.2. Cluster Dimension Directives: .explicitcluster  .explicitcluster Declare that Kernel must be launched with cluster dimensions explicitly specified.
Syntax .explicitcluster Description Declares that this Kernel should be launched with cluster dimension explicitly specified.
Semantics Kernels with .explicitcluster directive must be launched with cluster dimension explicitly specified (either at launch time or via .reqnctapercluster ), otherwise program will fail with runtime error or kernel launch failure.
Examples .entry foo .explicitcluster { . 11.7.3. Cluster Dimension Directives: .maxclusterrank  .maxclusterrank Declare the maximum number of CTAs that can be part of the cluster.
Syntax .maxclusterrank n Description Declare the maximum number of thread blocks (CTAs) allowed to be part of the cluster.
Semantics Product of the number of CTAs in each cluster dimension specified in any invocation of the kernel is required to be less or equal to that specified in this directive.
The .maxclusterrank directive cannot be used in conjunction with the .reqnctapercluster directive.
Examples .entry foo ..maxclusterrank 8 { . 12. Release Notes  This section describes the history of change in the PTX ISA and implementation.
The first section describes ISA and implementation changes in the current release of PTX ISA version 8.5, and the remaining sections provide a record of changes in previous releases of PTX ISA versions back to PTX ISA version 2.0.
Changes in PTX ISA Version 8.5  New Features PTX ISA version 8.5 introduces the following new features: Adds support for mma.sp::ordered_metadata instruction.
Semantic Changes and Clarifications Values 0b0000 , 0b0101 , 0b1010 , 0b1111 for sparsity metadata (operand e ) of instruction mma.sp are invalid and their usage results in undefined behavior. 12.2. Changes in PTX ISA Version 8.4  New Features PTX ISA version 8.4 introduces the following new features: Extends ld , st and atom instructions with .b128 type to support .sys scope.
Extends integer wgmma.mma_async instruction to support .u8.s8 and .s8.u8 as .atype and .btype respectively.
Semantic Changes and Clarifications None. 12.3. Changes in PTX ISA Version 8.3  New Features PTX ISA version 8.3 introduces the following new features: Adds support for pragma used_bytes_mask that is used to specify mask for used bytes for a load operation.
Extends isspacep , cvta.to , ld and st instructions to accept ::entry and ::func sub-qualifiers with .param state space qualifier.
Add support for instructions tensormap.replace , tensormap.cp_fenceproxy and support for qualifier .to_proxykind::from_proxykind on instruction fence.proxy to support modifying tensor-map . 12.4. Changes in PTX ISA Version 8.2  New Features PTX ISA version 8.2 introduces the following new features: Adds support for .mmio qualifier on ld and st instructions.
Extends multimem.ld_reduce instruction to support .acc::f32 qualifer to allow .f32 precision of the intermediate accumulation.
Extends the asynchronous warpgroup-level matrix multiply-and-accumulate operation wgmma.mma_async to support .sp modifier that allows matrix multiply-accumulate operation when input matrix A is sparse.
Semantic Changes and Clarifications The .multicast::cluster qualifier on cp.async.bulk and cp.async.bulk.tensor instructions is optimized for target architecture sm_90a and may have substantially reduced performance on other targets and hence .multicast::cluster is advised to be used with sm_90a . 12.5. Changes in PTX ISA Version 8.1  New Features PTX ISA version 8.1 introduces the following new features: Adds support for st.async and red.async instructions for asynchronous store and asynchronous reduction operations respectively on shared memory.
Adds support for .satfinite saturation modifer on cvt instruction for .f16 , .bf16 and .tf32 formats.
Adds support for multimem.ld_reduce , multimem.st and multimem.red instructions to perform memory operations on multimem addresses. 12.6. Changes in PTX ISA Version 8.0  New Features PTX ISA version 8.0 introduces the following new features: Adds support for target sm_90a that supports specialized accelerated features.
Adds support for asynchronous warpgroup-level matrix multiply-and-accumulate operation wgmma .
Extends the asynchronous copy operations with bulk operations that operate on large data, including tensor data.
Extends integer arithmetic instruction add to allow packed integer types .u16x2 and .s16x2 .
Extends integer arithmetic instructions min and max to allow packed integer types .u16x2 and .s16x2 , as well as saturation modifier .relu on .s16x2 and .s32 types.
Adds support for special register %current_graph_exec that identifies the currently executing CUDA device graph.
Extends the fence instruction to allow opcode-specific synchronizaion using op_restrict qualifier.
Adds support for .cluster scope on mbarrier.arrive , mbarrier.arrive_drop , mbarrier.test_wait and mbarrier.try_wait operations.
Adds support for transaction count operations on mbarrier objects, specified with .expect_tx and .complete_tx qualifiers. 12.7. Changes in PTX ISA Version 7.8  New Features PTX ISA version 7.8 introduces the following new features: Adds support for sm_89 target architecture.
Adds support for movmatrix instruction which transposes a matrix in registers across a warp.
Extends the .f64 floating point type mma operation with shapes .m16n8k4 , .m16n8k8 , and .m16n8k16 .
Extends add , sub , mul , set , setp , cvt , tanh , ex2 , atom and red instructions with bf16 alternate floating point data format.
Adds support for griddepcontrol instruction as a communication mechanism to control the execution of dependent grids.
Adds support for new thread scope .cluster which is a set of Cooperative Thread Arrays (CTAs).
Adds support for extended visibility of shared state space to all threads within a cluster.
Extends .shared state space qualifier with ::cluster sub-qualifier for cluster-level visibility of shared memory.
Extends isspacep , cvta , ld , st , atom , and red instructions to accept ::cluster sub-qualifier with .shared state space qualifier.
Adds support for mapa instruction to map a shared memory address to the corresponding address in a different CTA within the cluster.
Adds support for getctarank instruction to query the rank of the CTA that contains a given address.
Adds support for special registers related to cluster information: %is_explicit_cluster , %clusterid , %nclusterid , %cluster_ctaid , %cluster_nctaid , %cluster_ctarank , %cluster_nctarank .
Adds support for cluster dimension directives .reqnctapercluster , .explicitcluster , and .maxclusterrank . 12.8. Changes in PTX ISA Version 7.7  New Features PTX ISA version 7.7 introduces the following new features: Extends isspacep and cvta instructions to include the .param state space for kernel function parameters.
12.9. Changes in PTX ISA Version 7.6  New Features PTX ISA version 7.6 introduces the following new features: Support for szext instruction which performs sign-extension or zero-extension on a specified value.
Support for bmsk instruction which creates a bitmask of the specified width starting at the specified bit position.
Support for special registers %reserved_smem_offset_begin , %reserved_smem_offset_end , %reserved_smem_offset_cap , %reserved_smem_offset . 12.10. Changes in PTX ISA Version 7.5  New Features PTX ISA version 7.5 introduces the following new features: Debug information enhancements to support label difference and negative values in the .section debugging directive.
Extensions to the memory consistency model to introduce the following new concepts: A memory proxy as an abstract label for different methods of memory access.
Support for new fence.proxy and membar.proxy instructions to allow synchronization of memory accesses performed via virtual aliases. 12.11. Changes in PTX ISA Version 7.4  New Features PTX ISA version 7.4 introduces the following new features: Support for sm_87 target architecture.
Support for .level::eviction_priority qualifier which allows specifying cache eviction priority hints on ld , ld.global.nc , st , and prefetch instructions.
Support for .level::prefetch_size qualifier which allows specifying data prefetch hints on ld and cp.async instructions.
Support for createpolicy instruction which allows construction of different types of cache eviction policies.
Support for .level::cache_hint qualifier which allows the use of cache eviction policies with ld , ld.global.nc , st , atom , red and cp.async instructions.
Support for applypriority and discard operations on cached data. 12.12. Changes in PTX ISA Version 7.3  New Features PTX ISA version 7.3 introduces the following new features: Extends mask() operator used in initializers to also support integer constant expression.
Adds support for stack manpulation instructions that allow manipulating stack using stacksave and stackrestore instructions and allocation of per-thread stack using alloca instruction.
Semantic Changes and Clarifications The unimplemented version of alloca from the older PTX ISA specification has been replaced with new stack manipulation instructions in PTX ISA version 7.3. 12.13. Changes in PTX ISA Version 7.2  New Features PTX ISA version 7.2 introduces the following new features: Enhances .loc directive to represent inline function information.
Extends min and max instructions to support .xorsign and .abs modifiers. 12.14. Changes in PTX ISA Version 7.1  New Features PTX ISA version 7.1 introduces the following new features: Support for sm_86 target architecture.
Adds a new operator, mask() , to extract a specific byte from variable’s address used in initializers.
Extends tex and tld4 instructions to return an optional predicate that indicates if data at specified coordinates is resident in memory.
Extends mma instruction to support .sp modifier that allows matrix multiply-accumulate operation when input matrix A is sparse.
Extends mbarrier.test_wait instruction to test the completion of specific phase parity. 12.15. Changes in PTX ISA Version 7.0  New Features PTX ISA version 7.0 introduces the following new features: Support for sm_80 target architecture.
Adds support for asynchronous copy instructions that allow copying of data asynchronously from one state space to another.
Adds support for mbarrier instructions that allow creation of mbarrier objects in memory and use of these objects to synchronize threads and asynchronous copy operations initiated by threads.
Adds support for redux.sync instruction which allows reduction operation across threads in a warp.
Extends mma instruction to support new shapes .m8n8k128 , .m16n8k4 , .m16n8k16 , .m16n8k32 , .m16n8k64 , .m16n8k128 and .m16n8k256 .
Extends min and max instructions to support .NaN modifier and .f16 , .f16x2 , .bf16 and .bf16x2 data formats.
Extends fma instruction to support .relu saturation mode and .bf16 and .bf16x2 data formats.
Extends cvt instruction to support .relu saturation mode and .f16 , .f16x2 , .bf16 , .bf16x2 and .tf32 destination formats.
Extends ex2 instruction to support .f16 and .f16x2 types. 12.16. Changes in PTX ISA Version 6.5  New Features PTX ISA version 6.5 introduces the following new features: Adds support for integer destination types for half precision comparison instruction set .
Adds support for cvt.pack instruction which allows converting two integer values and packing the results together.
Adds support for ldmatrix instruction which loads one or more matrices from shared memory for mma instruction.
Removed Features PTX ISA version 6.5 removes the following features: Support for .satfinite qualifier on floating point wmma.mma instruction has been removed.
This support was deprecated since PTX ISA version 6.4. 12.17. Changes in PTX ISA Version 6.4  New Features PTX ISA version 6.4 introduces the following new features: Adds support for .noreturn directive which can be used to indicate a function does not return to it’s caller function.
Adds support for mma instruction which allows performing matrix multiply-and-accumulate operation.
Deprecated Features PTX ISA version 6.4 deprecates the following features: Support for .satfinite qualifier on floating point wmma.mma instruction.
Removed Features PTX ISA version 6.4 removes the following features: Support for shfl and vote instructions without the .sync qualifier has been removed for .target sm_70 and higher.
This support was deprecated since PTX ISA version 6.0 as documented in PTX ISA version 6.2.
Semantic Changes and Clarifications Clarified that resolving references of a .weak symbol considers only .weak or .visible symbols with the same name and does not consider local symbols with the same name.
Clarified that in cvt instruction, modifier .ftz can only be specified when either .atype or .dtype is .f32 . 12.18. Changes in PTX ISA Version 6.3  New Features PTX ISA version 6.3 introduces the following new features: Support for sm_75 target architecture.
Adds support for a new instruction nanosleep that suspends a thread for a specified duration.
The wmma instructions are extended to support multiplicand matrices of type .s8 , .u8 , .s4 , .u4 , .b1 and accumulator matrices of type .s32 .
Semantic Changes and Clarifications Introduced the mandatory .aligned qualifier for all wmma instructions.
Specified the alignment required for the base address and stride parameters passed to wmma.load and wmma.store .
Clarified that layout of fragment returned by wmma operation is architecture dependent and passing wmma fragments around functions compiled for different link compatible SM architectures may not work as expected.
Clarified that atomicity for {atom/red}.f16x2} operations is guranteed separately for each of the two .f16 elements but not guranteed to be atomic as single 32-bit access. 12.19. Changes in PTX ISA Version 6.2  New Features PTX ISA version 6.2 introduces the following new features: A new instruction activemask for querying active threads in a warp.
Extends atomic and reduction instructions to perform .f16x2 addition operation with mandatory .noftz qualifier.
Deprecated Features PTX ISA version 6.2 deprecates the following features: The use of shfl and vote instructions without the .sync is deprecated retrospectively from PTX ISA version 6.0, which introduced the sm_70 architecture that implements Independent Thread Scheduling .
Semantic Changes and Clarifications Clarified that wmma instructions can be used in conditionally executed code only if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined.
In the memory consistency model, the definition of morally strong operations was updated to exclude fences from the requirement of complete overlap since fences do not access memory. 12.20. Changes in PTX ISA Version 6.1  New Features PTX ISA version 6.1 introduces the following new features: Support for sm_72 target architecture.
Support for new matrix shapes 32x8x16 and 8x32x16 in wmma instruction. 12.21. Changes in PTX ISA Version 6.0  New Features PTX ISA version 6.0 introduces the following new features: Support for sm_70 target architecture.
Specifies the memory consistency model for programs running on sm_70 and later architectures.
Various extensions to memory instructions to specify memory synchronization semantics and scopes at which such synchronization can be observed.
New instruction wmma for matrix operations which allows loading matrices from memory, performing multiply-and-accumulate on them and storing result in memory.
Extends vote and shfl instructions with .sync modifier which waits for specified threads before executing the vote and shfl operation respectively.
A new instruction match.sync which allows broadcasting and comparing a value across threads in warp.
A new instruction brx.idx which allows branching to a label indexed from list of potential targets.
Support for unsized array parameter for .func which can be used to implement variadic functions.
Semantic Changes and Clarifications Semantics of bar instruction were updated to indicate that executing thread waits for other non-exited threads from it’s warp.
Support for indirect branch introduced in PTX 2.1 which was unimplemented has been removed from the spec.
Support for taking address of labels, using labels in initializers which was unimplemented has been removed from the spec.
Support for variadic functions which was unimplemented has been removed from the spec. 12.22. Changes in PTX ISA Version 5.0  New Features PTX ISA version 5.0 introduces the following new features: Support for sm_60 , sm_61 , sm_62 target architecture.
A new .common directive to permit linking multiple object files containing declarations of the same symbol with different size.
Semantic Changes and Clarifications Semantics of cache modifiers on ld and st instructions were clarified to reflect cache operations are treated as performance hint only and do not change memory consistency behavior of the program.
Semantics of volatile operations on ld and st instructions were clarified to reflect how volatile operations are handled by optimizing compiler. 12.23. Changes in PTX ISA Version 4.3  New Features PTX ISA version 4.3 introduces the following new features: A new lop3 instruction which allows arbitrary logical operation on 3 inputs.
Extends tex and tld4 instructions to support optional operands for offset vector and depth compare.
Extends txq instruction to support querying texture fields from specific LOD. 12.24. Changes in PTX ISA Version 4.2  New Features PTX ISA version 4.2 introduces the following new features: Support for sm_53 target architecture.
Support for memory_layout field for surfaces and suq instruction support for querying this field.
Semantic Changes and Clarifications Semantics for parameter passing under ABI were updated to indicate ld.param and st.param instructions used for argument passing cannot be predicated.
Semantics of {atom/red}.add.f32 were updated to indicate subnormal inputs and results are flushed to sign-preserving zero for atomic operations on global memory; whereas atomic operations on shared memory preserve subnormal inputs and results and don’t flush them to zero. 12.25. Changes in PTX ISA Version 4.1  New Features PTX ISA version 4.1 introduces the following new features: Support for sm_37 and sm_52 target architectures.
Support for new fields array_size , num_mipmap_levels and num_samples for Textures, and the txq instruction support for querying these fields.
Support for new field array_size for Surfaces, and the suq instruction support for querying this field.
Support for special registers %total_smem_size and %dynamic_smem_size . 12.26. Changes in PTX ISA Version 4.0  New Features PTX ISA version 4.0 introduces the following new features: Support for sm_32 and sm_50 target architectures.
A new instruction, rsqrt.approx.ftz.f64 has been added to compute a fast approximation of the square root reciprocal of a value.
Semantic Changes and Clarifications The vote instruction semantics were updated to clearly indicate that an inactive thread in a warp contributes a 0 for its entry when participating in vote.ballot.b32 . 12.27. Changes in PTX ISA Version 3.2  New Features PTX ISA version 3.2 introduces the following new features: The texture instruction supports reads from multi-sample and multisample array textures.
Semantic Changes and Clarifications The vavrg2 and vavrg4 instruction semantics were updated to indicate that instruction adds 1 only if Va[i] + Vb[i] is non-negative, and that the addition result is shifted by 1 (rather than being divided by 2). 12.28. Changes in PTX ISA Version 3.1  New Features PTX ISA version 3.1 introduces the following new features: Support for sm_35 target architecture.
Support for CUDA Dynamic Parallelism, which enables a kernel to create and synchronize new work.
Extends atomic and reduction instructions to perform 64-bit {and, or, xor} operations, and 64-bit integer {min, max} operations.
Extends support for generic addressing to include the .const state space, and adds a new operator, generic() , to form a generic address for .global or .const variables used in initializers.
A new .weak directive to permit linking multiple object files containing declarations of the same symbol.
Semantic Changes and Clarifications PTX 3.1 redefines the default addressing for global variables in initializers, from generic addresses to offsets in the global state space.
Instruction mad.f32 requires a rounding modifier for sm_20 and higher targets. 12.29. Changes in PTX ISA Version 3.0  New Features PTX ISA version 3.0 introduces the following new features: Support for sm_30 target architectures.
Platform option .target debug to declare that a PTX module contains DWARF debug information.
Semantic Changes and Clarifications Special register %gridid has been extended from 32-bits to 64-bits.
PTX ISA version 3.0 deprecates module-scoped .reg and .local variables when compiling to the Application Binary Interface (ABI).
The shfl instruction semantics were updated to clearly indicate that value of source operand a is unpredictable for inactive and predicated-off threads within the warp.
{u32,s32,f32} have been removed. 12.30. Changes in PTX ISA Version 2.3  New Features PTX 2.3 adds support for texture arrays.
The texture array feature supports access to an array of 1D or 2D textures, where an integer indexes into the array of textures, and then one or two single-precision floating point coordinates are used to address within the selected 1D or 2D texture.
Semantic Changes and Clarifications The semantics of the .maxntid directive have been updated to match the current implementation.
Specifically, .maxntid only guarantees that the total number of threads in a thread block does not exceed the maximum.
Previously, the semantics indicated that the maximum was enforced separately in each dimension, which is not the case.
Bit field extract and insert instructions BFE and BFI now indicate that the len and pos operands are restricted to the value range 0..255 .
{min,max}.f32 have been removed. 12.31. Changes in PTX ISA Version 2.2  New Features PTX 2.2 adds a new directive for specifying kernel parameter attributes; specifically, there is a new directives for specifying that a kernel parameter is a pointer, for specifying to which state space the parameter points, and for optionally specifying the alignment of the memory to which the parameter points.
This field is used in the independent texturing mode to override the normalized_coords field in the texture header.
This field is needed to support languages such as OpenCL, which represent the property of normalized/unnormalized coordinates in the sampler header rather than in the texture header.
PTX 2.2 deprecates explicit constant banks and supports a large, flat address space for the .const state space.
PTX 2.2 adds a new tld4 instruction for loading a component ( r , g , b , or a ) from the four texels compising the bilinear interpolation footprint of a given texture location.
This instruction may be used to compute higher-precision bilerp results in software, or for performing higher-bandwidth texture loads. 12.32. Changes in PTX ISA Version 2.1  New Features The underlying, stack-based ABI is supported in PTX ISA version 2.1 for sm_2x targets.
New directives, .branchtargets and .calltargets , have been added for specifying potential targets for indirect branches and indirect function calls.
A .callprototype directive has been added for declaring the type signatures for indirect function calls.
The names of .global and .const variables can now be specified in variable initializers to represent their addresses.
A set of thirty-two driver-specific execution environment special registers has been added.
Textures and surfaces have new fields for channel data type and channel order, and the txq and suq instructions support queries for these fields.
A new instruction, rcp.approx.ftz.f64 , has been added to compute a fast, gross approximate reciprocal.
Semantic Changes and Clarifications A warning is emitted if .minnctapersm is specified without also specifying .maxntid . 12.33. Changes in PTX ISA Version 2.0  New Features Floating Point Extensions This section describes the floating-point changes in PTX ISA version 2.0 for sm_20 targets.
The goal is to achieve IEEE 754 compliance wherever possible, while maximizing backward compatibility with legacy PTX ISA version 1.x code and sm_1x targets.
The changes from PTX ISA version 1.x are as follows: Single-precision instructions support subnormal numbers by default for sm_20 targets.
Single-precision add , sub , and mul now support .rm and .rp rounding modifiers for sm_20 targets.
A single-precision fused multiply-add (fma) instruction has been added, with support for IEEE 754 compliant rounding modifiers and support for subnormal numbers.
The mad.f32 instruction has been extended with rounding modifiers so that it’s synonymous with fma.f32 for sm_20 targets.
The mad.f32 instruction without rounding is retained so that compilers can generate code for sm_1x targets.
Single- and double-precision div , rcp , and sqrt with IEEE 754 compliant rounding have been added.
Instructions {atom,red} .shared have been extended to handle 64-bit data types for sm_20 targets.
The bar instruction has been extended as follows: A bar.arrive instruction has been added.
Instruction isspacep for querying whether a generic address falls within a specified state space window has been added.
Instruction cvta for converting global, local, and shared addresses to generic address and vice-versa has been added.
Other New Features Instructions ld , ldu , st , prefetch , prefetchu , isspacep , cvta , atom , and red now support generic addressing.
New special registers %nwarpid , %nsmid , %clock64 , %lanemask_{eq,le,lt,ge,gt} have been added.
Cache operations have been added to instructions ld , st , suld , and sust , e.g., for prefetching to specified level of memory hierarchy.
The .maxnctapersm directive was deprecated and replaced with .minnctapersm to better match its behavior and usage.
A new directive, .section , has been added to replace the @@DWARF syntax for passing DWARF-format debugging information through PTX.
A new directive, .pragma nounroll , has been added to allow users to disable loop unrolling.
Semantic Changes and Clarifications The errata in cvt.ftz for PTX ISA versions 1.4 and earlier, where single-precision subnormal inputs and results were not flushed to zero if either source or destination type size was 64-bits, has been fixed.
In PTX ISA version 1.5 and later, cvt.ftz (and cvt for .target sm_1x , where .ftz is implied) instructions flush single-precision subnormal inputs and results to sign-preserving zero for all combinations of floating-point instruction types.
To maintain compatibility with legacy PTX code, if .version is 1.4 or earlier, single-precision subnormal inputs and results are flushed to sign-preserving zero only when neither source nor destination type size is 64-bits.
Components of special registers %tid , %ntid , %ctaid , and %nctaid have been extended from 16-bits to 32-bits.
The number of samplers available in independent texturing mode was incorrectly listed as thirty-two in PTX ISA version 1.5; the correct number is sixteen. 14. Descriptions of .pragma Strings  This section describes the .pragma strings defined by ptxas.
14.1. Pragma Strings: “nounroll”  “nounroll” Disable loop unrolling in optimizing the backend compiler.
Syntax .pragma "nounroll"; Description The "nounroll" pragma is a directive to disable loop unrolling in the optimizing backend compiler.
The "nounroll" pragma is allowed at module, entry-function, and statement levels, with the following meanings: module scope disables unrolling for all loops in module, including loops preceding the .pragma .
statement-level pragma disables unrolling of the loop for which the current block is the loop header.
Note that in order to have the desired effect at statement level, the "nounroll" directive must appear before any instruction statements in the loop header basic block for the desired loop.
The loop header block is defined as the block that dominates all blocks in the loop body and is the target of the loop backedge.
Statement-level "nounroll" directives appearing outside of loop header blocks are silently ignored.
Examples .entry foo (...) .pragma "nounroll";   do not unroll any loop in this function { ...
L1_continue: bra L1_head; L1_end: ... } 14.2. Pragma Strings: “used_bytes_mask”  “used_bytes_mask” Mask for indicating used bytes in data of ld operation.
Syntax .pragma "used_bytes_mask mask"; Description The "used_bytes_mask" pragma is a directive that specifies used bytes in a load operation based on the mask provided.
"used_bytes_mask" pragma needs to be specified prior to a load instruction for which information about bytes used from the load operation is needed.
For a load instruction without this pragma, all bytes from the load operation are assumed to be used.
Operand mask is a 32-bit integer with set bits indicating the used bytes in data of load operation.
Semantics Each bit in mask operand corresponds to a byte data where each set bit represents the used byte.
For 4 bytes load with only lower 3 bytes used .pragma "used_bytes_mask 0x7"; ld.global.u32 %r0, [gbl];   Higher 1 byte from %r0 is unused   For vector load of 16 bytes with lower 12 bytes used .pragma "used_bytes_mask 0xfff"; ld.global.v4.u32 {%r0, %r1, %r2, %r3}, [gbl];   %r3 unused PTX ISA Notes Introduced in PTX ISA version 8.3.
Examples .pragma "used_bytes_mask 0xfff"; ld.global.v4.u32 {%r0, %r1, %r2, %r3}, [gbl];   Only lower 12 bytes used 15.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 15.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 15.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Introduction v12.5 | PDF | Archive PTX Writer’s Guide to Interoperability The guide to writing ABI-compliant PTX.
Introduction  This document defines the Application Binary Interface (ABI) for the CUDA ® architecture when generating PTX.
By following the ABI, external developers can generate compliant PTX code that can be linked with other code.
PTX is a low-level parallel-thread-execution virtual machine and ISA (Instruction Set Architecture).
PTX is meant to be GPU-architecture independent, so that the same code can be reused for different GPU architectures.
For more information on PTX, refer to the latest version of the PTX ISA reference document .
Programs conforming to an ABI are expected to be executed on the appropriate architecture GPU, and can assume that instructions from that ISA are available. 2. Data Representation  2.1.
Any PTX producer must use these sizes and alignments in order for its PTX to be compatible with PTX generated by other producers.
PTX has an .address_size directive that specifies the address size used throughout the PTX code.
PTX Type Size (bytes) Align (bytes) Hardware Representation .b8 1 1 untyped byte .b16 2 2 untyped halfword .b32 4 4 untyped word .b64 8 8 untyped doubleword .s8 1 1 signed integral byte .s16 2 2 signed integral halfword .s32 4 4 signed integral word .s64 8 8 signed integral doubleword .u8 1 1 unsigned integral byte .u16 2 2 unsigned integral halfword .u32 4 4 unsigned integral word .u64 8 8 unsigned integral doubleword .f16 2 2 IEEE half precision .f32 4 4 IEEE single precision .f64 8 8 IEEE double precision 2.2.
Aggregates and Unions  Beyond the scalar types, PTX also supports native-vector types of these scalar types, with both its vector syntax and its byte-array syntax.
For scalar types with a size no greater than four bytes, vector types with 1, 2, 3, and 4 elements exist; for all other types, only 1 and 2 element vector types exist.
For a non-native-vector type, an entire aggregate or union is aligned on the same boundary as its most strictly aligned member.
For example, in OpenCL built-in vector data types have their alignment set to the size of the built-in data type in bytes.
For a native vector type – discussed at the start of this section – the alignment is defined as follows.
(For the definitions below, the native vector has n elements and has an element type t.) For a vector with an odd number of elements, its alignment is the same as its member: alignof(t).
For a vector with an even number of elements, its alignment is set to number of elements times the alignment of its member: n*alignof(t).
The size of an aggregate or union, if necessary, is increased to make it a multiple of the alignment of the aggregate or union.
This may require tail padding, depending on the last member. 2.3. Bit Fields  C structure and union definitions may have bit fields that define integral objects with a specified number of bits.
Bit Field Type Width w Range signed char 1 to 8 -2 w-1 to 2 w-1 - 1 unsigned char 1 to 8 0 to 2 w - 1 signed short 1 to 16 -2 w-1 to 2 w-1 - 1 unsigned short 1 to 16 0 to 2 w - 1 signed int 1 to 32 -2 w-1 to 2 w-1 - 1 unsigned int 1 to 32 0 to 2 w - 1 signed long long 1 to 64 -2 w-1 to 2 w-1 - 1 unsigned long long 1 to 64 0 to 2 w - 1 Current GPUs only support little-endian memory, so the below assumes little-endian layout.
Bit fields obey the same size and alignment rules as other structure and union members, with the following modifications.
Bit fields are allocated in memory from right to left (least to more significant) for little endian.
Bit fields may share a storage unit with other structure and union members, including members that are not bit fields, as long as there is enough space within the storage unit.
Zero-length bit fields force the alignment of the following member of a structure to the next alignment boundary corresponding to the bit-field type.
An unnamed, zero-length bit field will not force the external alignment of the structure to that boundary.
If an unnamed, zero-length bit field has a stricter alignment than the external alignment, there is no guarantee that the stricter alignment will be maintained when the structure or union gets allocated to memory.
Figure 1 shows the byte offsets (upper corners) and the bit numbers (lower corners) that are used in the examples.
Bit Numbering  Bit-field Allocation  Boundary Alignment  Storage Unit Sharing  Union Allocation  Unnamed Bit Fields  2.4.
Texture, Sampler, and Surface Types  Texture, sampler and surface types are used to define references to texture and surface memory.
The CUDA architecture provides hardware and instructions to efficiently read data from texture or surface memory as opposed to global memory.
References to textures are bound through runtime functions to device read-only regions of memory, called a texture memory, before they can be used by a kernel.
At the PTX level objects that access texture or surface memory are referred to as opaque objects.
Textures are expressed by either a .texref or .samplerref type and surfaces are expressed by the .surfref type.
The data of opaque objects can be accessed by specific instructions (TEX for .texref/.samplerref and SULD/SUST for .surfref).
The attributes of opaque objects are implemented by allocating a descriptor in memory which is populated by the driver.
The internal format of the descriptor varies with each architecture and should not be relied on by the user.
The data and the attributes of an opaque object may be accessed directly if the texture or surface reference is known at compile time or indirectly.
If the reference is not known during compile time all information required to read data and attributes is contained in a .b64 value called the handle.
The handle can be used to pass and return oqaque object references to and from functions as well as to reference external textures, samplers and surfaces. 3. Function Calling Sequence  This section describes the PTX-level function calling sequence, including register usage, stack-frame layout, and parameter passing.
The PTX-level function calling sequence describes what gets represented in PTX to enable function calls.
Most of the details associated with the function calling sequence are handled at the SASS level.
PTX versions earlier than 2.0 do not conform to the ABI defined in this document, and cannot perform ABI compatible function calls.
For the calling convention to work PTX version 2.0 or greater must be used. 3.1. Registers  At the PTX level, the registers that are specified are virtual.
The PTX-to-SASS translation also converts parameters and return values to physical registers or stack locations. 3.2. Stack Frame  The PTX level has no concept of the software stack.
Manipulation of the stack is completely defined at the SASS level, and gets allocated during the PTX-to-SASS translation process. 3.3. Parameter Passing  At the PTX level, all parameters and return values present in a device function use the parameter state space (.param).
The below table contains the rules for handling parameters and return values that are defined at the source level.
For each source-level type, the corresponding PTX-level type that should be used is provided.
Source Type Size in Bits PTX Type Integral types 8 to 32 (A) .u32 (if unsigned) or .s32 (if signed) Integral types 64 .u64 (if unsigned) or .s64 (if signed) Pointers (B) 32 .u32 Pointers (B) 64 .u64 Floating-point types (C) 32 .f32 Floating-point types (C) 64 .f64 Aggregates or unions Any size .align align .b8 name [ size ] Where align is overall aggregate-or-union alignment in bytes (D), name is variable name associated with aggregate or union, and size is the aggregate-or-union size in bytes.
Handles (E) 64 .b64 (assigned from .texref, .sampleref, .surfref) NOTES: Values shorter than 32-bits are sign extended or zero extended, depending on whether they are signed or unsigned types.
Unless the memory type is specified in the function declaration, all pointers passed at the PTX level must use a generic address.
The PTX built-in opaque types such as texture, sampler, and surface types are can be passed into functions as parameters and be returned by them through 64-bit handles.
The handle contains the necessary information to access the actual data from the texture or surface memory as well as the attributes of the object stored in its type descriptor.
See section Texture, Sampler, and Surface Types for more information on handles. 4. System Calls  System calls are calls into the driver operating system code.
A prototype must be provided in the PTX file, but the implementation of the function is provided by the driver.
param t2 valist ) The following are the definitions for the vprintf parameters and return value.
uni ( _ ), vprintf , ( param0 , param1 ); For this code, _fmt is the format string in global memory, and _valist_array is the valist of arguments.
param t2 size ) The following are the definitions for the malloc parameters and return value.
The malloc and free system calls are emitted as part of the malloc and free functions defined in “malloc.h”.
In order to support assert, the PTX function call __assertfail is used whenever the assert expression produces a false value.
charSize : The size in bytes of the characters contained in the __assertfail parameter strings.
The __assertfail system call is emitted as part of the assert macro defined in “assert.h”. 5. Debug Information  Debug information is encoded in DWARF (Debug With Arbitrary Record Format).
5.1. Generation of Debug Information  The responsibility for generating debug information is split between the PTX producer and the PTX-to-SASS backend.
The PTX producer is responsible for emitting binary DWARF into the PTX file, using the .section and .b8-.b16-.b32-and-.b64 directives in PTX.
This should contain the .debug_info and .debug_abbrev sections, and possibly optional sections .debug_pubnames and .debug_aranges.
These sections are standard DWARF2 sections that refer to labels and registers in the PTX.
The PTX-to-SASS backend is responsible for generating the .debug_line section from the .file and .loc directives in the PTX file.
The backend also generates the .debug_frame section. 5.2. CUDA-Specific DWARF Definitions  In order to support debugging of multiple memory segments, address class codes are defined to reflect the memory space of variables.
The address-class values are emitted as the DW_AT_address_class attribute for all variable and parameter Debugging Information Entries.
Code Value Description ADDR_code_space 1 Code storage ADDR_reg_space 2 Register storage ADDR_sreg_space 3 Special register storage ADDR_const_space 4 Constant storage ADDR_global_space 5 Global storage ADDR_local_space 6 Local storage ADDR_param_space 7 Parameter storage ADDR_shared_space 8 Shared storage ADDR_surf_space 9 Surface storage ADDR_tex_space 10 Texture storage ADDR_tex_sampler_space 11 Texture sampler storage ADDR_generic_space 12 Generic-address storage 6.
Example  The following is example PTX with debug information for implementing the following program that makes a call: __device__ __noinline__ int foo ( int i , int j ) { return i + j ; } __global__ void test ( int * p ) { * p = foo ( 1 , 2 ); } The resulting PTX would be something like: .
b8 8 , 1 , 108 , 103 , 101 , 110 , 102 , 101 , 58 , 32 , 69 , 68 , 71 , 32 , 52 , 46 , 57 .
b8 1 , 17 , 1 , 37 , 8 , 19 , 11 , 3 , 8 , 17 , 1 , 16 , 6 , 27 , 8 , 0 , 0 , 2 , 46 , 1 , 135 .
b8 64 , 8 , 3 , 8 , 58 , 6 , 59 , 6 , 73 , 19 , 63 , 12 , 17 , 1 , 18 , 1 , 64 , 10 , 0 , 0 .
b8 3 , 5 , 0 , 3 , 8 , 58 , 6 , 59 , 6 , 73 , 19 , 2 , 10 , 51 , 11 , 0 , 0 , 4 , 36 , 0 , 3 .
b8 8 , 62 , 11 , 11 , 6 , 0 , 0 , 5 , 59 , 0 , 3 , 8 , 0 , 0 , 6 , 15 , 0 , 73 , 19 , 51 , 11 .
Exceptions and try/catch blocks RTTI STL library Global constructors and destructors Virtual functions and classes across host and device (i.e., vtables cannot be used across host and device) There are also a few C features that are not currently supported: stdio other than printf 8.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 8.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 8.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Using Inline PTX Assembly in CUDA v12.5 | PDF | Archive Inline PTX Assembly in CUDA The reference guide for inlining PTX (parallel thread execution) assembly statements into CUDA.
Using Inline PTX Assembly in CUDA  The NVIDIA ® CUDA ® programming environment provides a parallel thread execution (PTX) instruction set architecture (ISA) for using the GPU as a data-parallel computing device.
For more information on the PTX ISA, refer to the latest version of the PTX ISA reference document .
This application note describes how to inline PTX assembly language statements into CUDA code. 1.1. Assembler (ASM) Statements  Assembler statements, asm() , provide a way to insert arbitrary PTX code into your CUDA program.
A simple example is: asm ( "membar.gl;" ); This inserts a PTX membar.gl into your generated PTX code at the point of the asm() statement. 1.1.1. Parameters  An asm() statement becomes more complicated, and more useful, when we pass values in and out of the asm.
The basic syntax is as follows: asm ( "template-string" : "constraint" ( output ) : "constraint" ( input )); where you can have multiple input or output operands separated by commas.
A simple example is as follows: asm ( "add.s32 %0, %1, %2;" : "=r" ( i ) : "r" ( j ), "r" ( k )); Each %n in the template string is an index into the following list of operands, in text order.
Since the output operands are always listed ahead of the input operands, they are assigned the smallest indices.
s32 i , j , k ; Note that the numbered references in the string can be in arbitrary order.
The following is equivalent to the above example: asm ( "add.s32 %0, %2, %1;" : "=r" ( i ) : "r" ( k ), "r" ( j )); You can also repeat a reference, e.g.
: asm ( "mov.s32 %0, 2;" : "=r" ( i )); If there is no output operand, the colon separators are adjacent, e.g.
: asm ( "mov.s32 r1, %0;" :: "r" ( i )); If you want the % in a ptx instruction, then you should escape it with double %% , e.g.
: asm ( "mov.u32 %0, %%clock;" : "=r" ( x )); The above was simplified to explain the ordering of the string % references.
In reality, the operand values are passed via whatever mechanism the constraint specifies.
The full list of constraints will be explained later, but the “r” constraint refers to a 32bit integer register.
So the earlier example asm() statement: asm ( "add.s32 %0, %1, %2;" : "=r" ( i ) : "r" ( j ), "r" ( k )); produces the following code sequence in the output generated by the compiler: ld .
s32 [ i ], r3 ; This is where the distinction between input and output operands becomes important.
The input operands are loaded into registers before the asm() statement, then the result register is stored to the output operand.
There is also available a “+” modifier that specifies the register is both read and written, e.g.
: asm ( "add.s32 %0, %0, %1;" : "+r" ( i ) : "r" ( j )); Multiple instructions can be combined into a single asm() statement; basically, anything legal can be put into the asm string.
Multiple instructions can be split across multiple lines by making use of C/C++’s implicit string concatenation.
Both C++ style line end comments “ ” and classical C-style comments “/**/” can be interspersed with these strings.
To generate readable output in the PTX intermediate file it is best practice to terminate each instruction string except the last one with “nt”.
For example, a cube routine could be written as: __device__ int cube ( int x ) { int y ; asm ( ".reg .u32 t1;  \t "   temp reg t1 " mul.lo.u32 t1, %1, %1;  \t "   t1 = x * x " mul.lo.u32 %0, t1, %1;"   y = t1 * x : "=r" ( y ) : "r" ( x )); return y ; } If an output operand is conditionally updated by the asm instructions, then the “+” modifier should be used.
For example, __device__ int cond ( int x ) { int y = 0 ; asm ( "{  \t " " .reg .pred %p;  \t " " setp.eq.s32 %p, %1, 34;  \t "   x == 34? " @%p mov.s32 %0, 1;  \t "   set y to 1 if true "}"   conceptually y = (x==34)?1:y : "+r" ( y ) : "r" ( x )); return y ; } 1.1.2.
f32 [ x ], f1 ; The constraint "n" may be used for immediate integer operands with a known value.
u32 r1 , r1 , 42 ; The constraint "C" can be used for operand of type ‘array of const char’, where the array contents are known at compile time.
It is intended to allow customization of PTX instruction modes based on compile time computation (see examples).
Here is the specification for the "C" constraint: 'C' ( constant - expression ) The constant-expression is evaluated during compilation and shall generate the address of a variable V , where: V has static storage duration .
If V is a static class member, then V ’s initializing declaration is the declaration within the class.
During translation, the compiler will replace a reference to the operand within the Assembler Template with the contents of V ’s initializer, except for the last trailing zero.
(terms in italics are C++ standard terms and/or terms from the GNU inline asm specification).
PTX instructions types accepting 8-bit wide types permit operands to be wider than the instruction-type size .
Example: __device__ void copy_u8 ( char * in , char * out ) { int d ; asm ( "ld.u8 %0, [%1];" : "=r" ( d ) : "l" ( in )); * out = d ; } generates: ld .
u8 [ rd2 ], r1 ; The behavior of using a constraint string that is not one of those specified above is undefined. 1.2. Pitfalls  Although asm() statements are very flexible and powerful, you may encounter some pitfalls—these are listed in this section.
1.2.1. Namespace Conflicts  If the cube function (described before) is called and inlined multiple times in the code, it generates an error about duplicate definitions of the temp register t1.
To avoid this error you need to: not inline the cube function, or, nest the t1 use inside {} so that it has a separate scope for each invocation, e.g.
: __device__ int cube ( int x ) { int y ; asm ( "{  \t "   use braces for local scope " reg .u32 t1;  \t "   temp reg t1, " mul.lo.u32 t1, %1, %1;  \t "   t1 = x * x " mul.lo.u32 %0, t1, %1;  \t "   y = t1 * x "}" : "=r" ( y ) : "r" ( x )); return y ; } Note that you can similarly use braces for local labels inside the asm() statement. 1.2.2. Memory Space Conflicts  Since asm() statements have no way of knowing what memory space a register is in, the user must make sure that the appropriate PTX instruction is used.
For sm_20 and greater, any pointer argument to an asm() statement is passed as a generic address. 1.2.3. Incorrect Optimization  The compiler assumes that an asm() statement has no side effects except to change the output operands.
To ensure that the asm is not deleted or moved during generation of PTX, you should use the volatile keyword, e.g.
: asm volatile ( "mov.u32 %0, %%clock;" : "=r" ( x )); Normally any memory that is written to will be specified as an out operand, but if there is a hidden side effect on user memory (for example, indirect access of a memory location via an operand), or if you want to stop any memory optimizations around the asm() statement performed during generation of PTX, you can add a “memory” clobbers specification after a 3rd colon, e.g.
: asm volatile ( "mov.u32 %0, %%clock;" : "=r" ( x ) :: "memory" ); asm ( "st.u32 [%0], %1;" :: "l" ( p ), "r" ( x ) : "memory" ); 1.2.4.
Incorrect PTX  The compiler front end does not parse the asm() statement template string and does not know what it means or even whether it is valid PTX input.
For example, if you pass a value with an “r” constraint but use it in an add.f64 you will get a parse error from ptxas.
For example, in asm ( "mov.u32 %0, %n1;" : "=r" ( n ) : "r" ( 1 )); the ‘n’ modifier in “%n1” is not supported and will be passed to ptxas , where it can cause undefined behavior.
Refer to the document nvcc.pdf for further compiler related details. 1.3. Error Checking  The following are some of the error checks that the compiler will do on inlinePTXasm: Multiple constraint letters for a single asm operand are not allowed, e.g.
: asm ( "add.s32 %0, %1, %2;" : "=r" ( i ) : "rf" ( j ), "r" ( k )); error: an asm operand may specify only one constraint letter in a __device__/__global__ function Only scalar variables are allowed as asm operands.
int4 i4 ; asm ( "add.s32 %0, %1, %2;" : "=r" ( i4 ) : "r" ( j ), "r" ( k )); error: an asm operand must have scalar type The type and size implied by a PTX asm constraint must match that of the associated operand.
Example where size does not match: For ‘char’ type variable “ci”, asm ( "add.s32 %0,%1,%2;" : "=r" ( ci ) : "r" ( j ), "r" ( k )); error: asm operand type size(1) does not match type/size implied by constraint ‘r’ In order to use ‘char’ type variables “ci”, “cj”, and “ck” in the above asm statement, code segment similar to the following may be used, int temp = ci ; asm ( "add.s32 %0,%1,%2;" : "=r" ( temp ) : "r" (( int ) cj ), "r" (( int ) ck )); ci = temp ; Another example where type does not match: For ‘float’ type variable “fi”, asm ( "add.s32 %0,%1,%2;" : "=r" ( fi ) : "r" ( j ), "r" ( k )); error: asm operand type size(4) does not match type/size implied by constraint ‘r’ 2.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 2.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 2.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit v12.5.1 CUDA Runtime API 1.
Deprecated List Search Results CUDA Runtime API ( PDF ) - v12.5.1 ( older ) - Last updated July 1, 2024 - Send Feedback Table of Contents 1.
Deprecated List Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024 NVIDIA Corporation var switchTo5x=true; stLight.options({publisher: "998dc202-a267-4d8e-bce9-14debadb8d92", doNotHash: false, doNotCopy: false, hashAddressBar: false}); _satellite.pageBottom();
NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit v12.5.1 CUDA Driver API 1.
Deprecated List Search Results CUDA Driver API ( PDF ) - v12.5.1 ( older ) - Last updated July 1, 2024 - Send Feedback Table of Contents 1.
Deprecated List Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024 NVIDIA Corporation var switchTo5x=true; stLight.options({publisher: "998dc202-a267-4d8e-bce9-14debadb8d92", doNotHash: false, doNotCopy: false, hashAddressBar: false}); _satellite.pageBottom();
1.
Notices CUDA Math API Reference Manual » CUDA Math API Reference Manual v12.5 | PDF | Archive CUDA Math API Reference Manual  CUDA mathematical functions are always available in device code.
Host implementations of the common mathematical functions are mapped in a platform-specific way to standard math library functions, provided by the host compiler and respective host libm where available.
Some functions, not available with the host compilers, are implemented in crt/math_functions.hpp header file.
Other, less common functions, like rhypot() , cyl_bessel_i0() are only available in device code.
Note that many floating-point and integer functions names are overloaded for different argument types.
For example, the log() function has the following prototypes: double log ( double x ); float log ( float x ); float logf ( float x ); Note also that due to implementation constraints, certain math functions from std:: namespace may be callable in device code even via explicitly qualified std:: names.
However, such use is discouraged, since this capability is unsupported, unverified, undocumented, not portable, and may change without notice.
Notices Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates.
Introduction v12.5 | PDF | Archive cuBLAS The API Reference guide for cuBLAS, the CUDA Basic Linear Algebra Subroutine library.
Introduction  The cuBLAS library is an implementation of BLAS (Basic Linear Algebra Subprograms) on top of the NVIDIA®CUDA™ runtime.
It allows the user to access the computational resources of NVIDIA Graphics Processing Unit (GPU).
The cuBLAS Library exposes four sets of APIs: The cuBLAS API , which is simply called cuBLAS API in this document (starting with CUDA 6.0), The cuBLASXt API (starting with CUDA 6.0), and The cuBLASLt API (starting with CUDA 10.1) The cuBLASDx API (not shipped with the CUDA Toolkit) To use the cuBLAS API, the application must allocate the required matrices and vectors in the GPU memory space, fill them with data, call the sequence of desired cuBLAS functions, and then upload the results from the GPU memory space back to the host.
The cuBLAS API also provides helper functions for writing and retrieving data from the GPU.
To use the cuBLASXt API, the application may have the data on the Host or any of the devices involved in the computation, and the Library will take care of dispatching the operation to, and transferring the data to, one or multiple GPUs present in the system, depending on the user request.
The cuBLASLt is a lightweight library dedicated to GEneral Matrix-to-matrix Multiply (GEMM) operations with a new flexible API.
This library adds flexibility in matrix data layouts, input types, compute types, and also in choosing the algorithmic implementations and heuristics through parameter programmability.
After a set of options for the intended GEMM operation are identified by the user, these options can be used repeatedly for different inputs.
This is analogous to how cuFFT and FFTW first create a plan and reuse for same size and type FFTs with different input data. 1.1. Data Layout  For maximum compatibility with existing Fortran environments, the cuBLAS library uses column-major storage, and 1-based indexing.
Since C and C++ use row-major storage, applications written in these languages can not use the native array semantics for two-dimensional arrays.
Instead, macros or inline functions should be defined to implement matrices on top of one-dimensional arrays.
For Fortran code ported to C in mechanical fashion, one may chose to retain 1-based indexing to avoid the need to transform loops.
In this case, the array index of a matrix element in row “i” and column “j” can be computed via the following macro #define IDX2F(i,j,ld) (j)-1)*(ld))+((i)-1)) Here, ld refers to the leading dimension of the matrix, which in the case of column-major storage is the number of rows of the allocated matrix (even if only a submatrix of it is being used).
For natively written C and C++ code, one would most likely choose 0-based indexing, in which case the array index of a matrix element in row “i” and column “j” can be computed via the following macro #define IDX2C(i,j,ld) (((j)*(ld))+(i)) 1.2.
New and Legacy cuBLAS API  Starting with version 4.0, the cuBLAS Library provides a new API, in addition to the existing legacy API.
This section discusses why a new API is provided, the advantages of using it, and the differences with the existing legacy API.
It has the following features that the legacy cuBLAS API does not have: The handle to the cuBLAS library context is initialized using the function and is explicitly passed to every subsequent library function call.
This allows the user to have more control over the library setup when using multiple host threads and multiple GPUs.
The scalars \(\alpha\) and \(\beta\) can be passed by reference on the host or the device, instead of only being allowed to be passed by value on the host.
This change allows library functions to execute asynchronously using streams even when \(\alpha\) and \(\beta\) are generated by a previous kernel.
When a library routine returns a scalar result, it can be returned by reference on the host or the device, instead of only being allowed to be returned by value only on the host.
This change allows library routines to be called asynchronously when the scalar result is generated and returned by reference on the device resulting in maximum parallelism.
Note that cublasStatus was renamed cublasStatus_t to be more consistent with other types in the cuBLAS library.
This change removes these unnecessary wrappers around cudaMalloc() and cudaFree() , respectively.
The function cublasSetKernelStream() was renamed cublasSetStream() to be more consistent with the other CUDA libraries.
The legacy cuBLAS API, explained in more detail in Using the cuBLAS Legacy API , can be used by including the header file cublas.h .
Since the legacy API is identical to the previously released cuBLAS library API, existing applications will work out of the box and automatically use this legacy API without any source code changes.
The current and the legacy cuBLAS APIs cannot be used simultaneously in a single translation unit: including both cublas.h and cublas_v2.h header files will lead to compilation errors due to incompatible symbol redeclarations.
In general, new applications should not use the legacy cuBLAS API, and existing applications should convert to using the new API if it requires sophisticated and optimal stream parallelism, or if it calls cuBLAS routines concurrently from multiple threads.
For the rest of the document, the new cuBLAS Library API will simply be referred to as the cuBLAS Library API.
As mentioned earlier the interfaces to the legacy and the cuBLAS library APIs are the header file cublas.h and cublas_v2.h , respectively.
In addition, applications using the cuBLAS library need to link against: The DSO cublas.so for Linux, The DLL cublas.dll for Windows, or The dynamic library cublas.dylib for Mac OS X.
Note The same dynamic library implements both the new and legacy cuBLAS APIs. 1.3. Example Code  For sample code references please see the two examples below.
They show an application written in C using the cuBLAS library API with two indexing styles (Example 1.
In that case, the results are not guaranteed to be bit-wise reproducible because atomics are used for the computation. 2.1.5. Scalar Parameters  There are two categories of the functions that use scalar parameters : Functions that take alpha and/or beta parameters by reference on the host or the device as scaling factors, such as gemm .
Functions that return a scalar result on the host or the device such as amax() , amin , asum() , rotg() , rotmg() , dot() and nrm2() .
For the functions of the first category, when the pointer mode is set to CUBLAS_POINTER_MODE_HOST , the scalar parameters alpha and/or beta can be on the stack or allocated on the heap, shouldn’t be placed in managed memory.
Underneath, the CUDA kernels related to those functions will be launched with the value of alpha and/or beta .
Therefore if they were allocated on the heap, they can be freed just after the return of the call even though the kernel launch is asynchronous.
When the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE , alpha and/or beta must be accessible on the device and their values should not be modified until the kernel is done.
Note that since cudaFree() does an implicit cudaDeviceSynchronize() , cudaFree() can still be called on alpha and/or beta just after the call but it would defeat the purpose of using this pointer mode in that case.
For the functions of the second category, when the pointer mode is set to CUBLAS_POINTER_MODE_HOST , these functions block the CPU, until the GPU has completed its computation and the results have been copied back to the Host.
When the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE , these functions return immediately.
In this case, similar to matrix and vector results, the scalar result is ready only when execution of the routine on the GPU has completed.
In either case, the pointer mode CUBLAS_POINTER_MODE_DEVICE allows the library functions to execute completely asynchronously from the Host even when alpha and/or beta are generated by a previous kernel.
For example, this situation can arise when iterative methods for solution of linear systems and eigenvalue problems are implemented using the cuBLAS library. 2.1.6. Parallelism with Streams  If the application uses the results computed by multiple independent tasks, CUDA™ streams can be used to overlap the computation performed in these tasks.
In order to achieve the overlap of computation between the tasks, the user should create CUDA™ streams using the function cudaStreamCreate() and set the stream to be used by each individual cuBLAS library routine by calling cublasSetStream() just before calling the actual cuBLAS routine.
Note that cublasSetStream() resets the user-provided workspace to the default workspace pool; see cublasSetWorkspace() .
Then, the computation performed in separate streams would be overlapped automatically when possible on the GPU.
This approach is especially useful when the computation performed by a single task is relatively small and is not enough to fill the GPU with work.
We recommend using the new cuBLAS API with scalar parameters and results passed by reference in the device memory to achieve maximum overlap of the computation when using streams.
A particular application of streams, batching of multiple small kernels, is described in the following section. 2.1.7. Batching Kernels  In this section, we explain how to use streams to batch the execution of small kernels.
For instance, suppose that we have an application where we need to make many small independent matrix-matrix multiplications with dense matrices.
It is clear that even with millions of small independent matrices we will not be able to achieve the same GFLOPS rate as with a one large matrix.
For example, a single \(n \times n\) large matrix-matrix multiplication performs \(n^{3}\) operations for \(n^{2}\) input size, while 1024 \(\frac{n}{32} \times \frac{n}{32}\) small matrix-matrix multiplications perform \(1024\left( \frac{n}{32}  ight)^{3} = \frac{n^{3}}{32}\) operations for the same input size.
However, it is also clear that we can achieve a significantly better performance with many small independent matrices compared with a single small matrix.
Hence, in order to batch the execution of independent kernels, we can run each of them in a separate stream.
In particular, in the above example we could create 1024 CUDA™ streams using the function cudaStreamCreate() , then preface each call to cublasgemm() with a call to cublasSetStream() with a different stream for each of the matrix-matrix multiplications (note that cublasSetStream() resets user-provided workspace to the default workspace pool, see cublasSetWorkspace() ).
This will ensure that when possible the different computations will be executed concurrently.
Although the user can create many streams, in practice it is not possible to have more than 32 concurrent kernels executing at the same time. 2.1.8. Cache Configuration  On some devices, L1 cache and shared memory use the same hardware resources.
The cache configuration can be set directly with the CUDA Runtime function cudaDeviceSetCacheConfig.
The cache configuration can also be set specifically for some functions using the routine cudaFuncSetCacheConfig.
Please refer to the CUDA Runtime API documentation for details about the cache configuration settings.
Because switching from one configuration to another can affect kernels concurrency, the cuBLAS Library does not set any cache configuration preference and relies on the current setting.
However, some cuBLAS routines, especially Level-3 routines, rely heavily on shared memory.
Thus the cache preference setting might affect adversely their performance. 2.1.9. Static Library Support  The cuBLAS Library is also delivered in a static form as libcublas_static.a on Linux.
The static cuBLAS library and all other static math libraries depend on a common thread abstraction layer library called libculibos.a .
For example, on Linux, to compile a small application using cuBLAS, against the dynamic library, the following command can be used: nvcc myCublasApp .
c - lcublas - o myCublasApp Whereas to compile against the static cuBLAS library, the following command must be used: nvcc myCublasApp .
c - lcublas_static - lculibos - o myCublasApp It is also possible to use the native Host C++ compiler.
Depending on the Host operating system, some additional libraries like pthread or dl might be needed on the linking line.
c - lcublas_static - lculibos - lcudart_static - lpthread - ldl - I / include - L / lib64 - o myCublasApp Note that in the latter case, the library cuda is not needed.
In the case of a system which does not have the CUDA driver installed, this allows the application to gracefully manage this issue and potentially run if a CPU-only path is available.
Starting with release 11.2, using the typed functions instead of the extension functions (cublas**Ex()) helps in reducing the binary size when linking to static cuBLAS Library. 2.1.10. GEMM Algorithms Numerical Behavior  Some GEMM algorithms split the computation along the dimension K to increase the GPU occupancy, especially when the dimension K is large compared to dimensions M and N.
When this type of algorithm is chosen by the cuBLAS heuristics or explicitly by the user, the results of each split is summed deterministically into the resulting matrix to get the final result.
For the routines cublasgemmEx and cublasGemmEx() , when the compute type is greater than the output type, the sum of the split chunks can potentially lead to some intermediate overflows thus producing a final resulting matrix with some overflows.
Those overflows might not have occurred if all the dot products had been accumulated in the compute type before being converted at the end in the output type.
This computation side-effect can be easily exposed when the computeType is CUDA_R_32F and Atype, Btype and Ctype are in CUDA_R_16F.
This behavior can be controlled using the compute precision mode CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION with cublasSetMathMode() 2.1.11.
Tensor Core Usage  Tensor cores were first introduced with Volta GPUs (compute capability 7.0 and above) and significantly accelerate matrix multiplications.
Starting with cuBLAS version 11.0.0, the library may automatically make use of Tensor Core capabilities wherever possible, unless they are explicitly disabled by selecting pedantic compute modes in cuBLAS (see cublasSetMathMode() , cublasMath_t ).
It should be noted that the library will pick a Tensor Core enabled implementation wherever it determines that it would provide the best performance.
The best performance when using Tensor Cores can be achieved when the matrix dimensions and pointers meet certain memory alignment requirements.
Specifically, all of the following conditions must be satisfied to get the most performance out of Tensor Cores: ((op_A == CUBLAS_OP_N ? m : k) * AtypeSize) % 16 == 0 ((op_B == CUBLAS_OP_N ? k : n) * BtypeSize) % 16 == 0 (m * CtypeSize) % 16 == 0 (lda * AtypeSize) % 16 == 0 (ldb * BtypeSize) % 16 == 0 (ldc * CtypeSize) % 16 == 0 intptr_t(A) % 16 == 0 intptr_t(B) % 16 == 0 intptr_t(C) % 16 == 0 To conduct matrix multiplication with FP8 types (see 8-bit Floating Point Data Types (FP8) Usage ), you must ensure that your matrix dimensions and pointers meet the optimal requirements listed above.
Aside from FP8, there are no longer any restrictions on matrix dimensions and memory alignments to use Tensor Cores (starting with cuBLAS version 11.0.0). 2.1.12. CUDA Graphs Support  cuBLAS routines can be captured in CUDA Graph stream capture without restrictions in most situations.
cublasdot while pointer mode CUBLAS_POINTER_MODE_HOST is configured), as it enforces synchronization.
For input coefficients (such as alpha , beta ) behavior depends on the pointer mode setting: In the case of CUBLAS(LT)_POINTER_MODE_HOST , coefficient values are captured in the graph.
In the case of pointer modes with device pointers, coefficient value is accessed using the device pointer at the time of graph execution.
Note When captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync .
However, as there is currently no support for memory nodes in child graphs or graphs launched from the device , attempts to capture cuBLAS routines in such scenarios may fail.
To avoid this issue, use the cublasSetWorkspace() function to provide user-owned workspace memory. 2.1.13. 64-bit Integer Interface  cuBLAS version 12 introduced 64-bit integer capable functions.
Each 64-bit integer function is equivalent to a 32-bit integer function with the following changes: The function name has _64 suffix.
For instance, cublasSetMathMode() doesn’t have any arguments that could meaningfully be int64_t .
For documentation brevity, the 64-bit integer APIs are not explicitly listed, but only mentioned that they exist for the relevant functions. 2.2. cuBLAS Datatypes Reference  2.2.1.
cublasHandle_t  The cublasHandle_t type is a pointer type to an opaque structure holding the cuBLAS library context.
The cuBLAS library context must be initialized using cublasCreate() and the returned handle must be passed to all subsequent library function calls.
The context should be destroyed at the end using cublasDestroy() . 2.2.2. cublasStatus_t  The type is used for function status returns.
This is usually caused by the lack of a prior cublasCreate() call, an error in the CUDA Runtime API called by the cuBLAS routine, or an error in the hardware setup.
To correct: call cublasCreate() before the function call; and check that the hardware, an appropriate version of the driver, and the cuBLAS library are correctly installed.
To correct: prior to the function call, deallocate previously allocated memory as much as possible.
CUBLAS_STATUS_INVALID_VALUE An unsupported value or parameter was passed to the function (a negative vector size, for example).
CUBLAS_STATUS_ARCH_MISMATCH The function requires a feature absent from the device architecture; usually caused by compute capability lower than 5.0.
To correct: compile and run the application on a device with appropriate compute capability.
CUBLAS_STATUS_MAPPING_ERROR An access to GPU memory space failed, which is usually caused by a failure to bind a texture.
This is often caused by a launch failure of the kernel on the GPU, which can be caused by multiple reasons.
To correct: check that the hardware, an appropriate version of the driver, and the cuBLAS library are correctly installed.
Also, check that the memory passed as a parameter to the routine is not being deallocated prior to the routine’s completion.
CUBLAS_STATUS_LICENSE_ERROR The functionality requested requires some license and an error was detected when trying to check the current licensing.
This error can happen if the license is not present or is expired or if the environment variable NVIDIA_LICENSE_FILE is not set properly. 2.2.3. cublasOperation_t  The cublasOperation_t type indicates which operation needs to be performed with the dense matrix.
Its values correspond to Fortran characters ‘N’ or ‘n’ (non-transpose), ‘T’ or ‘t’ (transpose) and ‘C’ or ‘c’ (conjugate transpose) that are often used as parameters to legacy BLAS implementations.
CUBLAS_OP_C The conjugate transpose operation is selected. 2.2.4. cublasFillMode_t  The type indicates which part (lower or upper) of the dense matrix was filled and consequently should be used by the function.
Its values correspond to Fortran characters L or l (lower) and U or u (upper) that are often used as parameters to legacy BLAS implementations.
CUBLAS_FILL_MODE_FULL The full matrix is filled. 2.2.5. cublasDiagType_t  The type indicates whether the main diagonal of the dense matrix is unity and consequently should not be touched or modified by the function.
Its values correspond to Fortran characters ‘N’ or ‘n’ (non-unit) and ‘U’ or ‘u’ (unit) that are often used as parameters to legacy BLAS implementations.
CUBLAS_DIAG_UNIT The matrix diagonal has unit elements. 2.2.6. cublasSideMode_t  The type indicates whether the dense matrix is on the left or right side in the matrix equation solved by a particular function.
Its values correspond to Fortran characters ‘L’ or ‘l’ (left) and ‘R’ or ‘r’ (right) that are often used as parameters to legacy BLAS implementations.
CUBLAS_SIDE_RIGHT The matrix is on the right side in the equation. 2.2.7. cublasPointerMode_t  The cublasPointerMode_t type indicates whether the scalar values are passed by reference on the host or device.
It is important to point out that if several scalar values are present in the function call, all of them must conform to the same single pointer mode.
The pointer mode can be set and retrieved using cublasSetPointerMode() and cublasGetPointerMode() routines, respectively.
CUBLAS_POINTER_MODE_DEVICE The scalars are passed by reference on the device. 2.2.8. cublasAtomicsMode_t  The type indicates whether cuBLAS routines which has an alternate implementation using atomics can be used.
The atomics mode can be set and queried using cublasSetAtomicsMode() and cublasGetAtomicsMode() and routines, respectively.
CUBLAS_ATOMICS_ALLOWED The usage of atomics is allowed. 2.2.9. cublasGemmAlgo_t  cublasGemmAlgo_t type is an enumerant to specify the algorithm for matrix-matrix multiplication on GPU architectures up to sm_75 .
cuBLAS has the following algorithm options: Value Meaning CUBLAS_GEMM_DEFAULT Apply Heuristics to select the GEMM algorithm CUBLAS_GEMM_ALGO0 to CUBLAS_GEMM_ALGO23 Explicitly choose an Algorithm [0,23].
CUBLAS_GEMM_DEFAULT_TENSOR_OP [DEPRECATED] This mode is deprecated and will be removed in a future release.
Apply Heuristics to select the GEMM algorithm, while allowing use of reduced precision CUBLAS_COMPUTE_32F_FAST_16F kernels (for backward compatibility).
CUBLAS_GEMM_ALGO0_TENSOR_OP to CUBLAS_GEMM_ALGO15_TENSOR_OP [DEPRECATED] Those values are deprecated and will be removed in a future release.
Allows use of reduced precision CUBLAS_COMPUTE_32F_FAST_16F kernels (for backward compatibility). 2.2.10. cublasMath_t  cublasMath_t enumerate type is used in cublasSetMathMode() to choose compute precision modes as defined in the following table.
Since this setting does not directly control the use of Tensor Cores, the mode CUBLAS_TENSOR_OP_MATH is being deprecated, and will be removed in a future release.
Value Meaning CUBLAS_DEFAULT_MATH This is the default and highest-performance mode that uses compute and intermediate storage precisions with at least the same number of mantissa and exponent bits as requested.
CUBLAS_PEDANTIC_MATH This mode uses the prescribed precision and standardized arithmetic for all phases of calculations and is primarily intended for numerical robustness studies, testing, and debugging.
CUBLAS_TF32_TENSOR_OP_MATH Enable acceleration of single-precision routines using TF32 tensor cores.
CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION Forces any reductions during matrix multiplications to use the accumulator type (that is, compute type) and not the output type in case of mixed precision routines where output type precision is less than the compute type precision.
This is a flag that can be set (using a bitwise or operation) alongside any of the other values.
CUBLAS_TENSOR_OP_MATH [DEPRECATED] This mode is deprecated and will be removed in a future release.
For single precision GEMM routines cuBLAS will use the CUBLAS_COMPUTE_32F_FAST_16F compute type. 2.2.11. cublasComputeType_t  cublasComputeType_t enumerate type is used in cublasGemmEx() and cublasLtMatmul() (including all batched and strided batched variants) to choose compute precision modes as defined below.
Value Meaning CUBLAS_COMPUTE_16F This is the default and highest-performance mode for 16-bit half precision floating point and all compute and intermediate storage precisions with at least 16-bit half precision.
CUBLAS_COMPUTE_16F_PEDANTIC This mode uses 16-bit half precision floating point standardized arithmetic for all phases of calculations and is primarily intended for numerical robustness studies, testing, and debugging.
This mode might not be as performant as the other modes since it disables use of tensor cores.
CUBLAS_COMPUTE_32F This is the default 32-bit single precision floating point and uses compute and intermediate storage precisions of at least 32-bits.
CUBLAS_COMPUTE_32F_PEDANTIC Uses 32-bit single precision floatin point arithmetic for all phases of calculations and also disables algorithmic optimizations such as Gaussian complexity reduction (3M).
CUBLAS_COMPUTE_32F_FAST_16F Allows the library to use Tensor Cores with automatic down-conversion and 16-bit half-precision compute for 32-bit input and output matrices.
CUBLAS_COMPUTE_32F_FAST_16BF Allows the library to use Tensor Cores with automatic down-convesion and bfloat16 compute for 32-bit input and output matrices.
CUBLAS_COMPUTE_32F_FAST_TF32 Allows the library to use Tensor Cores with TF32 compute for 32-bit input and output matrices.
CUBLAS_COMPUTE_64F This is the default 64-bit double precision floating point and uses compute and intermediate storage precisions of at least 64-bits.
CUBLAS_COMPUTE_64F_PEDANTIC Uses 64-bit double precision floatin point arithmetic for all phases of calculations and also disables algorithmic optimizations such as Gaussian complexity reduction (3M).
CUBLAS_COMPUTE_32I This is the default 32-bit integer mode and uses compute and intermediate storage precisions of at least 32-bits.
CUBLAS_COMPUTE_32I_PEDANTIC Uses 32-bit integer arithmetic for all phases of calculations.
Note Setting the environment variable NVIDIA_TF32_OVERRIDE = 0 will override any defaults or programmatic configuration of NVIDIA libraries, and consequently, cuBLAS will not accelerate FP32 computations with TF32 tensor cores. 2.3. CUDA Datatypes Reference  The chapter describes types shared by multiple CUDA Libraries and defined in the header file library_types.h .
2.3.1. cudaDataType_t  The cudaDataType_t type is an enumerant to specify the data precision.
It is used when the data reference does not carry the type itself (e.g void *) For example, it is used in the routine cublasSgemmEx() .
Value Meaning CUDA_R_16F the data type is a 16-bit real half precision floating-point CUDA_C_16F the data type is a 32-bit structure comprised of two half precision floating-points representing a complex number.
CUDA_R_16BF the data type is a 16-bit real bfloat16 floating-point CUDA_C_16BF the data type is a 32-bit structure comprised of two bfloat16 floating-points representing a complex number.
CUDA_R_32F the data type is a 32-bit real single precision floating-point CUDA_C_32F the data type is a 64-bit structure comprised of two single precision floating-points representing a complex number.
CUDA_R_64F the data type is a 64-bit real double precision floating-point CUDA_C_64F the data type is a 128-bit structure comprised of two double precision floating-points representing a complex number.
CUDA_R_8I the data type is a 8-bit real signed integer CUDA_C_8I the data type is a 16-bit structure comprised of two 8-bit signed integers representing a complex number.
CUDA_R_8U the data type is a 8-bit real unsigned integer CUDA_C_8U the data type is a 16-bit structure comprised of two 8-bit unsigned integers representing a complex number.
CUDA_R_32I the data type is a 32-bit real signed integer CUDA_C_32I the data type is a 64-bit structure comprised of two 32-bit signed integers representing a complex number.
CUDA_R_8F_E4M3 the data type is an 8-bit real floating point in E4M3 format CUDA_R_8F_E5M2 the data type is an 8-bit real floating point in E5M2 format 2.3.2.
libraryPropertyType_t  The libraryPropertyType_t is used as a parameter to specify which property is requested when using the routine cublasGetProperty() Value Meaning MAJOR_VERSION enumerant to query the major version MINOR_VERSION enumerant to query the minor version PATCH_LEVEL number to identify the patch level 2.4.
cublasCreate()  cublasStatus_t cublasCreate ( cublasHandle_t * handle ) This function initializes the cuBLAS library and creates a handle to an opaque structure holding the cuBLAS library context.
It allocates hardware resources on the host and device and must be called prior to making any other cuBLAS library calls.
To use the library on multiple devices, one cuBLAS handle needs to be created for each device.
Furthermore, for a given device, multiple cuBLAS handles with different configurations can be created.
Because cublasCreate() allocates some internal resources and the release of those resources by calling cublasDestroy() will implicitly call cudaDeviceSynchronize() , it is recommended to minimize the number of times these functions are called.
For multi-threaded applications that use the same device from different threads, the recommended programming model is to create one cuBLAS handle per thread and use that cuBLAS handle for the entire life of the thread.
Return Value Meaning CUBLAS_STATUS_SUCCESS the initialization succeeded CUBLAS_STATUS_NOT_INITIALIZED the CUDA™ Runtime initialization failed CUBLAS_STATUS_ALLOC_FAILED the resources could not be allocated CUBLAS_STATUS_INVALID_VALUE handle == NULL 2.4.2.
cublasDestroy()  cublasStatus_t cublasDestroy ( cublasHandle_t handle ) This function releases hardware resources used by the cuBLAS library.
Return Value Meaning CUBLAS_STATUS_SUCCESS the shut down succeeded CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 2.4.3.
cublasGetVersion()  cublasStatus_t cublasGetVersion ( cublasHandle_t handle , int * version ) This function returns the version number of the cuBLAS library.
Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the provided storage for library version number is not initialized (NULL) Note This function can be safely called with the handle set to NULL.
Another way to do this is with cublasGetProperty() . 2.4.4. cublasGetProperty()  cublasStatus_t cublasGetProperty ( libraryPropertyType type , int * value ) This function returns the value of the requested property in memory pointed to by value.
Return Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully CUBLAS_STATUS_INVALID_VALUE Invalid type value If invalid type value or value == NULL 2.4.5.
cublasGetStatusName()  const char * cublasGetStatusName ( cublasStatus_t status ) This function returns the string representation of a given status.
Return Value Meaning NULL-terminated string The string representation of the status 2.4.6.
cublasGetStatusString()  const char * cublasGetStatusString ( cublasStatus_t status ) This function returns the description string for a given status.
cublasSetStream()  cublasStatus_t cublasSetStream ( cublasHandle_t handle , cudaStream_t streamId ) This function sets the cuBLAS library stream, which will be used to execute all subsequent calls to the cuBLAS library functions.
In particular, this routine can be used to change the stream between kernel launches and then to reset the cuBLAS library stream back to NULL .
Additionally this function unconditionally resets the cuBLAS library workspace back to the default workspace pool (see cublasSetWorkspace() ).
Return Value Meaning CUBLAS_STATUS_SUCCESS the stream was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 2.4.8.
cublasSetWorkspace()  cublasStatus_t cublasSetWorkspace ( cublasHandle_t handle , void * workspace , size_t workspaceSizeInBytes ) This function sets the cuBLAS library workspace to a user-owned device buffer, which will be used to execute all subsequent calls to the cuBLAS library functions (on the currently set stream).
If the cuBLAS library workspace is not set, all kernels will use the default workspace pool allocated during the cuBLAS context creation.
The workspace pointer has to be aligned to at least 256 bytes, otherwise CUBLAS_STATUS_INVALID_VALUE error is returned.
The cublasSetStream() function unconditionally resets the cuBLAS library workspace back to the default workspace pool.
Calling this function, including with workspaceSizeInBytes equal to 0, will prevent the cuBLAS library from utilizing the default workspace.
Too small workspaceSizeInBytes may cause some routines to fail with CUBLAS_STATUS_ALLOC_FAILED error returned or cause large regressions in performance.
Workspace size equal to or larger than 16KiB is enough to prevent CUBLAS_STATUS_ALLOC_FAILED error, while a larger workspace can provide performance benefits for some routines.
Note If the stream set by cublasSetStream() is cudaStreamPerThread and there are multiple threads using the same cuBLAS library handle, then users must manually manage synchronization to avoid possible race conditions in the user provided workspace.
Alternatively, users may rely on the default workspace pool which safely guards against race conditions.
This is based on the cuBLAS default workspace pool size which is GPU architecture dependent.
GPU Architecture Recommended workspace size NVIDIA Hopper Architecture 32 MiB Other 4 MiB The possible error values returned by this function and their meanings are listed below.
Return Value Meaning CUBLAS_STATUS_SUCCESS the stream was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the workspace pointer wasn’t aligned to at least 256 bytes 2.4.9.
cublasGetStream()  cublasStatus_t cublasGetStream ( cublasHandle_t handle , cudaStream_t * streamId ) This function gets the cuBLAS library stream, which is being used to execute all calls to the cuBLAS library functions.
Return Value Meaning CUBLAS_STATUS_SUCCESS the stream was returned successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE streamId == NULL 2.4.10.
cublasGetPointerMode()  cublasStatus_t cublasGetPointerMode ( cublasHandle_t handle , cublasPointerMode_t * mode ) This function obtains the pointer mode used by the cuBLAS library.
Return Value Meaning CUBLAS_STATUS_SUCCESS the pointer mode was obtained successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE mode == NULL 2.4.11.
cublasSetPointerMode()  cublasStatus_t cublasSetPointerMode ( cublasHandle_t handle , cublasPointerMode_t mode ) This function sets the pointer mode used by the cuBLAS library.
Return Value Meaning CUBLAS_STATUS_SUCCESS the pointer mode was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE mode is not CUBLAS_POINTER_MODE_HOST or CUBLAS_POINTER_MODE_DEVICE 2.4.12.
cublasSetVector()  cublasStatus_t cublasSetVector ( int n , int elemSize , const void * x , int incx , void * y , int incy ) This function supports the 64-bit Integer Interface .
This function copies n elements from a vector x in host memory space to a vector y in GPU memory space.
The storage spacing between consecutive elements is given by incx for the source vector x and by incy for the destination vector y .
Since column-major format for two-dimensional matrices is assumed, if a vector is part of a matrix, a vector increment equal to 1 accesses a (partial) column of that matrix.
Similarly, using an increment equal to the leading dimension of the matrix results in accesses to a (partial) row of that matrix.
Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters incx , incy , elemSizesymv and cublashemv have an alternate implementation that use atomics to cumulate results.
This implementation is generally significantly faster but can generate results that are not strictly identical from one run to the others.
Mathematically, those different results are not significant but when debugging those differences can be prejudicial.
This function allows or disallows the usage of atomics in the cuBLAS library for all routines which have an alternate implementation.
When not explicitly specified in the documentation of any cuBLAS routine, it means that this routine does not have an alternate implementation that use atomics.
When atomics mode is disabled, each cuBLAS routine should produce the same results from one run to the other when called with identical parameters on the same Hardware.
The default atomics mode of default initialized cublasHandle_t object is CUBLAS_ATOMICS_NOT_ALLOWED .
Return Value Meaning CUBLAS_STATUS_SUCCESS the atomics mode was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 2.4.21.
cublasGetAtomicsMode()  cublasStatus_t cublasGetAtomicsMode ( cublasHandle_t handle , cublasAtomicsMode_t * mode ) This function queries the atomic mode of a specific cuBLAS context.
Return Value Meaning CUBLAS_STATUS_SUCCESS the atomics mode was queried successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the argument mode is a NULL pointer 2.4.22.
cublasSetMathMode()  cublasStatus_t cublasSetMathMode ( cublasHandle_t handle , cublasMath_t mode ) The cublasSetMathMode() function enables you to choose the compute precision modes as defined by cublasMath_t .
Users are allowed to set the compute precision mode as a logical combination of them (except the deprecated CUBLAS_TENSOR_OP_MATH ).
For example, cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH | CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION) .
For matrix and compute precisions allowed for cublasGemmEx() and cublasLtMatmul() APIs and their strided variants please refer to: cublasGemmEx() , cublasGemmBatchedEx() , cublasGemmStridedBatchedEx() , and cublasLtMatmul() .
CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized. 2.4.23. cublasGetMathMode()  cublasStatus_t cublasGetMathMode ( cublasHandle_t handle , cublasMath_t * mode ) This function returns the math mode used by the library routines.
CUBLAS_STATUS_INVALID_VALUE if mode is NULL. 2.4.24. cublasSetSmCountTarget()  cublasStatus_t cublasSetSmCountTarget ( cublasHandle_t handle , int smCountTarget ) The cublasSetSmCountTarget() function allows overriding the number of multiprocessors available to the library during kernels execution.
This option can be used to improve the library performance when cuBLAS routines are known to run concurrently with other work on different CUDA streams.
a NVIDIA A100 GPU has 108 SM and there is a concurrent kenrel running with grid size of 8, one can use cublasSetSmCountTarget() with value 100 to override the library heuristics to optimize for running on 100 multiprocessors.
The input value should not exceed the device’s multiprocessor count, which can be obtained using cudaDeviceGetAttribute .
The user must ensure thread safety when modifying the library handle with this routine similar to when using cublasSetStream() , etc.
CUBLAS_STATUS_INVALID_VALUE the value of smCountTarget outside of the allowed range. 2.4.25. cublasGetSmCountTarget()  cublasStatus_t cublasGetSmCountTarget ( cublasHandle_t handle , int * smCountTarget ) This function obtains the value previously programmed to the library handle.
CUBLAS_STATUS_INVALID_VALUE smCountTarget is NULL. 2.4.26. cublasLoggerConfigure()  cublasStatus_t cublasLoggerConfigure ( int logIsOn , int logToStdOut , int logToStdErr , const char * logFileName ) This function configures logging during runtime.
Besides this type of configuration, it is possible to configure logging with special environment variables which will be checked by libcublas: CUBLAS_LOGINFO_DBG - Setup env.
By default is off, but is turned on by calling cublasSetLoggerCallback() to user defined callback function.
Returns CUBLAS_STATUS_SUCCESS Success. 2.4.27. cublasGetLoggerCallback()  cublasStatus_t cublasGetLoggerCallback ( cublasLogCallback * userCallback ) This function retrieves function pointer to previously installed custom user defined callback function via cublasSetLoggerCallback() or zero otherwise.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE userCallback is NULL 2.4.28.
cublasSetLoggerCallback()  cublasStatus_t cublasSetLoggerCallback ( cublasLogCallback userCallback ) This function installs a custom user defined callback function via cublas C public API.
Parameters userCallback Input . 2.5. cuBLAS Level-1 Function Reference  In this chapter we describe the Level-1 Basic Linear Algebra Subprograms (BLAS1) functions that perform scalar and vector based operations.
We will use abbreviations for type and for the corresponding short type to make a more concise and clear presentation of the implemented functions.
Unless otherwise specified and have the following meanings: Meaning float ‘s’ or ‘S’ real single-precision double ‘d’ or ‘D’ real double-precision cuComplex ‘c’ or ‘C’ complex single-precision cuDoubleComplex ‘z’ or ‘Z’ complex double-precision When the parameters and returned values of the function differ, which sometimes happens for complex input, the can also have the following meanings Sc , Cs , Dz and Zd .
The abbreviation \(\mathbf{Re}(\cdot)\) and \(\mathbf{Im}(\cdot)\) will stand for the real and imaginary part of a number, respectively.
Since imaginary part of a real number does not exist, we will consider it to be zero and can usually simply discard it from the equation where it is being used.
In general throughout the documentation, the lower case Greek symbols \(\alpha\) and \(\beta\) will denote scalars, lower case English letters in bold type \(\mathbf{x}\) and \(\mathbf{y}\) will denote vectors and capital English letters \(A\) , \(B\) and \(C\) will denote matrices. 2.5.1. cublasIamax()  cublasStatus_t cublasIsamax ( cublasHandle_t handle , int n , const float * x , int incx , int * result ) cublasStatus_t cublasIdamax ( cublasHandle_t handle , int n , const double * x , int incx , int * result ) cublasStatus_t cublasIcamax ( cublasHandle_t handle , int n , const cuComplex * x , int incx , int * result ) cublasStatus_t cublasIzamax ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , int * result ) This function supports the 64-bit Integer Interface .
Hence, the result is the first \(i\) such that \(\left| \mathbf{Im}\left( {x\lbrack j brack}  ight) \middle| + \middle| \mathbf{Re}\left( {x\lbrack j brack}  ight)  ight|\) is maximum for \(i = 1,\ldots,n\) and \(j = 1 + \left( {i - 1}  ight)*\text{ incx}\) .
Notice that the last equation reflects 1-based indexing used for compatibility with Fortran.
result host or device output the resulting index, which is 0 if n,incxamin()  cublasStatus_t cublasIsamin ( cublasHandle_t handle , int n , const float * x , int incx , int * result ) cublasStatus_t cublasIdamin ( cublasHandle_t handle , int n , const double * x , int incx , int * result ) cublasStatus_t cublasIcamin ( cublasHandle_t handle , int n , const cuComplex * x , int incx , int * result ) cublasStatus_t cublasIzamin ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , int * result ) This function supports the 64-bit Integer Interface .
Hence, the result is the first \(i\) such that \(\left| \mathbf{Im}\left( {x\lbrack j brack}  ight) \middle| + \middle| \mathbf{Re}\left( {x\lbrack j brack}  ight)  ight|\) is minimum for \(i = 1,\ldots,n\) and \(j = 1 + \left( {i - 1}  ight)*\text{incx}\) Notice that the last equation reflects 1-based indexing used for compatibility with Fortran.
result host or device output the resulting index, which is 0 if n,incxasum()  cublasStatus_t cublasSasum ( cublasHandle_t handle , int n , const float * x , int incx , float * result ) cublasStatus_t cublasDasum ( cublasHandle_t handle , int n , const double * x , int incx , double * result ) cublasStatus_t cublasScasum ( cublasHandle_t handle , int n , const cuComplex * x , int incx , float * result ) cublasStatus_t cublasDzasum ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , double * result ) This function supports the 64-bit Integer Interface .
\sum_{i = 1}^{n} \middle| \mathbf{Im}\left( {x\lbrack j brack}  ight) \middle| + \middle| \mathbf{Re}\left( {x\lbrack j brack}  ight)  ight|\) where \(j = 1 + \left( {i - 1}  ight)*\text{incx}\) .
result host or device output the resulting index, which is 0.0 if n,incxaxpy()  cublasStatus_t cublasSaxpy ( cublasHandle_t handle , int n , const float * alpha , const float * x , int incx , float * y , int incy ) cublasStatus_t cublasDaxpy ( cublasHandle_t handle , int n , const double * alpha , const double * x , int incx , double * y , int incy ) cublasStatus_t cublasCaxpy ( cublasHandle_t handle , int n , const cuComplex * alpha , const cuComplex * x , int incx , cuComplex * y , int incy ) cublasStatus_t cublasZaxpy ( cublasHandle_t handle , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface .
This function multiplies the vector x by the scalar \(\alpha\) and adds it to the vector y overwriting the latest vector with the result.
Hence, the performed operation is \(\mathbf{y}\lbrack j brack = \alpha \times \mathbf{x}\lbrack k brack + \mathbf{y}\lbrack j brack\) for \(i = 1,\ldots,n\) , \(k = 1 + \left( {i - 1}  ight)*\text{incx}\) and \(j = 1 + \left( {i - 1}  ight)*\text{incy}\) .
Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: saxpy , daxpy , caxpy , zaxpy 2.5.5.
cublascopy()  cublasStatus_t cublasScopy ( cublasHandle_t handle , int n , const float * x , int incx , float * y , int incy ) cublasStatus_t cublasDcopy ( cublasHandle_t handle , int n , const double * x , int incx , double * y , int incy ) cublasStatus_t cublasCcopy ( cublasHandle_t handle , int n , const cuComplex * x , int incx , cuComplex * y , int incy ) cublasStatus_t cublasZcopy ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface .
Hence, the performed operation is \(\mathbf{y}\lbrack j brack = \mathbf{x}\lbrack k brack\) for \(i = 1,\ldots,n\) , \(k = 1 + \left( {i - 1}  ight)*\text{incx}\) and \(j = 1 + \left( {i - 1}  ight)*\text{incy}\) .
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: scopy , dcopy , ccopy , zcopy 2.5.6.
Hence, the result is \(\sum_{i = 1}^{n}\left( {\mathbf{x}\lbrack k brack \times \mathbf{y}\lbrack j brack}  ight)\) where \(k = 1 + \left( {i - 1}  ight)*\text{incx}\) and \(j = 1 + \left( {i - 1}  ight)*\text{incy}\) .
Notice that in the first equation the conjugate of the element of vector x should be used if the function name ends in character ‘c’ and that the last two equations reflect 1-based indexing used for compatibility with Fortran.
result host or device output the resulting dot product, which is 0.0 if nnrm2()  cublasStatus_t cublasSnrm2 ( cublasHandle_t handle , int n , const float * x , int incx , float * result ) cublasStatus_t cublasDnrm2 ( cublasHandle_t handle , int n , const double * x , int incx , double * result ) cublasStatus_t cublasScnrm2 ( cublasHandle_t handle , int n , const cuComplex * x , int incx , float * result ) cublasStatus_t cublasDznrm2 ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , double * result ) This function supports the 64-bit Integer Interface .
The code uses a multiphase model of accumulation to avoid intermediate underflow and overflow, with the result being equivalent to \(\sqrt{\sum_{i = 1}^{n}\left( {\mathbf{x}\lbrack j brack \times \mathbf{x}\lbrack j brack}  ight)}\) where \(j = 1 + \left( {i - 1}  ight)*\text{incx}\) in exact arithmetic.
This function applies Givens rotation matrix (i.e., rotation in the x,y plane counter-clockwise by angle defined by cos(alpha)=c, sin(alpha)=s): \(G = \begin{pmatrix} c & s \\ {- s} & c \\ \end{pmatrix}\) to vectors x and y .
Hence, the result is \(\mathbf{x}\lbrack k brack = c \times \mathbf{x}\lbrack k brack + s \times \mathbf{y}\lbrack j brack\) and \(\mathbf{y}\lbrack j brack = - s \times \mathbf{x}\lbrack k brack + c \times \mathbf{y}\lbrack j brack\) where \(k = 1 + \left( {i - 1}  ight)*\text{incx}\) and \(j = 1 + \left( {i - 1}  ight)*\text{incy}\) .
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srot , drot , crot , csrot , zrot , zdrot 2.5.9.
cublasrotg()  cublasStatus_t cublasSrotg ( cublasHandle_t handle , float * a , float * b , float * c , float * s ) cublasStatus_t cublasDrotg ( cublasHandle_t handle , double * a , double * b , double * c , double * s ) cublasStatus_t cublasCrotg ( cublasHandle_t handle , cuComplex * a , cuComplex * b , float * c , cuComplex * s ) cublasStatus_t cublasZrotg ( cublasHandle_t handle , cuDoubleComplex * a , cuDoubleComplex * b , double * c , cuDoubleComplex * s ) This function supports the 64-bit Integer Interface .
This function constructs the Givens rotation matrix \(G = \begin{pmatrix} c & s \\ {- s} & c \\ \end{pmatrix}\) that zeros out the second entry of a \(2 \times 1\) vector \(\left( {a,b}  ight)^{T}\) .
Then, for real numbers we can write \(\begin{pmatrix} c & s \\ {- s} & c \\ \end{pmatrix}\begin{pmatrix} a \\ b \\ \end{pmatrix} = \begin{pmatrix} r \\ 0 \\ \end{pmatrix}\) where \(c^{2} + s^{2} = 1\) and \(r = a^{2} + b^{2}\) .
The value of \(z\) is such that \(c\) and \(s\) may be recovered using the following rules: \(\left( {c,s}  ight) = \begin{cases} \left( {\sqrt{1 - z^{2}},z}  ight) & {\text{ if }\left| z \middle| 1  ight.} \\ \end{cases}\) For complex numbers we can write \(\begin{pmatrix} c & s \\ {- \bar{s}} & c \\ \end{pmatrix}\begin{pmatrix} a \\ b \\ \end{pmatrix} = \begin{pmatrix} r \\ 0 \\ \end{pmatrix}\) where \(c^{2} + \left( {\bar{s} \times s}  ight) = 1\) and \(r = \frac{a}{|a|} \times \parallel \left( {a,b}  ight)^{T} \parallel_{2}\) with \(\parallel \left( {a,b}  ight)^{T} \parallel_{2} = \sqrt{\left| a|^{2} + \middle| b|^{2}  ight.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srotg , drotg , crotg , zrotg 2.5.10.
cublasrotm()  cublasStatus_t cublasSrotm ( cublasHandle_t handle , int n , float * x , int incx , float * y , int incy , const float * param ) cublasStatus_t cublasDrotm ( cublasHandle_t handle , int n , double * x , int incx , double * y , int incy , const double * param ) This function supports the 64-bit Integer Interface .
This function applies the modified Givens transformation \(H = \begin{pmatrix} h_{11} & h_{12} \\ h_{21} & h_{22} \\ \end{pmatrix}\) to vectors x and y .
Hence, the result is \(\mathbf{x}\lbrack k brack = h_{11} \times \mathbf{x}\lbrack k brack + h_{12} \times \mathbf{y}\lbrack j brack\) and \(\mathbf{y}\lbrack j brack = h_{21} \times \mathbf{x}\lbrack k brack + h_{22} \times \mathbf{y}\lbrack j brack\) where \(k = 1 + \left( {i - 1}  ight)*\text{incx}\) and \(j = 1 + \left( {i - 1}  ight)*\text{incy}\) .
The elements , , and of matrix \(H\) are stored in param[1] , param[2] , param[3] and param[4] , respectively.
The flag=param[0] defines the following predefined values for the matrix \(H\) entries flag=-1.0 flag= 0.0 flag= 1.0 flag=-2.0 \(\begin{pmatrix} h_{11} & h_{12} \\ h_{21} & h_{22} \\ \end{pmatrix}\) \(\begin{pmatrix} {1.0} & h_{12} \\ h_{21} & {1.0} \\ \end{pmatrix}\) \(\begin{pmatrix} h_{11} & {1.0} \\ {- 1.0} & h_{22} \\ \end{pmatrix}\) \(\begin{pmatrix} {1.0} & {0.0} \\ {0.0} & {1.0} \\ \end{pmatrix}\) Notice that the values -1.0, 0.0 and 1.0 implied by the flag are not stored in param.
param host or device input vector of 5 elements, where param[0] and param[1-4] contain the flag and matrix \(H\) .
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srotm , drotm 2.5.11.
cublasrotmg()  cublasStatus_t cublasSrotmg ( cublasHandle_t handle , float * d1 , float * d2 , float * x1 , const float * y1 , float * param ) cublasStatus_t cublasDrotmg ( cublasHandle_t handle , double * d1 , double * d2 , double * x1 , const double * y1 , double * param ) This function supports the 64-bit Integer Interface .
This function constructs the modified Givens transformation \(H = \begin{pmatrix} h_{11} & h_{12} \\ h_{21} & h_{22} \\ \end{pmatrix}\) that zeros out the second entry of a \(2 \times 1\) vector \(\left( {\sqrt{d1}*x1,\sqrt{d2}*y1}  ight)^{T}\) .
param host or device output vector of 5 elements, where param[0] and param[1-4] contain the flag and matrix \(H\) .
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srotmg , drotmg 2.5.12.
cublasscal()  cublasStatus_t cublasSscal ( cublasHandle_t handle , int n , const float * alpha , float * x , int incx ) cublasStatus_t cublasDscal ( cublasHandle_t handle , int n , const double * alpha , double * x , int incx ) cublasStatus_t cublasCscal ( cublasHandle_t handle , int n , const cuComplex * alpha , cuComplex * x , int incx ) cublasStatus_t cublasCsscal ( cublasHandle_t handle , int n , const float * alpha , cuComplex * x , int incx ) cublasStatus_t cublasZscal ( cublasHandle_t handle , int n , const cuDoubleComplex * alpha , cuDoubleComplex * x , int incx ) cublasStatus_t cublasZdscal ( cublasHandle_t handle , int n , const double * alpha , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface .
This function scales the vector x by the scalar \(\alpha\) and overwrites it with the result.
Hence, the performed operation is \(\mathbf{x}\lbrack j brack = \alpha \times \mathbf{x}\lbrack j brack\) for \(i = 1,\ldots,n\) and \(j = 1 + \left( {i - 1}  ight)*\text{incx}\) .
:class: table-no-stripes  Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sscal , dscal , csscal , cscal , zdscal , zscal 2.5.13.
cublasswap()  cublasStatus_t cublasSswap ( cublasHandle_t handle , int n , float * x , int incx , float * y , int incy ) cublasStatus_t cublasDswap ( cublasHandle_t handle , int n , double * x , int incx , double * y , int incy ) cublasStatus_t cublasCswap ( cublasHandle_t handle , int n , cuComplex * x , int incx , cuComplex * y , int incy ) cublasStatus_t cublasZswap ( cublasHandle_t handle , int n , cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface .
\mathbf{y}\lbrack j brack\Leftrightarrow\mathbf{x}\lbrack k brack  ight.\) for \(i = 1,\ldots,n\) , \(k = 1 + \left( {i - 1}  ight)*\text{incx}\) and \(j = 1 + \left( {i - 1}  ight)*\text{incy}\) .
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sswap , dswap , cswap , zswap 2.6.
This function performs the banded matrix-vector multiplication \(\mathbf{y} = \alpha\text{ op}(A)\mathbf{x} + \beta\mathbf{y}\) where \(A\) is a banded matrix with \(kl\) subdiagonals and \(ku\) superdiagonals, \(\mathbf{x}\) and \(\mathbf{y}\) are vectors, and \(\alpha\) and \(\beta\) are scalars.
Also, for matrix \(A\) \(\text{ op}(A) = \begin{cases} A & \text{ if transa == $\mathrm{CUBLAS\_OP\_N}$} \\ A^{T} & \text{ if transa == $\mathrm{CUBLAS\_OP\_T}$} \\ A^{H} & \text{ if transa == $\mathrm{CUBLAS\_OP\_C}$} \\ \end{cases}\) The banded matrix \(A\) is stored column by column, with the main diagonal stored in row \(ku + 1\) (starting in first position), the first superdiagonal stored in row \(ku\) (starting in second position), the first subdiagonal stored in row \(ku + 2\) (starting in first position), etc.
So that in general, the element \(A\left( {i,j}  ight)\) is stored in the memory location A(ku+1+i-j,j) for \(j = 1,\ldots,n\) and \(i \in \left\lbrack {\max\left( {1,j - ku}  ight),\min\left( {m,j + kl}  ight)}  ight brack\) .
Also, the elements in the array \(A\) that do not conceptually correspond to the elements in the banded matrix (the top left \(ku \times ku\) and bottom right \(kl \times kl\) triangles) are not referenced.
beta host or device input scalar used for multiplication, if beta == 0 then y does not have to be a valid input.
This function performs the matrix-vector multiplication \(\textbf{y} = \alpha\text{ op}(A)\textbf{x} + \beta\textbf{y}\) where \(A\) is a \(m \times n\) matrix stored in column-major format, \(\mathbf{x}\) and \(\mathbf{y}\) are vectors, and \(\alpha\) and \(\beta\) are scalars.
Also, for matrix \(A\) \(\text{ op}(A) = \begin{cases} A & \text{ if transa == $\mathrm{CUBLAS\_OP\_N}$} \\ A^{T} & \text{ if transa == $\mathrm{CUBLAS\_OP\_T}$} \\ A^{H} & \text{ if transa == $\mathrm{CUBLAS\_OP\_C}$} \\ \end{cases}\) Param.
Before entry, the leading m by n part of the array A must contain the matrix of coefficients.
x device input vector at least (1+(n-1)*abs(incx)) elements if transa==CUBLAS_OP_N and at least (1+(m-1)*abs(incx)) elements otherwise.
beta host or device input scalar used for multiplication, if beta==0 then y does not have to be a valid input.
y device in/out vector at least (1+(m-1)*abs(incy)) elements if transa==CUBLAS_OP_N and at least (1+(n-1)*abs(incy)) elements otherwise.
incy input stride between consecutive elements of y The possible error values returned by this function and their meanings are listed below.
This function performs the rank-1 update \(A = \begin{cases} {\alpha\mathbf{xy}^{T} + A} & \text{if ger(),geru() is called} \\ {\alpha\mathbf{xy}^{H} + A} & \text{if gerc() is called} \\ \end{cases}\) where \(A\) is a \(m \times n\) matrix stored in column-major format, \(\mathbf{x}\) and \(\mathbf{y}\) are vectors, and \(\alpha\) is a scalar.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m sbmv()  cublasStatus_t cublasSsbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const float * alpha , const float * A , int lda , const float * x , int incx , const float * beta , float * y , int incy ) cublasStatus_t cublasDsbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const double * alpha , const double * A , int lda , const double * x , int incx , const double * beta , double * y , int incy ) This function supports the 64-bit Integer Interface .
This function performs the symmetric banded matrix-vector multiplication \(\textbf{y} = \alpha A\textbf{x} + \beta\textbf{y}\) where \(A\) is a \(n \times n\) symmetric banded matrix with \(k\) subdiagonals and superdiagonals, \(\mathbf{x}\) and \(\mathbf{y}\) are vectors, and \(\alpha\) and \(\beta\) are scalars.
If uplo == CUBLAS_FILL_MODE_LOWER then the symmetric banded matrix \(A\) is stored column by column, with the main diagonal of the matrix stored in row 1, the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc.
So that in general, the element \(A(i,j)\) is stored in the memory location A(1+i-j,j) for \(j = 1,\ldots,n\) and \(i \in \lbrack j,\min(m,j + k) brack\) .
Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the bottom right \(k \times k\) triangle) are not referenced.
If uplo == CUBLAS_FILL_MODE_UPPER then the symmetric banded matrix \(A\) is stored column by column, with the main diagonal of the matrix stored in row k+1 , the first superdiagonal in row k (starting at second position), the second superdiagonal in row k-1 (starting at third position), etc.
So that in general, the element \(A(i,j)\) is stored in the memory location A(1+k+i-j,j) for \(j = 1,\ldots,n\) and \(i \in \lbrack\max(1,j - k),j brack\) .
Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the top left \(k \times k\) triangle) are not referenced.
uplo input indicates if matrix A lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n spmv()  cublasStatus_t cublasSspmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * AP , const float * x , int incx , const float * beta , float * y , int incy ) cublasStatus_t cublasDspmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * AP , const double * x , int incx , const double * beta , double * y , int incy ) This function supports the 64-bit Integer Interface .
This function performs the symmetric packed matrix-vector multiplication \(\textbf{y} = \alpha A\textbf{x} + \beta\textbf{y}\) where \(A\) is a \(n \times n\) symmetric matrix stored in packed format, \(\mathbf{x}\) and \(\mathbf{y}\) are vectors, and \(\alpha\) and \(\beta\) are scalars.
If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the symmetric matrix \(A\) are packed together column by column without gaps, so that the element \(A(i,j)\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \(j = 1,\ldots,n\) and \(i \geq j\) .
Consequently, the packed format requires only \(\frac{n(n + 1)}{2}\) elements for storage.
If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the symmetric matrix \(A\) are packed together column by column without gaps, so that the element \(A(i,j)\) is stored in the memory location AP[i+(j*(j+1))/2] for \(j = 1,\ldots,n\) and \(i \leq j\) .
uplo input indicates if matrix \(A\) lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n spr()  cublasStatus_t cublasSspr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , float * AP ) cublasStatus_t cublasDspr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * x , int incx , double * AP ) This function supports the 64-bit Integer Interface .
This function performs the packed symmetric rank-1 update \(A = \alpha\textbf{x}\textbf{x}^{T} + A\) where \(A\) is a \(n \times n\) symmetric matrix stored in packed format, \(\mathbf{x}\) is a vector, and \(\alpha\) is a scalar.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n spr2()  cublasStatus_t cublasSspr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , const float * y , int incy , float * AP ) cublasStatus_t cublasDspr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * x , int incx , const double * y , int incy , double * AP ) This function supports the 64-bit Integer Interface .
This function performs the packed symmetric rank-2 update \(A = \alpha\left( {\textbf{x}\textbf{y}^{T} + \textbf{y}\textbf{x}^{T}}  ight) + A\) where \(A\) is a \(n \times n\) symmetric matrix stored in packed format, \(\mathbf{x}\) is a vector, and \(\alpha\) is a scalar.
\(\textbf{y} = \alpha A\textbf{x} + \beta\textbf{y}\) where \(A\) is a \(n \times n\) symmetric matrix stored in lower or upper mode, \(\mathbf{x}\) and \(\mathbf{y}\) are vectors, and \(\alpha\) and \(\beta\) are scalars.
This function has an alternate faster implementation using atomics that can be enabled with cublasSetAtomicsMode() .
Please see the section on the function cublasSetAtomicsMode() for more details about the usage of atomics.
uplo input indicates if matrix lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements.
This function performs the symmetric rank-1 update \(A = \alpha\textbf{x}\textbf{x}^{T} + A\) where \(A\) is a \(n \times n\) symmetric matrix stored in column-major format, \(\mathbf{x}\) is a vector, and \(\alpha\) is a scalar.
This function performs the symmetric rank-2 update \(A = \alpha\left( {\textbf{x}\textbf{y}^{T} + \textbf{y}\textbf{x}^{T}}  ight) + A\) where \(A\) is a \(n \times n\) symmetric matrix stored in column-major format, \(\mathbf{x}\) and \(\mathbf{y}\) are vectors, and \(\alpha\) is a scalar.
This function performs the triangular banded matrix-vector multiplication \(\textbf{x} = \text{op}(A)\textbf{x}\) where \(A\) is a triangular banded matrix, and \(\mathbf{x}\) is a vector.
Also, for matrix \(A\) \(\text{op}(A) = \left\{ \begin{matrix} A & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_N}$}} \\ A^{T} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_T}$}} \\ A^{H} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_C}$}} \\ \end{matrix}  ight.\) If uplo == CUBLAS_FILL_MODE_LOWER then the triangular banded matrix \(A\) is stored column by column, with the main diagonal of the matrix stored in row 1 , the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc.
If uplo == CUBLAS_FILL_MODE_UPPER then the triangular banded matrix \(A\) is stored column by column, with the main diagonal of the matrix stored in row k+1 , the first superdiagonal in row k (starting at second position), the second superdiagonal in row k-1 (starting at third position), etc.
So that in general, the element \(A(i,j)\) is stored in the memory location A(1+k+i-j,j) for \(j = 1,\ldots,n\) and \(i \in \lbrack\max(1,j - k,j) brack\) .
uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements.
diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed.
This function solves the triangular banded linear system with a single right-hand-side \(\text{op}(A)\textbf{x} = \textbf{b}\) where \(A\) is a triangular banded matrix, and \(\mathbf{x}\) and \(\mathbf{b}\) are vectors.
Also, for matrix \(A\) \(\text{op}(A) = \left\{ \begin{matrix} A & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_N}$}} \\ A^{T} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_T}$}} \\ A^{H} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_C}$}} \\ \end{matrix}  ight.\) The solution \(\mathbf{x}\) overwrites the right-hand-sides \(\mathbf{b}\) on exit.
If uplo == CUBLAS_FILL_MODE_LOWER then the triangular banded matrix \(A\) is stored column by column, with the main diagonal of the matrix stored in row 1 , the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc.
This function performs the triangular packed matrix-vector multiplication \(\textbf{x} = \text{op}(A)\textbf{x}\) where \(A\) is a triangular matrix stored in packed format, and \(\mathbf{x}\) is a vector.
Also, for matrix \(A\) \(\text{op}(A) = \left\{ \begin{matrix} A & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_N}$}} \\ A^{T} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_T}$}} \\ A^{H} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_C}$}} \\ \end{matrix}  ight.\) If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the triangular matrix \(A\) are packed together column by column without gaps, so that the element \(A(i,j)\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \(j = 1,\ldots,n\) and \(i \geq j\) .
If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the triangular matrix \(A\) are packed together column by column without gaps, so that the element \(A(i,j)\) is stored in the memory location AP[i+(j*(j+1))/2] for \(A(i,j)\) and \(i \leq j\) .
This function solves the packed triangular linear system with a single right-hand-side \(\text{op}(A)\textbf{x} = \textbf{b}\) where \(A\) is a triangular matrix stored in packed format, and \(\mathbf{x}\) and \(\mathbf{b}\) are vectors.
If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the triangular matrix \(A\) are packed together column by column without gaps, so that the element \(A(i,j)\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \(j = 1,\ldots,n\) and \(i \geq j\) .
If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the triangular matrix \(A\) are packed together column by column without gaps, so that the element \(A(i,j)\) is stored in the memory location AP[i+(j*(j+1))/2] for \(j = 1,\ldots,n\) and \(i \leq j\) .
diag input indicates if the elements on the main diagonal of matrix are unity and should not be accessed.
This function performs the triangular matrix-vector multiplication \(\textbf{x} = \text{op}(A)\textbf{x}\) where \(A\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, and \(\mathbf{x}\) is a vector.
Also, for matrix \(A\) \(\text{op}(A) = \left\{ \begin{matrix} A & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_N}$}} \\ A^{T} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_T}$}} \\ A^{H} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_C}$}} \\ \end{matrix}  ight.\) Param.
trans input operation op( A ) (that is, non- or conj.) A device input array of dimensions lda x n , with lda>=max(1,n) .
This function solves the triangular linear system with a single right-hand-side \(\text{op}(A)\textbf{x} = \textbf{b}\) where \(A\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, and \(\mathbf{x}\) and \(\mathbf{b}\) are vectors.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n hemv()  cublasStatus_t cublasChemv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZhemv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface .
This function performs the Hermitian matrix-vector multiplication \(\textbf{y} = \alpha A\textbf{x} + \beta\textbf{y}\) where \(A\) is a \(n \times n\) Hermitian matrix stored in lower or upper mode, \(\mathbf{x}\) and \(\mathbf{y}\) are vectors, and \(\alpha\) and \(\beta\) are scalars.
This function has an alternate faster implementation using atomics that can be enabled with Please see the section on the for more details about the usage of atomics Param.
uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n hbmv()  cublasStatus_t cublasChbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZhbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface .
This function performs the Hermitian banded matrix-vector multiplication \(\textbf{y} = \alpha A\textbf{x} + \beta\textbf{y}\) where \(A\) is a \(n \times n\) Hermitian banded matrix with \(k\) subdiagonals and superdiagonals, \(\mathbf{x}\) and \(\mathbf{y}\) are vectors, and \(\alpha\) and \(\beta\) are scalars.
If uplo == CUBLAS_FILL_MODE_LOWER then the Hermitian banded matrix \(A\) is stored column by column, with the main diagonal of the matrix stored in row 1 , the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc.
If uplo == CUBLAS_FILL_MODE_UPPER then the Hermitian banded matrix \(A\) is stored column by column, with the main diagonal of the matrix stored in row k+1 , the first superdiagonal in row k (starting at second position), the second superdiagonal in row k-1 (starting at third position), etc.
beta host or device input scalar used for multiplication, if beta==0 then does not have to be a valid input.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n hpmv()  cublasStatus_t cublasChpmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * AP , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZhpmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * AP , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface .
This function performs the Hermitian packed matrix-vector multiplication \(\textbf{y} = \alpha A\textbf{x} + \beta\textbf{y}\) where \(A\) is a \(n \times n\) Hermitian matrix stored in packed format, \(\mathbf{x}\) and \(\mathbf{y}\) are vectors, and \(\alpha\) and \(\beta\) are scalars.
If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the Hermitian matrix \(A\) are packed together column by column without gaps, so that the element \(A(i,j)\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \(j = 1,\ldots,n\) and \(i \geq j\) .
If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the Hermitian matrix \(A\) are packed together column by column without gaps, so that the element \(A(i,j)\) is stored in the memory location AP[i+(j*(j+1))/2] for \(j = 1,\ldots,n\) and \(i \leq j\) .
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n her()  cublasStatus_t cublasCher ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const cuComplex * x , int incx , cuComplex * A , int lda ) cublasStatus_t cublasZher ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const cuDoubleComplex * x , int incx , cuDoubleComplex * A , int lda ) This function supports the 64-bit Integer Interface .
This function performs the Hermitian rank-1 update \(A = \alpha\textbf{x}\textbf{x}^{H} + A\) where \(A\) is a \(n \times n\) Hermitian matrix stored in column-major format, \(\mathbf{x}\) is a vector, and \(\alpha\) is a scalar.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n her2()  cublasStatus_t cublasCher2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * A , int lda ) cublasStatus_t cublasZher2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * A , int lda ) This function supports the 64-bit Integer Interface .
This function performs the Hermitian rank-2 update \(A = \alpha\textbf{x}\textbf{y}^{H} + \overset{ˉ}{\alpha}\textbf{y}\textbf{x}^{H} + A\) where \(A\) is a \(n \times n\) Hermitian matrix stored in column-major format, \(\mathbf{x}\) and \(\mathbf{y}\) are vectors, and \(\alpha\) is a scalar.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n hpr()  cublasStatus_t cublasChpr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const cuComplex * x , int incx , cuComplex * AP ) cublasStatus_t cublasZhpr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const cuDoubleComplex * x , int incx , cuDoubleComplex * AP ) This function supports the 64-bit Integer Interface .
This function performs the packed Hermitian rank-1 update \(A = \alpha\textbf{x}\textbf{x}^{H} + A\) where \(A\) is a \(n \times n\) Hermitian matrix stored in packed format, \(\mathbf{x}\) is a vector, and \(\alpha\) is a scalar.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n hpr2()  cublasStatus_t cublasChpr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * AP ) cublasStatus_t cublasZhpr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * AP ) This function supports the 64-bit Integer Interface .
This function performs the packed Hermitian rank-2 update \(A = \alpha\textbf{x}\textbf{y}^{H} + \overset{ˉ}{\alpha}\textbf{y}\textbf{x}^{H} + A\) where \(A\) is a \(n \times n\) Hermitian matrix stored in packed format, \(\mathbf{x}\) and \(\mathbf{y}\) are vectors, and \(\alpha\) is a scalar.
This function performs the matrix-vector multiplication of a batch of matrices and vectors.
all instances have the same dimensions (m, n), leading dimension (lda), increments (incx, incy) and transposition (trans) for their respective A matrix, x and y vectors.
The address of the input matrix and vector, and the output vector of each instance of the batch are read from arrays of pointers passed to the function by the caller.
\(\textbf{y}\lbrack i brack = \alpha\text{op}(A\lbrack i brack)\textbf{x}\lbrack i brack + \beta\textbf{y}\lbrack i brack,\text{ for i} \in \lbrack 0,batchCount - 1 brack\) where \(\alpha\) and \(\beta\) are scalars, and \(A\) is an array of pointers to matrice \(A\lbrack i brack\) stored in column-major format with dimension \(m \times n\) , and \(\textbf{x}\) and \(\textbf{y}\) are arrays of pointers to vectors.
Also, for matrix \(A\lbrack i brack\) , \(\text{op}(A\lbrack i brack) = \left\{ \begin{matrix} {A\lbrack i brack} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_N}$}} \\ {A\lbrack i brack}^{T} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_T}$}} \\ {A\lbrack i brack}^{H} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_C}$}} \\ \end{matrix}  ight.\) Note \(\textbf{y}\lbrack i brack\) vectors must not overlap, i.e.
the individual gemv operations must be computable independently; otherwise, undefined behavior is expected.
On certain problem sizes, it might be advantageous to make multiple calls to cublasgemv in different CUDA streams, rather than use this API.
trans input operation op( A[i] ) that is non- or (conj.) m input number of rows of matrix A[i] .
xarray device input array of pointers to array, with each dimension n if trans==CUBLAS_OP_N and m otherwise.
If math mode enables fast math modes when using cublasSgemvBatched() , pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors.
Otherwise it is recommended that they meet the following rule: if k % 4==0 then ensure intptr_t(ptr) % 16 == 0 , The possible error values returned by this function and their meanings are listed below.
Input matrix A and vector x, and output vector y for each instance of the batch are located at fixed offsets in number of elements from their locations in the previous instance.
Pointers to A matrix, x and y vectors for the first instance are passed to the function by the user along with offsets in number of elements - strideA, stridex and stridey that determine the locations of input matrices and vectors, and output vectors in future instances.
\(\textbf{y} + i*{stridey} = \alpha\text{op}(A + i*{strideA})(\textbf{x} + i*{stridex}) + \beta(\textbf{y} + i*{stridey}),\text{ for i } \in \lbrack 0,batchCount - 1 brack\) where \(\alpha\) and \(\beta\) are scalars, and \(A\) is an array of pointers to matrix stored in column-major format with dimension \(A\lbrack i brack\) \(m \times n\) , and \(\textbf{x}\) and \(\textbf{y}\) are arrays of pointers to vectors.
Also, for matrix \(A\lbrack i brack\) \(\text{op}(A\lbrack i brack) = \left\{ \begin{matrix} {A\lbrack i brack} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_N}$}} \\ {A\lbrack i brack}^{T} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_T}$}} \\ {A\lbrack i brack}^{H} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_C}$}} \\ \end{matrix}  ight.\) Note \(\textbf{y}\lbrack i brack\) matrices must not overlap, i.e.
Note In the table below, we use A[i], x[i], y[i] as notation for A matrix, and x and y vectors in the ith instance of the batch, implicitly assuming they are respectively offsets in number of elements strideA, stridex, stridey away from A[i-1], x[i-1], y[i-1] .
A device input * pointer to the A matrix corresponding to the first instance of the batch, with dimensions lda x n with lda>=max(1,m) .
strideA input Value of type long long int that gives the offset in number of elements between A[i] and A[i+1] x device input * pointer to the x vector corresponding to the first instance of the batch, with each dimension n if trans==CUBLAS_OP_N and m otherwise.
stridex input Value of type long long int that gives the offset in number of elements between x[i] and x[i+1] beta host or device input scalar used for multiplication.
y device in/out * pointer to the y vector corresponding to the first instance of the batch, with each dimension m if trans==CUBLAS_OP_N and n otherwise.
stridey input Value of type long long int that gives the offset in number of elements between y[i] and y[i+1] batchCount input number of GEMVs to perform in the batch.
This function performs the matrix-matrix multiplication \(C = \alpha\text{op}(A)\text{op}(B) + \beta C\) where \(\alpha\) and \(\beta\) are scalars, and \(A\) , \(B\) and \(C\) are matrices stored in column-major format with dimensions \(\text{op}(A)\) \(m \times k\) , \(\text{op}(B)\) \(k \times n\) and \(C\) \(m \times n\) , respectively.
Also, for matrix \(A\) \(\text{op}(A) = \left\{ \begin{matrix} A & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_N}$}} \\ A^{T} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_T}$}} \\ A^{H} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_C}$}} \\ \end{matrix}  ight.\) and \(\text{op}(B)\) is defined similarly for matrix \(B\) .
transa input operation op( A ) that is non- or (conj.) transb input operation op( B ) that is non- or (conj.) m input number of rows of matrix op( A ) and C .
A device input array of dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise.
B device input array of dimension ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m , n , k gemm3m()  cublasStatus_t cublasCgemm3m ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZgemm3m ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface .
This function performs the complex matrix-matrix multiplication, using Gauss complexity reduction algorithm.
This can lead to an increase in performance up to 25% \(C = \alpha\text{op}(A)\text{op}(B) + \beta C\) where \(\alpha\) and \(\beta\) are scalars, and \(A\) , \(B\) and \(C\) are matrices stored in column-major format with dimensions \(\text{op}(A)\) \(m \times k\) , \(\text{op}(B)\) \(k \times n\) and \(C\) \(m \times n\) , respectively.
Note These 2 routines are only supported on GPUs with architecture capabilities equal to or greater than 5.0 Param.
transa input Operation op( A ) that is non- or (conj.) transb input Operation op( B ) that is non- or (conj.) m input Number of rows of matrix op( A ) and C .
The possible error values returned by this function and their meanings are listed in the following table: Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully.
all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective A, B and C matrices.
The address of the input matrices and the output matrix of each instance of the batch are read from arrays of pointers passed to the function by the caller.
\(C\lbrack i brack = \alpha\text{op}(A\lbrack i brack)\text{op}(B\lbrack i brack) + \beta C\lbrack i brack,\text{ for i } \in \lbrack 0,batchCount - 1 brack\) where \(\alpha\) and \(\beta\) are scalars, and \(A\) , \(B\) and \(C\) are arrays of pointers to matrices stored in column-major format with dimensions \(\text{op}(A\lbrack i brack)\) \(m \times k\) , \(\text{op}(B\lbrack i brack)\) \(k \times n\) and \(C\lbrack i brack\) \(m \times n\) , respectively.
Also, for matrix \(A\) \(\text{op}(A) = \left\{ \begin{matrix} A & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_N}$}} \\ A^{T} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_T}$}} \\ A^{H} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_C}$}} \\ \end{matrix}  ight.\) and \(\text{op}(B\lbrack i brack)\) is defined similarly for matrix \(B\lbrack i brack\) .
Note \(C\lbrack i brack\) matrices must not overlap, that is, the individual gemm operations must be computable independently; otherwise, undefined behavior is expected.
On certain problem sizes, it might be advantageous to make multiple calls to cublasgemm in different CUDA streams, rather than use this API.
transa input operation op( A[i] ) that is non- or (conj.) transb input operation op( B[i] ) that is non- or (conj.) m input number of rows of matrix op( A[i] ) and C[i] .
lda x k with lda>=max(1,m) if transa==CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise.
ldb x n with ldb>=max(1,k) if transb==CUBLAS_OP_N and ldb x k with ldb>=max(1,n) max(1,) otherwise.
If math mode enables fast math modes when using cublasSgemmBatched() , pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors.
Otherwise it is recommended that they meet the following rule: if k%4==0 then ensure intptr_t(ptr) % 16 == 0 , The possible error values returned by this function and their meanings are listed below.
Input matrices A, B and output matrix C for each instance of the batch are located at fixed offsets in number of elements from their locations in the previous instance.
Pointers to A, B and C matrices for the first instance are passed to the function by the user along with offsets in number of elements - strideA, strideB and strideC that determine the locations of input and output matrices in future instances.
\(C + i*{strideC} = \alpha\text{op}(A + i*{strideA})\text{op}(B + i*{strideB}) + \beta(C + i*{strideC}),\text{ for i } \in \lbrack 0,batchCount - 1 brack\) where \(\alpha\) and \(\beta\) are scalars, and \(A\) , \(B\) and \(C\) are arrays of pointers to matrices stored in column-major format with dimensions \(\text{op}(A\lbrack i brack)\) \(m \times k\) , \(\text{op}(B\lbrack i brack)\) \(k \times n\) and \(C\lbrack i brack\) \(m \times n\) , respectively.
the individual gemm operations must be computable independently; otherwise, undefined behavior is expected.
Note In the table below, we use A[i], B[i], C[i] as notation for A, B and C matrices in the ith instance of the batch, implicitly assuming they are respectively offsets in number of elements strideA, strideB, strideC away from A[i-1], B[i-1], C[i-1] .
A device input * pointer to the A matrix corresponding to the first instance of the batch, with dimensions lda x k with lda>=max(1,m) if transa==CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise.
strideA input Value of type long long int that gives the offset in number of elements between A[i] and A[i+1] B device input * pointer to the B matrix corresponding to the first instance of the batch, with dimensions ldb x n with ldb>=max(1,k) if transb==CUBLAS_OP_N and ldb x k with ldb>=max(1,n) max(1,) otherwise.
strideB input Value of type long long int that gives the offset in number of elements between B[i] and B[i+1] beta host or device input scalar used for multiplication.
C device in/out * pointer to the C matrix corresponding to the first instance of the batch, with dimensions ldc x n with ldc>=max(1,m) .
strideC input Value of type long long int that gives the offset in number of elements between C[i] and C[i+1] batchCount input number of GEMMs to perform in the batch.
However, the dimensions, leading dimensions, transpositions, and scaling factors (alpha, beta) may vary between groups.
This is functionally equivalent to the following: idx = 0 ; for i = 0 : group_count - 1 for j = 0 : group_size [ i ] - 1 gemm ( transa_array [ i ], transb_array [ i ], m_array [ i ], n_array [ i ], k_array [ i ], alpha_array [ i ], Aarray [ idx ], lda_array [ i ], Barray [ idx ], ldb_array [ i ], beta_array [ i ], Carray [ idx ], ldc_array [ i ]); idx += 1 ; end end where \(\text{$\mathrm{alpha\_array}$}\) and \(\text{$\mathrm{beta\_array}$}\) are arrays of scaling factors, and \(\text{Aarray}\) , \(\text{Barray}\) and \(\text{Carray}\) are arrays of pointers to matrices stored in column-major format.
For a given index, \(\text{idx}\) , that is part of group \(i\) , the dimensions are: \(\text{op}(\text{Aarray}\lbrack\text{idx} brack)\) : \(\text{$\mathrm{m\_array}$}\lbrack i brack \times \text{$\mathrm{k\_array}$}\lbrack i brack\) \(\text{op}(\text{Barray}\lbrack\text{idx} brack)\) : \(\text{$\mathrm{k\_array}$}\lbrack i brack \times \text{$\mathrm{n\_array}$}\lbrack i brack\) \(\text{Carray}\lbrack\text{idx} brack\) : \(\text{$\mathrm{m\_array}$}\lbrack i brack \times \text{$\mathrm{n\_array}$}\lbrack i brack\) Note This API takes arrays of two different lengths.
Note \(C\lbrack\text{idx} brack\) matrices must not overlap, that is, the individual gemm operations must be computable independently; otherwise, undefined behavior is expected.
On certain problem sizes, it might be advantageous to make multiple calls to cublasgemmBatched in different CUDA streams, rather than use this API.
transa_array host input array containing the operations, op( A[idx] ), that is non- or (conj.) transpose for each group.
group_count transb_array host input array containing the operations, op( B[idx] ), that is non- or (conj.) group_count m_array host input array containing the number of rows of matrix op( A[idx] ) and C[idx] for each group.
group_count n_array host input array containing the number of columns of op( B[idx] ) and C[idx] for each group.
group_count k_array host input array containing the number of columns of op( A[idx] ) and rows of op( B[idx] ) for each group.
group_count alpha_array host input array containing the scalar used for multiplication for each group.
lda[i] x k[i] with lda[i]>=max(1,m[i]) if transa[i]==CUBLAS_OP_N and lda[i] x m[i] with lda[i]>=max(1,k[i]) otherwise.
problem_count lda_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix A[idx] for each group.
ldb[i] x n[i] with ldb[i]>=max(1,k[i]) if transb[i]==CUBLAS_OP_N and ldb[i] x k[i] with ldb[i]>=max(1,n[i]) otherwise.
problem_count ldb_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix B[idx] for each group.
group_count beta_array host input array containing the scalar used for multiplication for each group.
problem_count ldc_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix C[idx] for each group.
group_count group_count host input number of groups group_size host input array containg the number of pointers contained in Aarray, Barray and Carray for each group.
group_count If math mode enables fast math modes when using cublasSgemmGroupedBatched() , pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors.
Otherwise it is required that they meet the following rule: if k%4==0 then ensure intptr_t(ptr) % 16 == 0 , The possible error values returned by this function and their meanings are listed below.
This function performs the symmetric matrix-matrix multiplication \(C = \left\{ \begin{matrix} {\alpha AB + \beta C} & {\text{if }\textsf{side == $\mathrm{CUBLAS\_SIDE\_LEFT}$}} \\ {\alpha BA + \beta C} & {\text{if }\textsf{side == $\mathrm{CUBLAS\_SIDE\_RIGHT}$}} \\ \end{matrix}  ight.\) where \(A\) is a symmetric matrix stored in lower or upper mode, \(B\) and \(C\) are \(m \times n\) matrices, and \(\alpha\) and \(\beta\) are scalars.
A device input array of dimension lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise.
beta host or device input scalar used for multiplication, if beta == 0 then C does not have to be a valid input.
This function performs the symmetric rank- \(k\) update \(C = \alpha\text{op}(A)\text{op}(A)^{T} + \beta C\) where \(\alpha\) and \(\beta\) are scalars, \(C\) is a symmetric matrix stored in lower or upper mode, and \(A\) is a matrix with dimensions \(\text{op}(A)\) \(n \times k\) .
Also, for matrix \(A\) \(\text{op}(A) = \left\{ \begin{matrix} A & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_N}$}} \\ A^{T} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_T}$}} \\ \end{matrix}  ight.\) Param.
uplo input indicates if matrix C lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements.
A device input array of dimension lda x k with lda>=max(1,n) if trans == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise.
beta host or device input scalar used for multiplication, if beta==0 then C does not have to be a valid input.
This function performs the symmetric rank- \(2k\) update \(C = \alpha(\text{op}(A)\text{op}(B)^{T} + \text{op}(B)\text{op}(A)^{T}) + \beta C\) where \(\alpha\) and \(\beta\) are scalars, \(C\) is a symmetric matrix stored in lower or upper mode, and \(A\) and \(B\) are matrices with dimensions \(\text{op}(A)\) \(n \times k\) and \(\text{op}(B)\) \(n \times k\) , respectively.
Also, for matrix \(A\) and \(B\) \(\text{op(}A\text{) and op(}B\text{)} = \left\{ \begin{matrix} {A\text{ and }B} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_N}$}} \\ {A^{T}\text{ and }B^{T}} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_T}$}} \\ \end{matrix}  ight.\) Param.
uplo input indicates if matrix C lower or upper part, is stored, the other symmetric part is not referenced and is inferred from the stored elements.
A device input array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise.
B device input array of dimensions ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise.
beta host or device input scalar used for multiplication, if beta==0 , then C does not have to be a valid input.
This function performs a variation of the symmetric rank- \(k\) update \(C = \alpha\text{op}(A)\text{op}(B)^{T} + \beta C\) where \(\alpha\) and \(\beta\) are scalars, \(C\) is a symmetric matrix stored in lower or upper mode, and \(A\) and \(B\) are matrices with dimensions \(\text{op}(A)\) \(n \times k\) and \(\text{op}(B)\) \(n \times k\) , respectively.
Also, for matrices \(A\) and \(B\) \(\text{op(}A\text{) and op(}B\text{)} = \left\{ \begin{matrix} {A\text{ and }B} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_N}$}} \\ {A^{T}\text{ and }B^{T}} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_T}$}} \\ \end{matrix}  ight.\) This routine can be used when B is in such way that the result is guaranteed to be symmetric.
A usual example is when the matrix B is a scaled form of the matrix A: this is equivalent to B being the product of the matrix A and a diagonal matrix.
For an efficient computation of the product of a regular matrix with a diagonal matrix, refer to the routine cublasdgmm .
This function performs the triangular matrix-matrix multiplication \(C = \left\{ \begin{matrix} {\alpha\text{op}(A)B} & {\text{if }\textsf{side == $\mathrm{CUBLAS\_SIDE\_LEFT}$}} \\ {\alpha B\text{op}(A)} & {\text{if }\textsf{side == $\mathrm{CUBLAS\_SIDE\_RIGHT}$}} \\ \end{matrix}  ight.\) where \(A\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \(B\) and \(C\) are \(m \times n\) matrix, and \(\alpha\) is a scalar.
Also, for matrix \(A\) \(\text{op}(A) = \left\{ \begin{matrix} A & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_N}$}} \\ A^{T} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_T}$}} \\ A^{H} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_C}$}} \\ \end{matrix}  ight.\) Notice that in order to achieve better parallelism cuBLAS differs from the BLAS API only for this routine.
The BLAS API assumes an in-place implementation (with results written back to B), while the cuBLAS API assumes an out-of-place implementation (with results written into C).
The application can obtain the in-place functionality of BLAS in the cuBLAS API by passing the address of the matrix B in place of the matrix C.
alpha host or device input scalar used for multiplication, if alpha==0 then A is not referenced and B does not have to be a valid input.
This function solves the triangular linear system with multiple right-hand-sides \(\left\{ \begin{matrix} {\text{op}(A)X = \alpha B} & {\text{if }\textsf{side == $\mathrm{CUBLAS\_SIDE\_LEFT}$}} \\ {X\text{op}(A) = \alpha B} & {\text{if }\textsf{side == $\mathrm{CUBLAS\_SIDE\_RIGHT}$}} \\ \end{matrix}  ight.\) where \(A\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \(X\) and \(B\) are \(m \times n\) matrices, and \(\alpha\) is a scalar.
Also, for matrix \(A\) \(\text{op}(A) = \left\{ \begin{matrix} A & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_N}$}} \\ A^{T} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_T}$}} \\ A^{H} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_C}$}} \\ \end{matrix}  ight.\) The solution \(X\) overwrites the right-hand-sides \(B\) on exit.
This function solves an array of triangular linear systems with multiple right-hand-sides \(\left\{ \begin{matrix} {\text{op}(A\lbrack i brack)X\lbrack i brack = \alpha B\lbrack i brack} & {\text{if }\textsf{side == $\mathrm{CUBLAS\_SIDE\_LEFT}$}} \\ {X\lbrack i brack\text{op}(A\lbrack i brack) = \alpha B\lbrack i brack} & {\text{if }\textsf{side == $\mathrm{CUBLAS\_SIDE\_RIGHT}$}} \\ \end{matrix}  ight.\) where \(A\lbrack i brack\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \(X\lbrack i brack\) and \(B\lbrack i brack\) are \(m \times n\) matrices, and \(\alpha\) is a scalar.
Also, for matrix \(A\) \(\text{op}(A\lbrack i brack) = \left\{ \begin{matrix} {A\lbrack i brack} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_N}$}} \\ {A^{T}\lbrack i brack} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_T}$}} \\ {A^{H}\lbrack i brack} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_C}$}} \\ \end{matrix}  ight.\) The solution \(X\lbrack i brack\) overwrites the right-hand-sides \(B\lbrack i brack\) on exit.
This function works for any sizes but is intended to be used for matrices of small sizes where the launch overhead is a significant factor.
For bigger sizes, it might be advantageous to call batchCount times the regular cublastrsm within a set of CUDA streams.
The current implementation is limited to devices with compute capability above or equal 2.0.
uplo input indicates if matrix A[i] lower or upper part is stored, the other part is not referenced and is inferred from the stored elements.
diag input indicates if the elements on the main diagonal of matrix A[i] are unity and should not be accessed.
alpha host or device input scalar used for multiplication, if alpha==0 then A[i] is not referenced and B[i] does not have to be a valid input.
lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m hemm()  cublasStatus_t cublasChemm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZhemm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface .
This function performs the Hermitian matrix-matrix multiplication \(C = \left\{ \begin{matrix} {\alpha AB + \beta C} & {\text{if }\textsf{side == $\mathrm{CUBLAS\_SIDE\_LEFT}$}} \\ {\alpha BA + \beta C} & {\text{if }\textsf{side == $\mathrm{CUBLAS\_SIDE\_RIGHT}$}} \\ \end{matrix}  ight.\) where \(A\) is a Hermitian matrix stored in lower or upper mode, \(B\) and \(C\) are \(m \times n\) matrices, and \(\alpha\) and \(\beta\) are scalars.
A device input array of dimension lda x m with lda>=max(1,m) if side==CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise.
beta input scalar used for multiplication, if beta==0 then C does not have to be a valid input.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m herk()  cublasStatus_t cublasCherk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const cuComplex * A , int lda , const float * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZherk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const cuDoubleComplex * A , int lda , const double * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface .
This function performs the Hermitian rank- \(k\) update \(C = \alpha\text{op}(A)\text{op}(A)^{H} + \beta C\) where \(\alpha\) and \(\beta\) are scalars, \(C\) is a Hermitian matrix stored in lower or upper mode, and \(A\) is a matrix with dimensions \(\text{op}(A)\) \(n \times k\) .
Also, for matrix \(A\) \(\text{op}(A) = \left\{ \begin{matrix} A & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_N}$}} \\ A^{H} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_C}$}} \\ \end{matrix}  ight.\) Param.
uplo input indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n her2k()  cublasStatus_t cublasCher2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const float * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZher2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const double * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface .
This function performs the Hermitian rank- \(2k\) update \(C = \alpha\text{op}(A)\text{op}(B)^{H} + \overset{ˉ}{\alpha}\text{op}(B)\text{op}(A)^{H} + \beta C\) where \(\alpha\) and \(\beta\) are scalars, \(C\) is a Hermitian matrix stored in lower or upper mode, and \(A\) and \(B\) are matrices with dimensions \(\text{op}(A)\) \(n \times k\) and \(\text{op}(B)\) \(n \times k\) , respectively.
Also, for matrix \(A\) and \(B\) \(\text{op(}A\text{) and op(}B\text{)} = \left\{ \begin{matrix} {A\text{ and }B} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_N}$}} \\ {A^{H}\text{ and }B^{H}} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_C}$}} \\ \end{matrix}  ight.\) Param.
B device input array of dimension ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise.
beta host or device input scalar used for multiplication, if beta==0 then C does not have to be a valid input.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n herkx()  cublasStatus_t cublasCherkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const float * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZherkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const double * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface .
This function performs a variation of the Hermitian rank- \(k\) update \(C = \alpha\text{op}(A)\text{op}(B)^{H} + \beta C\) where \(\alpha\) and \(\beta\) are scalars, \(C\) is a Hermitian matrix stored in lower or upper mode, and \(A\) and \(B\) are matrices with dimensions \(\text{op}(A)\) \(n \times k\) and \(\text{op}(B)\) \(n \times k\) , respectively.
Also, for matrix \(A\) and \(B\) \(\text{op(}A\text{) and op(}B\text{)} = \left\{ \begin{matrix} {A\text{ and }B} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_N}$}} \\ {A^{H}\text{ and }B^{H}} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_C}$}} \\ \end{matrix}  ight.\) This routine can be used when the matrix B is in such way that the result is guaranteed to be hermitian.
An usual example is when the matrix B is a scaled form of the matrix A: this is equivalent to B being the product of the matrix A and a diagonal matrix.
beta host or device input real scalar used for multiplication, if beta==0 then C does not have to be a valid input.
This function performs the matrix-matrix addition/transposition \(C = \alpha\text{op}(A) + \beta\text{op}(B)\) where \(\alpha\) and \(\beta\) are scalars, and \(A\) , \(B\) and \(C\) are matrices stored in column-major format with dimensions \(\text{op}(A)\) \(m \times n\) , \(\text{op}(B)\) \(m \times n\) and \(C\) \(m \times n\) , respectively.
The in-place mode supports the following two operations, \(C = \alpha\text{*}C + \beta\text{op}(B)\) \(C = \alpha\text{op}(A) + \beta\text{*}C\) For in-place mode, if C = A , ldc = lda and transa = CUBLAS_OP_N .
The operation includes the following special cases: the user can reset matrix C to zero by setting *alpha=*beta=0 .
A device input array of dimensions lda x n with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,n) otherwise.
B device input array of dimension ldb x n with ldb>=max(1,m) if transb == CUBLAS_OP_N and ldb x m with ldb>=max(1,n) otherwise.
This function performs the matrix-matrix multiplication \(C = \left\{ \begin{matrix} {A \times diag(X)} & {\text{if }\textsf{mode == $\mathrm{CUBLAS\_SIDE\_RIGHT}$}} \\ {diag(X) \times A} & {\text{if }\textsf{mode == $\mathrm{CUBLAS\_SIDE\_LEFT}$}} \\ \end{matrix}  ight.\) where \(A\) and \(C\) are matrices stored in column-major format with dimensions \(m \times n\) .
\(X\) is a vector of size \(n\) if mode == CUBLAS_SIDE_RIGHT and of size \(m\) if mode == CUBLAS_SIDE_LEFT .
The formula of X is \(X\lbrack j brack = \left\{ \begin{matrix} {x\lbrack j \times incx brack} & {\text{if }incx \geq 0} \\ {x\lbrack(\chi - 1) \times |incx| - j \times |incx| brack} & {\text{if }incx geam() with *beta=0 and transa == CUBLAS_OP_N or cublasdgmm() with incx=0 and x[0]=alpha .
mode input left multiply if mode == CUBLAS_SIDE_LEFT or right multiply if mode == CUBLAS_SIDE_RIGHT m input number of rows of matrix A and C .
A device input array of dimensions lda x n with lda>=max(1,m) lda input leading dimension of two-dimensional array used to store the matrix A .
x device input one-dimensional array of size \(|inc| \times m\) if mode == CUBLAS_SIDE_LEFT and \(|inc| \times n\) if mode == CUBLAS_SIDE_RIGHT incx input stride of one-dimensional array x .
This function performs the LU factorization of each Aarray[i] for i = 0, …, batchSize-1 by the following equation \(\text{P}\text{*}{Aarray}\lbrack i brack = L\text{*}U\) where P is a permutation matrix which represents partial pivoting with row interchanges.
Formally P is written by a product of permutation matrices Pj , for j = 1,2,...,n , say P = P1 * P2 * P3 * .
Pj can be constructed by j element of PivotArray[i] by the following Matlab code   In Matlab PivotArray[i] is an array of base-1.
Pj = eye ( n ); swap Pj ( j , : ) and Pj ( PivotArray [ i ][ j ] , : ) L and U are written back to original matrix A , and diagonal elements of L are discarded.
The L and U can be constructed by the following Matlab code   A is a matrix of nxn after getrf.
L = eye ( n ); for j = 1 : n L ( j + 1 : n , j ) = A ( j + 1 : n , j ) end U = zeros ( n ); for i = 1 : n U ( i , i : n ) = A ( i , i : n ) end If matrix A(=Aarray[i]) is singular, getrf still works and the value of info(=infoArray[i]) reports first row index that LU factorization cannot proceed.
The equation P*A=L*U still holds, however L and U reconstruction needs different Matlab code as follows:   A is a matrix of nxn after getrf.
L = eye ( n ); for j = 1 : k -1 L ( j + 1 : n , j ) = A ( j + 1 : n , j ) end U = zeros ( n ); for i = 1 : k -1 U ( i , i : n ) = A ( i , i : n ) end for i = k : n U ( i , k : n ) = A ( i , k : n ) end This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor.
lda input leading dimension of two-dimensional array used to store each matrix Aarray[i] .
PivotArray device output array of size n x batchSize that contains the pivoting sequence of each factorization of Aarray[i] stored in a linear fashion.
infoArray device output array of size batchSize that info(=infoArray[i]) contains the information of factorization of Aarray[i] .
batchSize input number of pointers contained in A The possible error values returned by this function and their meanings are listed below.
Also, for matrix \(A\) \(\text{op}(A\lbrack i brack) = \left\{ \begin{matrix} {A\lbrack i brack} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_N}$}} \\ {A^{T}\lbrack i brack} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_T}$}} \\ {A^{H}\lbrack i brack} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_C}$}} \\ \end{matrix}  ight.\) This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor.
devIpiv device input array of size n x batchSize that contains the pivoting sequence of each factorization of Aarray[i] stored in a linear fashion.
ldb input leading dimension of two-dimensional array used to store each solution matrix Barray[i] .
Prior to calling cublasgetriBatched, the matrix A[i] must be factorized first using the routine cublasgetrfBatched.
After the call of cublasgetrfBatched, the matrix pointing by Aarray[i] will contain the LU factors of the matrix A[i] and the vector pointing by (PivotArray+i) will contain the pivoting sequence.
Following the LU factorization, cublasgetriBatched uses forward and backward triangular solvers to complete inversion of matrices A[i] for i = 0, …, batchSize-1 .
The inversion is out-of-place, so memory space of Carray[i] cannot overlap memory space of Array[i].
Aarray[i] is n*n matrix A[i] cublasDgetrfBatched ( handle , n , Aarray , lda , PivotArray , infoArray , batchSize );   check infoArray[i] to see if factorization of A[i] is successful or not.
Array[i] contains LU factorization of A[i]   step 2: perform out-of-place inversion, Carray[i] = inv(A[i]) cublasDgetriBatched ( handle , n , Aarray , lda , PivotArray , Carray , ldc , infoArray , batchSize );   check infoArray[i] to see if inversion of A[i] is successful or not.
This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor.
If cublasgetrfBatched is performed by non-pivoting, PivotArray of cublasgetriBatched should be NULL.
Aarray device input array of pointers to array, with each array of dimension n*n with lda>=max(1,n) .
PivotArray device output array of size n*batchSize that contains the pivoting sequence of each factorization of Aarray[i] stored in a linear fashion.
Carray device output array of pointers to array, with each array of dimension n*n with ldc>=max(1,n) .
ldc input leading dimension of two-dimensional array used to store each matrix Carray[i] .
infoArray device output array of size batchSize that info(=infoArray[i]) contains the information of inversion of A[i] .
If the matrix A[i] is singular, then info[i] reports singularity, the same as cublasgetrfBatched .
A device input array of pointers to array, with each array of dimension n*n with lda>=max(1,n) .
Ainv device output array of pointers to array, with each array of dimension n*n with lda_inv>=max(1,n) .
lda_inv input leading dimension of two-dimensional array used to store each matrix Ainv[i] .
info device output array of size batchSize that info[i] contains the information of inversion of A[i] .
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n 32 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU 2.8.7.
This function performs the QR factorization of each Aarray[i] for i = 0, ...,batchSize-1 using Householder reflections.
Each matrix Q[i] is represented as a product of elementary reflectors and is stored in the lower part of each Aarray[i] as follows : Q[j] = H[j][1] H[j][2] . . H[j](k), where k = min(m,n).
Each H[j][i] has the form H[j][i] = I - tau[j] * v * v' where tau[j] is a real scalar, and v is a real vector with v(1:i-1) = 0 and v(i) = 1 ; v(i+1:m) is stored on exit in Aarray[j][i+1:m,i] , and tau in TauArray[j][i] .
This function find the least squares solution of a batch of overdetermined systems: it solves the least squares problem described as follows : minimize || Carray [ i ] - Aarray [ i ] * Xarray [ i ] || , with i = 0 , ..., batchSize -1 On exit, each Aarray[i] is overwritten with their QR factorization and each Carray[i] is overwritten with the least square solution cublasgelsBatched supports only the non-transpose operation and only solves over-determined systems (m >= n).
trans input operation op( Aarray[i] ) that is non- or (conj.) Only non-transpose operation is currently supported.
m input number of rows of each Aarray[i] and Carray[i] if trans == CUBLAS_OP_N , numbers of columns of each Aarray[i] otherwise (not supported currently).
n input number of columns of each Aarray[i] if trans == CUBLAS_OP_N , and number of rows of each Aarray[i] and Carray[i] otherwise (not supported currently).
m x n with lda>=max(1,m) if trans == CUBLAS_OP_N , and n x m with lda>=max(1,n) otherwise (not supported currently).
m x nrhs with ldc>=max(1,m) if trans == CUBLAS_OP_N , and n x nrhs with lda>=max(1,n) otherwise (not supported currently).
info host output If info=0, the parameters passed to the function are valid If info 0 : the V-th diagonal element of the Aarray[i] is zero.
batchSize input number of pointers contained in Aarray and Carray The possible error values returned by this function and their meanings are listed below.
If uplo == CUBLAS_FILL_MODE_UPPER then the elements of AP are copied into the upper triangular part of the triangular matrix A and the lower part of A is left untouched.
If uplo == CUBLAS_FILL_MODE_UPPER then then the upper triangular part of the triangular matrix A is copied into the array AP .
In this function the input matrices and output matrices can have a lower precision but the computation is still done in the type .
For example, in the type float for cublasSgemmEx() and in the type cuComplex for cublasCgemmEx() .
\(C = \alpha\text{op}(A)\text{op}(B) + \beta C\) where \(\alpha\) and \(\beta\) are scalars, and \(A\) , \(B\) and \(C\) are matrices stored in column-major format with dimensions \(\text{op}(A)\) \(m \times k\) , \(\text{op}(B)\) \(k \times n\) and \(C\) \(m \times n\) , respectively.
The matrix types combinations supported for cublasSgemmEx() are listed below: C A/B CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_8I CUDA_R_16BF CUDA_R_16F CUDA_R_32F The matrix types combinations supported for cublasCgemmEx() are listed below : C A/B CUDA_C_32F CUDA_C_8I CUDA_C_32F The possible error values returned by this function and their meanings are listed below.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ARCH_MISMATCH cublasCgemmEx() is only supported for GPU with architecture capabilities equal or greater than 5.0 CUBLAS_STATUS_NOT_SUPPORTED the combination of the parameters Atype , Btype and Ctype is not supported CUBLAS_STATUS_INVALID_VALUE If m gemm that allows the user to individually specify the data types for each of the A, B and C matrices, the precision of computation and the GEMM algorithm to be run.
Note The second variant of cublasGemmEx() function is provided for backward compatibility with C++ applications code, where the computeType parameter is of cudaDataType instead of cublasComputeType_t .
alpha host or device input Scaling factor for A*B of the type that corresponds to the computeType and Ctype, see the table below for details.
beta host or device input Scaling factor for C of the type that corresponds to the computeType and Ctype, see the table below for details.
For better performance, it is also recommended that IMMA kernels requirements for a regular data ordering listed here are met.
The possible error values returned by this function and their meanings are listed in the following table.
CUBLAS_STATUS_ARCH_MISMATCH cublasGemmEx() is only supported for GPU with architecture capabilities equal or greater than 5.0.
CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters Atype , Btype and Ctype or the algorithm, algo is not supported.
CUBLAS_STATUS_INVALID_VALUE If m gemmBatched that performs the matrix-matrix multiplication of a batch of matrices and allows the user to individually specify the data types for each of the A, B and C matrix arrays, the precision of computation and the GEMM algorithm to be run.
Note The second variant of cublasGemmBatchedEx() function is provided for backward compatibility with C++ applications code, where the computeType parameter is of cudaDataType instead of cublasComputeType_t .
transa input Operation op( A[i] ) that is non- or (conj.) transb input Operation op( B[i] ) that is non- or (conj.) m input Number of rows of matrix op( A[i] ) and C[i] .
lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise.
ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise.
Otherwise it is recommended that they meet the following rule: if k%8==0 then ensure intptr_t(ptr) % 16 == 0 , if k%2==0 then ensure intptr_t(ptr) % 4 == 0 .
Note Compute types CUBLAS_COMPUTE_32I and CUBLAS_COMPUTE_32I_PEDANTIC are only supported with all pointers A[i] , B[i] being 4-byte aligned and lda, ldb being multiples of 4.
For a better performance, it is also recommended that IMMA kernels requirements for the regular data ordering listed here are met.
CUBLAS_STATUS_ARCH_MISMATCH cublasGemmBatchedEx() is only supported for GPU with architecture capabilities equal to or greater than 5.0.
CUBLAS_STATUS_INVALID_VALUE If m gemmStridedBatched that performs the matrix-matrix multiplication of a batch of matrices and allows the user to individually specify the data types for each of the A, B and C matrices, the precision of computation and the GEMM algorithm to be run.
Pointers to A, B and C matrices for the first instance are passed to the function by the user along with the offsets in number of elements - strideA, strideB and strideC that determine the locations of input and output matrices in future instances.
Note The second variant of cublasGemmStridedBatchedEx() function is provided for backward compatibility with C++ applications code, where the computeType parameter is of cudaDataType_t instead of cublasComputeType_t .
A device input Pointer to matrix, A, corresponds to the first instance of the batch, with dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise.
strideA input Value of type long long int that gives the offset in number of elements between A[i] and A[i+1] .
B device input Pointer to matrix, B, corresponds to the first instance of the batch, with dimensions ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise.
strideB input Value of type long long int that gives the offset in number of elements between B[i] and B[i+1] .
C device in/out Pointer to matrix, C, corresponds to the first instance of the batch, with dimensions ldc x n with ldc>=max(1,m) .
strideC input Value of type long long int that gives the offset in number of elements between C[i] and C[i+1] .
CUBLAS_STATUS_ARCH_MISMATCH cublasGemmBatchedEx() is only supported for GPU with architecture capabilities equal or greater than 5.0.
lda_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix A[idx] for each group.
ldb_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix B[idx] for each group.
ldc_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix C[idx] for each group.
cublasGemmGroupedBatchedEx() supports the following Compute Type, Scale Type, Atype/Btype, and Ctype: Compute Type Scale Type (alpha and beta) Atype/Btype Ctype CUBLAS_COMPUTE_32F CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_32F CUDA_R_32F CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F If Atype is CUDA_R_16F or CUDA_R_16BF or if the computeType is any of the FAST options, pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors.
Otherwise it is required that they meet the following rule: if (k * AtypeSize) % 16 == 0 then ensure intptr_t(ptr) % 16 == 0 , if (k * AtypeSize) % 4 == 0 then ensure intptr_t(ptr) % 4 == 0 .
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If transa_array , transb_array , m_array , n_array , k_array , alpha_array , lda_array , ldb_array , beta_array , ldc_array , or group_size are NULL or if group_count scalar used for multiplication.
beta host or device input scalar used for multiplication, if beta==0 then C does not have to be a valid input.
The matrix types combinations supported for cublasCsyrkEx() are listed below: A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and their meanings are listed below.
The matrix types combinations supported for cublasCsyrk3mEx() are listed below : A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and their meanings are listed below.
The matrix types combinations supported for cublasCherkEx() are listed in the following table: A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and their meanings are listed below.
The matrix types combinations supported for cublasCherk3mEx() are listed in the following table: A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and their meanings are listed below.
CUBLAS_STATUS_INVALID_VALUE If n nrm2 where input data, output data and compute type can be specified independently.
result host or device output the resulting norm, which is 0.0 if n,incxaxpy where input data, output data and compute type can be specified independently.
executionType input Enumerant specifying the datatype in which the computation is executed.
The datatypes combinations currently supported for cublasAxpyEx() are listed in the following table: alpha x y execution CUDA_R_32F CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F The possible error values returned by this function and their meanings are listed below.
CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters xType , yType , and executionType is not supported.
CUBLAS_STATUS_INVALID_VALUE alphaType or xType or yType or executionType is not supported.
cublasDotEx()  cublasStatus_t cublasDotEx ( cublasHandle_t handle , int n , const void * x , cudaDataType xType , int incx , const void * y , cudaDataType yType , int incy , void * result , cudaDataType resultType , cudaDataType executionType ); cublasStatus_t cublasDotcEx ( cublasHandle_t handle , int n , const void * x , cudaDataType xType , int incx , const void * y , cudaDataType yType , int incy , void * result , cudaDataType resultType , cudaDataType executionType ); These functions support the 64-bit Integer Interface .
These functions are an API generalization of the routines cublasdot and cublasdotc where input data, output data and compute type can be specified independently.
result host or device output The resulting dot product, which is 0.0 if nrot where input data, output data, cosine/sine type, and compute type can be specified independently.
executionType input enumerant specifying the datatype in which the computation is executed.
The datatypes combinations currently supported for cublasRotEx() are listed below : executionType xType / yType csType CUDA_R_32F CUDA_R_16BF CUDA_R_16F CUDA_R_32F CUDA_R_16BF CUDA_R_16F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_R_32F CUDA_C_32F CUDA_C_64F CUDA_C_64F CUDA_C_64F CUDA_R_64F CUDA_C_64F The possible error values returned by this function and their meanings are listed below.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srot , drot , crot , csrot , zrot , zdrot 2.8.24.
cublasScalEx()  cublasStatus_t cublasScalEx ( cublasHandle_t handle , int n , const void * alpha , cudaDataType alphaType , void * x , cudaDataType xType , int incx , cudaDataType executionType ); This function supports the 64-bit Integer Interface .
The datatypes combinations currently supported for cublasScalEx() are listed below : alpha x execution CUDA_R_32F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_16BF CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_64F CUDA_C_64F CUDA_C_64F The possible error values returned by this function and their meanings are listed below.
Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_NOT_SUPPORTED the combination of the parameters xType and executionType is not supported CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_INVALID_VALUE alphaType or xType or executionType is not supported For references please refer to: sscal , dscal , csscal , cscal , zdscal , zscal 3.
General Description  The cuBLASLt library is a new lightweight library dedicated to GEneral Matrix-to-matrix Multiply (GEMM) operations with a new flexible API.
This new library adds flexibility in matrix data layouts, input types, compute types, and also in choosing the algorithmic implementations and heuristics through parameter programmability.
Once a set of options for the intended GEMM operation are identified by the user, these options can be used repeatedly for different inputs.
Note The cuBLASLt library does not guarantee the support of all possible sizes and configurations, however, since CUDA 12.2 update 2, the problem size limitations on m, n, and batch size have been largely resolved.
The main focus of the library is to provide the most performant kernels, which might have some implied limitations.
Some non-standard configurations may require a user to handle them manually, typically by decomposing the problem into smaller parts (see Problem Size Limitations ). 3.1.1. Problem Size Limitations  There are inherent problem size limitations that are a result of limitations in CUDA grid dimensions.
For example, many kernels do not support batch sizes greater than 65535 due to a limitation on the z dimension of a grid.
In cases where a problem cannot be run by a single kernel, cuBLASLt will attempt to decompose the problem into multiple sub-problems and solve it by running the kernel on each sub-problem.
There are some restrictions on cuBLASLt internal problem decomposition which are summarized below: Amax computations are not supported.
This means that CUBLASLT_MATMUL_DESC_AMAX_D_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER must be left unset (see cublasLtMatmulDescAttributes_t ) All matrix layouts must have CUBLASLT_MATRIX_LAYOUT_ORDER set to CUBLASLT_ORDER_COL (see cublasLtOrder_t ) cuBLASLt will not partition along the n dimension when CUBLASLT_MATMUL_DESC_EPILOGUE is set to CUBLASLT_EPILOGUE_DRELU_BGRAD or CUBLASLT_EPILOGUE_DGELU_BGRAD (see cublasLtEpilogue_t ) To overcome these limitations, a user may want to partition the problem themself, launch kernels for each sub-problem, and compute any necessary reductions to combine the results. 3.1.2. Heuristics Cache  cuBLASLt uses heuristics to pick the most suitable matmul kernel for execution based on the problem sizes, GPU configuration, and other parameters.
This requires performing some computations on the host CPU, which could take tens of microseconds.
To overcome this overhead, it is recommended to query the heuristics once using cublasLtMatmulAlgoGetHeuristic() and then reuse the result for subsequent computations using cublasLtMatmul() .
For the cases where querying heuristics once and then reusing them is not feasible, cuBLASLt implements a heuristics cache that maps matmul problems to kernels previously selected by heuristics.
The user can control the heuristics cache capacity with the CUBLASLT_HEURISTICS_CACHE_CAPACITY environment variable or with the cublasLtHeuristicsCacheSetCapacity() function which has higher precedence.
The capacity is measured in number of entries and might be rounded up to the nearest multiple of some factor for performance reasons.
This can be useful for workloads that do not have a steady state and for which cache operations may have higher overhead than regular heuristics computations.
Note The cache is not ideal for performance reasons, so it is sometimes necessary to increase its capacity 1.5x-2.x over the anticipated number of unique matmul problems to achieve a nearly perfect hit rate.
See: cublasLtLoggerSetCallback() , cublasLtLoggerSetFile() , cublasLtLoggerOpenFile() , cublasLtLoggerSetLevel() , cublasLtLoggerSetMask() , cublasLtLoggerForceDisable() 3.1.4.
8-bit Floating Point Data Types (FP8) Usage  FP8 was first introduced with Ada and Hopper GPUs (compute capability 8.9 and above) and is designed to further accelerate matrix multiplications.
There are two types of FP8 available: CUDA_R_8F_E4M3 is designed to be accurate at a smaller dynamic range than half precision.
In order to maintain accurate FP8 matrix multiplications, we define native compute FP8 matrix multiplication as follows: \[D = scale_D \cdot (\alpha \cdot scale_A \cdot scale_B \cdot \text{op}(A) \text{op}(B) + \beta \cdot scale_C \cdot C)\] where A, B, and C are input matrices, and scaleA, scaleB, scaleC, scaleD, alpha, and beta are input scalars.
This differs from the other matrix multiplication routines because of this addition of scaling factors for each matrix.
The scaleA, scaleB, and scaleC are used for de-quantization, and scaleD is used for quantization.
This means that sometimes it is necessary to use a scaling factor or its reciprocal depending on the context in which it is applied.
For FP8 matrix multiplications, epilogues and amaxD may be computed as follows: \[\begin{split}D_{temp}, Aux_{temp} & = \mathop{Epilogue}(\alpha \cdot scale_A \cdot scale_B \cdot \text{op}(A) \text{op}(B) + \beta \cdot scale_C \cdot C) \\ amax_{D} & = \mathop{absmax}(D_{temp}) \\ amax_{Aux} & = \mathop{absmax}(Aux_{temp}) \\ D & = scale_D * D_{temp} \\ Aux & = scale_{Aux} * Aux_{temp} \\\end{split}\] Here Aux is an auxiliary output of an epilogue function like GELU, scaleAux is an optional scaling factor that can be applied to Aux, and amaxAux is the maximum absolute value in Aux before scaling.
For more information, see attributes CUBLASLT_MATMUL_DESC_AMAX_D_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER in cublasLtMatmulDescAttributes_t . 3.1.5. Disabling CPU Instructions  As mentioned in the Heuristics Cache section, cuBLASLt heuristics perform some compute-intensive operations on the host CPU.
To speed-up the operations, the implementation detects CPU capabilities and may use special instructions, such as Advanced Vector Extensions (AVX) on x86-64 CPUs.
For instance, using advanced instructions may result in CPU running at a lower frequency, which would affect performance of the other host code.
The user can optionally instruct the cuBLASLt library to not use some CPU instructions with the CUBLASLT_DISABLE_CPU_INSTRUCTIONS_MASK environment variable or with the cublasLtDisableCpuInstructionsSetMask() function which has higher precedence.
Please check cublasLtDisableCpuInstructionsSetMask() for more information. 3.1.6. Atomics Synchronization  Atomics synchronization allows optimizing matmul workloads by enabling cublasLtMatmul() to have a producer or consumer relationship with another concurrently running kernel.
Conceptually, matmul is provided with an array containing 32-bit integer counters, and then: In the consumer mode, either matrix A is partitioned into chunks by rows, or matrix B is partitioned into chunks by columns 1 .
A chunk can be read from memory and used in computations only when the corresponding atomic counter reaches value of 0.
The producer should execute a memory fence to ensure that the written value is visible to the concurrently running matmul kernel 2 .
In the producer mode, the output matrix C (or D in the out-of-place mode), is partitioned by rows or columns, and after a chunk is computed, the corresponding atomic counter is set to 0.
1 The current implementation allows partitioning either the rows or the columns of the matrixes, but not both.
2 One possible implementation of a memory fence is cuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope::thread_scope_device) (see cuda::atomic_thread_fence() for more details).
The array of counters are passed to matmuls via the CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_IN_COUNTERS_POINTER and CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_OUT_COUNTERS_POINTER compute descriptor attributes for the consumer and producer modes respectively 3 .
3 The current implementation allows to only enable either the producer or the consumer mode, but not both.
Matmul will return an error if both input and output counter pointers to a non-NULL value.
The number of chunks is controlled by CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_ROWS and CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_COLS compute descriptor attributes.
Both of these attributes must be set to a value greater than zero for the feature to be enabled.
For the column-major layout, the number of chunks must satisfy: \[\begin{split}0 \leq \text{$\mathrm{NUM\_CHUNKS\_ROWS}$} \leq & \mathop{\text{floor}}\left( \frac{\text{M}}{\text{$\mathrm{TILE\_SIZE\_M}$} * \text{$\mathrm{CLUSTER\_SHAPE\_M}$}}  ight) \\ 0 \leq \text{$\mathrm{NUM\_CHUNKS\_COLS}$} \leq & \mathop{\text{floor}}\left( \frac{\text{N}}{\text{$\mathrm{TILE\_SIZE\_N}$} * \text{$\mathrm{CLUSTER\_SHAPE\_N}$}}  ight)\end{split}\] For row-major layout, M and N in tile size and cluster shape must be swapped.
These restrictions mean that it is required to first query heuristic via cublasLtMatmulAlgoGetHeuristic() and inspect the result for tile and cluster shapes, and only then set the number of chunks.
The pseudocode below shows the principles of operation:   The code below shows operation when partitioning over   rows assuming column-major layout and TN case.
The case when partitioning is done over columns or   row-major case are handled in a similar fashion,   with the main difference being the offsets   computations.
Note that the actual implementation does not   guarantee in which order the chunks are computed,   and may employ various optimizations to improve   overall performance.
Here:   - A, B, C -- input matrices in the column-major layout   - lda -- leading dimension of matrix A   - M, N, K -- the original problem dimensions   - counters_in[] and counters_out[] -- the arrays of   input and output atomic counters   for ( int i = 0 ; i 1.
Each algorithm can support some custom options that don’t fit the description of the other configuration attributes.
See the CUBLASLT_ALGO_CAP_CUSTOM_OPTION_MAX of cublasLtMatmulAlgoCapAttributes_t for the accepted range for a specific case.
cublasLtMatmulDesc_t  The cublasLtMatmulDesc_t is a pointer to an opaque structure holding the description of the matrix multiplication operation cublasLtMatmul() .
A descriptor can be created by calling cublasLtMatmulDescCreate() and destroyed by calling cublasLtMatmulDescDestroy() . 3.3.9. cublasLtMatmulDescAttributes_t  cublasLtMatmulDescAttributes_t is a descriptor structure containing the attributes that define the specifics of the matrix multiply operation.
Use cublasLtMatmulDescGetAttribute() and cublasLtMatmulDescSetAttribute() to get and set the attribute value of a matmul descriptor.
Defines the data type used for multiply and accumulate operations, and the accumulator during the matrix multiplication.
The accumulator value and the value from matrix C are typically converted to scale type before final scaling.
The value is then converted from scale type to the type of matrix D before storing in memory.
int32_t CUBLASLT_MATMUL_DESC_POINTER_MODE Specifies alpha and beta are passed by reference, whether they are scalars on the host or on the device, or device vectors.
int32_t CUBLASLT_MATMUL_DESC_TRANSA Specifies the type of transformation operation that should be performed on matrix A.
int32_t CUBLASLT_MATMUL_DESC_TRANSB Specifies the type of transformation operation that should be performed on matrix B.
int32_t CUBLASLT_MATMUL_DESC_TRANSC Specifies the type of transformation operation that should be performed on matrix C.
int32_t CUBLASLT_MATMUL_DESC_FILL_MODE Indicates whether the lower or upper part of the dense matrix was filled, and consequently should be used by the function.
uint32_t CUBLASLT_MATMUL_DESC_BIAS_POINTER Bias or Bias gradient vector pointer in the device memory.
Input vector with length that matches the number of rows of matrix D when one of the following epilogues is used: CUBLASLT_EPILOGUE_BIAS , CUBLASLT_EPILOGUE_RELU_BIAS , CUBLASLT_EPILOGUE_RELU_AUX_BIAS , CUBLASLT_EPILOGUE_GELU_BIAS , CUBLASLT_EPILOGUE_GELU_AUX_BIAS .
Output vector with length that matches the number of rows of matrix D when one of the following epilogues is used: CUBLASLT_EPILOGUE_DRELU_BGRAD , CUBLASLT_EPILOGUE_DGELU_BGRAD , CUBLASLT_EPILOGUE_BGRADA .
Output vector with length that matches the number of columns of matrix D when one of the following epilogues is used: CUBLASLT_EPILOGUE_BGRADB .
Bias vector elements are the same type as alpha and beta (see CUBLASLT_MATMUL_DESC_SCALE_TYPE in this table) when matrix D datatype is CUDA_R_8I and same as matrix D datatype otherwise.
void * / const void * CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE Stride (in elements) to the next bias or bias gradient vector for strided batch operations.
Output vector for ReLu bit-mask in forward pass when CUBLASLT_EPILOGUE_RELU_AUX or CUBLASLT_EPILOGUE_RELU_AUX_BIAS epilogue is used.
Input vector for ReLu bit-mask in backward pass when CUBLASLT_EPILOGUE_DRELU or CUBLASLT_EPILOGUE_DRELU_BGRAD epilogue is used.
Output of GELU input matrix in forward pass when CUBLASLT_EPILOGUE_GELU_AUX_BIAS epilogue is used.
Input of GELU input matrix for backward pass when CUBLASLT_EPILOGUE_DGELU or CUBLASLT_EPILOGUE_DGELU_BGRAD epilogue is used.
Routines that don’t dereference this pointer, like cublasLtMatmulAlgoGetHeuristic() depend on its value to determine expected pointer alignment.
void * / const void * CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_LD Leading dimension for epilogue auxiliary buffer.
bits) when CUBLASLT_EPILOGUE_RELU_AUX , CUBLASLT_EPILOGUE_RELU_AUX_BIAS , CUBLASLT_EPILOGUE_DRELU_BGRAD , or CUBLASLT_EPILOGUE_DRELU_BGRAD epilogue is used.
GELU input matrix leading dimension in elements when CUBLASLT_EPILOGUE_GELU_AUX_BIAS , CUBLASLT_EPILOGUE_DGELU , or CUBLASLT_EPILOGUE_DGELU_BGRAD epilogue used.
int64_t CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_BATCH_STRIDE Batch stride for epilogue auxiliary buffer.
bits) when CUBLASLT_EPILOGUE_RELU_AUX , CUBLASLT_EPILOGUE_RELU_AUX_BIAS or CUBLASLT_EPILOGUE_DRELU_BGRAD epilogue is used.
GELU input matrix batch stride in elements when CUBLASLT_EPILOGUE_GELU_AUX_BIAS , CUBLASLT_EPILOGUE_DRELU , or CUBLASLT_EPILOGUE_DGELU_BGRAD epilogue used.
Used together with CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST when matrix D’s CUBLASLT_MATRIX_LAYOUT_BATCH_COUNT is greater than 1.
If CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO is set then CUBLASLT_MATMUL_DESC_ALPHA_VECTOR_BATCH_STRIDE must be set to 0 as this mode doesn’t support batched alpha vector.
int64_t CUBLASLT_MATMUL_DESC_SM_COUNT_TARGET Number of SMs to target for parallel execution.
Optimizes heuristics for execution on a different number of SMs when user expects a concurrent stream to be using some of the device resources.
int32_t CUBLASLT_MATMUL_DESC_A_SCALE_POINTER Device pointer to the scale factor value that converts data in matrix A to the compute data type range.
If set for an unsupported matrix data, scale, and compute type combination, calling cublasLtMatmul() will return CUBLAS_INVALID_VALUE .
Default value: NULL const void* CUBLASLT_MATMUL_DESC_B_SCALE_POINTER Equivalent to CUBLASLT_MATMUL_DESC_A_SCALE_POINTER for matrix B.
Default value: NULL const void* CUBLASLT_MATMUL_DESC_C_SCALE_POINTER Equivalent to CUBLASLT_MATMUL_DESC_A_SCALE_POINTER for matrix C.
Default value: NULL const void* CUBLASLT_MATMUL_DESC_D_SCALE_POINTER Equivalent to CUBLASLT_MATMUL_DESC_A_SCALE_POINTER for matrix D.
Default value: NULL const void* CUBLASLT_MATMUL_DESC_AMAX_D_POINTER Device pointer to the memory location that on completion will be set to the maximum of absolute values in the output matrix.
Default value: NULL void * CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_DATA_TYPE The type of the data that will be stored in CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER .
If unset (or set to the default value of -1), the data type is set to be the output matrix element data type (DType) with some exceptions: ReLu uses a bit-mask.
For FP8 kernels with an output type (DType) of CUDA_R_8F_E4M3 , the data type can be set to a non-default value if: AType and BType are CUDA_R_8F_E4M3 .
CType is CUDA_R_16BF or CUDA_R_16F CUBLASLT_MATMUL_DESC_EPILOGUE is set to CUBLASLT_EPILOGUE_GELU_AUX When CType is CUDA_R_16BF , the data type may be set to CUDA_R_16BF or CUDA_R_8F_E4M3 .
Default value: -1 int32_t based on cudaDataType CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER Device pointer to the scaling factor value to convert results from compute type data range to storage data range in the auxiliary matrix that is set via CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER .
Default value: NULL void * CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER Device pointer to the memory location that on completion will be set to the maximum of absolute values in the buffer that is set via CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER .
If set for an unsupported matrix data, scale, and compute type combination, calling cublasLtMatmul() will return CUBLAS_INVALID_VALUE.
Default value: NULL void * CUBLASLT_MATMUL_DESC_FAST_ACCUM Flag for managing FP8 fast accumulation mode.
When enabled, problem execution might be faster but at the cost of lower accuracy because intermediate results will not periodically be promoted to a higher precision.
Default value: 0 - fast accumulation mode is disabled int8_t CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE Type of the bias or bias gradient vector in the device memory.
If unset (or set to the default value of -1), the bias vector elements are the same type as the elements of the output matrix (Dtype) with the following exceptions: IMMA kernels with computeType= CUDA_R_32I and Ctype=CUDA_R_8I where the bias vector elements are the same type as alpha, beta ( CUBLASLT_MATMUL_DESC_SCALE_TYPE=CUDA_R_32F ) For FP8 kernels with an output type of CUDA_R_32F , CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2 .
Default value: -1 int32_t based on cudaDataType CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_IN_COUNTERS_POINTER Pointer to a device array of input atomic counters consumed by a matmul.
When a counter reaches zero, computation of the corresponding chunk of the output tensor is allowed to start.
int32_t * CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_OUT_COUNTERS_POINTER Pointer to a device array of output atomic counters produced by a matmul.
A matmul kernel sets a counter to zero when the computations of the corresponding chunk of the output tensor have completed.
int32_t * CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_ROWS Number of atomic synchronization chunks in the row dimension of the output matrix D.
int32_t CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_COLS Number of atomic synchronization chunks in the column dimension of the output matrix D.
cublasLtMatmulHeuristicResult_t  cublasLtMatmulHeuristicResult_t is a descriptor that holds the configured matrix multiplication algorithm descriptor and its runtime properties.
Member Description cublasLtMatmulAlgo_t algo Must be initialized with cublasLtMatmulAlgoInit() if the preference CUBLASLT_MATMUL_PERF_SEARCH_MODE is set to CUBLASLT_SEARCH_LIMITED_BY_ALGO_ID.
Other fields are valid only if, after call to cublasLtMatmulAlgoGetHeuristic() , this member is set to CUBLAS_STATUS_SUCCESS.
A wavesCount value of 1.0f suggests that when the kernel is launched it will fully occupy the GPU.
int reserved[4]; Reserved. 3.3.11. cublasLtMatmulInnerShape_t  cublasLtMatmulInnerShape_t is an enumerated type used to configure various aspects of the internal kernel design.
CUBLASLT_MATMUL_INNER_SHAPE_MMA16816 Inner shape is MMA16816. 3.3.12. cublasLtMatmulPreference_t  The cublasLtMatmulPreference_t is a pointer to an opaque structure holding the description of the preferences for cublasLtMatmulAlgoGetHeuristic() configuration.
Use cublasLtMatmulPreferenceCreate() to create one instance of the descriptor and cublasLtMatmulPreferenceDestroy() to destroy a previously created descriptor and release the resources. 3.3.13. cublasLtMatmulPreferenceAttributes_t  cublasLtMatmulPreferenceAttributes_t is an enumerated type used to apply algorithm search preferences while fine-tuning the heuristic function.
Use cublasLtMatmulPreferenceGetAttribute() and cublasLtMatmulPreferenceSetAttribute() to get and set the attribute value of a matmul preference descriptor.
Only algorithm configurations specifying CUBLASLT_ALGO_CONFIG_REDUCTION_SCHEME that is not masked out by this attribute are allowed.
For example, a mask value of 0x03 will allow only INPLACE and COMPUTE_TYPE reduction schemes.
uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_A_BYTES Minimum buffer alignment for matrix A (in bytes).
Selecting a smaller value will exclude algorithms that can not work with matrix A, which is not as strictly aligned as the algorithms need.
uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_B_BYTES Minimum buffer alignment for matrix B (in bytes).
Selecting a smaller value will exclude algorithms that can not work with matrix B, which is not as strictly aligned as the algorithms need.
uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_C_BYTES Minimum buffer alignment for matrix C (in bytes).
Selecting a smaller value will exclude algorithms that can not work with matrix C, which is not as strictly aligned as the algorithms need.
uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_D_BYTES Minimum buffer alignment for matrix D (in bytes).
Selecting a smaller value will exclude algorithms that can not work with matrix D, which is not as strictly aligned as the algorithms need.
Selecting a non-zero value will exclude algorithms that report device utilization higher than specified.
cublasLtMatmulSearch_t  cublasLtMatmulSearch_t is an enumerated type that contains the attributes for heuristics search type.
Value Description Data Type CUBLASLT_SEARCH_BEST_FIT Request heuristics for the best algorithm for the given use case.
CUBLASLT_SEARCH_LIMITED_BY_ALGO_ID Request heuristics only for the pre-configured algo id. 3.3.15. cublasLtMatmulTile_t  cublasLtMatmulTile_t is an enumerated type used to set the tile size in rows x columns.
CUBLASLT_MATMUL_TILE_128x96 Tile size is 128 rows x 96 columns. 3.3.16. cublasLtMatmulStages_t  cublasLtMatmulStages_t is an enumerated type used to configure the size and number of shared memory buffers where input elements are staged.
CUBLASLT_MATMUL_STAGES_8xAUTO Stage size is 8, number of stages is selected automatically.
CUBLASLT_MATMUL_STAGES_16xAUTO Stage size is 16, number of stages is selected automatically.
CUBLASLT_MATMUL_STAGES_32xAUTO Stage size is 32, number of stages is selected automatically.
CUBLASLT_MATMUL_STAGES_64xAUTO Stage size is 64, number of stages is selected automatically.
CUBLASLT_MATMUL_STAGES_128xAUTO Stage size is 128, number of stages is selected automatically. 3.3.17. cublasLtNumericalImplFlags_t  cublasLtNumericalImplFlags_t : a set of bit-flags that can be specified to select implementation details that may affect numerical behavior of algorithms.
Value Description CUBLASLT_NUMERICAL_IMPL_FLAGS_FMA Specify that the implementation is based on [H,F,D]FMA (fused multiply-add) family instructions.
CUBLASLT_NUMERICAL_IMPL_FLAGS_HMMA Specify that the implementation is based on HMMA (tensor operation) family instructions.
CUBLASLT_NUMERICAL_IMPL_FLAGS_IMMA Specify that the implementation is based on IMMA (integer tensor operation) family instructions.
CUBLASLT_NUMERICAL_IMPL_FLAGS_DMMA Specify that the implementation is based on DMMA (double precision tensor operation) family instructions.
CUBLASLT_NUMERICAL_IMPL_FLAGS_TENSOR_OP_MASK Mask to filter implementations using any of the above kinds of tensor operations.
CUBLASLT_NUMERICAL_IMPL_FLAGS_OP_TYPE_MASK Mask to filter implementation details about multiply-accumulate instructions used.
CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_16F Specify that the implementation’s inner dot product is using half precision accumulator.
CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_32F Specify that the implementation’s inner dot product is using single precision accumulator.
CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_64F Specify that the implementation’s inner dot product is using double precision accumulator.
CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_32I Specify that the implementation’s inner dot product is using 32 bit signed integer precision accumulator.
CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_TYPE_MASK Mask to filter implementation details about accumulator used.
CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_16F Specify that the implementation’s inner dot product multiply-accumulate instruction is using half-precision inputs.
CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_16BF Specify that the implementation’s inner dot product multiply-accumulate instruction is using bfloat16 inputs.
CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_TF32 Specify that the implementation’s inner dot product multiply-accumulate instruction is using TF32 inputs.
CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_32F Specify that the implementation’s inner dot product multiply-accumulate instruction is using single-precision inputs.
CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_64F Specify that the implementation’s inner dot product multiply-accumulate instruction is using double-precision inputs.
CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_8I Specify that the implementation’s inner dot product multiply-accumulate instruction is using 8-bit integer inputs.
CUBLASLT_NUMERICAL_IMPL_FLAGS_OP_INPUT_TYPE_MASK Mask to filter implementation details about accumulator input used.
CUBLASLT_NUMERICAL_IMPL_FLAGS_GAUSSIAN Specify that the implementation applies Gauss complexity reduction algorithm to reduce arithmetic complexity of the complex matrix multiplication problem 3.3.18.
cublasLtMatrixLayout_t  The cublasLtMatrixLayout_t is a pointer to an opaque structure holding the description of a matrix layout.
Use cublasLtMatrixLayoutCreate() to create one instance of the descriptor and cublasLtMatrixLayoutDestroy() to destroy a previously created descriptor and release the resources. 3.3.19. cublasLtMatrixLayoutAttribute_t  cublasLtMatrixLayoutAttribute_t is a descriptor structure containing the attributes that define the details of the matrix operation.
Use cublasLtMatrixLayoutGetAttribute() and cublasLtMatrixLayoutSetAttribute() to get and set the attribute value of a matrix layout descriptor.
Attribute Name Description Data Type CUBLASLT_MATRIX_LAYOUT_TYPE Specifies the data precision type.
uint32_t CUBLASLT_MATRIX_LAYOUT_ORDER Specifies the memory order of the data of the matrix.
Must be large enough so that matrix memory locations are not overlapping (e.g., greater or equal to CUBLASLT_MATRIX_LAYOUT_ROWS in case of CUBLASLT_ORDER_COL).
int64_t CUBLASLT_MATRIX_LAYOUT_BATCH_COUNT Number of matmul operations to perform in the batch.
int32_t CUBLASLT_MATRIX_LAYOUT_STRIDED_BATCH_OFFSET Stride (in elements) to the next matrix for the strided batch operation.
When matrix type is planar-complex (CUBLASLT_MATRIX_LAYOUT_PLANE_OFFSET != 0), batch stride is interpreted by cublasLtMatmul() in number of real valued sub-elements.
for data of type CUDA_C_16F, offset of 1024B is encoded as a stride of value 512 (since each element of the real and imaginary matrices is a 2B (16bit) floating point type).
NOTE: A bug in cublasLtMatrixTransform() causes it to interpret the batch stride for a planar-complex matrix as if it was specified in number of complex elements.
Therefore an offset of 1024B must be encoded as stride value 256 when calling cublasLtMatrixTransform() (each complex element is 4B with real and imaginary values 2B each).
int64_t CUBLASLT_MATRIX_LAYOUT_PLANE_OFFSET Stride (in bytes) to the imaginary plane for planar-complex layout.
Default value is 0, indicating that the layout is regular (real and imaginary parts of complex numbers are interleaved in memory for each element).
cublasLtMatrixTransformDesc_t  The cublasLtMatrixTransformDesc_t is a pointer to an opaque structure holding the description of a matrix transformation operation.
Use cublasLtMatrixTransformDescCreate() to create one instance of the descriptor and cublasLtMatrixTransformDescDestroy() to destroy a previously created descriptor and release the resources. 3.3.21. cublasLtMatrixTransformDescAttributes_t  cublasLtMatrixTransformDescAttributes_t is a descriptor structure containing the attributes that define the specifics of the matrix transform operation.
Use cublasLtMatrixTransformDescGetAttribute() and cublasLtMatrixTransformDescSetAttribute() to set the attribute value of a matrix transform descriptor.
Transform Attribute Name Description Data Type CUBLASLT_MATRIX_TRANSFORM_DESC_SCALE_TYPE Scale type.
Inputs are converted to the scale type for scaling and summation, and results are then converted to the output type to store in the memory.
int32_t CUBLASLT_MATRIX_TRANSFORM_DESC_POINTER_MODE Specifies the scalars alpha and beta are passed by reference whether on the host or on the device.
int32_t CUBLASLT_MATRIX_TRANSFORM_DESC_TRANSA Specifies the type of operation that should be performed on the matrix A.
int32_t CUBLASLT_MATRIX_TRANSFORM_DESC_TRANSB Specifies the type of operation that should be performed on the matrix B.
cublasLtOrder_t  cublasLtOrder_t is an enumerated type used to indicate the data ordering of the matrix.
The leading dimension is the stride (in elements) to the beginning of next column in memory.
The leading dimension is the stride (in elements) to the beginning of next group of 32-columns.
For example, if the matrix has 33 columns and 2 rows, then the leading dimension must be at least (32) * 2 = 64.
CUBLASLT_ORDER_COL4_4R2_8C Data is ordered in column-major ordered tiles of composite tiles with total 32 columns and 8 rows.
A tile is composed of interleaved inner tiles of 4 columns within 4 even or odd rows in an alternating pattern.
The leading dimension is the stride (in elements) to the beginning of the first 32 column x 8 row tile for the next 32-wide group of columns.
For example, if the matrix has 33 columns and 1 row, the leading dimension must be at least (32 * 8) * 1 = 256.
CUBLASLT_ORDER_COL32_2R_4R4 Data is ordered in column-major ordered tiles of composite tiles with total 32 columns ands 32 rows.
Leading dimension is the stride (in elements) to the beginning of the first 32 column x 32 row tile for the next 32-wide group of columns.
if matrix has 33 columns and 1 row, ld must be at least (32*32)*1 = 1024. 3.3.23. cublasLtPointerMode_t  cublasLtPointerMode_t is an enumerated type used to set the pointer mode for the scaling factors alpha and beta .
Value Description CUBLASLT_POINTER_MODE_HOST = CUBLAS_POINTER_MODE_HOST Matches CUBLAS_POINTER_MODE_HOST, and the pointer targets a single value host memory.
CUBLASLT_POINTER_MODE_DEVICE = CUBLAS_POINTER_MODE_DEVICE Matches CUBLAS_POINTER_MODE_DEVICE, and the pointer targets a single value device memory.
CUBLASLT_POINTER_MODE_DEVICE_VECTOR = 2 Pointers target device memory vectors of length equal to the number of rows of matrix D.
CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO = 3 alpha pointer targets a device memory vector of length equal to the number of rows of matrix D, and beta is zero.
CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST = 4 alpha pointer targets a device memory vector of length equal to the number of rows of matrix D, and beta is a single value in host memory. 3.3.24. cublasLtPointerModeMask_t  cublasLtPointerModeMask_t is an enumerated type used to define and query the pointer mode capability.
Value Description CUBLASLT_POINTER_MODE_MASK_HOST = 1 See CUBLASLT_POINTER_MODE_HOST in cublasLtPointerMode_t .
CUBLASLT_POINTER_MODE_MASK_DEVICE = 2 See CUBLASLT_POINTER_MODE_DEVICE in cublasLtPointerMode_t .
CUBLASLT_POINTER_MODE_MASK_DEVICE_VECTOR = 4 See CUBLASLT_POINTER_MODE_DEVICE_VECTOR in cublasLtPointerMode_t CUBLASLT_POINTER_MODE_MASK_ALPHA_DEVICE_VECTOR_BETA_ZERO = 8 See CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO in cublasLtPointerMode_t CUBLASLT_POINTER_MODE_MASK_ALPHA_DEVICE_VECTOR_BETA_HOST = 16 See CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST in cublasLtPointerMode_t 3.3.25.
cublasLtReductionScheme_t  cublasLtReductionScheme_t is an enumerated type used to specify a reduction scheme for the portions of the dot-product calculated in parallel (i.e., “split - K”).
CUBLASLT_REDUCTION_SCHEME_INPLACE Reduction is performed “in place” using the output buffer, parts are added up in the output data type.
CUBLASLT_REDUCTION_SCHEME_COMPUTE_TYPE Reduction done out of place in a user-provided workspace.
The intermediate results are stored in the compute type in the workspace and reduced in a separate step.
CUBLASLT_REDUCTION_SCHEME_OUTPUT_TYPE Reduction done out of place in a user-provided workspace.
The intermediate results are stored in the output type in the workspace and reduced in a separate step.
CUBLASLT_REDUCTION_SCHEME_MASK Allows all reduction schemes. 3.4. cuBLASLt API Reference  3.4.1.
cublasLtCreate()  cublasStatus_t cublasLtCreate ( cublasLtHandle_t * lighthandle ) This function initializes the cuBLASLt library and creates a handle to an opaque structure holding the cuBLASLt library context.
It allocates light hardware resources on the host and device, and must be called prior to making any other cuBLASLt library calls.
To use the library on multiple devices, one cuBLASLt handle should be created for each device.
Parameters: Parameter Memory Input / Output Description lightHandle Output Pointer to the allocated cuBLASLt handle for the created cuBLASLt context.
Returns: Return Value Description CUBLAS_STATUS_SUCCESS The allocation completed successfully.
This usually happens: when cublasLtCreate() is not called first an error in the CUDA Runtime API called by the cuBLASLt routine, or an error in the hardware setup.
To correct: prior to the function call, deallocate the previously allocated memory as much as possible.
CUBLAS_STATUS_INVALID_VALUE lighthandle == NULL See cublasStatus_t for a complete list of valid return codes. 3.4.2. cublasLtDestroy()  cublasStatus_t cublasLtDestroy ( cublasLtHandle_t lightHandle ) This function releases hardware resources used by the cuBLASLt library.
Because cublasLtCreate() allocates some internal resources and the release of those resources by calling cublasLtDestroy() will implicitly call cudaDeviceSynchronize() , it is recommended to minimize the number of times these functions are called.
Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the cuBLASLt handle to be destroyed.
Returns : Return Value Meaning CUBLAS_STATUS_SUCCESS The cuBLASLt context was successfully destroyed.
CUBLAS_STATUS_INVALID_VALUE lightHandle == NULL See cublasStatus_t for a complete list of valid return codes. 3.4.3. cublasLtDisableCpuInstructionsSetMask()  unsigned cublasLtDisableCpuInstructionsSetMask ( unsigned mask ); Instructs cuBLASLt library to not use CPU instructions specified by the flags in the mask .
The function takes precedence over the CUBLASLT_DISABLE_CPU_INSTRUCTIONS_MASK environment variable.
Parameters: mask – the flags combined with bitwise OR(|) operator that specify which CPU instructions should not be used.
Returns: the previous value of the mask . 3.4.4. cublasLtGetCudartVersion()  size_t cublasLtGetCudartVersion ( void ); This function returns the version number of the CUDA Runtime library.
Returns: size_t - The version number of the CUDA Runtime library. 3.4.5. cublasLtGetProperty()  cublasStatus_t cublasLtGetProperty ( libraryPropertyType type , int * value ); This function returns the value of the requested property by writing it to the memory location pointed to by the value parameter.
Parameters : Parameter Memory Input / Output Description type Input Of the type libraryPropertyType , whose value is requested from the property.
value Output Pointer to the host memory location where the requested information should be written.
Returns : Return Value Meaning CUBLAS_STATUS_SUCCESS The requested libraryPropertyType information is successfully written at the provided address.
CUBLAS_STATUS_INVALID_VALUE If invalid value of the type input argument or value == NULL See cublasStatus_t for a complete list of valid return codes. 3.4.6. cublasLtGetStatusName()  const char * cublasLtGetStatusName ( cublasStatus_t status ); Returns the string representation of a given status.
Returns: const char* - the NULL-terminated string. 3.4.7. cublasLtGetStatusString()  const char * cublasLtGetStatusString ( cublasStatus_t status ); Returns the description string for a given status.
3.4.8. cublasLtHeuristicsCacheGetCapacity()  cublasStatus_t cublasLtHeuristicsCacheGetCapacity ( size_t * capacity ); Returns the Heuristics Cache capacity.
Returns: Return Value Description CUBLAS_STATUS_SUCCESS The capacity was successfully written.
CUBLAS_STATUS_INVALID_VALUE The capacity was successfully set. 3.4.9. cublasLtHeuristicsCacheSetCapacity()  cublasStatus_t cublasLtHeuristicsCacheSetCapacity ( size_t capacity ); Sets the Heuristics Cache capacity.
This function takes precedence over CUBLASLT_HEURISTICS_CACHE_CAPACITY environment variable.
Returns: Return Value Description CUBLAS_STATUS_SUCCESS The capacity was successfully set. 3.4.10. cublasLtGetVersion()  size_t cublasLtGetVersion ( void ); This function returns the version number of cuBLASLt library.
Returns: size_t - The version number of cuBLASLt library. 3.4.11. cublasLtLoggerSetCallback()  cublasStatus_t cublasLtLoggerSetCallback ( cublasLtLoggerCallback_t callback ); Experimental: This function sets the logging callback function.
Parameters : Parameter Memory Input / Output Description callback Input Pointer to a callback function.
Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the callback function was successfully set.
See cublasStatus_t for a complete list of valid return codes. 3.4.12. cublasLtLoggerSetFile()  cublasStatus_t cublasLtLoggerSetFile ( FILE * file ); Experimental: This function sets the logging output file.
Note: once registered using this function call, the provided file handle must not be closed unless the function is called again to switch to a different file handle.
Parameters : Parameter Memory Input / Output Description file Input Pointer to an open file.
Returns : Return Value Description CUBLAS_STATUS_SUCCESS If logging file was successfully set. 3.4.13. cublasLtLoggerOpenFile()  cublasStatus_t cublasLtLoggerOpenFile ( const char * logFile ); Experimental: This function opens a logging output file in the given path.
Parameters : Parameter Memory Input / Output Description logFile Input Path of the logging output file.
Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the logging file was successfully opened. 3.4.14. cublasLtLoggerSetLevel()  cublasStatus_t cublasLtLoggerSetLevel ( int level ); Experimental: This function sets the value of the logging level.
Parameters : Parameter Memory Input / Output Description level Input Value of the logging level.
Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If the value was not a valid logging level.
CUBLAS_STATUS_SUCCESS If the logging level was successfully set. 3.4.15. cublasLtLoggerSetMask()  cublasStatus_t cublasLtLoggerSetMask ( int mask ); Experimental: This function sets the value of the logging mask.
Parameters : Parameter Memory Input / Output Description mask Input Value of the logging mask.
Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the logging mask was successfully set. 3.4.16. cublasLtLoggerForceDisable()  cublasStatus_t cublasLtLoggerForceDisable (); Experimental: This function disables logging for the entire run.
Returns : Return Value Description CUBLAS_STATUS_SUCCESS If logging was successfully disabled. 3.4.17. cublasLtMatmul()  cublasStatus_t cublasLtMatmul ( cublasLtHandle_t lightHandle , cublasLtMatmulDesc_t computeDesc , const void * alpha , const void * A , cublasLtMatrixLayout_t Adesc , const void * B , cublasLtMatrixLayout_t Bdesc , const void * beta , const void * C , cublasLtMatrixLayout_t Cdesc , void * D , cublasLtMatrixLayout_t Ddesc , const cublasLtMatmulAlgo_t * algo , void * workspace , size_t workspaceSizeInBytes , cudaStream_t stream ); This function computes the matrix multiplication of matrices A and B to produce the output matrix D, according to the following operation: D = alpha*(A*B) + beta*(C), where A , B , and C are input matrices, and alpha and beta are input scalars.
Note This function supports both in-place matrix multiplication ( C == D and Cdesc == Ddesc ) and out-of-place matrix multiplication ( C != D , both matrices must have the same data type, number of rows, number of columns, batch size, and memory order).
In the out-of-place case, the leading dimension of C can be different from the leading dimension of D.
The recommendations on workspaceSizeInBytes are the same as mentioned in the cublasSetWorkspace() section.
Datatypes Supported: cublasLtMatmul() supports the following computeType, scaleType, Atype/Btype, and Ctype.
When A, B, C, and D are Regular Column- or Row-major Matrices  computeType scaleType Atype/Btype Ctype Bias Type 5 CUBLAS_COMPUTE_16F or CUBLAS_COMPUTE_16F_PEDANTIC CUDA_R_16F CUDA_R_16F CUDA_R_16F CUDA_R_16F 5 CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I CUDA_R_32I Non-default epilogue not supported.
CUBLAS_COMPUTE_32F or CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16BF 5 CUDA_R_16F CUDA_R_16F CUDA_R_16F 5 CUDA_R_8I CUDA_R_32F Non-default epilogue not supported.
CUDA_R_16BF CUDA_R_32F CUDA_R_32F 5 CUDA_R_16F CUDA_R_32F CUDA_R_32F 5 CUDA_R_32F CUDA_R_32F CUDA_R_32F 5 CUDA_C_32F 6 CUDA_C_8I 6 CUDA_C_32F 6 Non-default epilogue not supported.
CUDA_C_32F 6 CUDA_C_32F 6 CUBLAS_COMPUTE_32F_FAST_16F or CUBLAS_COMPUTE_32F_FAST_16BF or CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F 5 CUDA_C_32F 6 CUDA_C_32F 6 CUDA_C_32F 6 Non-default epilogue not supported.
CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F 5 CUDA_C_64F 6 CUDA_C_64F 6 CUDA_C_64F 6 Non-default epilogue not supported.
To use IMMA kernels, one of the following sets of requirements, with the first being the preferred one, must be met: Using a regular data ordering: All matrix pointers must be 4-byte aligned.
Pointer mode can be CUBLASLT_POINTER_MODE_HOST , CUBLASLT_POINTER_MODE_DEVICE or CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST .
With the latter mode, the kernels support the CUBLASLT_MATMUL_DESC_ALPHA_VECTOR_BATCH_STRIDE attribute.
Using the IMMA-specific data ordering on Ampere or Turing (but not Hopper) architecture - CUBLASLT_ORDER_COL32` for matrices A, C, D, and CUBLASLT_ORDER_COL4_4R2_8C (on Turing or Ampere architecture) or CUBLASLT_ORDER_COL32_2R_4R4 (on Ampere architecture) for matrix B: Leading dimensions of matrices A, B, C must fulfill conditions specific to the memory ordering (see cublasLtOrder_t ).
Matmul descriptor must specify CUBLAS_OP_T on matrix B and CUBLAS_OP_N (default) on matrix A and C.
If scaleType CUDA_R_32I is used, the only supported values for alpha and beta are 0 or 1 .
Pointer mode can be CUBLASLT_POINTER_MODE_HOST , CUBLASLT_POINTER_MODE_DEVICE , CUBLASLT_POINTER_MODE_DEVICE_VECTOR or CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO .
When A, B, C, and D Use Layouts for IMMA  computeType scaleType Atype/Btype Ctype Bias Type CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I CUDA_R_32I Non-default epilogue not supported.
CUDA_R_32F CUDA_R_8I CUDA_R_8I CUDA_R_32F To use FP8 kernels, the following set of requirements must be satisfied: All matrix dimensions must meet the optimal requirements listed in Tensor Core Usage (i.e.
When A, B, C, and D are Planar-Complex Matrices  computeType scaleType Atype/Btype Ctype CUBLAS_COMPUTE_32F CUDA_C_32F CUDA_C_16F 6 CUDA_C_16F 6 CUDA_C_32F 6 CUDA_C_16BF 6 CUDA_C_16BF 6 CUDA_C_32F 6 NOTES: 5 ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 ) ReLU, dReLu, GELU, dGELU and Bias epilogue modes (see CUBLASLT_MATMUL_DESC_EPILOGUE in cublasLtMatmulDescAttributes_t ) are not supported when D matrix memory order is defined as CUBLASLT_ORDER_ROW .
For best performance when using the bias vector, specify zero beta and set pointer mode to CUBLASLT_POINTER_MODE_HOST .
6 ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 ) Use of CUBLAS_ORDER_ROW together with CUBLAS_OP_C (Hermitian operator) is not supported unless all of A, B, C, and D matrices use the CUBLAS_ORDER_ROW ordering.
Parameters: Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context.
computeDesc Input Handle to a previously created matrix multiplication descriptor of type cublasLtMatmulDesc_t .
A, B, and C Device Input Pointers to the GPU memory associated with the corresponding descriptors Adesc, Bdesc and Cdesc.
Ddesc Input Handle to the previous created descriptor of the type cublasLtMatrixLayout_t .
When NULL, an implicit heuritics query with default search preferences will be performed to determine actual algorithm to use.
Returns: Return Value Description CUBLAS_STATUS_NOT_INITIALIZED If cuBLASLt handle has not been initialized.
CUBLAS_STATUS_INVALID_VALUE If the parameters are unexpectedly NULL, in conflict or in an impossible configuration.
For example, when workspaceSizeInBytes is less than workspace required by the configured algo.
CUBLAS_STATUS_NOT_SUPPORTED If the current implementation on the selected device doesn’t support the configured operation.
CUBLAS_STATUS_ARCH_MISMATCH If the configured operation cannot be run using the selected device.
CUBLAS_STATUS_SUCCESS If the operation completed successfully. 3.4.18. cublasLtMatmulAlgoCapGetAttribute()  cublasStatus_t cublasLtMatmulAlgoCapGetAttribute ( const cublasLtMatmulAlgo_t * algo , cublasLtMatmulAlgoCapAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried capability attribute for an initialized cublasLtMatmulAlgo_t descriptor structure.
The capability attribute value is retrieved from the enumerated type cublasLtMatmulAlgoCapAttributes_t .
For example, to get list of supported Tile IDs: cublasLtMatmulTile_t tiles [ CUBLASLT_MATMUL_TILE_END ]; size_t num_tiles , size_written ; if ( cublasLtMatmulAlgoCapGetAttribute ( algo , CUBLASLT_ALGO_CAP_TILE_IDS , tiles , sizeof ( tiles ), & size_written ) == CUBLAS_STATUS_SUCCESS ) { num_tiles = size_written / sizeof ( tiles [ 0 ]);} Parameters : Parameter Memory Input / Output Description algo Input Pointer to the previously created opaque structure holding the matrix multiply algorithm descriptor.
If sizeInBytes is non-zero: then sizeWritten is the number of bytes actually written; if sizeInBytes is 0: then sizeWritten is the number of bytes needed to write full contents.
It checks whether the descriptor is supported on the current device, and returns the result containing the required workspace and the calculated wave count.
Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context.
operationDesc Input Handle to a previously created matrix multiplication descriptor of type cublasLtMatmulDesc_t .
Adesc, Bdesc, Cdesc, and Ddesc Input Handles to the previously created matrix layout descriptors of the type cublasLtMatrixLayout_t .
algo Input Descriptor which specifies which matrix multiplication algorithm should be used.
Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If matrix layout descriptors or the operation descriptor do not match the algo descriptor.
CUBLAS_STATUS_NOT_SUPPORTED If the algo configuration or data type combination is not currently supported on the given device.
CUBLAS_STATUS_ARCH_MISMATCH If the algo configuration cannot be run using the selected device.
CUBLAS_STATUS_SUCCESS If the check was successful. 3.4.20. cublasLtMatmulAlgoConfigGetAttribute()  cublasStatus_t cublasLtMatmulAlgoConfigGetAttribute ( const cublasLtMatmulAlgo_t * algo , cublasLtMatmulAlgoConfigAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried configuration attribute for an initialized cublasLtMatmulAlgo_t descriptor.
The configuration attribute value is retrieved from the enumerated type cublasLtMatmulAlgoConfigAttributes_t .
Parameters : Parameter Memory Input / Output Description algo Input Pointer to the previously created opaque structure holding the matrix multiply algorithm descriptor.
See cublasLtMatmulAlgoConfigAttributes_t . 3.4.21. cublasLtMatmulAlgoConfigSetAttribute()  cublasStatus_t cublasLtMatmulAlgoConfigSetAttribute ( cublasLtMatmulAlgo_t * algo , cublasLtMatmulAlgoConfigAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified configuration attribute for an initialized cublasLtMatmulAlgo_t descriptor.
The configuration attribute is an enumerant of the type cublasLtMatmulAlgoConfigAttributes_t .
Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If buf is NULL or sizeInBytes doesn’t match the size of the internal storage for the selected attribute.
CUBLAS_STATUS_SUCCESS If the attribute was set successfully. 3.4.22. cublasLtMatmulAlgoGetHeuristic()  cublasStatus_t cublasLtMatmulAlgoGetHeuristic ( cublasLtHandle_t lightHandle , cublasLtMatmulDesc_t operationDesc , cublasLtMatrixLayout_t Adesc , cublasLtMatrixLayout_t Bdesc , cublasLtMatrixLayout_t Cdesc , cublasLtMatrixLayout_t Ddesc , cublasLtMatmulPreference_t preference , int requestedAlgoCount , cublasLtMatmulHeuristicResult_t heuristicResultsArray [] int * returnAlgoCount ); This function retrieves the possible algorithms for the matrix multiply operation cublasLtMatmul() function with the given input matrices A, B and C, and the output matrix D.
The output is placed in heuristicResultsArray[] in the order of increasing estimated compute time.
preference Input Pointer to the structure holding the heuristic search preferences descriptor.
heuristicResultsArray[] Output Array containing the algorithm heuristics and associated runtime characteristics, returned by this function, in the order of increasing estimated compute time.
Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If requestedAlgoCount is less or equal to zero.
Inspect heuristicResultsArray[0 to (returnAlgoCount -1)].state for the status of the results.
Note This function may load some kernels using CUDA Driver API which may fail when there is no available GPU memory.
Do not allocate the entire VRAM before running cublasLtMatmulAlgoGetHeuristic() . 3.4.23. cublasLtMatmulAlgoGetIds()  cublasStatus_t cublasLtMatmulAlgoGetIds ( cublasLtHandle_t lightHandle , cublasComputeType_t computeType , cudaDataType_t scaleType , cudaDataType_t Atype , cudaDataType_t Btype , cudaDataType_t Ctype , cudaDataType_t Dtype , int requestedAlgoCount , int algoIdsArray [], int * returnAlgoCount ); This function retrieves the IDs of all the matrix multiply algorithms that are valid, and can potentially be run by the cublasLtMatmul() function, for given types of the input matrices A, B and C, and of the output matrix D.
To make sure the best possible algo is contained in the list, make requestedAlgoCount large enough to receive the full list.
Inspect returnAlgoCount to get actual number of IDs available. 3.4.24. cublasLtMatmulAlgoInit()  cublasStatus_t cublasLtMatmulAlgoInit ( cublasLtHandle_t lightHandle , cublasComputeType_t computeType , cudaDataType_t scaleType , cudaDataType_t Atype , cudaDataType_t Btype , cudaDataType_t Ctype , cudaDataType_t Dtype , int algoId , cublasLtMatmulAlgo_t * algo ); This function initializes the matrix multiply algorithm structure for the cublasLtMatmul() , for a specified matrix multiply algorithm and input matrices A, B and C, and the output matrix D.
Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context.
Atype, Btype, Ctype, and Dtype Input Datatype precision for the input and output matrices.
Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If algo is NULL or algoId is outside the recognized range.
CUBLAS_STATUS_NOT_SUPPORTED If algoId is not supported for given combination of data types.
CUBLAS_STATUS_SUCCESS If the structure was successfully initialized. 3.4.25. cublasLtMatmulDescCreate()  cublasStatus_t cublasLtMatmulDescCreate ( cublasLtMatmulDesc_t * matmulDesc , cublasComputeType_t computeType , cudaDataType_t scaleType ); This function creates a matrix multiply descriptor by allocating the memory needed to hold its opaque structure.
Parameters : Parameter Memory Input / Output Description matmulDesc Output Pointer to the structure holding the matrix multiply descriptor created by this function.
computeType Input Enumerant that specifies the data precision for the matrix multiply descriptor this function creates.
scaleType Input Enumerant that specifies the data precision for the matrix transform descriptor this function creates.
Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If memory could not be allocated.
CUBLAS_STATUS_SUCCESS If the descriptor was created successfully. 3.4.26. cublasLtMatmulDescInit()  cublasStatus_t cublasLtMatmulDescInit ( cublasLtMatmulDesc_t matmulDesc , cublasComputeType_t computeType , cudaDataType_t scaleType ); This function initializes a matrix multiply descriptor in a previously allocated one.
Parameters : Parameter Memory Input / Output Description matmulDesc Output Pointer to the structure holding the matrix multiply descriptor initialized by this function.
computeType Input Enumerant that specifies the data precision for the matrix multiply descriptor this function initializes.
scaleType Input Enumerant that specifies the data precision for the matrix transform descriptor this function initializes. 3.4.27. cublasLtMatmulDescDestroy()  cublasStatus_t cublasLtMatmulDescDestroy ( cublasLtMatmulDesc_t matmulDesc ); This function destroys a previously created matrix multiply descriptor object.
Parameters : Parameter Memory Input / Output Description matmulDesc Input Pointer to the structure holding the matrix multiply descriptor that should be destroyed by this function.
Returns : Return Value Description CUBLAS_STATUS_SUCCESS If operation was successful. 3.4.28. cublasLtMatmulDescGetAttribute()  cublasStatus_t cublasLtMatmulDescGetAttribute ( cublasLtMatmulDesc_t matmulDesc , cublasLtMatmulDescAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to a previously created matrix multiply descriptor.
Parameters : Parameter Memory Input / Output Description matmulDesc Input Pointer to the previously created structure holding the matrix multiply descriptor queried by this function.
buf Output Memory address containing the attribute value retrieved by this function. 3.4.29. cublasLtMatmulDescSetAttribute()  cublasStatus_t cublasLtMatmulDescSetAttribute ( cublasLtMatmulDesc_t matmulDesc , cublasLtMatmulDescAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix multiply descriptor.
buf Input The value to which the specified attribute should be set. 3.4.30. cublasLtMatmulPreferenceCreate()  cublasStatus_t cublasLtMatmulPreferenceCreate ( cublasLtMatmulPreference_t * pref ); This function creates a matrix multiply heuristic search preferences descriptor by allocating the memory needed to hold its opaque structure.
Parameters : Parameter Memory Input / Output Description pref Output Pointer to the structure holding the matrix multiply preferences descriptor created by this function.
See cublasLtMatrixLayout_t . 3.4.31. cublasLtMatmulPreferenceInit()  cublasStatus_t cublasLtMatmulPreferenceInit ( cublasLtMatmulPreference_t pref ); This function initializes a matrix multiply heuristic search preferences descriptor in a previously allocated one.
3.4.32. cublasLtMatmulPreferenceDestroy()  cublasStatus_t cublasLtMatmulPreferenceDestroy ( cublasLtMatmulPreference_t pref ); This function destroys a previously created matrix multiply preferences descriptor object.
Parameters : Parameter Memory Input / Output Description pref Input Pointer to the structure holding the matrix multiply preferences descriptor that should be destroyed by this function.
Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the operation was successful. 3.4.33. cublasLtMatmulPreferenceGetAttribute()  cublasStatus_t cublasLtMatmulPreferenceGetAttribute ( cublasLtMatmulPreference_t pref , cublasLtMatmulPreferenceAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to a previously created matrix multiply heuristic search preferences descriptor.
Parameters : Parameter Memory Input / Output Description pref Input Pointer to the previously created structure holding the matrix multiply heuristic search preferences descriptor queried by this function.
See cublasLtMatmulPreferenceAttributes_t . 3.4.34. cublasLtMatmulPreferenceSetAttribute()  cublasStatus_t cublasLtMatmulPreferenceSetAttribute ( cublasLtMatmulPreference_t pref , cublasLtMatmulPreferenceAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix multiply preferences descriptor.
Parameters : Parameter Memory Input / Output Description pref Input Pointer to the previously created structure holding the matrix multiply preferences descriptor queried by this function. 3.4.35. cublasLtMatrixLayoutCreate()  cublasStatus_t cublasLtMatrixLayoutCreate ( cublasLtMatrixLayout_t * matLayout , cudaDataType type , uint64_t rows , uint64_t cols , int64_t ld ); This function creates a matrix layout descriptor by allocating the memory needed to hold its opaque structure.
Parameters : Parameter Memory Input / Output Description matLayout Output Pointer to the structure holding the matrix layout descriptor created by this function.
type Input Enumerant that specifies the data precision for the matrix layout descriptor this function creates.
Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If the memory could not be allocated. 3.4.36. cublasLtMatrixLayoutInit()  cublasStatus_t cublasLtMatrixLayoutInit ( cublasLtMatrixLayout_t matLayout , cudaDataType type , uint64_t rows , uint64_t cols , int64_t ld ); This function initializes a matrix layout descriptor in a previously allocated one.
Parameters : Parameter Memory Input / Output Description matLayout Output Pointer to the structure holding the matrix layout descriptor initialized by this function.
type Input Enumerant that specifies the data precision for the matrix layout descriptor this function initializes. 3.4.37. cublasLtMatrixLayoutDestroy()  cublasStatus_t cublasLtMatrixLayoutDestroy ( cublasLtMatrixLayout_t matLayout ); This function destroys a previously created matrix layout descriptor object.
Parameters : Parameter Memory Input / Output Description matLayout Input Pointer to the structure holding the matrix layout descriptor that should be destroyed by this function. 3.4.38. cublasLtMatrixLayoutGetAttribute()  cublasStatus_t cublasLtMatrixLayoutGetAttribute ( cublasLtMatrixLayout_t matLayout , cublasLtMatrixLayoutAttribute_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to the specified matrix layout descriptor.
Parameters : Parameter Memory Input / Output Description matLayout Input Pointer to the previously created structure holding the matrix layout descriptor queried by this function.
See cublasLtMatrixLayoutAttribute_t . 3.4.39. cublasLtMatrixLayoutSetAttribute()  cublasStatus_t cublasLtMatrixLayoutSetAttribute ( cublasLtMatrixLayout_t matLayout , cublasLtMatrixLayoutAttribute_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix layout descriptor.
Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If buf is NULL or sizeInBytes doesn’t match size of internal storage for the selected attribute.
CUBLAS_STATUS_SUCCESS If attribute was set successfully. 3.4.40. cublasLtMatrixTransform()  cublasStatus_t cublasLtMatrixTransform ( cublasLtHandle_t lightHandle , cublasLtMatrixTransformDesc_t transformDesc , const void * alpha , const void * A , cublasLtMatrixLayout_t Adesc , const void * beta , const void * B , cublasLtMatrixLayout_t Bdesc , void * C , cublasLtMatrixLayout_t Cdesc , cudaStream_t stream ); This function computes the matrix transformation operation on the input matrices A and B, to produce the output matrix C, according to the below operation: C = alpha*transformation(A) + beta*transformation(B), where A , B are input matrices, and alpha and beta are input scalars.
This function can be used to change the memory order of data or to scale and shift the values.
transformDesc Input Pointer to the opaque descriptor holding the matrix transformation operation.
A, B, and C Device Input Pointers to the GPU memory associated with the corresponding descriptors Adesc , Bdesc and Cdesc .
Adesc or Bdesc can be NULL if corresponding pointer is NULL and corresponding scalar is zero.
Returns : Return Value Description CUBLAS_STATUS_NOT_INITIALIZED If cuBLASLt handle has not been initialized.
CUBLAS_STATUS_INVALID_VALUE If the parameters are in conflict or in an impossible configuration.
CUBLAS_STATUS_NOT_SUPPORTED If the current implementation on the selected device does not support the configured operation. 3.4.41. cublasLtMatrixTransformDescCreate()  cublasStatus_t cublasLtMatrixTransformDescCreate ( cublasLtMatrixTransformDesc_t * transformDesc , cudaDataType scaleType ); This function creates a matrix transform descriptor by allocating the memory needed to hold its opaque structure.
Parameters : Parameter Memory Input / Output Description transformDesc Output Pointer to the structure holding the matrix transform descriptor created by this function. 3.4.42. cublasLtMatrixTransformDescInit()  cublasStatus_t cublasLtMatrixTransformDescInit ( cublasLtMatrixTransformDesc_t transformDesc , cudaDataType scaleType ); This function initializes a matrix transform descriptor in a previously allocated one.
Parameters : Parameter Memory Input / Output Description transformDesc Output Pointer to the structure holding the matrix transform descriptor initialized by this function. 3.4.43. cublasLtMatrixTransformDescDestroy()  cublasStatus_t cublasLtMatrixTransformDescDestroy ( cublasLtMatrixTransformDesc_t transformDesc ); This function destroys a previously created matrix transform descriptor object.
Parameters : Parameter Memory Input / Output Description transformDesc Input Pointer to the structure holding the matrix transform descriptor that should be destroyed by this function. 3.4.44. cublasLtMatrixTransformDescGetAttribute()  cublasStatus_t cublasLtMatrixTransformDescGetAttribute ( cublasLtMatrixTransformDesc_t transformDesc , cublasLtMatrixTransformDescAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to a previously created matrix transform descriptor.
Parameters : Parameter Memory Input / Output Description transformDesc Input Pointer to the previously created structure holding the matrix transform descriptor queried by this function.
See cublasLtMatrixTransformDescAttributes_t . 3.4.45. cublasLtMatrixTransformDescSetAttribute()  cublasStatus_t cublasLtMatrixTransformDescSetAttribute ( cublasLtMatrixTransformDesc_t transformDesc , cublasLtMatrixTransformDescAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix transform descriptor.
Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If buf is NULL or sizeInBytes does not match size of the internal storage for the selected attribute. 4. Using the cuBLASXt API  4.1.
General description  The cuBLASXt API of cuBLAS exposes a multi-GPU capable host interface: when using this API the application only needs to allocate the required matrices on the host memory space.
Additionally, the current implementation supports managed memory on Linux with GPU devices that have compute capability 6.x or greater but treats it as host memory.
There are no restriction on the sizes of the matrices as long as they can fit into the host memory.
The cuBLASXt API takes care of allocating the memory across the designated GPUs and dispatched the workload between them and finally retrieves the results back to the host.
The cuBLASXt API supports only the compute-intensive BLAS3 routines (e.g matrix-matrix operations) where the PCI transfers back and forth from the GPU can be amortized.
Starting with release 8.0, cuBLASXt API allows any of the matrices to be located on a GPU device.
Note : The cuBLASXt API is only supported on 64-bit platforms. 4.1.1. Tiling design approach  To be able to share the workload between multiples GPUs, the cuBLASXt API uses a tiling strategy : every matrix is divided in square tiles of user-controllable dimension BlockDim x BlockDim.
The resulting matrix tiling defines the static scheduling policy : each resulting tile is affected to a GPU in a round robin fashion One CPU thread is created per GPU and is responsible to do the proper memory transfers and cuBLAS operations to compute all the tiles that it is responsible for.
From a performance point of view, due to this static scheduling strategy, it is better that compute capabilites and PCI bandwidth are the same for every GPU.
To compute the first tile G0 from C, the CPU thread 0 responsible of GPU0, have to load 3 tiles from the first row of A and tiles from the first columun of B in a pipeline fashion in order to overlap memory transfer and computations and sum the results into the first tile G0 of C before to move on to the next tile G0.
Example of cublasXtgemm tiling for 3 Gpus  When the tile dimension is not an exact multiple of the dimensions of C, some tiles are partially filled on the right border or/and the bottom border.
The current implementation does not pad the incomplete tiles but simply keep track of those incomplete tiles by doing the right reduced cuBLAS opearations : this way, no extra computation is done.
However it still can lead to some load unbalance when all GPUS do not have the same number of incomplete tiles to work on.
When one or more matrices are located on some GPU devices, the same tiling approach and workload sharing is applied.
However, when the computation of a tile and some data are located on the same GPU device, the memory transfer to/from the local data into tiles is bypassed and the GPU operates directly on the local data.
This can lead to a significant performance increase, especially when only one GPU is used for the computation.
The matrices can be located on any GPU device, and do not have to be located on the same GPU device.
Furthermore, the matrices can even be located on a GPU device that do not participate to the computation.
On the contrary of the cuBLAS API, even if all matrices are located on the same device, the cuBLASXt API is still a blocking API from the host point of view : the data results wherever located will be valid on the call return and no device synchronization is required. 4.1.2. Hybrid CPU-GPU computation  In the case of very large problems, the cuBLASXt API offers the possibility to offload some of the computation to the host CPU.
This feature can be setup with the routines cublasXtSetCpuRoutine() and cublasXtSetCpuRatio() The workload affected to the CPU is put aside : it is simply a percentage of the resulting matrix taken from the bottom and the right side whichever dimension is bigger.
If any of the matrices is located on a GPU device, the feature is ignored and all computation will be done only on the GPUs This feature should be used with caution because it could interfere with the CPU threads responsible of feeding the GPUs.
Currently, only the routine cublasXtgemm supports this feature. 4.1.3. Results reproducibility  Currently all cuBLASXt API routines from a given toolkit version, generate the same bit-wise results when the following conditions are respected : all GPUs particating to the computation have the same compute capabilities and the same number of SMs.
either the CPU hybrid computation is not used or the CPU Blas provided is also guaranteed to produce reproducible results. 4.2. cuBLASXt API Datatypes Reference  4.2.1.
cublasXtHandle_t  The cublasXtHandle_t type is a pointer type to an opaque structure holding the cuBLASXt API context.
The cuBLASXt API context must be initialized using cublasXtCreate() and the returned handle must be passed to all subsequent cuBLASXt API function calls.
The context should be destroyed at the end using cublasXtDestroy() . 4.2.2. cublasXtOpType_t  The cublasOpType_t enumerates the four possible types supported by BLAS routines.
This enum is used as parameters of the routines cublasXtSetCpuRoutine and cublasXtSetCpuRatio to setup the hybrid configuration.
Value Meaning CUBLASXT_FLOAT float or single precision type CUBLASXT_DOUBLE double precision type CUBLASXT_COMPLEX single precision complex CUBLASXT_DOUBLECOMPLEX double precision complex 4.2.3.
cublasXtBlasOp_t  The cublasXtBlasOp_t type enumerates the BLAS3 or BLAS-like routine supported by cuBLASXt API.
Value Meaning CUBLASXT_GEMM GEMM routine CUBLASXT_SYRK SYRK routine CUBLASXT_HERK HERK routine CUBLASXT_SYMM SYMM routine CUBLASXT_HEMM HEMM routine CUBLASXT_TRSM TRSM routine CUBLASXT_SYR2K SYR2K routine CUBLASXT_HER2K HER2K routine CUBLASXT_SPMM SPMM routine CUBLASXT_SYRKX SYRKX routine CUBLASXT_HERKX HERKX routine 4.2.4.
cublasXtPinningMemMode_t  The type is used to enable or disable the Pinning Memory mode through the routine cubasMgSetPinningMemMode Value Meaning CUBLASXT_PINNING_DISABLED the Pinning Memory mode is disabled CUBLASXT_PINNING_ENABLED the Pinning Memory mode is enabled 4.3.
cublasXtCreate()  cublasStatus_t cublasXtCreate ( cublasXtHandle_t * handle ) This function initializes the cuBLASXt API and creates a handle to an opaque structure holding the cuBLASXt API context.
It allocates hardware resources on the host and device and must be called prior to making any other cuBLASXt API calls.
Return Value Meaning CUBLAS_STATUS_SUCCESS the initialization succeeded CUBLAS_STATUS_ALLOC_FAILED the resources could not be allocated CUBLAS_STATUS_NOT_SUPPORTED cuBLASXt API is only supported on 64-bit platform 4.3.2.
cublasXtDestroy()  cublasStatus_t cublasXtDestroy ( cublasXtHandle_t handle ) This function releases hardware resources used by the cuBLASXt API context.
Return Value Meaning CUBLAS_STATUS_SUCCESS the shut down succeeded CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 4.3.3.
cublasXtDeviceSelect()  cublasXtDeviceSelect ( cublasXtHandle_t handle , int nbDevices , int deviceId []) This function allows the user to provide the number of GPU devices and their respective Ids that will participate to the subsequent cuBLASXt API Math function calls.
Currently the device configuration is static and cannot be changed between Math function calls.
To be able to run multiple configurations, multiple cuBLASXt API contexts should be created.
Return Value Meaning CUBLAS_STATUS_SUCCESS User call was sucessful CUBLAS_STATUS_INVALID_VALUE Access to at least one of the device could not be done or a cuBLAS context could not be created on at least one of the device CUBLAS_STATUS_ALLOC_FAILED Some resources could not be allocated. 4.3.4. cublasXtSetBlockDim()  cublasXtSetBlockDim ( cublasXtHandle_t handle , int blockDim ) This function allows the user to set the block dimension used for the tiling of the matrices for the subsequent Math function calls.
This function can be called anytime and will take effect for the following Math function calls.
The block dimension should be chosen in a way to optimize the math operation and to make sure that the PCI transfers are well overlapped with the computation.
Return Value Meaning CUBLAS_STATUS_SUCCESS the call has been successful CUBLAS_STATUS_INVALID_VALUE blockDim for type and for the corresponding short type to make a more concise and clear presentation of the implemented functions.
A host or device input array of dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise.
B host or device input array of dimension ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise.
A host or device input array of dimension lda x m with lda>=max(1,m) if side==CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise.
beta host input scalar used for multiplication, if beta==0 then C does not have to be a valid input.
A host or device input array of dimension lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise.
beta host input scalar used for multiplication, if beta == 0 then C does not have to be a valid input.
A host or device input array of dimension lda x k with lda>=max(1,n) if trans == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise.
beta host input scalar used for multiplication, if beta==0 then C does not have to be a valid input.
A host or device input array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise.
B host or device input array of dimensions ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise.
beta host input scalar used for multiplication, if beta==0 , then C does not have to be a valid input.
Also, for matrix \(A\) and \(B\) \(\text{op(}A\text{) and op(}B\text{)} = \left\{ \begin{matrix} {A\text{ and }B} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_N}$}} \\ {A^{T}\text{ and }B^{T}} & {\text{if }\textsf{trans == $\mathrm{CUBLAS\_OP\_T}$}} \\ \end{matrix}  ight.\) This routine can be used when B is in such way that the result is guaranteed to be symmetric.
An usual example is when the matrix B is a scaled form of the matrix A : this is equivalent to B being the product of the matrix A and a diagonal matrix.
B host or device input array of dimension ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise.
For an efficient computation of the product of a regular matrix with a diagonal matrix, refer to the routine cublasXtdgmm .
beta host input real scalar used for multiplication, if beta==0 then C does not have to be a valid input.
alpha host input scalar used for multiplication, if alpha==0 then A is not referenced and B does not have to be a valid input.
Also, for matrix \(A\) \(\text{op}(A) = \left\{ \begin{matrix} A & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_N}$}} \\ A^{T} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_T}$}} \\ A^{H} & {\text{if }\textsf{transa == $\mathrm{CUBLAS\_OP\_C}$}} \\ \end{matrix}  ight.\) Notice that in order to achieve better parallelism, similarly to the cublas API, cuBLASXt API differs from the BLAS API for this routine.
The BLAS API assumes an in-place implementation (with results written back to B), while the cuBLASXt API assumes an out-of-place implementation (with results written into C).
The application can still obtain the in-place functionality of BLAS in the cuBLASXt API by passing the address of the matrix B in place of the matrix C.
Note The packed matrix AP must be located on the host or managed memory whereas the other matrices can be located on the host or any GPU device Param.
cuBLAS Fortran Bindings  The cuBLAS library is implemented using the C-based CUDA toolchain.
This makes interfacing to applications written in C and C++ trivial, but the library can also be used by applications written in Fortran.
In particular, the cuBLAS library uses 1-based indexing and Fortran-style column-major storage for multidimensional data to simplify interfacing to Fortran applications.
Unfortunately, Fortran-to-C calling conventions are not standardized and differ by platform and toolchain.
In particular, differences may exist in the following areas: symbol names (capitalization, name decoration) argument passing (by value or reference) passing of string arguments (length information) passing of pointer arguments (size of the pointer) returning floating-point or compound data types (for example single-precision or complex data types) To provide maximum flexibility in addressing those differences, the cuBLAS Fortran interface is provided in the form of wrapper functions and is part of the Toolkit delivery.
The C source code of those wrapper functions is located in the src directory and provided in two different forms: the thunking wrapper interface located in the file fortran_thunking.c the direct wrapper interface located in the file fortran.c The code of one of those two files needs to be compiled into an application for it to call the cuBLAS API functions.
Providing source code allows users to make any changes necessary for a particular platform and toolchain.
The code in those two C files has been used to demonstrate interoperability with the compilers g77 3.2.3 and g95 0.91 on 32-bit Linux, g77 3.4.5 and g95 0.91 on 64-bit Linux, Intel Fortran 9.0 and Intel Fortran 10.0 on 32-bit and 64-bit Microsoft Windows XP, and g77 3.4.0 and g95 0.92 on Mac OS X.
Note that for g77, use of the compiler flag -fno-second-underscore is required to use these wrappers as provided.
Also, the use of the default calling conventions with regard to argument and return value passing is expected.
Using the flag -fno-f2c changes the default calling convention with respect to these two items.
The thunking wrappers allow interfacing to existing Fortran applications without any changes to the application.
During each call, the wrappers allocate GPU memory, copy source data from CPU memory space to GPU memory space, call cuBLAS, and finally copy back the results to CPU memory space and deallocate the GPU memory.
As this process causes very significant call overhead, these wrappers are intended for light testing, not for production code.
To use the thunking wrappers, the application needs to be compiled with the file fortran_thunking.c .
The direct wrappers, intended for production code, substitute device pointers for vector and matrix arguments in all BLAS functions.
To use these interfaces, existing applications need to be modified slightly to allocate and deallocate data structures in GPU memory space (using cuBLAS_ALLOC and cuBLAS_FREE ) and to copy data between GPU and CPU memory spaces (using cuBLAS_SET_VECTOR , cuBLAS_GET_VECTOR , cuBLAS_SET_MATRIX , and cuBLAS_GET_MATRIX ).
The sample wrappers provided in fortran.c map device pointers to the OS-dependent type size_t , which is 32-bit wide on 32-bit platforms and 64-bit wide on a 64-bit platforms.
One approach to deal with index arithmetic on device pointers in Fortran code is to use C-style macros, and use the C preprocessor to expand these, as shown in the example below.
On Linux and Mac OS X, one way of pre-processing is to use the option -E -x f77-cpp-input when using g77 compiler, or simply the option -cpp when using g95 or gfortran.
On Windows platforms with Microsoft Visual C/C++, using ’cl -EP’ achieves similar results.
Inadvertently exceeding the maximum line length can lead to run-time errors that are difficult to find, so care should be taken not to exceed the 72-column limit if fixed form is retained.
The examples in this chapter show a small application implemented in Fortran 77 on the host and the same application with the non-thunking wrappers after it has been ported to use the cuBLAS library.
The second example should be compiled with ARCH_64 defined as 1 on 64-bit OS system and as 0 on 32-bit OS system.
For example for g95 or gfortran, this can be done directly on the command line by using the option -cpp -DARCH_64=1 .
0 ) then write ( * , * ) "device memory allocation failed" call cublas_shutdown stop endif stat = cublas_set_matrix ( M , N , sizeof_real , a , M , devPtrA , M ) if ( stat .
0 ) then call cublas_free ( devPtrA ) write ( * , * ) "data download failed" call cublas_shutdown stop endif — — Code block continues below.
— — call modify ( devPtrA , M , N , 2 , 3 , 16.0 , 12.0 ) stat = cublas_get_matrix ( M , N , sizeof_real , devPtrA , M , a , M ) if ( stat .
NE .0 ) then call cublas_free ( devPtrA ) write ( * , * ) "data upload failed" call cublas_shutdown stop endif call cublas_free ( devPtrA ) call cublas_shutdown do j = 1 , N do i = 1 , M write ( * , "(F7.0$)" ) a ( i , j ) enddo write ( * , * ) "" enddo stop end 8.
Interaction with Other Libraries and Tools  This section describes important requirements and recommendations that ensure correct use of cuBLAS with other libraries and utilities. 8.1. nvprune  nvprune enables pruning relocatable host objects and static libraries to only contain device code for the specific target architectures.
In case of cuBLAS, particular care must be taken if using nvprune with compute capabilities, whose minor revision number is different than 0.
To reduce binary size, cuBLAS may only store major revision equivalents of CUDA binary files for kernels reused between different minor revision versions.
Therefore, to ensure that a pruned library does not fail for arbitrary problems, the user must keep binaries for a selected architecture and all prior minor architectures in its major architecture.
For example, the following call prunes libcublas_static.a to contain only sm_75 (Turing) and sm_70 (Volta) cubins: nvprune -- generate - code code = sm_70 -- generate - code code = sm_75 libcublasLt_static .
Acknowledgements  NVIDIA would like to thank the following individuals and institutions for their contributions: Portions of the SGEMM, DGEMM, CGEMM and ZGEMM library routines were written by Vasily Volkov of the University of California.
Portions of the SGEMM, DGEMM and ZGEMM library routines were written by Davide Barbieri of the University of Rome Tor Vergata.
Portions of the DGEMM and SGEMM library routines optimized for Fermi architecture were developed by the University of Tennessee.
Subsequently, several other routines that are optimized for the Fermi architecture have been derived from these initial DGEMM and SGEMM implementations.
The substantial optimizations of the STRSV, DTRSV, CTRSV and ZTRSV library routines were developed by Jonathan Hogg of The Science and Technology Facilities Council (STFC).
Subsequently, some optimizations of the STRSM, DTRSM, CTRSM and ZTRSM have been derived from these TRSV implementations.
Substantial optimizations of the SYMV and HEMV library routines were developed by Ahmad Abdelfattah, David Keyes and Hatem Ltaief of King Abdullah University of Science and Technology (KAUST).
Substantial optimizations of the TRMM and TRSM library routines were developed by Ali Charara, David Keyes and Hatem Ltaief of King Abdullah University of Science and Technology (KAUST).
This product includes {fmt} - A modern formatting library https: fmt.dev Copyright (c) 2012 - present, Victor Zverovich.
This product includes SIMD Library for Evaluating Elementary Functions, vectorized libm and DFT https: sleef.org Boost Software License - Version 1.0 - August 17th, 2003.
This product includes Frozen - a header-only, constexpr alternative to gperf for C++14 users.
This product includes Boost C++ Libraries - free peer-reviewed portable C++ source libraries https: www.boost.org/ Boost Software License - Version 1.0 - August 17th, 2003.
This product includes Zstandard - a fast lossless compression algorithm, targeting real-time compression scenarios at zlib-level and better compression ratios.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 10.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 10.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit v12.5.1 cuDLA API 1.
Data Fields Search Results cuDLA API ( PDF ) - v12.5.1 ( older ) - Last updated July 1, 2024 - Send Feedback 1.
CUDLA_NUM_OUTPUT_TASK_STATISTICS = 4 Flag to retrieve total number of output task statistics buffer.
CUDLA_OUTPUT_TASK_STATISTICS_DESCRIPTORS = 5 Flag to retrieve all the output task statistics descriptors.
CUDLA_MODULE_ENABLE_FAULT_DIAGNOSTICS = 1 Flag to load a module that is used to perform permanent fault diagnostics for DLA HW.
cudlaErrorInvalidParam = 1 This indicates that one or more parameters passed to the API is/are incorrect.
cudlaErrorOutOfResources = 2 This indicates that the API call failed due to lack of underlying resources.
cudlaErrorCreationFailed = 3 This indicates that an internal error occurred during creation of device handle.
cudlaErrorInvalidAddress = 4 This indicates that the memory object being passed in the API call has not been registered before.
cudlaErrorCuda = 6 This indicates that there was an error in a CUDA operation as part of the API call.
cudlaErrorUmd = 7 This indicates that there was an error in the DLA runtime for the API call.
cudlaErrorInvalidDevice = 8 This indicates that the device handle passed to the API call is invalid.
cudlaErrorInvalidAttribute = 9 This indicates that an invalid attribute is being requested.
cudlaErrorIncompatibleDlaSWVersion = 10 This indicates that the underlying DLA runtime is incompatible with the current cuDLA version.
cudlaErrorMemoryRegistered = 11 This indicates that the memory object is already registered.
cudlaErrorUnsupportedOperation = 13 This indicates that the operation being requested by the API call is unsupported.
cudlaErrorNvSci = 14 This indicates that the NvSci operation requested by the API call failed.
Values CUDLA_SUBMIT_NOOP = 1 Flag to specify that the submitted task must be bypassed for execution.
CUDLA_SUBMIT_SKIP_LOCK_ACQUIRE = 1<<1 Flag to specify that the global lock acquire must be skipped.
CUDLA_SUBMIT_DIAGNOSTICS_TASK = 1<<2 Flag to specify that the submitted task is to run permanent fault diagnostics for DLA HW. 2. Data Structures Here are the data structures with brief descriptions: cudlaDevAttribute cudlaExternalMemoryHandleDesc cudlaExternalSemaphoreHandleDesc CudlaFence cudlaModuleAttribute cudlaModuleTensorDescriptor cudlaSignalEvents cudlaTask cudlaWaitEvents 2.1.
Public Variables uint32_t deviceVersion uint8_t unifiedAddressingSupported Variables uint32_t cudlaDevAttribute :: deviceVersion [inherited] DLA device version.
uint8_t cudlaDevAttribute :: unifiedAddressingSupported [inherited] Returns 0 if unified addressing is not supported. 2.2. cudlaExternalMemoryHandleDesc_t Struct Reference [ Data types used by cuDLA driver ] External memory handle descriptor.
Public Variables const void * extBufObject unsigned long long size Variables const void * cudlaExternalMemoryHandleDesc_t :: extBufObject [inherited] A handle representing an external memory object.
unsigned long long cudlaExternalMemoryHandleDesc_t :: size [inherited] Size of the memory allocation 2.3.
cudlaExternalSemaphoreHandleDesc_t Struct Reference [ Data types used by cuDLA driver ] External semaphore handle descriptor.
Public Variables const void * extSyncObject Variables const void * cudlaExternalSemaphoreHandleDesc_t :: extSyncObject [inherited] A handle representing an external synchronization object. 2.4. CudlaFence Struct Reference [ Data types used by cuDLA driver ] Fence description.
Public Variables void * fence cudlaFenceType type Variables void * CudlaFence :: fence [inherited] Fence.
cudlaFenceType CudlaFence :: type [inherited] Fence type. 2.5. cudlaModuleAttribute Union Reference [ Data types used by cuDLA driver ] Module attribute.
Public Variables cudlaModuleTensorDescriptor * inputTensorDesc uint32_t numInputTensors uint32_t numOutputTensors cudlaModuleTensorDescriptor * outputTensorDesc Variables cudlaModuleTensorDescriptor * cudlaModuleAttribute :: inputTensorDesc [inherited] Returns an array of input tensor descriptors.
uint32_t cudlaModuleAttribute :: numInputTensors [inherited] Returns the number of input tensors.
uint32_t cudlaModuleAttribute :: numOutputTensors [inherited] Returns the number of output tensors.
cudlaModuleTensorDescriptor * cudlaModuleAttribute :: outputTensorDesc [inherited] Returns an array of output tensor descriptors. 2.6. cudlaModuleTensorDescriptor Struct Reference [ Data types used by cuDLA driver ] Tensor descriptor.
2.7. cudlaSignalEvents Struct Reference [ Data types used by cuDLA driver ] Signal events for cudlaSubmitTask Public Variables const * devPtrs CudlaFence * eofFences uint32_t numEvents Variables const * cudlaSignalEvents :: devPtrs [inherited] Array of registered synchronization objects (via cudlaImportExternalSemaphore ).
CudlaFence * cudlaSignalEvents :: eofFences [inherited] Array of fences pointers for all the signal events corresponding to the synchronization objects.
uint32_t cudlaSignalEvents :: numEvents [inherited] Total number of signal events. 2.8. cudlaTask Struct Reference [ Data types used by cuDLA driver ] Structure of Task.
Public Variables const * inputTensor cudlaModule moduleHandle uint32_t numInputTensors uint32_t numOutputTensors const * outputTensor cudlaSignalEvents * signalEvents const cudlaWaitEvents * waitEvents Variables const * cudlaTask :: inputTensor [inherited] Array of input tensors.
const cudlaWaitEvents * cudlaTask :: waitEvents [inherited] Wait events. 2.9. cudlaWaitEvents Struct Reference [ Data types used by cuDLA driver ] Wait events for cudlaSubmitTask .
Public Variables uint32_t numEvents const CudlaFence * preFences Variables uint32_t cudlaWaitEvents :: numEvents [inherited] Total number of wait events.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.
Trademarks NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2021-2024 NVIDIA Corporation var switchTo5x=true; stLight.options({publisher: "998dc202-a267-4d8e-bce9-14debadb8d92", doNotHash: false, doNotCopy: false, hashAddressBar: false}); _satellite.pageBottom();
1.
Introduction v12.5 | PDF | Archive NVBLAS The User guide for NVBLAS, drop-in BLAS replacement, multi-GPUs accelerated 1.
Introduction  The NVBLAS Library is a GPU-accelerated Libary that implements BLAS (Basic Linear Algebra Subprograms).
It can accelerate most BLAS Level-3 routines by dynamically routing BLAS calls to one or more NVIDIA GPUs present in the system, when the charateristics of the call make it speed up on a GPU. 2. NVBLAS Overview  The NVBLAS Library is built on top of the cuBLAS Library using only the CUBLASXT API (refer to the CUBLASXT API section of the cuBLAS Documentation for more details).
Depending on the charateristics of those BLAS calls, NVBLAS will redirect the calls to the GPUs present in the system or to CPU.
That decision is based on a simple heuristic that estimates if the BLAS call will execute for long enough to amortize the PCI transfers of the input and output data to the GPU.
Because NVBLAS does not support all standard BLAS routines, it might be necessary to associate it with an existing full BLAS Library.
Please refer to the Usage section for more details. 3. GPU Accelerated Routines  NVBLAS offloads only the compute-intensive BLAS3 routines which have the best potential for acceleration on GPUs.
The following table shows the currently supported routines: Routine Types Operation gemm S,D,C,Z Multiplication of 2 matrices syrk S,D,C,Z Symmetric rank-k update herk C,Z Hermitian rank-k update syr2k S,D,C,Z Symmetric rank-2k update her2k C,Z Hermitian rank-2k update trsm S,D,C,Z Triangular solve with multiple right-hand sides trmm S,D,C,Z Triangular matrix-matrix multiplication symm S,D,C,Z Symmetric matrix-matrix multiplication hemm C,Z Hermitian matrix-matrix multiplication 4.
BLAS Symbols Interception  Standard BLAS Library implementations usually expose multiple symbols for the same routines.
Let’s say func is a BLAS routine name, func_ or/and func are usually defined as extern symbols.
The user needs to make sure that the application intended to be GPU-accelerated by NVBLAS actually calls those defined symbols.
Any other symbols will not be intercepted and the original BLAS routine will be executed for those cases. 5. Device Memory Support  Starting with Release 8.0, data can be located on any GPU device, even on GPU devices that are not configured to be part of the computation.
When any of the data is located on a GPU, the computation will be exclusively done on GPU whatever the size of the problem.
Also, this feature has to be used with caution: the user has to be sure that the BLAS call will indeed be intercepted by NVBLAS, otherwise it will result in a crash when the CPU BLAS tries to execute it. 6. Security Precaution  Because the NVBLAS Library relies on a symbols interception mechanism, it is essential to make sure it has not been compromised.
In that regard, NVBLAS should never be used from a process running at elevated privileges, such as Administrator on Windows or root on Linux. 7. Configuration  Because NVBLAS is a drop-in replacement of BLAS, it must be configured through an ASCII text file that describes how many and which GPUs can participate in the intercepted BLAS calls.
The format of the configuration file is based on keywords optionally followed by one or more user-defined parameters.
Blank lines or lines beginning with the character # are ignored. 7.1. NVBLAS_CONFIG_FILE Environment Variable  The location and name of the configuration file must be defined by the environment variable NVBLAS_CONFIG_FILE .
By default, if NVBLAS_CONFIG_FILE is not defined, NVBLAS will try to open the file nvblas.conf in the current directory.
For a safe use of NVBLAS, the configuration file should have have restricted write permissions. 7.2. Configuration Keywords  The configuration keywords syntax is described in the following subsections.
7.2.1. NVBLAS_LOGFILE  This keyword defines the file where NVBLAS should print status and error messages.
It is advised to define this keyword early in the configuration to capture errors in parsing that file itself. 7.2.2. NVBLAS_TRACE_LOG_ENABLED  When this keyword is defined, every intercepted BLAS calls will be logged into the NVBLAS_LOGFILE.
This feature, even though intrusive, can be useful for debugging purposes. 7.2.3. NVBLAS_CPU_BLAS_LIB  This keyword defines the CPU BLAS dynamic library file (for example, .so file on Linux or .dll on Windows) that NVBLAS should open to find the CPU BLAS symbols definitions.
Because CPU Blas libraries are often composed of multiple files, even though this keyword is set to the full path to the main file of the CPU library, it might still be necessary to define the right path to find the rest of the library files in the environment of your system.
On Linux, this can be done by setting the environment variable LD_LIBRARY_PATH whereas on Windows, this can be done by setting the environment variable PATH .
For a safe use of NVBLAS, the following precautions are strongly advised: The CPU BLAS Library should be located where ordinary users do not have write permissions.
The path specified should be absolute, not relative. 7.2.4. NVBLAS_GPU_LIST  This keyword defines the list of GPUs that should participate in the computation of the intercepted BLAS calls.
If not defined, only GPU device 0 is used, since that is normally the most compute-capable GPU installed in the system.
Also the following wildcard keywords are also accepted for simplicity : Keyword Meaning ALL All compute-capable GPUs detected on the system will be used by NVBLAS ALL0 GPU device 0, AND all others GPUs detected that have the same compute-capabilities as device 0 will be used by NVBLAS Note In the current release of CUBLAS, the CUBLASXT API supports two GPUs if they are on the same board such as Tesla K10 or GeForce GTX690 and one GPU otherwise.
If access to more GPUs devices is needed, details of the licensing are described at cublasXt . 7.2.5. NVBLAS_TILE_DIM  This keyword defines the tile dimension that should be used to divide the matrices involved in the computation.
Refer to cuBLAS documentation to understand the tradeoffs associated with setting this to a larger or a smaller value. 7.2.6. NVBLAS_GPU_DISABLED_  This keyword, appended with the name of a BLAS routine disables NVBLAS from running a specified routine on the GPU.
By default, all supported BLAS routines are enabled. 7.2.7. NVBLAS_CPU_RATIO_  This keyword, appended with the name of ta BLAS routine defines the ratio of the workload that should remain on the CPU in the event that the NVBLAS decides to offload work for that routine on the GPU.
Please refer to the cuBLAS documentation for details and for the list of routines which support this feature. 7.2.8. NVBLAS_AUTOPIN_MEM_ENABLED  This keyword enables the Pinning Memory mode.
This functionality is directly mapped to the cublasXt API routine cublasXtSetPinningMemMode .
If this keyowrd is not present in the configuration file, the Pinning Memory mode will be set to CUBLASXT_PINNING_DISABLED .
Note There are some restrictions to use this feature as specified in the cuBLAS documentation of the underlying routine cublasXtSetPinningMemMode .
Specifically when NVBLAS is used in a multi-threaded applications, this option should not be used if there is a chance that matrices used by different threads overlaps while calling NVBLAS.
Please refer to the cuBLAS Documentation of the routine `cublasXtSetPinningMemMode `__ for details. 7.2.9. Configuration File Example  The following example shows a typical NVBLAS configuration file : # This is the configuration file to use NVBLAS Library # Setup the environment variable NVBLAS_CONFIG_FILE to specify your own config file.
NVBLAS Installation  The NVBLAS Library is part of the CUDA Toolkit, and will be installed along all the other CUDA libraries.
NVBLAS Library is built on top of cuBLAS, so the cuBLAS library needs to be accessible by NVBLAS. 9. Usage  To use the NVBLAS Library, the user application must be relinked against NVBLAS in addition to the original CPU Blas (technically only NVBLAS is needed unless some BLAS routines not supported by NVBLAS are used by the application).
To be sure that the linker links against the exposed symbols of NVBLAS and not the ones from the CPU BLAS, the NVBLAS Library needs to be put before the CPU BLAS on the linkage command line.
On Linux, an alternative way to use NVBLAS Library is to use the LD_PRELOAD environment variable; this technique has the advantage of avoiding the relinkage step.
However, the user should avoid defining that environment variable globally because it will cause the NVBLAS library to be loaded by every shell command executed on the system, thus leading to a lack of responsiveness of the system.
Finally, mathematical tools and libraries often offer the opportunity to specify the BLAS Library to be used through an environment variable or a configuration file.
Because NVBLAS does not support all the standard BLAS routines, it might be necessary to pair NVBLAS with a full BLAS library, even though your application only calls supported NVBLAS routines.
Fortunately, those tools and libraries usually offer a way to specify multiple BLAS Libraries.
Please refer to the documentation of the appropriate tools and libraries for details. 10. Notices  10.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 10.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 10.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Introduction v12.5 | PDF | Archive NVVM IR Specification Reference guide to the NVVM compiler (intermediate representation) based on the LLVM IR.
Introduction  NVVM IR is a compiler IR (intermediate representation) based on the LLVM IR.
The PTX codegen part of a NVVM compiler needs to know the source language because of the difference in DCI (driver/compiler interface).
Technically speaking, NVVM IR is LLVM IR with a set of rules, restrictions, and conventions, plus a set of supported intrinsic functions.
Accepted and ignored: The NVVM compiler will accept this IR feature, but will ignore the required semantics.
This applies to some IR features that do not have meaningful semantics on GPUs and that can be ignored.
Illegal, not supported: The specified semantics is not supported, such as a fence instruction.
Future versions of NVVM may either support or accept and ignore IRs that are illegal in the current version.
This document describes version 2.0 of the NVVM IR and version 3.1 of the NVVM debug metadata (see Source Level Debugging Support ).
For the complete semantics of the IR, readers of this document should check the official LLVM Language Reference Manual ( https: releases.llvm.org/7.0.1/docs/LangRef.html ). 2. Identifiers  The name of a named global identifier must have the form: @[a-zA-Z$_][a-zA-Z$_0-9]* Note that it cannot contain the .
Linkage Types  Supported: private internal available_externally linkonce weak common linkonce_odr weak_odr external Not supported: appending extern_weak See NVVM ABI for PTX for details on how linkage types are translated to PTX. 3.2. Calling Conventions  All LLVM calling convention markings are accepted and ignored.
Functions and calls are generated according to the PTX calling convention. 3.2.1. Rules and Restrictions  When an argument with width less than 32-bit is passed, the zeroext/signext parameter attribute should be set.
When a value with width less than 32-bit is returned, the zeroext/signext parameter attribute should be set.
Arguments of aggregate or vector types that are passed by value can be passed by pointer with the byval attribute set (referred to as the by-pointer-byval case below).
The align attribute must be set if the type requires a non-natural alignment (natural alignment is the alignment inferred for the aggregate type according to the Data Layout section).
If a function has an argument of aggregate or vector type that is passed by value directly and the type has a non-natural alignment requirement, the alignment must be annotated by the global property annotation , where alignment is a 32-bit integer whose upper 16 bits represent the argument position (starting from 1) and the lower 16 bits represent the alignment.
If the return type of a function is an aggregate or a vector that has a non-natural alignment, then the alignment requirement must be annotated by the global property annotation , where the upper 16 bits is 0, and the lower 16 bits represent the alignment.
If annotated, the alignment must match the natural alignment or the align attribute in the by-pointer-byval case.
For an indirect call instruction of a function that has a non-natural alignment for its return value or one of its arguments that is not expressed in alignment in the by-pointer-byval case, the call instruction must have an attached metadata of kind callalign .
The metadata contains a sequence of i32 fields each of which represents a non-natural alignment requirement.
The upper 16 bits of an i32 field represent the argument position (0 for return value, 1 for the first argument, and so on) and the lower 16 bits represent the alignment.
S % arg2 ), ! callalign ! 10 ! 10 = ! { i32 8 , i32 520 }; It is not required to have an i32 metadata field for the other arguments or the return value otherwise.
If presented, the alignment must match the natural alignment or the align attribute in the by-pointer-byval case .
If attached, the alignment must match the natural alignment or the alignment in the by-pointer-byval case.
The absence of the metadata in an indirect call instruction means using natural alignment or the align attribute in the by-pointer-byval case. 3.3. Visibility Styles  All styles—default, hidden, and protected—are accepted and ignored.
3.11. Global Variables  A global variable, that is not an intrinsic global variable, may be optionally declared to reside in one of the following address spaces: global shared constant If no address space is explicitly specified, the global variable is assumed to reside in the global address space with a generic address value.
Use undef initialization. 3.12. Functions  The following are not supported on functions: Alignment Explicit section Garbage collector name Prefix data Prologue Personality 3.13.
Aliases  Supported only as aliases of non-kernel functions. 3.14. Ifuncs  Not supported.
3.15. Named Metadata  Accepted and ignored, except for the following: !nvvm.annotations : see Global Property Annotation !nvvmir.version !llvm.dbg.cu !llvm.module.flags The NVVM IR version is specified using a named metadata called !nvvmir.version .
The !nvvmir.version named metadata may have one metadata node that contains the NVVM IR version for that module.
If multiple such modules are linked together, the named metadata in the linked module may have more than one metadata node with each node containing a version.
A metadata node with NVVM IR version takes either of the following forms: It may consist of two i32 values—the first denotes the NVVM IR major version number and the second denotes the minor version number.
If absent, the version number is assumed to be 1.0, which can be specified as: !nvvmir.version = ! {!0} !0 = ! {i32 1, i32 0} It may consist of four i32 values—the first two denote the NVVM IR major and minor versions respectively.
The third value denotes the NVVM IR debug metadata major version number, and the fourth value denotes the corresponding minor version number.
The version of NVVM IR debug metadata described in this document is 3.1. 3.16. Parameter Attributes  Fully supported, except the following: Accepted and ignored: inreg nest Not supported: inalloca swiftself swifterror See Calling Conventions for the use of the attributes.
The set of supported attributes is equal to the set of attributes accepted where the attribute group is used. 3.21. Function Attributes  Supported: allocsize alwaysinline cold convergent inaccessiblememonly inaccessiblemem_or_argmemonly inlinehint minsize no-jump-tables noduplicate noinline noreturn norecurse nounwind "null-pointer-is-valid" optforfuzzing optnone optsize readnone readonly writeonly argmemonly speculatable strictfp Not Supported: alignstack builtin nonlazybind naked nobuiltin noimplicitfloat noredzone "patchable-function" probe-stack returns_twice sanitize_address sanitize_memory sanitize_thread sanitize_hwaddress ssp sspreq sspstrong "stack-probe-size" "no-stack-arg-probe" uwtable jumptable safestack "thunk" nocf_check shadowcallstack 3.22.
3.25. Data Layout  Only the following data layout is supported: 64-bit e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-i128:128:128-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64 The following data layouts are deprecated and will be removed in a future release.
32-bit e-p:32:32:32-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-i128:128:128-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64 e-p:32:32:32-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64 64-bit e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64 3.26.
Target Triple  Only the following target triple is supported, where * can be any name: 64-bit: nvptx64-*-cuda The following target triple is deprecated, and will be removed in future release: 32-bit: nvptx-*-cuda 3.27.
Note that for code generation: ld.volatile and st.volatile will be generated. 3.29. Memory Model for Concurrent Operations  Not applicable.
Threads in an NVVM IR program must use atomic operations or barrier synchronization to communicate. 3.30. Atomic Memory Ordering Constraints  Atomic loads and stores are not supported.
Other atomic operations on other than 32-bit or 64-bit operands are not supported. 3.31. Fast-Math Flags  Supported.
4. Type System  Fully supported, except for the following: Floating point types half , fp128 , x86_fp80 and ppc_fp128 are not supported.
The non-integral pointer type is not supported. 5. Constants  Fully supported, except for the following: Token constants is not supported.
For a constant expression that is used as the initializer of a global variable @g1 , if the constant expression contains a global identifier @g2 , then the constant expression is supported if it can be reduced to the form of bitcast+offset , where offset is an integer number (including 0 ) 6.
Inline Assembler Expressions  Inline assembler of PTX instructions is supported, with the following supported constraints: Constraint Type c i8 h i16 r i32 l i64 f f32 d f64 The inline asm metadata !srcloc is accepted and ignored.
The following metadata are understood by the NVVM compiler: Specialized Metadata Nodes llvm.loop.unroll.count llvm.loop.unroll.disable llvm.loop.unroll.full callalign (see Rules and Restrictions for Calling Conventions) Module flags metadata ( llvm.module.flags ) is supported and verified, but the metadata values will be ignored.
The llvm.compiler.used global variable is supported The llvm.global_ctors global variable is not supported The llvm.global_dtors global variable is not supported 10.
Terminator Instructions  Supported: ret br switch unreachable Unsupported: indirectbr invoke resume catchswitch catchret cleanupret 10.2.
Binary Operations  Supported: add fadd sub fsub mul fmul udiv sdiv fdiv urem srem frem 10.3.
alloca Instruction  The alloca instruction returns a generic pointer to the local address space.
The addrspace() specifier is supported only if num is 0. 10.6.2. load Instruction  load atomic is not supported.
Use NVVM intrinsic functions instead. 10.6.5. cmpxchg Instruction  Supported for i32 , i64 , and i128 types, with the following restrictions: The pointer must be either a global pointer, a shared pointer, or a generic pointer that points to either the global address space or the shared address space.
The i128 type is only supported on compute_90 and above. 10.6.6. atomicrmw Instruction  nand is not supported.
The other keywords are supported for i32 , i64 , and i128 types, with the following restrictions.
The pointer must be either a global pointer, a shared pointer, or a generic pointer that points to either the global address space or the shared address space.
For i128 , only xchg is supported, and only on compute_90 and above. 10.6.7. getelementptr Instruction  Fully supported.
to See Conversion for a special use case of bitcast . 10.8. Other Operations  Supported: icmp fcmp phi select va_arg call (See Calling Conventions for other rules and restrictions.) Unsupported: landingpad catchpad cleanuppad 11.
Note that the constant address space cannot be used as the destination since it is read-only.
llvm.maxnum Not supported. 11.5. Bit Manipulations Intrinsics  llvm.bitreverse Supported for i8 , i16 , i32 , and i64 .
llvm.fshr Supported for i8 , i16 , i32 , and i64 . 11.6. Specialised Arithmetic Intrinsics  llvm.fmuladd Supported.
llvm.canonicalize Not supported. 11.7. Arithmetic with Overflow Intrinsics  Supported for i16 , i32 , and i64 .
11.8. Half Precision Floating Point Intrinsics  Supported: llvm.convert.to.fp16 , llvm.convert.from.fp16 11.9.
11.18. Memory Use Markers  Supported: llvm.lifetime.start , llvm.lifetime.end , llvm.invariant.start , and llvm.invariant.end .
Not supported: llvm.launder.invariant.group , llvm.strip.invariant.group . 11.19. General Intrinsics  llvm.var.annotation Accepted and ignored.
Address Spaces  NVVM IR has a set of predefined memory address spaces, whose semantics are similar to those defined in CUDA C/C++, OpenCL C and PTX.
Name Address Space Number Semantics/Example code 0 functions, code CUDA C/C++ function OpenCL C function generic 0 Can only be used to qualify the pointee of a pointer Pointers in CUDA C/C++ global 1 CUDA C/C++ __device__ OpenCL C global shared 3 CUDA C/C++ __shared__ OpenCL C local constant 4 CUDA C/C++ __constant__ OpenCL C constant local 5 CUDA C/C++ local OpenCL C private 2, 101 and above Each global variable, that is not an intrinsic global variable, can be declared to reside in a specific non-zero address space, which can only be one of the following: global , shared or constant .
If a non-intrinsic global variable is declared without any address space number or with the address space number 0, then this global variable resides in address space global and the pointer of this global variable holds a generic pointer value.
The predefined NVVM memory spaces are needed for the language front-ends to model the memory spaces in the source languages.
For example,   CUDA C/C++ __constant__ int c; __device__ int g; ; NVVM IR @c = addrspace(4) global i32 0, align 4 @g = addrspace(1) global [2 x i32] zeroinitializer, align 4 Address space numbers 2 and 101 or higher are reserved for NVVM compiler internal use only.
No language front-end should generate code that uses these address spaces directly. 12.2. Generic Pointers and Non-Generic Pointers  12.2.1.
In NVVM IR, a generic pointer has a pointer type with the address space generic , while a non-generic pointer has a pointer type with a non-generic address space.
Note that the address space number for the generic address space is 0—the default in both NVVM IR and LLVM IR.
Loads/stores via generic pointers are supported, as well as loads/stores via non-generic pointers.
Loads/stores via function pointers are not supported @a = addrspace(1) global i32 0, align 4 ; 'global' addrspace, @a holds a specific value @b = global i32 0, align 4 ; 'global' addrspace, @b holds a generic value @c = addrspace(4) global i32 0, align 4 ; 'constant' addrspace, @c holds a specific value ...
Conversion  The bit value of a generic pointer that points to a specific object may be different from the bit value of a specific pointer that points to the same object.
The addrspacecast IR instruction should be used to perform pointer casts across address spaces (generic to non-generic or non-generic to generic).
Casting from a generic to a non-generic pointer is undefined if the generic pointer does not point to an object in the target non-generic address space.
inttoptr and ptrtoint are value preserving instructions when the two operands are of the same size.
The following intrinsic can be used to query if the argument pointer was derived from the address of a kernel function parameter that has the grid_constant property: i1 @llvm.nvvm.isspacep.grid_const(i8*) The following intrinsic can be used to query if the input generic pointer was derived from the address of a variable allocated in the shared address space, in a CTA that is part of the same cluster as the parent CTA of the invoking thread.
i1 @llvm.nvvm.isspacep.cluster_shared(i8*) The following intrinsics can be used to query if a generic pointer can be safely cast to a specific non-generic address space: i1 @llvm.nvvm.isspacep.const(i8*) i1 @llvm.nvvm.isspacep.global(i8*) i1 @llvm.nvvm.isspacep.local(i8*) i1 @llvm.nvvm.isspacep.shared(i8*) bitcast on pointers is supported, though LLVM IR forbids bitcast from being used to change the address space of a pointer. 12.2.3. No Aliasing between Two Different Specific Address Spaces  Two different specific address spaces do not overlap.
NVVM compiler assumes two memory accesses via non-generic pointers that point to different address spaces are not aliased. 12.3. The alloca Instruction  The alloca instruction returns a generic pointer that only points to address space local .
Overview  NVVM uses Named Metadata to annotate IR objects with properties that are otherwise not representable in the IR.
The NVVM IR producers can use the Named Metadata to annotate the IR with properties, which the NVVM compiler can process. 13.2. Representation of Properties  For each translation unit (that is, per bitcode file), there is a named metadata called nvvm.annotations .
The first operand of each MDNode is an entity that the node is annotating using the remaining operands.
Multiple MDNodes may provide annotations for the same entity, in which case their first operands will be same.
Starting with the operand after the annotated entity, every alternate operand specifies a property.
!nvvm.annotations = ! {!12, !13} !12 = ! {void (i32, i32)* @_Z6kernelii, ! "kernel", i32 1} !13 = ! {void ()* @_Z7kernel2v, ! "kernel", i32 1, ! "maxntidx", i32 16} If two bitcode files are being linked and both have a named metadata nvvm.annotations , the linked file will have a single merged named metadata.
If both files define properties for the same entity foo , the linked file will have two MDNodes defining properties for foo .
It is illegal for the files to have conflicting properties for the same entity. 13.3. Supported Properties  Property Name Annotated On Description maxntid{x, y, z} kernel function Maximum expected CTA size from any launch.
minctasm kernel function Hint/directive to the compiler/driver, asking it to put at least these many CTAs on an SM.
grid_constant kernel function The argument is a metadata node, which contains a list of integers, where each integer n denotes that the nth parameter has the grid_constant annotation (numbering from 1).
align function Signifies that the value in low 16-bits of the 32-bit value contains alignment of n th parameter type if its alignment is not the natural alignment.
managed global variable Signifies that variable is a UVM managed variable. 14. Texture and Surface  14.1.
Texture Variable and Surface Variable  A texture or a surface variable can be declared/defined as a global variable of i64 type with annotation texture or surface in the global address space.
A texture or surface variable must have a name, which must follow identifier naming conventions.
A texture or a surface variable may only have the following uses: In a metadata node As an intrinsic function argument as shown below In llvm.used Global Variable 14.2.
Accessing Texture Memory or Surface Memory  Texture memory and surface memory can be accessed using texture or surface handles.
NVVM provides the following intrinsic function to get a texture or surface handle from a texture or surface variable.
p1i64 ( metadata , i64 addrspace ( 1 ) * ) The first argument to the intrinsic is a metadata holding the texture or surface variable.
The returned handle value from the intrinsic call can be used as an operand (with a constraint of l) in a PTX inline asm to access the texture or surface memory. 15. NVVM Specific Intrinsic Functions  15.1.
Atomic  Besides the atomic instructions, the following extra atomic intrinsic functions are supported.
declare float @llvm.nvvm.atomic.load.add.f32.p0f32(float* address, float val) declare float @llvm.nvvm.atomic.load.add.f32.p1f32(float addrspace(1)* address, float val) declare float @llvm.nvvm.atomic.load.add.f32.p3f32(float addrspace(3)* address, float val) declare double @llvm.nvvm.atomic.load.add.f64.p0f64(double* address, double val) declare double @llvm.nvvm.atomic.load.add.f64.p1f64(double addrspace(1)* address, double val) declare double @llvm.nvvm.atomic.load.add.f64.p3f64(double addrspace(3)* address, double val) reads the single/double precision floating point value old located at the address address , computes old+val , and stores the result back to memory at the same address.
declare i32 @llvm.nvvm.atomic.load.inc.32.p0i32(i32* address, i32 val) declare i32 @llvm.nvvm.atomic.load.inc.32.p1i32(i32 addrspace(1)* address, i32 val) declare i32 @llvm.nvvm.atomic.load.inc.32.p3i32(i32 addrspace(3)* address, i32 val) reads the 32-bit word old located at the address address , computes ((old >= val) ? 0 : (old+1)) , and stores the result back to memory at the same address.
declare i32 @llvm.nvvm.atomic.load.dec.32.p0i32(i32* address, i32 val) declare i32 @llvm.nvvm.atomic.load.dec.32.p1i32(i32 addrspace(1)* address, i32 val) declare i32 @llvm.nvvm.atomic.load.dec.32.p3i32(i32 addrspace(3)* address, i32 val) reads the 32-bit word old located at the address address , computes (((old == 0) | (old > val)) ? val : (old-1) ) , and stores the result back to memory at the same address. 15.2. Barrier and Memory Fence  declare void @llvm.nvvm.barrier0() waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to llvm.nvvm.barrier0() are visible to all threads in the block.
declare i32 @llvm.nvvm.barrier0.popc(i32) is identical to llvm.nvvm.barrier0() with the additional feature that it evaluates predicate for all threads of the block and returns the number of threads for which predicate evaluates to non-zero.
declare i32 @llvm.nvvm.barrier0.and(i32) is identical to llvm.nvvm.barrier0() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for all of them.
declare i32 @llvm.nvvm.barrier0.or(i32) is identical to llvm.nvvm.barrier0() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for any of them.
declare void @llvm.nvvm.cluster.barrier(i32 %flags) Synchronize and communicate among threads in the same cluster.
The %flags is encoded according to the following table: %flags bits Meaning 31-8 Reserved 7-4 Memory ordering (See Cluster Barrier Memory Ordering Encoding below) 3-0 Operation mode (See Cluster Barrier Operation Mode Encoding below) Cluster Barrier Operation Mode Encoding Encoding Mode Description 0 Arrive Arrive at cluster barrier 1 Wait Wait at cluster barrier 2-15 RESERVED RESERVED Cluster Barrier Memory Ordering Encoding Encoding Mode Description 0 Default All synchronous memory accesses requested by the executing entry prior to arrive are performed and are visible to all the entrys in the cluster after wait.
1 Relaxed All previously fenced memory accesses requested by the executing entry prior to arrive are performed and are visible to all the entrys in the cluster after wait.
2-15 RESERVED RESERVED declare void @llvm.nvvm.membar.cta() is a memory fence at the thread block level.
declare void @llvm.nvvm.membar(i32 %flags) Wait for all prior memory accesses requested by this thread to be performed at a membar level defined by the membar mode below.
For horizontal synchronization, a barrier should be used instead, or in addition to membar.
The %flags is encoded according to the following table: %flags bits Meaning 31-4 Reserved 3-0 Membar modes (See Membar Mode Encoding.) Membar Mode Encoding Encoding Mode Description 0 GLOBAL Membar at the global level 1 CTA Membar at the CTA level 2 SYSTEM Membar at the system level 3 RESERVED RESERVED 4 CLUSTER Membar at the cluster level, only on Hopper+ 5-15 RESERVED RESERVED 15.3.
Address space conversion  Note Attention: Please use the addrspacecast IR instruction for address space conversion. 15.4. Special Registers  The following intrinsic functions are provided to support reading special PTX registers: declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() declare i32 @llvm.nvvm.read.ptx.sreg.tid.y() declare i32 @llvm.nvvm.read.ptx.sreg.tid.z() declare i32 @llvm.nvvm.read.ptx.sreg.ntid.x() declare i32 @llvm.nvvm.read.ptx.sreg.ntid.y() declare i32 @llvm.nvvm.read.ptx.sreg.ntid.z() declare i32 @llvm.nvvm.read.ptx.sreg.ctaid.x() declare i32 @llvm.nvvm.read.ptx.sreg.ctaid.y() declare i32 @llvm.nvvm.read.ptx.sreg.ctaid.z() declare i32 @llvm.nvvm.read.ptx.sreg.nctaid.x() declare i32 @llvm.nvvm.read.ptx.sreg.nctaid.y() declare i32 @llvm.nvvm.read.ptx.sreg.nctaid.z() declare i32 @llvm.nvvm.read.ptx.sreg.warpsize() 15.5.
Texture/Surface Access  The following intrinsic function is provided to convert a global texture/surface variable into a texture/surface handle.
declare i64 %llvm.nvvm.texsurf.handle.p1i64(metadata, i64 addrspace(1)*) See Accessing Texture Memory or Surface Memory for details.
The following IR definitions apply to all intrinsics in this section: type %float4 = { float, float, float, float } type %long2 = { i64, i64 } type %int4 = { i32, i32, i32, i32 } type %int2 = { i32, i32 } type %short4 = { i16, i16, i16, i16 } type %short2 = { i16, i16 } 15.5.1.
Surface Loads  In the following intrinsics, represents the surface clamp mode and can be one of the following: clamp , trap , or zero .
For surface load instructions that operate on 8-bit data channels, the output operands are of type i16 .
It is trap for the formatted stores, and can be one of the following for unformatted stores: clamp , trap , or zero .
For surface store instructions that operate on 8-bit data channels, the input operands are of type i16 .
(i64 %tex, i32 %x, i32 %r, i32 %g, i32 %b, i32 %a) ;; Formatted void @llvm.nvvm.sust.p.1d.i32.
(i64 %tex, i32 %x, i32 %r, i32 %g, i32 %b, i32 %a) Writing a 1D surface array: ;; Unformatted void @llvm.nvvm.sust.b.1d.array.i8.
(i64 %tex, i32 %idx, i32 %x, i16 %r, i16 %g, i16 %b, i16 %a) void @llvm.nvvm.sust.b.1d.array.v4i16.
(i64 %tex, i32 %idx, i32 %x, i16 %r, i16 %g, i16 %b, i16 %a) void @llvm.nvvm.sust.b.1d.array.v4i32.
(i64 %tex, i32 %idx, i32 %x, i32 %r, i32 %g, i32 %b, i32 %a) ;; Formatted void @llvm.nvvm.sust.p.1d.array.i32.
(i64 %tex, i32 %idx, i32 %x, i32 %r, i32 %g, i32 %b, i32 %a) Writing a 2D surface: ;; Unformatted void @llvm.nvvm.sust.b.2d.i8.
(i64 %tex, i32 %x, i32 %y, i16 %r, i16 %g, i16 %b, i16 %a) void @llvm.nvvm.sust.b.2d.v4i16.
(i64 %tex, i32 %x, i32 %y, i16 %r, i16 %g, i16 %b, i16 %a) void @llvm.nvvm.sust.b.2d.v4i32.
(i64 %tex, i32 %x, i32 %y, i32 %r, i32 %g, i32 %b, i32 %a) ;; Formatted void @llvm.nvvm.sust.p.2d.i32.
(i64 %tex, i32 %x, i32 %y, i32 %r, i32 %g, i32 %b, i32 %a) Writing a 2D surface array: ;; Unformatted void @llvm.nvvm.sust.b.2d.array.i8.
(i64 %tex, i32 %idx, i32 %x, i32 %y, i16 %r, i16 %g) void @llvm.nvvm.sust.b.2d.array.v2i16.
(i64 %tex, i32 %idx, i32 %x, i32 %y, i16 %r, i16 %g) void @llvm.nvvm.sust.b.2d.array.v2i32.
(i64 %tex, i32 %idx, i32 %x, i32 %y, i32 %r, i32 %g) void @llvm.nvvm.sust.b.2d.array.v2i64.
(i64 %tex, i32 %idx, i32 %x, i32 %y, i64 %r, i64 %g) void @llvm.nvvm.sust.b.2d.array.v4i8.
(i64 %tex, i32 %idx, i32 %x, i32 %y, i16 %r, i16 %g, i16 %b, i16 %a) void @llvm.nvvm.sust.b.2d.array.v4i16.
(i64 %tex, i32 %idx, i32 %x, i32 %y, i16 %r, i16 %g, i16 %b, i16 %a) void @llvm.nvvm.sust.b.2d.array.v4i32.
(i64 %tex, i32 %idx, i32 %x, i32 %y, i32 %r, i32 %g, i32 %b, i32 %a) ;; Formatted void @llvm.nvvm.sust.p.2d.array.i32.
(i64 %tex, i32 %idx, i32 %x, i32 %y, i32 %r, i32 %g) void @llvm.nvvm.sust.p.2d.array.v4i32.
(i64 %tex, i32 %idx, i32 %x, i32 %y, i32 %r, i32 %g, i32 %b, i32 %a) Writing a 3D surface: ;; Unformatted void @llvm.nvvm.sust.b.3d.i8.
(i64 %tex, i32 %x, i32 %y, i32 %z, i16 %r, i16 %g, i16 %b, i16 %a) void @llvm.nvvm.sust.b.3d.v4i16.
(i64 %tex, i32 %x, i32 %y, i32 %z, i16 %r, i16 %g, i16 %b, i16 %a) void @llvm.nvvm.sust.b.3d.v4i32.
(i64 %tex, i32 %x, i32 %y, i32 %z, i32 %r, i32 %g, i32 %b, i32 %a) ;; Formatted void @llvm.nvvm.sust.p.3d.i32.
Barrier Synchronization  The following intrinsic performs a barrier synchronization among a subset of threads in a warp.
declare void @llvm.nvvm.bar.warp.sync(i32 %membermask) This intrinsic causes executing thread to wait until all threads corresponding to %membermask have executed the same intrinsic with the same %membermask value before resuming execution.
The argument %membership is a 32bit mask, with each bit corresponding to a lane in the warp.
The behavior of this intrinsic is undefined if the executing thread is not in the %membermask .
For compute_62 or below, all threads in %membermask must call the same @llvm.nvvm.bar.warp.sync() in convergence, and only threads belonging to the %membermask can be active when the intrinsic is called.
Otherwise, the behavior is undefined. 15.6.2. Data Movement  The following intrinsic synchronizes a subset of threads in a warp and then performs data movement among these threads.
declare {i32, i1} @llvm.nvvm.shfl.sync.i32(i32 %membermask, i32 %mode, i32 %a, i32 %b, i32 %c) This intrinsic causes executing thread to wait until all threads corresponding to %membermask have executed the same intrinsic with the same %membermask value before reading data from other threads in the same warp.
Each thread in the currently executing warp will compute a source lane index j based on input arguments %b , %c , and %mode .
If the computed source lane index j is in range, the returned i32 value will be the value of %a from lane j; otherwise, it will be the the value of %a from the current thread.
If the thread corresponding to lane j is inactive, then the returned i32 value is undefined.
The returned i1 value is set to 1 if the source lane j is in range, and otherwise set to 0.
The argument %mode must be a constant and its encoding is specified in the following table.
Encoding Meaning 0 IDX 1 UP 2 DOWN 3 BFLY Argument %b specifies a source lane or source lane offset, depending on %mode .
Argument %c contains two packed values specifying a mask for logically splitting warps into sub-segments and an upper bound for clamping the source lane index.
How the elements of a matrix are distributed among the fragments is opaque to the user and is different for matrix A , B and the accumulator.
These intrinsics are only available on compute_70 or higher. 15.6.5.2. Store Fragments  The following intrinsics synchronize all threads in a warp and then store a fragment of a matrix for each thread.
Encoding Meaning 0 A fragment is row-major, B fragment is row-major 1 A fragment is row-major, B fragment is column-major 2 A fragment is column-major, B fragment is row-major 3 A fragment is column-major, B fragment is column-major Support for %satf has been removed and this operand must be a constant zero.
The behavior of these intrinsics are undefined if any thread in the warp has exited. 16. Source Level Debugging Support  To enable source level debugging of an IR module, NVVM IR supports debug intrinsics and debug information descriptors to express the debugging information.
For the complete semantics of the IR, readers of this chapter should check the official LLVM IR specialized metadata nodes documentation ( https: releases.llvm.org/7.0.1/docs/LangRef.html#specialized-metadata-nodes ) and the Source Level Debugging with LLVM Manual ( https: releases.llvm.org/7.0.1/docs/SourceLevelDebugging.html ).
The following metadata nodes need to be present in the module when debugging support is requested: Named metadata node !llvm.dbg.cu Module flags metadata for "Debug Info Version" flag: The behavior flag should be Error .
Named metadata !nvvmir.version containing a metadata node with the NVVM IR major and minor version values followed by the NVVM IR debug metadata major and minor version values.
The debug resolution (e.g., full, line info only) is controlled by the DICompileUnit’s emissionKind field: FullDebug (value: 1) : Generate symbolic debug and line information.
If there are multiple input NVVM IR modules, at most one module may have a single debug compile unit. 17. NVVM ABI for PTX  17.1.
Linkage Types  The following table provides the mapping of NVVM IR linkage types associated with functions and global variables to PTX linker directives .
LLVM Linkage Type PTX Linker Directive private , internal This is the default linkage type and does not require a linker directive.
external Function with definition .visible Global variable with initialization Function without definition .extern Global variable without initialization common .common for the global address space, otherwise .weak available_externally , linkonce , linkonce_odr , weak , weak_odr .weak All other linkage types Not supported. 17.2. Parameter Passing and Return  The following table shows the mapping of function argument and return types in NVVM IR to PTX types.
Source Type Size in Bits PTX Type Integer types <= 32 .u32 or .b32 (zero-extended if unsigned) .s32 or .b32 (sign-extended if signed) 64 .u64 or .b64 (if unsigned) .s64 or .b64 (if signed) Pointer types (without byval attribute) 32 .u32 or .b32 64 .u64 or .b64 Floating-point types 32 .f32 or .b32 64 .f64 or .b64 Aggregate types Any size .align align .b8 name [ size ] Where align is overall aggregate or vector alignment in bytes, name is variable name associated with aggregate or vector, and size is the aggregate or vector size in bytes.
Version 1.11 Added support for Hopper+ cluster intrinsics and max_blocks_per_cluster kernel property for CUDA 11.8.
Version 2.0 Updated the NVVM IR to version 2.0 which is incompatible with NVVM IR version 1.x Removed address space conversion intrinsics.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 19.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 19.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit v12.5.1 libdevice User's Guide 1.
__nv_ynf Search Results libdevice User's Guide ( PDF ) - v12.5.1 ( older ) - Last updated July 1, 2024 - Send Feedback Libdevice User's Guide User's guide to libdevice Table of Contents 1.
__nv_ynf Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation var switchTo5x=true; stLight.options({publisher: "998dc202-a267-4d8e-bce9-14debadb8d92", doNotHash: false, doNotCopy: false, hashAddressBar: false}); _satellite.pageBottom();
1.
Introduction  libNVVM API provides an interface for generating PTX code from both binary and text NVVM IR inputs.
Compatible input can be generated by tools and libraries that produce LLVM 7.0 IR and bitcode.
Support for reading the text NVVM IR representation is deprecated and may be removed in a later release. 1.2. Thread Safety  libNVVM API provides a thread-safe interface to libNVVM.
Clients can take advantage of improved compilation speeds by spawning multiple compilation threads concurrently. 1.3. Module  This chapter presents the API of the libNVVM library.
Functions const char * nvvmGetErrorString (nvvmResult result) Get the message string for the given nvvmResult code. 2.1. Enumerations  enum nvvmResult  NVVM API call result code.
Values: enumerator NVVM_SUCCESS  enumerator NVVM_ERROR_OUT_OF_MEMORY  enumerator NVVM_ERROR_PROGRAM_CREATION_FAILURE  enumerator NVVM_ERROR_IR_VERSION_MISMATCH  enumerator NVVM_ERROR_INVALID_INPUT  enumerator NVVM_ERROR_INVALID_PROGRAM  enumerator NVVM_ERROR_INVALID_IR  enumerator NVVM_ERROR_INVALID_OPTION  enumerator NVVM_ERROR_NO_MODULE_IN_PROGRAM  enumerator NVVM_ERROR_COMPILATION  2.2.
Functions  const char * nvvmGetErrorString ( nvvmResult result )  Get the message string for the given nvvmResult code.
Returns Message string for the given nvvmResult code. 3. General Information Query  Functions nvvmResult nvvmIRVersion (int *majorIR, int *minorIR, int *majorDbg, int *minorDbg) Get the NVVM IR version.
nvvmResult nvvmVersion (int *major, int *minor) Get the NVVM version. 3.1. Functions  nvvmResult nvvmIRVersion ( int * majorIR , int * minorIR , int * majorDbg , int * minorDbg )  Get the NVVM IR version.
Returns NVVM_SUCCESS nvvmResult nvvmVersion ( int * major , int * minor )  Get the NVVM version.
Compilation  Functions nvvmResult nvvmAddModuleToProgram (nvvmProgram prog, const char *buffer, size_t size, const char *name) Add a module level NVVM IR to a program.
nvvmResult nvvmCompileProgram (nvvmProgram prog, int numOptions, const char **options) Compile the NVVM program.
nvvmResult nvvmCreateProgram (nvvmProgram *prog) Create a program, and set the value of its handle to *prog .
nvvmResult nvvmGetCompiledResult (nvvmProgram prog, char *buffer) Get the compiled result.
nvvmResult nvvmGetCompiledResultSize (nvvmProgram prog, size_t *bufferSizeRet) Get the size of the compiled result.
nvvmResult nvvmGetProgramLog (nvvmProgram prog, char *buffer) Get the Compiler/Verifier Message.
nvvmResult nvvmGetProgramLogSize (nvvmProgram prog, size_t *bufferSizeRet) Get the Size of Compiler/Verifier Message.
nvvmResult nvvmLazyAddModuleToProgram (nvvmProgram prog, const char *buffer, size_t size, const char *name) Add a module level NVVM IR to a program.
nvvmResult nvvmVerifyProgram (nvvmProgram prog, int numOptions, const char **options) Verify the NVVM program.
Typedefs nvvmProgram NVVM Program. 4.1. Functions  nvvmResult nvvmAddModuleToProgram ( nvvmProgram prog , const char * buffer , size_t size , const char * name )  Add a module level NVVM IR to a program.
The module should have NVVM IR either in the LLVM 7.0.1 bitcode representation or in the LLVM 7.0.1 text representation.
Support for reading the text representation of NVVM IR is deprecated and may be removed in a later version.
Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_INVALID_INPUT NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmCompileProgram ( nvvmProgram prog , int numOptions , const char * * options )  Compile the NVVM program.
The target datalayout in the linked IR program is used to determine the address size (32bit vs 64bit).
Line number (line info) only generation is also enabled via NVVM IR Debug Metadata, there is no specific libNVVM API flag for that case.
Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_IR_VERSION_MISMATCH NVVM_ERROR_INVALID_PROGRAM NVVM_ERROR_INVALID_OPTION NVVM_ERROR_NO_MODULE_IN_PROGRAM NVVM_ERROR_COMPILATION nvvmResult nvvmCreateProgram ( nvvmProgram * prog )  Create a program, and set the value of its handle to *prog .
Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmDestroyProgram ( nvvmProgram * prog )  Destroy a program.
Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmGetCompiledResult ( nvvmProgram prog , char * buffer )  Get the compiled result.
Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmGetCompiledResultSize ( nvvmProgram prog , size_t * bufferSizeRet )  Get the size of the compiled result.
Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmGetProgramLog ( nvvmProgram prog , char * buffer )  Get the Compiler/Verifier Message.
The NULL terminated message string is stored in the memory pointed to by buffer when the return value is NVVM_SUCCESS.
Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmGetProgramLogSize ( nvvmProgram prog , size_t * bufferSizeRet )  Get the Size of Compiler/Verifier Message.
The size of the message string (including the trailing NULL) is stored into bufferSizeRet when the return value is NVVM_SUCCESS.
bufferSizeRet – [out] Size of the compilation/verification log (including the trailing NULL).
Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmLazyAddModuleToProgram ( nvvmProgram prog , const char * buffer , size_t size , const char * name )  Add a module level NVVM IR to a program.
A module added using this API is lazily loaded - the only symbols loaded are those that are required by module(s) loaded using nvvmAddModuleToProgram.
Compiler may also optimize entities in this module by making them internal to the linked NVVM IR module, making them eligible for other optimizations.
Due to these optimizations, this API to load a module is more efficient and should be used where possible.
Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_INVALID_INPUT NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmVerifyProgram ( nvvmProgram prog , int numOptions , const char * * options )  Verify the NVVM program.
Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_IR_VERSION_MISMATCH NVVM_ERROR_INVALID_PROGRAM NVVM_ERROR_INVALID_IR NVVM_ERROR_INVALID_OPTION NVVM_ERROR_NO_MODULE_IN_PROGRAM 4.2.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 5.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 5.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
CUDA for Tegra  This application note provides an overview of NVIDIA® Tegra® memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the Tegra® integrated GPU (iGPU).
It also discusses EGL interoperability. 2. Overview  This document provides an overview of NVIDIA® Tegra® memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the Tegra® integrated GPU (iGPU).
This guide is for developers who are already familiar with programming in CUDA®, and C/C++, and who want to develop applications for the Tegra® SoC.
Performance guidelines, best practices, terminology, and general information provided in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide are applicable to all CUDA-capable GPU architectures, including Tegra® devices.
The CUDA C++ Programming Guide and the CUDA C Best Practices Guide are available at the following web sites: CUDA C++ Programming Guide: https: docs.nvidia.com/cuda/cuda-c-programming-guide/index.html CUDA C++ Best Practices Guide: https: docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html 3.
Memory Management  In Tegra® devices, both the CPU (Host) and the iGPU share SoC DRAM memory.
A dGPU with separate DRAM memory can be connected to the Tegra device over PCIe or NVLink.
dGPU-connected Tegra Memory System  In Tegra, device memory, host memory, and unified memory are allocated on the same physical SoC DRAM.
The caching behavior in a Tegra system is different from that of an x86 system with a dGPU.
The caching and accessing behavior of different memory types in a Tegra system is shown in Table 1 .
Characteristics of Different Memory Types in a Tegra System  Memory Type CPU iGPU Tegra-connected dGPU Device memory Not directly accessible Cached Cached Pageable host memory Cached Not directly accessible Not directly accessible Pinned host memory Uncached where compute capability is less than 7.2.
Uncached Uncached Unified memory Cached Cached Not supported On Tegra, because device memory, host memory, and unified memory are allocated on the same physical SoC DRAM, duplicate memory allocations and data transfers can be avoided. 3.1. I/O Coherency  I/O coherency (also known as one-way coherency) is a feature with which an I/O device such as a GPU can read the latest updates in CPU caches.
It removes the need to perform CPU cache management operations when the same physical memory is shared between CPU and GPU.
The GPU cache management operations still need to be performed because the coherency is one way.
Please note that the CUDA driver internally performs the GPU cache management operations when managed memory or interop memory is used.
Applications should realize benefits from this HW feature without needing to make changes to the application’s code (see point 2 below).
The following functionalities depend on I/O coherency support: cudaHostRegister() / cuMemHostRegister() is supported only on platforms which are I/O coherent.
The host register support can be queried using the device attribute cudaDevAttrHostRegisterSupported / CU_DEVICE_ATTRIBUTE_HOST_REGISTER_SUPPORTED.
CPU cache for pinned memory allocated using cudaMallocHost() / cuMemHostAlloc() / cuMemAllocHost() is enabled only on platforms which are I/O coherent. 3.2. Estimating Total Allocatable Device Memory on an Integrated GPU Device  The cudaMemGetInfo() API returns the snapshot of free and total amount of memory available for allocation for the GPU.
The CPU can control the contents of DRAM and free DRAM memory by moving the contents of DMAR to SWAP area or vice versa.
The cudaMemGetInfo API may return a smaller size than the actually allocatable memory since the CPU may be able to free up some DRAM region by moving pages to the SWAP area.
In order to estimate the amount of allocatable device memory, CUDA application developers should consider following: On Linux and Android platforms: Device allocatable memory on Linux and Android depends mainly on the total and free sizes of swap space and main memory.
The following points can help users to estimate the total amount of device allocatable memory in various situations: Host allocated memory = Total used physical memory – Device allocated memory If (Host allocated memory Free Swap Space) then Device allocatable memory = Total Physical Memory – (Host allocated memory - Free swap space) Here, Device allocated memory is memory already allocated on the device.
It can be obtained from the NvMapMemUsed field in /proc/meminfo or from the total field of /sys/kernel/debug/nvmap/iovmm/clients .
If the free command is not available, the same information can be obtained from /proc/meminfo as: Total Used physical memory = MemTotal – MemFree Free swap space = SwapFree On QNX platforms: QNX does not use swap space, hence, cudaMemGetInfo.free will be a fair estimate of allocatable device memory as there is no swap space to move memory pages to swap area. 4. Porting Considerations  CUDA applications originally developed for dGPUs attached to x86 systems may require modifications to perform efficiently on Tegra systems.
This section describes the considerations for porting such applications to a Tegra system, such as selecting an appropriate memory buffer type (pinned memory, unified memory, and others) and selecting between iGPU and dGPU, to achieve efficient performance for the application. 4.1. Memory Selection  CUDA applications can use various kinds of memory buffers, such as device memory, pageable host memory, pinned memory, and unified memory.
Even though these memory buffer types are allocated on the same physical device, each has different accessing and caching behaviors, as shown in Table 1 .
It is important to select the most appropriate memory buffer type for efficient application execution.
For example, in an application with multiple kernels, there may be buffers that are used only by the intermediate kernels of the application as input or output.
Pageable Host Memory Use pageable host memory for buffers whose accessibility is limited to the CPU.
Pinned Memory Tegra® systems with different compute capabilities exhibit different behavior in terms of I/O coherency.
For example, Tegra® systems with compute capability greater than or equal to 7.2 are I/O coherent and others are not I/O coherent.
On Tegra® systems with I/O coherency, the CPU access time of pinned memory is as good as pageable host memory because it is cached on the CPU.
However, on Tegra® systems without I/O coherency, the CPU access time of pinned memory is higher, because it is not cached on the CPU.
Pinned memory is recommended for small buffers because the caching effect is negligible for such buffers and also because pinned memory does not involve any additional overhead, unlike Unified Memory.
With no additional overhead, pinned memory is also preferable for large buffers if the access pattern is not cache friendly on iGPU.
For large buffers, when the buffer is accessed only once on iGPU in a coalescing manner, performance on iGPU can be as good as unified memory on iGPU.
On Tegra®, using unified memory in applications requires additional coherency and cache maintenance operations during the kernel launch, synchronization and prefetching hint calls.
This coherency maintenance overhead is slightly higher on a Tegra® system with compute capability less than 7.2 as they lack I/O coherency.
On Tegra® devices with I/O coherency (with a compute capability of 7.2 or greater) where unified memory is cached on both CPU and iGPU, for large buffers which are frequently accessed by the iGPU and the CPU and the accesses on iGPU are repetitive , unified memory is preferable since repetitive accesses can offset the cache maintenance cost.
On Tegra® devices without I/O coherency (with a compute capability of less than 7.2), for large buffers which are frequently accessed by the CPU and the iGPU and the accesses on iGPU are not repetitive , unified memory is still preferable over pinned memory because pinned memory is not cached on both CPU and iGPU.
Pinned memory or unified memory can be used to reduce the data transfer overhead between CPU and iGPU as both memories are directly accessible from the CPU and the iGPU.
In an application, input and output buffers that must be accessible on both the host and the iGPU can be allocated using either unified memory or pinned memory.
Note The unified memory model requires the driver and system software to manage coherence on the current Tegra SOC.
Software managed coherence is by nature non-deterministic and not recommended in a safe context.
Evaluate the impact of unified memory overheads, pinned memory cache misses, and device memory data transfers in applications to determine the correct memory selection. 4.2. Pinned Memory  This section provides guidelines for porting applications that use pinned memory allocations in x86 systems with dGPUs to Tegra®.
CUDA applications developed for a dGPU attached to x86 system use pinned memory to reduce data transfer time and to overlap data transfers with kernel execution time.
For specific information on this topic, see “Data Transfer Between Host and Device” and “Asynchronous and Overlapping Transfers with Computation” at the following websites.
“Data Transfer Between Host and Device”: https: docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#data-transfer-between-host-and-device “Asynchronous and Overlapping Transfers with Computation”: https: docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation On Tegra® systems with no I/O coherency, repetitive access of pinned memory degrades application performance, because pinned memory is not cached on the CPU in such systems.
A sample application is shown below in which a set of filters and operations (k1, k2, and k3) are applied to an image.
Pinned memory is allocated to reduce data transfer time on an x86 system with a dGPU, increasing the overall application speed.
However, targeting a Tegra® device with the same code causes a drastic increase in the execution time of the readImage() function because it repeatedly accesses an uncached buffer.
If the time taken by readImage() is significantly higher compared to kernels execution time, it is recommended to use unified memory to reduce the readImage() time.
Otherwise, evaluate the application with pinned memory and unified memory by removing unnecessary data transfer calls to decide best suited memory.
UseImageonCPU ( h_d ); }   Porting the code on Tegra int main () { int * h_a , * d_b , * d_c , * h_d ; int height = 1024 ; int width = 1024 ; size_t sizeOfImage = width * height * sizeof ( int );   4MB image  Unified memory allocated for input and output  buffer of application pipeline cudaMallocManaged ( h_a , sizeOfImage , cudaMemAttachHost ); cudaMallocManaged ( h_d , sizeOfImage );  Intermediate buffers not needed on CPU side.
So allocate them on device memory cudaMalloc ( & d_b , sizeOfImage ); cudaMalloc ( & d_c , sizeOfImage );  CPU reads Image; readImage ( h_a );   Intialize the h_a buffer   - CUDA Application pipeline start -   Prefetch input image data to GPU cudaStreamAttachMemAsync ( NULL , h_a , 0 , cudaMemAttachGlobal ); k1 >> ( h_a , d_b ) k2 >> ( d_b , d_c ) k3 >> ( d_c , h_d )   Prefetch output image data to CPU cudaStreamAttachMemAsync ( NULL , h_d , 0 , cudaMemAttachHost ); cudaStreamSynchronize ( NULL );   - CUDA Application pipeline end -   Use processed Image i.e h_d on CPU side.
UseImageonCPU ( h_d ); } The cudaHostRegister() function The cudaHostRegister() function is not supported on Tegra® devices with compute capability less than 7.2, because those devices do not have I/O coherency.
Use other pinned memory allocation functions such as cudaMallocHost() and cudaHostAlloc() if cudaHostRegister() is not supported on the device.
GNU Atomic operations on pinned memory The GNU atomic operations on uncached memory is not supported on Tegra® CPU.
As pinned memory is not cached on Tegra® devices with compute capability less than 7.2, GNU atomic operations is not supported on pinned memory. 4.3. Effective Usage of Unified Memory on Tegra  Using unified memory in applications requires additional coherency and cache maintenance operations at kernel launch, synchronization, and prefetching hint calls.
These operations are performed synchronously with other GPU work which can cause unpredictable latencies in the application.
The performance of unified memory on Tegra® can be improved by providing data prefetching hints.
To prefetch the data, the cudaStreamAttachMemAsync() function can be used, in addition to the techniques described in the “Coherency and Concurrency” section of the CUDA C Programming Guide at the following link: https: docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-coherency-hd to prefetch the data.
The prefetching behavior of unified memory, as triggered by the changing states of the attachment flag, is shown in Table 2 .
Unified Memory Prefetching Behavior per Changing Attachment Flag States  Previous Flag Current Flag Prefetching Behavior cudaMemAttachGlobal/cudaMemAttachSingle cudaMemAttachHost Causes prefetch to CPU cudaMemAttachHost cudaMemAttachGlobal/ cudaMemAttachSingle Causes prefetch to GPU cudaMemAttachGlobal cudaMemAttachSingle No prefetch to GPU cudaMemAttachSingle cudaMemAttachGlobal No prefetch to GPU The following example shows usage of cudaStreamAttachMemAsync() to prefetch data.
Note However, not supported on Tegra® devices are the data prefetching techniques that use cudaMemPrefetchAsync() as described in the “Performance Tuning” section of the CUDA C++ Programming Guide at the following web site: https: docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-performance-tuning Note There are limitations in QNX system software which prevent implementation of all UVM optimizations.
Because of this, using cudaStreamAttachMemAsync() to prefetch hints on QNX does not benefit performance.
Any mismatch in the width and height values of frame with the values specified in cudaEGLStreamProducerConnect() leads to undefined behavior.
In the sample, the CUDA producer is sending a single frame, but it can send multiple frames over a loop.
The cudaEGLStreamProducerReturnFrame() call waits until it receives the released frame from the consumer.
Once the CUDA producer presents the first frame to EGLstream, at least one frame is always available for consumer acquisition until the producer disconnects.
This prevents the removal of the last frame from EGLStream, which would block cudaEGLStreamProducerReturnFrame ().
Use the EGL_NV_stream_reset extension to set EGLStream attribute EGL_SUPPORT_REUSE_NV to false to allow the last frame to be removed from EGLStream.
This allows removing or returning the last frame from EGLStream. 5.1.3. CUDA as Consumer  When CUDA is the consumer, the supported producers are CUDA, OpenGL, NvMedia, Argus, and Camera.
The following consumer side steps are shown in the sample code that follows: Connect consumer to EGLStream (line 5).
Connect consumer to EGLStream cudaEGLStreamConsumerConnect(&conn, eglStream);   consumer acquires a frame unsigned int timeout = 16000; cudaEGLStreamConsumerAcquireFrame(& conn, &cudaResource, eglStream, timeout);  consumer gets a cuda object pointer cudaGraphicsResourceGetMappedEglFrame(&cudaEgl, cudaResource, 0, 0); size_t numElem = cudaEgl->planeDesc[0].pitch * cudaEgl->planeDesc[0].height; .
int checkIfOne = 1;   Checks if each value in the buffer is 1, if any value is not 1, it sets checkIfOne = 0.
} In the sample code, the CUDA consumer receives a single frame, but it can also receive multiple frames over a loop.
If a CUDA consumer fails to receive a new frame in the specified time limit using cudaEGLStreamConsumerAcquireFrame() , it reacquires the previous frame from EGLStream.
The application can use eglQueryStreamKHR() to query for the availability of new frames using.
If the CUDA context is destroyed while connected to EGLStream, the stream is placed in the EGL_STREAM_STATE_DISCONNECTED_KHR state and the connection handle is invalidated. 5.1.4. Implicit Synchronization  EGLStream provides implicit synchronization in an application.
For example, in the previous code samples, both the producer and consumer threads are running in parallel and the K1 and K2 kernel processes access the same frame, but K2 execution in the consumer thread is guaranteed to occur only after kernel K1 in the producer thread finishes.
The cudaEGLStreamConsumerAcquireFrame() function waits on the GPU side until K1 finishes and ensures synchronization between producer and consumer.
Similarly, cudaEGLStreamProducerReturnFrame() in the producer thread is guaranteed to get the frame only after K2 finishes and the consumer releases the frame.
These non-blocking calls allow the CPU to do other computation in between, as synchronization is taken care of on the GPU side.
The EGLStreams_CUDA_Interop CUDA sample code shows the usage of EGLStream in detail. 5.1.5. Data Transfer Between Producer and Consumer  Data transfer between producer and consumer is avoided when they are present on the same device.
In a Tegra® platform that includes a dGPU however, such as is in NVIDIA DRIVE™ PX 2, the producer and consumer can be present on different devices.
In that case, an additional memory copy is required internally to move the frame between Tegra® SoC DRAM and dGPU DRAM.
Note On systems where a Tegra® device is connected to a dGPU, if a producer frame uses CUDA array, both producer and consumer should be on the same GPU.
But if a producer frame uses CUDA device pointers, the consumer can be present on any GPU. 5.1.6. EGLStream Pipeline  An application can use multiple EGL streams in a pipeline to pass the frames from one API to another.
For an application where NvMedia sends a frame to CUDA for computation, CUDA sends the same frame to OpenGL for rendering after the computation.
EGLStream Pipeline  NvMedia and CUDA connect as producer and consumer respectively to one EGLStream.
Using multiple EGLStreams in pipeline fashion gives the flexibility to send frames across multiple APIs without allocating additional memory or requiring explicit data transfers.
The above steps can be performed in a loop to facilitate the transfer of multiple frames in the EGLStream pipeline. 5.2. EGLImage  An EGLImage interop allows an EGL client API to share image data with other EGL client APIs.
For example, an application can use an EGLImage interop to share an OpenGL texture with CUDA without allocating any additional memory.
Note An EGLImage is created using eglCreateImageKHR() and destroyed using eglDestroyImageKHR() .
For more information see the EGLImage specification at the following web site: https: www.khronos.org/registry/EGL/extensions/KHR/EGL_KHR_image_base.txt 5.2.1.
CUDA interop with EGLImage  CUDA supports interoperation with EGLImage, allowing CUDA to read or modify the data of an EGLImage.
In CUDA, a single-planar EGLImage object is represented as a CUDA array or device pointer.
Similarly, a multi-planar EGLImage object is represented as an array of device pointers or CUDA arrays.
EGLImage is supported on Tegra® devices running the Linux, QNX, or Android operating systems.
An application can use cudaGraphicsResourceGetMappedEglFrame() to get a frame from the graphics resource object.
The frameType parameter in cudaEglFrame indicates if the frame is a CUDA device pointer or a CUDA array.
For a single planar graphics resource, an application can directly obtain a device pointer or CUDA array using cudaGraphicsResourceGetMappedPointer() or cudaGraphicsSubResourceGetMappedArray() respectively.
Also, cudaGraphicsEGLRegisterImage() expects only the ‘0’ flag as other API flags are for future use.
The pArray array is bound to a surface object to allow modification of the OpenGL texture in the changeTexture.
unsigned char* temp = (unsigned char*)(malloc(bufferSize * sizeof(unsigned char)));   Get the modified texture values GL_SAFE_CALL(glGetTexImage(GL_TEXTURE_2D, 0, GL_RGBA, GL_UNSIGNED_BYTE,(void*)temp)); .
This function check if the OpenGL texture got modified values checkbuf(temp,hostSurf);   Clean up CUDA cudaGraphicsUnregisterResource(pResource); cudaDestroySurfaceObject(inputSurfObj); eglDestroySyncKHR(eglDisplayHandle, eglsync1); eglDestroySyncKHR(eglDisplayHandle, eglsync2); cudaEventDestroy(egl_event); cudaEventDestroy(cuda_event); . 6. CUDA Upgradable Package for Jetson  CUDA introduced an upgrade path starting with JetPack SDK 5.0 which provides an option to update the CUDA driver and the CUDA toolkit to the latest version.
Prerequisite  The Jetson device must be installed with a compatible NVIDIA JetPack version.
Refer to Use the Right Upgrade Package for more info. 6.1.2. From Network Repositories or Local Installers  The CUDA Downloads page provides step-by-step instructions on how to download and use the local installer or CUDA network repositories to install the latest Toolkit.
The CUDA upgrade package gets downloaded and installed along with the corresponding CUDA toolkit for Linux-aarch64-jetson devices.
For use cases where applications are built on the host and require just the CUDA upgrade package to be installed independently on the target, the corresponding Debians can be found in CUDA Repos .
Taking 11.8 for example, this can be installed by running the command: $ sudo apt-get install -y cuda-compat-11-8 Note This is the recommended path for CUDA upgrade for devices that have disk space (secondary storage) limitations.
* - Just In Time - Link Time Optimization (CUDA 11.8 and later only) libnvidia-ptxjitcompiler.so.
* - the JIT (just-in-time) compiler for PTX files nvidia-cuda-mps-control - CUDA MPS control executable nvidia-cuda-mps-server - CUDA MPS server executable These files together implement the CUDA 11.8 driver interfaces.
Example The following commands show how the CUDA Upgrade package can be installed and used to run the applications.
The following additional packages will be installed: cuda-11-8 cuda-cccl-11-8 cuda-command-line-tools-11-8 cuda-compat-11-8 .
The following NEW packages will be installed: cuda cuda-11-8 cuda-cccl-11-8 cuda-command-line-tools-11-8 cuda-compat-11-8 .
Get:1 https: developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/arm64 cuda-compat-11-8 11.8.31339915-1 [15.8 MB] Fetched 15.7 MB in 12s (1,338 kB/s) Selecting previously unselected package cuda-compat-11-8.
148682 files and directories currently installed.) Preparing to unpack .../00-cuda-compat-11-8_11.8.30682616-1_arm64.deb ...
deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.8, CUDA Runtime Version = 11.8, NumDevs = 1 Result = PASS Only a single CUDA upgrade package can be installed at any point in time on a given system.
While installing a new CUDA upgrade package, the previous version of the installed upgrade package will be removed and replaced with the new one.
The default drivers (originally installed with the NVIDIA JetPack and part of the L4T BSP) will be retained by the installer.
The application has the ability to use either the default version of CUDA (originally installed with NVIDIA JetPack) or the one installed by the upgrade package.
In addition to LD_LIBRARY_PATH , CUDA MPS users must set the PATH variable in order to use the nvidia-cuda-mps-* executables installed by the upgrade package before starting MPS and running the CUDA applications that use MPS.
The MPS executables installed with the upgrade package are only compatible with the CUDA driver installed with the same upgrade package, and vice versa, which can be checked with the version info.
Installation of the upgrade package will fail if it is not compatible with the NVIDIA JetPack version. 6.2. Deployment Considerations for CUDA Upgrade Package  6.2.1.
Use the Right Upgrade Package  The CUDA upgrade package is named after the highest toolkit that it can support.
For example, if you are on the NVIDIA JetPack SDK 5.0 (11.4) driver but require 11.8 application support, install the CUDA upgrade package for 11.8.
Each CUDA release will support upgrades only for a specific set of NVIDIA JetPack releases.
If a new feature in the latest CUDA driver needs an updated NVIDIA JetPack SDK component/interface, it might not work and error out when used. 6.2.3. Check for Compatibility Support  In addition to the CUDA driver and certain compiler components, there are other drivers in the NVIDIA JetPack that remain at the default version.
A well-written application should use following error codes to determine if CUDA Upgrade is supported.
System administrators should be aware of these error codes to determine if there are errors in the deployment.
This error indicates that there is a mismatch between the versions of the upgraded CUDA driver and the already installed drivers on the system.
This error indicates that the system was updated to run with the CUDA upgrade package but the visible hardware detected by CUDA does not support this configuration. 7. cuDLA  DLA (Deep Learning Accelerator) is a fixed function accelerator present on the NVIDIA Tegra SoC and is used for inference applications.
The DLA HW has superior performance/W and can natively run many of the layers in modern neural networks, thus making it an attractive value proposition for embedded AI applications.
Programming the DLA typically consists of an offline and online step: in the offline step, an input network is parsed and compiled by the DLA compiler into a loadable and in the online step, that loadable is executed by the DLA HW to generate an inference result.
The SW stack that is currently provided by NVIDIA to perform the online or execution step consists of NvMediaDla and the DLA runtime/KMD.
Together, these APIs enable the user to submit a DLA task to the DLA HW for inferencing purposes.
DLA SW stack  It follows from the model above that users wishing to use GPU and DLA together in an application would have to use interop mechanisms such as EGLStreams/NvSci to share buffers as well as synchronization primitives between the GPU and DLA.
These interop mechanisms usually involve many steps for each buffer that is being shared and have limited ability to fine-tune the scheduling of tasks between the GPU and DLA.
cuDLA is an extension of the CUDA programming model that integrates DLA (Deep Learning Accelerator) with CUDA thereby making it possible to submit DLA tasks using CUDA programming constructs such as streams and graphs.
Managing shared buffers as well as synchronizing the tasks between GPU and DLA is transparently handled by cuDLA, freeing up the programmer to focus on the high-level usecase. 7.1. Developer Guide  This section describes the key principles involved in programming the DLA HW using cuDLA APIs.
The cuDLA interfaces expose mechanisms to initialize devices, manage memory and submit DLA tasks.
The detailed specification of these APIs is described in the API specification and should be referred while writing a cuDLA application.
Since cuDLA is an extension of CUDA, it is designed to work in conjunction with CUDA APIs that perform CUDA functions such as GPU management, context management etc.
Therefore, the current state of the application in terms of which GPU is selected and the current active context (and its lifecycle) are all important considerations while evaluating the behavior of a cuDLA API. 7.1.1. Device Model  To perform any DLA operation, it is necessary that an application first create a cuDLA device handle.
The cudlaCreateDevice() API creates a logical instance of a cuDLA device wherein the selected DLA HW instance is coupled with the current active GPU selected via CUDA.
For example, the following code snippet would create a logical instance consisting of the current GPU (set via cudaSetDevice() ) and DLA HW 0.
Currently, cuDLA supports only iGPU on Tegra and an attempt to create a device handle by setting the current GPU as a dGPU would result in a device creation error during cudlaCreateDevice() .
cudlaDevHandle devHandle ; cudlaStatus ret ; ret = cudlaCreateDevice ( 0 , & devHandle , CUDLA_CUDA_DLA ); Device model  The user can create any number of such logical instances using cudlaCreateDevice() using any combination of GPU and DLA HW instances (subject to system resource availability): Device model - multiple instances  In addition, cudlaCreateDevice() supports an alternative flag during device creation - CUDLA_STANDALONE.
This flag can be used by applications when they wish to create a cuDLA device in standalone mode i.e without coupling it with a GPU device.
All device submissions can be accomplished using cuDLA in standalone mode as well but in this mode there is no support for CUDA interactions.
Consequently, in what follows, two modes of execution are considered while describing a particular API or a particular usecase: the hybrid mode and the standalone mode.
The API spec has complete details about which API is supported in which mode. 7.1.2. Loading and Querying Modules  The cuDLA device handle needs an appropriate loadable to be associated with it before any DLA task submission occurs.
The loadable has information about the number of input and output tensors as well as their respective metadata and can be queried by the application to retrieve this information.
A typical application flow after a successful cuDLA device initialization would look like this (interspersed with some debug logs): DPRINTF ( "Device created successfully   " );   Load the loadable from 'loadableData' in which the loadable binary has   been copied from the location of the loadable - disk or otherwise.
err = cudlaModuleLoadFromMemory ( devHandle , loadableData , file_size , & amp ; moduleHandle , 0 ); if ( err != cudlaSuccess ) {   handle error }   Get tensor attributes.
uint32_t numInputTensors = 0 ; uint32_t numOutputTensors = 0 ; cudlaModuleAttribute attribute ; err = cudlaModuleGetAttributes ( moduleHandle , CUDLA_NUM_INPUT_TENSORS , & amp ; attribute ); if ( err != cudlaSuccess ) {   handle error } numInputTensors = attribute .
numInputTensors ; DPRINTF ( "numInputTensors = %d   " , numInputTensors ); err = cudlaModuleGetAttributes ( moduleHandle , CUDLA_NUM_OUTPUT_TENSORS , & amp ; attribute ); if ( err != cudlaSuccess ) {   handle error } numOutputTensors = attribute .
numOutputTensors ; DPRINTF ( "numOutputTensors = %d   " , numOutputTensors ); cudlaModuleTensorDescriptor * inputTensorDesc = ( cudlaModuleTensorDescriptor * ) malloc ( sizeof ( cudlaModuleTensorDescriptor ) * numInputTensors ); cudlaModuleTensorDescriptor * outputTensorDesc = ( cudlaModuleTensorDescriptor * ) malloc ( sizeof ( cudlaModuleTensorDescriptor ) * numOutputTensors ); if (( inputTensorDesc == NULL ) || ( outputTensorDesc == NULL )) {   handle error } attribute .
inputTensorDesc = inputTensorDesc ; err = cudlaModuleGetAttributes ( moduleHandle , CUDLA_INPUT_TENSOR_DESCRIPTORS , & amp ; attribute ); if ( err != cudlaSuccess ) {   handle error } attribute .
outputTensorDesc = outputTensorDesc ; err = cudlaModuleGetAttributes ( moduleHandle , CUDLA_OUTPUT_TENSOR_DESCRIPTORS , & amp ; attribute ); if ( err != cudlaSuccess ) {   handle error } Applications can use the retrieved tensor descriptors to setup their data buffers in terms of size and formats.
Detailed information about the contents of the tensor descriptors is present in the API specification section under cudlaModuleGetAttributes() . 7.1.3. Memory Model  The GPU and DLA have different MMUs that manage the VA to PA conversion while performing their respective functions.
The figure below shows an example where the GMMU performs a translation for GPU VAs and the SMMU performs a similar function for the VAs arriving from the DLA.
Virtual address to physical address conversion  In hybrid mode, before a CUDA pointer can be accessed by the DLA, it is necessary that the CUDA pointer be registered with the DLA.
This registration step creates an entry in the SMMU and returns the corresponding VA for use in task submissions.
The following code snippet shows an example registration for a device handle created with the flag CUDLA_CUDA_DLA :   Allocate memory on GPU.
void * buffer ; uint32_t size = 100 ; result = cudaMalloc ( & inputBufferGPU , size ); if ( result != cudaSuccess ) {   handle error }   Register the CUDA-allocated buffers.
uint64_t * bufferRegisteredPtr = NULL ; err = cudlaMemRegister ( devHandle , ( uint64_t * ) inputBufferGPU , size , & bufferRegisteredPtr , 0 ); if ( err != cudlaSuccess ) {   handle error } In standalone mode, cuDLA functions without the underlying CUDA device.
Consequently, in this mode, the memory allocations performed by the application (which need to be subsequently registered) need to come from outside CUDA.
On Tegra systems, cuDLA supports registration of NvSciBuf allocations via the cudlaImportExternalMemory() API as the following code snippet shows:   Allocate the NvSciBuf object.
NvSciBufObj inputBufObj ; sciError = NvSciBufObjAlloc ( reconciledInputAttrList , & inputBufObj ); if ( sciError != NvSciError_Success ) {   handle error } uint64_t * inputBufObjRegPtr = NULL ;   importing external memory cudlaExternalMemoryHandleDesc memDesc = { 0 }; memset ( & memDesc , 0 , sizeof ( memDesc )); memDesc .
size = size ; err = cudlaImportExternalMemory ( devHandle , & memDesc , & inputBufObjRegPtr , 0 ); if ( err != cudlaSuccess ) {   handle error } 7.1.4.
Task Execution  Submitting a DLA task for execution is similar to submitting a CUDA kernel to the GPU.
cuDLA natively supports CUDA streams and works seamlessly with the stream semantics to ensure that all tasks intended for the DLA are executed by the DLA HW only after the previous tasks on the stream have completed execution.
This enables applications to setup complex processing workflows between the GPU and the DLA using familiar stream semantics without having to manage memory coherency and execution dependencies between GPU and DLA.
DLA tasks can be interspersed with GPU tasks in a given stream or multiple streams and cudlaSubmitTask() handles all the memory/execution dependencies.
cuDLA task execution model  The submit task API needs the input and output tensors in the form of the addresses registered with the DLA (using cudlaMemRegister() ).
An application can pre-register all the required pointers with cuDLA and then use the registered pointers during cudlaSubmitTask() .
This API, in turn, ensures that the results of the previous operations on the underlying memory corresponding to the registered pointers is visible to the DLA before it begins execution of the current task.
A typical application code consisting of CUDA and cuDLA operations is shown in the snippet below: DPRINTF ( "ALL MEMORY REGISTERED SUCCESSFULLY   " );   Copy data from CPU buffers to GPU buffers.
size , cudaMemcpyHostToDevice , stream ); if ( result != cudaSuccess ) {   handle error } result = cudaMemsetAsync ( outputBufferGPU , 0 , outputTensorDesc [ 0 ].
signalEvents = NULL ; err = cudlaSubmitTask ( devHandle , & task , 1 , stream , 0 ); if ( err != cudlaSuccess ) {   handle error } DPRINTF ( "SUBMIT IS DONE ! !!   " ); result = cudaMemcpyAsync ( outputBuffer , outputBufferGPU , outputTensorDesc [ 0 ].
size , cudaMemcpyDeviceToHost , stream ); if ( result != cudaSuccess ) {   handle error } In standalone mode, the stream parameter in cudlaSubmitTask() must be specified as NULL as cuDLA is operating independently of CUDA.
In this case, the tasks submitted to the DLA are executed in FIFO order. 7.1.4.1.1. Multithreaded User Submission  Users can specify the CUDLA_SUBMIT_SKIP_LOCK_ACQUIRE flag during submission to a particular device handle if they are sure that submission to this particular device handle occurs only in this thread and that there is no shared data at the application level between this device handle and any other device handle which might be used in a parallel thread for submission.
This flag facilitates some optimizations in the submission path which might lead to better submission times from the application point of view. 7.1.4.2. Synchronization  Synchronization of tasks in hybrid mode does not need a different API.
Since DLA tasks are submitted to CUDA streams, it is sufficient to wait on the stream to complete its work in order to ensure that all DLA tasks submitted on that stream are completed.
In this regard DLA task synchronization is compatible with any of the different synchronization mechanisms available in CUDA – Event, Stream, Device – and the entire CUDA machinery is available for applications to setup different flows and usecases.
In standalone mode, however, the synchronization mechanisms are different given that cuDLA operates independently of CUDA.
In this mode, the cudlaTask structure has a provision to specify wait and signal events that cuDLA must wait on and signal respectively as part of cudlaSubmitTask() .
Each submitted task will wait for all its wait events to be signaled before beginning execution and will provide a signal event (if one is requested for during cudlaSubmitTask() ) that the application (or any other entity) can wait on to ensure that the submitted task has completed execution.
Furthermore, only NvSciSync objects can be registered and signaled as part of signal events and the fence corresponding to the signaled event is returned as part of cudlaSubmitTask() .
Like all memory operations, the underlying backing store for the events (in this case the NvSciSync object) must be registered with cuDLA before using it in a task submission.
extSyncObject = syncObj1 ; err = cudlaImportExternalSemaphore ( devHandle , & semaMemDesc , & nvSciSyncObjRegPtr1 , 0 ); if ( err != cudlaSuccess ) {   handle error } memset ( & semaMemDesc , 0 , sizeof ( semaMemDesc )); semaMemDesc .
extSyncObject = syncObj2 ; err = cudlaImportExternalSemaphore ( devHandle , & semaMemDesc , & nvSciSyncObjRegPtr2 , 0 ); if ( err != cudlaSuccess ) {   handle error } DPRINTF ( "ALL EXTERNAL SEMAPHORES REGISTERED SUCCESSFULLY   " ); 7.1.4.2.2.
Events setup for cudlaSubmitTask()    Wait events NvSciSyncFence preFence = NvSciSyncFenceInitializer ; sciError = NvSciSyncObjGenerateFence ( syncObj1 , & preFence ); if ( sciError != NvSciError_Success ) {   handle error } cudlaWaitEvents * waitEvents ; waitEvents = ( cudlaWaitEvents * ) malloc ( sizeof ( cudlaWaitEvents )); if ( waitEvents == NULL ) {   handle error } waitEvents -> numEvents = 1 ; CudlaFence * preFences = ( CudlaFence * ) malloc ( waitEvents -> numEvents * sizeof ( CudlaFence )); if ( preFences == NULL ) {   handle error } preFences [ 0 ].
type = CUDLA_NVSCISYNC_FENCE ; waitEvents -> preFences = preFences ;   Signal Events cudlaSignalEvents * signalEvents ; signalEvents = ( cudlaSignalEvents * ) malloc ( sizeof ( cudlaSignalEvents )); if ( signalEvents == NULL ) {   handle error } signalEvents -> numEvents = 1 ; uint64_t ** devPtrs = ( uint64_t ** ) malloc ( signalEvents -> numEvents * sizeof ( uint64_t * )); if ( devPtrs == NULL ) {   handle error } devPtrs [ 0 ] = nvSciSyncObjRegPtr2 ; signalEvents -> devPtrs = devPtrs ; signalEvents -> eofFences = ( CudlaFence * ) malloc ( signalEvents -> numEvents * sizeof ( CudlaFence )); if ( signalEvents -> eofFences == NULL ) {   handle error } NvSciSyncFence eofFence = NvSciSyncFenceInitializer ; signalEvents -> eofFences [ 0 ].
signalEvents = signalEvents ; err = cudlaSubmitTask ( devHandle , & task , 1 , NULL , 0 ); if ( err != cudlaSuccess ) {   handle error } DPRINTF ( "SUBMIT IS DONE !   " ); 7.1.4.2.3.
In practice, this signal will be done by another   entity or driver that provides the data input for this particular submitted task.
In practice, this wait will be done by   another entity or driver that is waiting for the output of the submitted task.
fence ), nvSciCtx , -1 ); if ( sciError != NvSciError_Success ) {   handle error } 7.1.4.2.4.
Supported Synchronization Primitives in cuDLA  cuDLA supports two types of NvSciSync object primitives.
By default, cuDLA prioritizes sync point primitive over deterministic semaphore primitive and sets these priorities in the NvSciSync attribute list when requested by the application using cudlaGetNvSciSyncAttributes() .
For Deterministic semaphore, the NvSciSync attribute list used to create the NvSciSync object must have the value of NvSciSyncAttrKey_RequireDeterministicFences key set to true.
Deterministic fences allow users to enqueue a wait over the semaphore object even before corresponding signal is enqueued.
For such semaphore object, cuDLA guarantees that each signal operation will increment the fence value by ‘1’.
Users are expected to keep track of signals enqueued on the semaphore object and insert waits accordingly. 7.1.4.2.5. Setting NvSciSyncAttrKey_RequireDeterministicFences key in NvSciSyncAttrList    Set NvSciSyncAttrKey_RequireDeterministicFences key to true in   NvScisyncAtrrList that is used to create NvSciSync object with   Deterministic Semaphore primitive.
len = sizeof ( detFenceReq ); return NvSciSyncAttrListSetAttrs ( list , keyValue , 2 ); 7.1.4.2.6.
Timestamp Support for NvSciFence  cuDLA supports the timestamp feature of NvSci in cuDLA standalone mode.
Timestamp support enables users to get the time at which a particular fence has been signaled.
cuDLA users can request timestamp support by setting the value of the NvSciSyncAttrKey_WaiterRequireTimestamps key as true while filling up the NvSci waiter attribute list.
The users can use this timestamp along with SOF(Start Of Frame) fence and EOF(End OF Frame) fence to get a snapshot of DLA clock just before start of task & after task completion respectively.
This enables users to calculate time taken by DLA to execute the submitted task. 7.1.4.2.7. Requesting Timestamp Support for NvSciSync Object  sciError fillCpuWaiterAttrList ( NvSciSyncAttrList list ) { bool cpuWaiter = true ; NvSciSyncAttrKeyValuePair keyValue [ 3 ]; memset ( keyValue , 0 , sizeof ( keyValue )); keyValue [ 0 ].
len = sizeof ( cpuWaiter ); NvSciSyncAccessPerm cpuPerm = NvSciSyncAccessPerm_WaitOnly ; keyValue [ 1 ].
Extracting Timestamp Value from Fence  Refer to these sections for more information: Registering an external semaphore: Events setup for cudlaSubmitTask() Waiting on the signal event   To extract Timestamp of the fence   Timestamp will be valid only after fence is signaled   hence Fence must be waited up on before extracting timestamp value uint64_t eofTimestampUS = 0UL ; sciError = NvSciSyncFenceGetTimestamp ( reinterpret_cast ( signalEvents -> eofFences .
fence ), & ( eofTimestampUS )); if (( sciError != NvSciError_Success ) || ( eofTimestampUS == 0UL )) {  handle error } 7.1.4.3.
Fault Diagnostics  To perform fault diagnostics for DLA HW, users should specify the CUDLA_MODULE_ENABLE_FAULT_DIAGNOSTICS flag to load the module and CUDLA_SUBMIT_DIAGNOSTICS_TASK during task submission.
With this flag set, in standalone mode the user is not allowed to do event only submissions, where tensor information is NULL and only events (wait/signal or both) are present in task.
This diagnostic module does not expect any input tensors and so does not require input tensor memory.
However the user is expected to query the number of output tensors, allocate the output tensor memory, and pass the same while using the submit task. 7.1.4.4. NOOP Submission  Users can mark certain tasks as noop tasks while calling cudlaSubmitTask() .
Specifically, the task is submitted to DLA, wait/signal events are considered before and after and stream semantics are respected.
This is supported in both hybrid and standalone modes. 7.1.5. Error Reporting Model  The asynchronous nature of task execution results in two kinds of errors that can get reported via cuDLA APIs: Synchronous errors Asynchronous errors Synchronous errors are those that are reported by the cuDLA APIs as part of their return code when they are invoked in an application.
Asynchronous errors are those that are detected later compared to sequential program execution.
The typical scenario here is that each task submitted to the DLA HW executes after a particular duration of time.
As a result, if there are errors in the task execution, they cannot be reported as part of the task submission APIs.
Depending on the timing of the errors, they are reported during a subsequent cuDLA API call or after a synchronization operation.
HW execution errors reported as part of cuDLA APIs are straightforward to handle at the application level.
However, if there is a no cuDLA API call currently executing or about to execute in the application, then the application needs to perform extra steps to handle asynchronous errors.
As mentioned in the device model section, cuDLA logically associates DLA with a GPU for the purposes of execution.
The user needs to check for DLA-specific errors from CUDA synchronization operations and then check the cuDLA device handle for the exact error using cudlaGetLastError() .
If there are multiple cuDLA device handles in the application and each of them have submitted some tasks to cuDLA in hybrid mode, then each and every device handle much be checked for errors.
The underlying model here is to use CUDA to detect DLA HW errors and then use cudlaGetLastError() on the relevant handle to report the exact error.
The code snippet below shows an example: result = cudaStreamSynchronize ( stream ); if ( result != cudaSuccess ) { DPRINTF ( "Error in synchronizing stream = %s   " , cudaGetErrorName ( result )); if ( result == cudaErrorExternalDevice ) { cudlaStatus hwStatus = cudlaGetLastError ( devHandle ); if ( hwStatus != cudlaSuccess ) { DPRINTF ( "Asynchronous error in HW = %u   " , hwStatus ); } } } This error reporting model is compatible with CUDA Driver APIs as well and therefore if the application uses CUDA Driver APIs for synchronization, similar error codes and error handling flow is applicable.
In standalone mode, the model is similar with the exception that there is no corresponding mechanism to detect errors as part of synchronization operations.
In this mode, the only option that an application has to wait on the submitted tasks is to wait on the NvSciSync fence returned by the latest submission.
As of this writing, NvSciSync does not support reporting DLA HW errors and therefore an application is expected to wait for the fence and then query cudlaGetLastError() for any errors during execution. 7.2. Migrating from NvMediaDla to cuDLA  NvMediaDla and cuDLA have different programming models with some degree of overlap in the functionality exposed by the respective APIs.
The following table provides a mapping from the NvMediaDla API to the equivalent cuDLA API or functionality.
This is intended to be used as a reference when migrating an NvMediaDla app to a cuDLA app.
NvMediaDla cuDLA NvMediaDlaGetVersion() cudlaGetVersion() NvMediaDlaPingById() Not required as ping is done inside cudlaCreateDevice and only upon successful ping does device handle creation succeed.
NvMediaDlaCreate() cudlaCreateDevice() NvMediaDlaDestroy() cudlaDestroyDevice() NvMediaDlaGetUMDVersion() Not available NvMediaDlaGetNumEngines() cudlaDeviceGetCount() NvMediaDlaGetMaxOutstandingTasks() Not available NvMediaDlaInit() cudlaCreateDevice (but specifying number of input tasks is not available) NvMediaDlaGetInstanceId() Not available NvMediaDlaGetNumTasks() Not available NvMediaDlaLoadableCreate() Not required as declaring a variable of type cudlaModule is sufficient alongwith cudlaModuleLoadFromMemory() .
NvMediaDlaLoadableDestroy() Not required as cuDLA modules are declared as variables of type cudlaModule .
NvMediaDlaAppendLoadable() Not required as this is done inside cudlaModuleLoadFromMemory() .
NvMediaDlaSetCurrentLoadable() Not required as this is done inside cudlaModuleLoadFromMemory() .
NvMediaDlaGetNumOfInputTensors() cudlaModuleGetAttributes() NvMediaDlaGetInputTensorDescriptor() cudlaModuleGetAttributes() NvMediaDlaGetNumOfOutputTensors() cudlaModuleGetAttributes() NvMediaDlaGetOutputTensorDescriptor() cudlaModuleGetAttributes() NvMediaDlaDataRegister() cudlaMemRegister() NvMediaDlaDataUnregister() cudlaMemUnregister() NvMediaDlaLoadLoadable() cudlaModuleLoadFromMemory() NvMediaDlaRemoveLoadable() cudlaModuleUnload() NvMediaDlaSubmit() cudlaSubmitTask() NvMediaDlaNvSciSyncGetVersion() Not available NvMediaDlaFillNvSciSyncAttrList() cudlaGetNvSciSyncAttributes() NvMediaDlaRegisterNvSciSyncObj() cudlaImportExternalSemaphore() NvMediaDlaUnregisterNvSciSyncObj() cudlaMemUnregister() NvMediaDlaSetNvSciSyncObjforEOF() Not required as cudlaTask structure has the required capability to specify this.
NvMediaDlaInsertPreNvSciSyncFence() Not required as cudlaTask structure has the required capability to specify this.
NvMediaDlaGetEOFNvSciSyncFence() Not required as cudlaTask structure has the required capability to retrieve this. 7.3. Profiling a cuDLA App  cuDLA APIs can be profiled using NVIDIA Nsight Systems.
cuDLA Release Notes  Known Issues in cuDLA 1.2.1: In hybrid mode, cuDLA internally allocates memory with CUDA using the primary context.
As a result, before destroying/resetting a CUDA primary context, it is mandatory that all cuDLA device initializations are destroyed.
Before destroying a cuDLA device handle, it is important to ensure that all tasks submitted previously to the device are completed.
Failure to do so can lead to application crashes as the internal memory allocations would still be in use.
NvSciBuf buffer allocations made by the application must adhere to DLA alignment constraints.
It is the application’s responsibility to ensure that there are no duplicate fences specified as part of wait events while submitting tasks.
In general, any synchronous or asynchronous error returned by cuDLA APIs must be treated as a non-recoverable error.
In this case, the application is expected to restart and initialize cuDLA again in order to submit DLA tasks.
The exception to this rule is cudlaErrorMemoryRegistered which is returned by cuDLA when the application tries to register a particular memory again without unregistering.
Applications are expected to detect these scenarios and respond accordingly. 8. Notices  8.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 8.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 8.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Introduction v12.5 | PDF | Archive Incomplete-LU and Cholesky Preconditioned Iterative Methods Using cuSPARSE and cuBLAS White paper describing how to use the cuSPARSE and cuBLAS libraries to achieve a 2x speedup over CPU in the incomplete-LU and Cholesky preconditioned iterative methods.
Introduction  The solution of large sparse linear systems is an important problem in computational mechanics, atmospheric modeling, geophysics, biology, circuit simulation and many other applications in the field of computational science and engineering.
In general, these linear systems can be solved using direct or preconditioned iterative methods.
Although the direct methods are often more reliable, they usually have large memory requirements and do not scale well on massively parallel computer platforms.
The iterative methods are more amenable to parallelism and therefore can be used to solve larger problems.
Currently, the most popular iterative schemes belong to the Krylov subspace family of methods.
They include Bi-Conjugate Gradient Stabilized (BiCGStab) and Conjugate Gradient (CG) iterative methods for nonsymmetric and symmetric positive definite (s.p.d.) linear systems, respectively [2] , [11] .
In practice, we often use a variety of preconditioning techniques to improve the convergence of the iterative methods.
In this white paper we focus on the incomplete-LU and Cholesky preconditioning [11] , which is one of the most popular of these preconditioning techniques.
It computes an incomplete factorization of the coefficient matrix and requires a solution of lower and upper triangular linear systems in every iteration of the iterative method.
In order to implement the preconditioned BiCGStab and CG we use the sparse matrix-vector multiplication [3] , [15] and the sparse triangular solve [8] , [16] implemented in the cuSPARSE library.
We point out that the underlying implementation of these algorithms takes advantage of the CUDA parallel programming paradigm [5] , [9] , [13] , which allows us to explore the computational resources of the graphical processing unit (GPU).
In our numerical experiments the incomplete factorization is performed on the CPU (host) and the resulting lower and upper triangular factors are then transferred to the GPU (device) memory before starting the iterative method.
However, the computation of the incomplete factorization could also be accelerated on the GPU.
We point out that the parallelism available in these iterative methods depends highly on the sparsity pattern of the coefficient matrix at hand.
In our numerical experiments the incomplete-LU and Cholesky preconditioned iterative methods achieve on average more than 2x speedup using the cuSPARSE and cuBLAS libraries on the GPU over the MKL [17] implementation on the CPU.
For example, the speedup for the preconditioned iterative methods with the incomplete-LU and Cholesky factorization with 0 fill-in (ilu0) is shown in Figure 1 for matrices resulting from a variety of applications.
Iterative Methods  In the next sections we briefly describe the methods of interest and comment on the role played in them by the parallel sparse matrix-vector multiplication and triangular solve algorithms. 2. Preconditioned Iterative Methods  Let us consider the linear system \(A\mathbf{x} = \mathbf{f}\) where \(A \in \mathbb{R}^{n \times n}\) is a nonsingular coefficient matrix and \(\mathbf{x},\mathbf{f} \in \mathbb{R}^{n}\) are the solution and right-hand-side vectors.
In general, the iterative methods start with an initial guess and perform a series of steps that find more accurate approximations to the solution.
There are two types of iterative methods: (i) the stationary iterative methods, such as the splitting-based Jacobi and Gauss-Seidel (GS), and (ii) the nonstationary iterative methods, such as the Krylov subspace family of methods, which includes CG and BiCGStab .
The convergence of the iterative methods depends highly on the spectrum of the coefficient matrix and can be significantly improved using preconditioning.
The preconditioning modifies the spectrum of the coefficient matrix of the linear system in order to reduce the number of iterative steps required for convergence.
It often involves finding a preconditioning matrix \(M\) , such that \(M^{- 1}\) is a good approximation of \(A^{- 1}\) and the systems with \(M\) are relatively easy to solve.
matrix \(A\) we can let \(M\) be its incomplete-Cholesky factorization, so that \(A \approx M = {\widetilde{R}}^{T}\widetilde{R}\) , where \(\widetilde{R}\) is an upper triangular matrix.
Let us assume that \(M\) is nonsingular, then \({\widetilde{R}}^{- T}A{\widetilde{R}}^{- 1}\) is s.p.d.
The corresponding CG code using the cuSPARSE and cuBLAS libraries in C programming language is shown below.
The appropriate memory has been allocated and set to zero. 3. The matrix A (valA, csrRowPtrA, csrColIndA) and the incomplete- Cholesky upper triangular factor R (valR, csrRowPtrR, csrColIndR) have been computed and are present in the device (GPU) memory.
*/  create the info and analyse the lower and upper triangular factors cusparseCreateSolveAnalysisInfo ( & inforRt ); cusparseCreateSolveAnalysisInfo ( & inforR ); cusparseDcsrsv_analysis ( handle , CUSPARSE_OPERATION_TRANSPOSE , n , descrR , valR , csrRowPtrR , csrColIndR , inforRt ); cusparseDcsrsv_analysis ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , descrR , valR , csrRowPtrR , csrColIndR , inforR );  1: compute initial residual r = f - A x0 (using initial guess in x) cusparseDcsrmv ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , n , 1.0 , descrA , valA , csrRowPtrA , csrColIndA , x , 0.0 , r ); cublasDscal ( n , -1.0 , r , 1 ); cublasDaxpy ( n , 1.0 , f , 1 , r , 1 ); nrmr0 = cublasDnrm2 ( n , r , 1 );  2: repeat until convergence (based on max.
The corresponding BiCGStab code using the cuSPARSE and cuBLAS libraries in C programming language is shown below.
The matrix A (valA, csrRowPtrA, csrColIndA) and the incomplete- LU lower L (valL, csrRowPtrL, csrColIndL) and upper U (valU, csrRowPtrU, csrColIndU) triangular factors have been computed and are present in the device (GPU) memory.
The sparse matrix-vector multiplication has already been extensively studied in the following references [3] , [15] .
The sparse triangular solve is not as well known, so we briefly point out the strategy used to explore parallelism in it and refer the reader to the NVIDIA technical report [8] for further details.
The Splitting of Total Time Taken on the GPU by the Preconditioned Iterative Method  To understand the main ideas behind the sparse triangular solve, notice that although the forward and back substitution is an inherently sequential algorithm for dense triangular systems, the dependencies on the previously obtained elements of the solution do not necessarily exist for the sparse triangular systems.
We pursue the strategy that takes advantage of the lack of these dependencies and split the solution process into two phases as mentioned in [1] , [4] , [6] , [7] , [8] , [10] , [12] , [14] .
The analysis phase builds the data dependency graph that groups independent rows into levels based on the matrix sparsity pattern.
The solve phase iterates across the constructed levels one-by-one and computes all elements of the solution corresponding to the rows at a single level in parallel.
Notice that by construction the rows within each level are independent of each other, but are dependent on at least one row from the previous level.
The analysis phase needs to be performed only once and is usually significantly slower than the solve phase, which can be performed multiple times.
This arrangement is ideally suited for the incomplete-LU and Cholesky preconditioned iterative methods.
Numerical Experiments  In this section we study the performance of the incomplete-LU and Cholesky preconditioned BiCGStab and CG iterative methods.
We use twelve matrices selected from The University of Florida Sparse Matrix Collection [18] in our numerical experiments.
and five nonsymmetric matrices with the respective number of rows (m), columns (n=m) and non-zero elements (nnz) are grouped and shown according to their increasing order in Table 1 .
Symmetric Positive Definite (s.p.d.) and Nonsymmetric Test Matrices  # Matrix m,n nnz s.p.d.
Application 1 offshore 259,789 4,242,673 yes Geophysics 2 af_shell3 504,855 17,562,051 yes Mechanics 3 parabolic_fem 525,825 3,674,625 yes General 4 apache2 715,176 4,817,870 yes Mechanics 5 ecology2 999,999 4,995,991 yes Biology 6 thermal2 1,228,045 8,580,313 yes Thermal Simulation 7 G3_circuit 1,585,478 7,660,826 yes Circuit Simulation 8 FEM_3D_thermal2 147,900 3,489,300 no Mechanics 9 thermomech_dK 204,316 2,846,228 no Mechanics 10 ASIC_320ks 321,671 1,316,08511 no Circuit Simulation 11 cage13 445,315 7,479,343 no Biology 12 atmosmodd 1,270,432 8,814,880 no Atmospheric Model In the following experiments we use the hardware system with NVIDIA C2050 (ECC on) GPU and Intel Core i7 CPU 950 @ 3.07GHz, using the 64-bit Linux operating system Ubuntu 10.04 LTS, cuSPARSE library 4.0 and MKL 10.2.3.029.
The MKL_NUM_THREADS and MKL_DYNAMIC environment variables are left unset to allow MKL to use the optimal number of threads.
We compute the incomplete-LU and Cholesky factorizations using the MKL routines csrilu0 and csrilut with 0 and threshold fill-in, respectively.
In the csrilut routine we allow three different levels of fill-in denoted by (5,10 -3 ), (10,10 -5 ) and (20,10 -7 ).
In general, the \(\left( k,\mathit{tol}  ight)\) fill-in is based on \(nnz/n + k\) maximum allowed number of elements per row and the dropping of elements with magnitude \(\left| l_{ij} \middle| , \middle| u_{ij} \middle| < \mathit{tol} \times \left\| \mathbf{a}_{i}^{T}  ight\|_{2}  ight.\) , where \(l_{ij}\) , \(u_{ij}\) and \(\mathbf{a}_{i}^{T}\) are the elements of the lower \(L\) , upper \(U\) triangular factors and the i -th row of the coefficient matrix \(A\) , respectively.
We compare the implementation of the BiCGStab and CG iterative methods using the cuSPARSE and cuBLAS libraries on the GPU and MKL on the CPU.
In our experiments we let the initial guess be zero, the right-hand-side \(\mathbf{f} = A\mathbf{e}\) where \(\mathbf{e}^{T}{= (1,\ldots,1)}^{T}\) , and the stopping criteria be the maximum number of iterations 2000 or relative residual \(\left\| \mathbf{r}_{i}  ight\|_{2}/\left\| \mathbf{r}_{0}  ight\|_{2} < 10^{- 7}\) , where \(\mathbf{r}_{i} = \mathbf{f} - A\mathbf{x}_{i}\) is the residual at i -th iteration.
time(s) copy time(s) solve time(s) \(\frac{\left\| \mathbf{r}_{i}  ight\|_{2}}{\left\| \mathbf{r}_{0}  ight\|_{2}}\) # it.
solve time(s) \(\frac{\left\| \mathbf{r}_{i}  ight\|_{2}}{\left\| \mathbf{r}_{0}  ight\|_{2}}\) # it.
ilu0 1 0.38 0.02 0.72 8.83E-08 25 1.52 8.83E-08 25 0.57 2 1.62 0.04 38.5 1.00E-07 569 33.9 9.69E-08 571 1.13 3 0.13 0.01 39.2 9.84E-08 1044 6.91 9.84E-08 1044 5.59 4 0.12 0.01 35.0 9.97E-08 713 12.8 9.97E-08 713 2.72 5 0.09 0.01 107 9.98E-08 1746 55.3 9.98E-08 1746 1.92 6 0.40 0.02 155 9.96E-08 1656 54.4 9.79E-08 1656 2.83 7 0.16 0.02 20.2 8.70E-08 183 8.61 8.22E-08 183 2.32 8 0.32 0.02 0.13 5.25E-08 4 0.52 5.25E-08 4 0.53 9 0.20 0.01 72.7 1.96E-04 2000 40.0 2.08E-04 2000 1.80 10 0.11 0.01 0.27 6.33E-08 6 0.12 6.33E-08 6 1.59 11 0.70 0.03 0.28 2.52E-08 2.5 0.15 2.52E-08 2.5 1.10 12 0.25 0.04 12.5 7.33E-08 76.5 4.30 9.69E-08 74.5 2.79 Table 3.
csrilut (5,10 -3 ) Preconditioned CG and BiCGStab Methods  ilut(5,10 -3 ) CPU GPU Speedup # fact.
), achieved relative residual ( \(\frac{\left\| \mathbf{r}_{i}  ight\|_{2}}{\left\| \mathbf{r}_{0}  ight\|_{2}}\) ) and time in seconds taken by the factorization (fact.
), iterative solution of the linear system (solve), and cudaMemcpy of the lower and upper triangular factors to the GPU (copy).
We include the time taken to compute the incomplete-LU and Cholesky factorization as well as to transfer the triangular factors from the CPU to the GPU memory in the computed speedup.
csrilut (10,10 -5 ) Preconditioned CG and BiCGStab Methods  ilut(10,10 -5 ) CPU GPU Speedup # fact.
ilu0 1 0.15 0.01 1.06 8.79E-08 34 1.96 8.79E-08 34 0.57 0.63 2 0.52 0.03 60.0 9.86E-08 748 38.7 9.86E-08 748 1.54 1.70 3 3.89 0.03 9.02 9.79E-08 147 5.42 9.78E-08 147 1.38 1.83 4 1.09 0.03 34.5 9.83E-08 454 38.2 9.83E-08 454 0.91 2.76 5 3.25 0.06 26.3 9.71E-08 272 55.2 9.71E-08 272 0.51 0.53 6 11.0 0.07 44.7 9.42E-08 263 84.0 9.44E-08 263 0.59 1.02 7 5.95 0.09 8.84 8.53E-08 43 17.0 8.53E-08 43 0.64 1.68 8 2.94 0.04 0.09 2.10E-08 1.5 1.75 2.10E-08 1.5 0.64 3.54 9 0.11 0.01 53.2 4.24E-03 2000 24.4 4.92E-03 2000 2.18 1.31 10 0.12 0.01 0.16 4.89E-11 4 0.08 6.45E-11 4 1.36 1.18 11 2.89 0.09 0.44 6.10E-09 2.5 0.48 6.10E-09 2.5 1.00 33.2 12 0.36 0.03 36.6 7.05E-08 278.5 10.6 8.82E-08 270.5 3.35 8.04 Table 5.
csrilut (20,10 -7 ) Preconditioned CG and BiCGStab Methods  ilut(20,10 -7 ) CPU GPU Speedup # fact.
Notice that in general in our numerical experiments the performance for the incomplete factorizations decreases as the threshold parameters are relaxed and the factorization becomes more dense, thus inhibiting parallelism due to data dependencies between rows in the sparse triangular solve.
For this reason, the best performance on the GPU is obtained for the incomplete-LU and Cholesky factorization with 0 fill-in, which will be our point of reference.
Performance of BiCGStab and CG with Incomplete-LU Cholesky Preconditioning  Although the incomplete factorizations with a more relaxed threshold are often closer to the exact factorization and thus result in fewer iterative steps, they are also much more expensive to compute.
Moreover, notice that even though the number of iterative steps decreases, each step is more computationally expensive.
As a result of these tradeoffs the total time, the sum of the time taken by the factorization and the iterative solve, for the iterative method does not necessarily decrease with a more relaxed threshold in our numerical experiments.
The speedup based on the total time taken by the preconditioned iterative method on the GPU with csrilu0 preconditioner and CPU with all four preconditioners is shown in Figure 4 .
Notice that for majority of matrices in our numerical experiments the implementation of the iterative method using the cuSPARSE and cuBLAS libraries does indeed outperform the MKL.
CPU (with all)  Finally, the average of the obtained speedups is shown in Figure 5 , where we have excluded the runs with cage13 matrix for ilut (10,10 -5 ) and runs with offshore and cage13 matrices for ilut (20,10 -7 ) incomplete factorizations because of their disproportional speedup.
Consequently, we can conclude that the incomplete-LU and Cholesky preconditioned BiCGStab and CG iterative methods obtain on average more than 2x speedup on the GPU over their CPU implementation.
Conclusion  The performance of the iterative methods depends highly on the sparsity pattern of the coefficient matrix at hand.
In our numerical experiments the incomplete-LU and Cholesky preconditioned BiCGStab and CG iterative methods implemented on the GPU using the cuSPARSE and cuBLAS libraries achieved an average of 2x speedup over their MKL implementation.
The sparse matrix-vector multiplication and triangular solve, which is split into a slower analysis phase that needs to be performed only once and a faster solve phase that can be performed multiple times, were the essential building blocks of these iterative methods.
In fact the obtained speedup was usually mostly influenced by the time taken by the solve phase of the algorithm.
Finally, we point out that the use of multiple-right-hand-sides would increase the available parallelism and can result in a significant relative performance improvement in the preconditioned iterative methods.
Also, the development of incomplete-LU and Cholesky factorizations using CUDA parallel programming paradigm can further improve the obtained speedup. 5. Acknowledgements  This white paper was authored by Maxim Naumov for NVIDIA Corporation.
Permission to make digital or hard copies of all or part of this work for any use is granted without fee provided that copies bear this notice and the full citation on the first page. 6. References  [1] E.
van der Vorst, Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods, SIAM, Philadelphia, PA, 1994.
Garland, Implementing Sparse Matrix-Vector Multiplication on Throughput-Oriented Processors, Proc.
Greenbaum, Solving Sparse Triangular Linear Systems using Fortran with Parallel Extensions on the NYU Ultracomputer Prototype, Report 99, NYU Ultracomputer Note, New York University, NY, April, 1986.
Mayer, Parallel Algorithms for Solving Linear Systems with Sparse Triangular Matrices, Computing, pp. 291-312 (86), 2009. [7] R.
Baxter, Run-Time Parallelization and Scheduling of Loops, IEEE Transactions on Computers, pp. (40), 1991. [8] M.
Naumov, Parallel Solution of Sparse Triangular Linear Systems in the Preconditioned Iterative Methods on the GPU, NVIDIA Technical Report, NVR-2011-001, 2011.
Gupta, Parallel ICCG on a Hierarchical Memory Multiprocessor - Addressing the Triangular Solve Bottleneck, Parallel Comput., pp. 719-741 (18), 1992. [11] Y.
Saltz, Aggregation Methods for Solving Sparse Triangular Systems on Multiprocessors, SIAM J.
Kandrot, CUDA by Example: An Introduction to General-Purpose GPU Programming, Addison-Wesley, 2010.
Demmel, Optimization of Sparse Matrix-Vector Multiplication on Emerging Multicore Platforms, Parallel Comput., pp. 178-194 (35-3), 2009. [16] NVIDIA cuSPARSE and cuBLAS Libraries, http: www.nvidia.com/object/cuda_develop.html [17] Intel Math Kernel Library, http: software.intel.com/en-us/articles/intel-mkl [18] The University of Florida Sparse Matrix Collection, http: www.cise.ufl.edu/research/sparse/matrices/ .
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 7.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 7.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2011-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Introduction v12.5 | PDF | Archive Floating Point and IEEE 754 Compliance for NVIDIA GPUs White paper covering the most common issues related to NVIDIA GPUs.
A number of issues related to floating point accuracy and compliance are a frequent source of confusion on both CPUs and GPUs.
The purpose of this white paper is to discuss the most common issues related to NVIDIA GPUs and to supplement the documentation in the CUDA C++ Programming Guide.
Introduction  Since the widespread adoption in 1985 of the IEEE Standard for Binary Floating-Point Arithmetic (IEEE 754-1985 [1] ) virtually all mainstream computing systems have implemented the standard, including NVIDIA with the CUDA architecture.
It is important to consider many aspects of floating point behavior in order to achieve the highest performance with the precision required for any specific application.
This is especially true in a heterogeneous computing environment where operations will be performed on different types of hardware.
Understanding some of the intricacies of floating point and the specifics of how NVIDIA hardware handles floating point is obviously important to CUDA programmers striving to implement correct numerical algorithms.
In addition, users of libraries such as cuBLAS and cuFFT will also find it informative to learn how NVIDIA handles floating point under the hood.
We also discuss the fused multiply-add operator, which was added to the IEEE 754 standard in 2008 [2] and is built into the hardware of NVIDIA GPUs.
In Chapter 3 we work through an example of computing the dot product of two short vectors to illustrate how different choices of implementation affect the accuracy of the final result.
In Chapter 4 we describe NVIDIA hardware versions and NVCC compiler options that affect floating point calculations.
Finally, in Chapter 6 we conclude with concrete recommendations to programmers that deal with numeric issues relating to floating point on the GPU. 2. Floating Point  2.1.
Formats  Floating point encodings and functionality are defined in the IEEE 754 Standard [2] last revised in 2008.
Goldberg [5] gives a good introduction to floating point and many of the issues that arise.
The standard mandates binary floating point data be encoded on three fields: a one bit sign field, followed by exponent bits encoding the exponent offset by a numeric bias specific to each format, and bits encoding the significand (or fraction).
In order to ensure consistent computations across platforms and to exchange floating point data, IEEE 754 defines basic and interchange formats.
The 32 and 64 bit basic binary floating point formats correspond to the C data types float and double .
Their corresponding representations have the following bit lengths: For numerical data representing finite values, the sign is either negative or positive, the exponent field encodes the exponent in base 2, and the fraction field encodes the significand without the most significant non-zero bit.
For example, the value -192 equals (-1) 1 x 2 7 x 1.5, and can be represented as having a negative sign, an exponent of 7, and a fractional part .5.
The exponents are biased by 127 and 1023, respectively, to allow exponents to extend from negative to positive.
Hence the exponent 7 is represented by bit strings with values 134 for float and 1030 for double.
Given that the fraction field uses a limited number of bits, not all real numbers can be represented exactly.
For example the mathematical value of the fraction 2/3 represented in binary is 0.10101010… which has an infinite number of bits after the binary point.
The value 2/3 must be rounded first in order to be represented as a floating point number with limited precision.
The most frequently used is the round-to-nearest-or-even mode (abbreviated as round-to-nearest).
The value 2/3 rounded in this mode is represented in binary as: The sign is positive and the stored exponent value represents an exponent of -1. 2.2. Operations and Accuracy  The IEEE 754 standard requires support for a handful of operations.
These include the arithmetic operations add, subtract, multiply, divide, square root, fused-multiply-add, remainder, conversion operations, scaling, sign operations, and comparisons.
The results of these operations are guaranteed to be the same for all implementations of the standard, for a given format and rounding mode.
The rules and properties of mathematical arithmetic do not hold directly for floating point arithmetic because of floating point’s limited precision.
For example, the table below shows single precision values A , B , and C , and the mathematical exact value of their sum computed using different associativity.
\(\begin{matrix} A & = & {2^{1} \times 1.00000000000000000000001} \\ B & = & {2^{0} \times 1.00000000000000000000001} \\ C & = & {2^{3} \times 1.00000000000000000000001} \\ {(A + B) + C} & = & {2^{3} \times 1.01100000000000000000001011} \\ {A + (B + C)} & = & {2^{3} \times 1.01100000000000000000001011} \\ \end{matrix}\) Mathematically, ( A + B ) + C does equal A + ( B + C ).
Performing these same computations in single precision floating point arithmetic in round-to-nearest mode according to IEEE 754, we obtain: \(\begin{matrix} {A + B} & = & {2^{1} \times 1.1000000000000000000000110000...} \\ {\text{rn}(A + B)} & = & {2^{1} \times 1.10000000000000000000010} \\ {B + C} & = & {2^{3} \times 1.0010000000000000000000100100...} \\ {\text{rn}(B + C)} & = & {2^{3} \times 1.00100000000000000000001} \\ {A + B + C} & = & {2^{3} \times 1.0110000000000000000000101100...} \\ {\text{rn}\left( \text{rn}(A + B) + C  ight)} & = & {2^{3} \times 1.01100000000000000000010} \\ {\text{rn}\left( A + \text{rn}(B + C)  ight)} & = & {2^{3} \times 1.01100000000000000000001} \\ \end{matrix}\) For reference, the exact, mathematical results are computed as well in the table above.
Not only are the results computed according to IEEE 754 different from the exact mathematical results, but also the results corresponding to the sum rn(rn(A + B) + C) and the sum rn(A + rn(B + C)) are different from each other.
In this case, rn(A + rn(B + C)) is closer to the correct mathematical result than rn(rn(A + B) + C).
This example highlights that seemingly identical computations can produce different results even if all basic operations are computed in compliance with IEEE 754.
These same results would be obtained using any microprocessor, CPU or GPU, which supports single precision floating point. 2.3. The Fused Multiply-Add (FMA)  In 2008 the IEEE 754 standard was revised to include the fused multiply-add operation ( FMA ).
Without the FMA operation the result would have to be computed as \(\text{rn}\left( \text{rn}(X \times Y) + Z  ight)\) with two rounding steps, one for multiply and one for add.
Let’s consider an example to illustrate how the FMA operation works using decimal arithmetic first for clarity.
Let’s compute \(x^{2} - 1\) with four digits of precision after the decimal point, or a total of five digits of precision including the leading digit before the decimal point.
For \(x = 1.0008\) , the correct mathematical result is \(x^{2} - 1 = 1.60064 \times 10^{- 4}\) .
The closest number using only four digits after the decimal point is \(1.6006 \times 10^{- 4}\) .
In this case \(\text{rn}\left( x^{2} - 1  ight) = 1.6006 \times 10^{- 4}\) which corresponds to the fused multiply-add operation \(\text{rn}\left( x \times x + ( - 1)  ight)\) .
The final result is \(\text{rn}\left( \text{rn}\left( x^{2}  ight) - 1  ight) = 1.6000 \times 10^{- 4}\) .
The corresponding FMA computation is wrong by only 0.00004, and its result is closest to the correct mathematical answer.
The alternative of computing the FMA \(\text{rn}(A \times A + B)\) provides a result equal to the mathematical value.
In general, the fused-multiply-add operation generates more accurate results than computing one multiply followed by one add.
The choice of whether or not to use the fused operation depends on whether the platform provides the operation and also on how the code is compiled.
Figure 1 shows CUDA C++ code and output corresponding to inputs A and B and operations from the example above.
The code is executed on two different hardware platforms: an x86-class CPU using SSE in single precision, and an NVIDIA GPU with compute capability 2.0.
At the time this paper is written (Spring 2011) there are no commercially available x86 CPUs which offer hardware FMA.
NVIDIA GPUs with compute capability 2.0 do offer hardware FMAs, so the result of executing this code will be the more accurate one by default.
The code fragment was compiled without any special intrinsics or compiler options for either platform.
Subtractive cancellation occurs during the addition of quantities of similar magnitude with opposite signs.
In this case many of the leading bits cancel, leaving fewer meaningful bits of precision in the result.
Thus even if subtractive cancellation occurs during the addition there are still enough valid bits remaining in the product to get a precise result with no loss of precision. 3. Dot Product: An Accuracy Example  Consider the problem of finding the dot product of two short vectors \(\overset{ ightarrow}{a}\) and \(\overset{ ightarrow}{b}\) , both with four elements.
\(\overset{ ightharpoonup}{a} = \begin{bmatrix} a_{1} \\ a_{2} \\ a_{3} \\ a_{4} \\ \end{bmatrix}\mspace{2mu}\quad\overset{ ightharpoonup}{b} = \begin{bmatrix} b_{1} \\ b_{2} \\ b_{3} \\ b_{4} \\ \end{bmatrix}\quad\overset{ ightharpoonup}{a} \cdot \overset{ ightharpoonup}{b} = a_{1}b_{1} + a_{2}b_{2} + a_{3}b_{3} + a_{4}b_{4}\) This operation is easy to write mathematically, but its implementation in software involves several choices.
All of the strategies we will discuss use purely IEEE 754 compliant operations. 3.1. Example Algorithms  We present three algorithms which differ in how the multiplications, additions, and possibly fused multiply-adds are organized.
Individual operation are shown as a circle with arrows pointing from arguments to operations.
 The serial method uses a simple loop with separate multiplies and adds to compute the do t product of the vectors.
The final result can be represented as (a 1 x b 1 ) + (a 2 x b 2 )) + (a 3 x b 3 )) + (a 4 x b 4 )).
 The FMA method uses a simple loop with fused multiply-adds to compute the dot product of the vectors.
The final result can be represented as a 4 x b 4 = (a 3 x b 3 + (a 2 x b 2 + (a 1 x b 1 + 0))).
A simple improvement to the algorithm is to use the fused multiply-add to do the multiply and addition in one step to improve accuracy.
Yet another way to compute the dot product is to use a divide-and-conquer strategy in which we first find the dot products of the first half and the second half of the vectors, then combine these results using addition.
This is a recursive strategy; the base case is the dot product of vectors of length 1 which is a single multiply.
We call this algorithm the parallel algorithm because the two sub-problems can be computed in parallel as they have no dependencies.
The algorithm does not require a parallel implementation, however; it can still be implemented with a single thread. 3.2. Comparison  All three algorithms for computing a dot product use IEEE 754 arithmetic and can be implemented on any system that supports the IEEE standard.
In fact, an implementation of the serial algorithm on multiple systems will give exactly the same result.
However, results computed by an implementation of the serial algorithm may differ from those computed by an implementation of the other two algorithms.
 The parallel method uses a tree to reduce all the products of individual elements into a final sum.
The final result can be represented as ((a 1 x b 1 ) + (a 2 x b 2 )) + ((a 3 x b 3 ) + (a 4 x b 4 )). 4. CUDA and Floating Point  NVIDIA has extended the capabilities of GPUs with each successive hardware generation.
Current generations of the NVIDIA architecture such as Tesla Kxx , GTX 8xx , and GTX 9xx , support both single and double precision with IEEE 754 precision and include hardware support for fused multiply-add in both single and double precision.
The runtime library supports a function call to determine the compute capability of a GPU at runtime; the CUDA C++ Programming Guide also includes a table of compute capabilities for many different devices [7] . 4.1. Compute Capability 2.0 and Above  Devices with compute capability 2.0 and above support both single and double precision IEEE 754 including fused multiply-add in both single and double precision.
Operations such as square root and division will result in the floating point value closest to the correct mathematical result in both single and double precision, by default. 4.2. Rounding Modes  The IEEE 754 standard defines four rounding modes: round-to-nearest, round towards positive, round towards negative, and round towards zero.
Compiler intrinsics like the ones listed in the tables below can be used to select other rounding modes for individual operations.
mode interpretation rn round to nearest, ties to even rz round towards zero ru round towards \(+ \text{∞}\) rd round towards \(- \text{∞}\) x + y __fadd_[rn | rz | ru | rd] (x, y) addition x * y __fmul_[rn | rz | ru | rd] (x, y) multiplication fmaf (x, y, z) __fmaf_[rn | rz | ru | rd] (x, y, z) FMA 1.0f / x __frcp_[rn | rz | ru | rd] (x) reciprocal x / y __fdiv_[rn | rz | ru | rd] (x, y) division sqrtf(x) __fsqrt_[rn | rz | ru | rd] (x) square root x + y __dadd_[rn | rz | ru | rd] (x, y) addition x * y __dmul_[rn | rz | ru | rd] (x, y) multiplication fma (x, y, z) __fma_[rn | rz | ru | rd] (x, y, z) FMA 1.0 / x __drcp_[rn | rz | ru | rd] (x) reciprocal x / y __ddiv_[rn | rz | ru | rd] (x, y) division sqrtf(x) __dsqrt_[rn | rz | ru | rd] (x) square root 4.3.
Controlling Fused Multiply-add  In general, the fused multiply-add operation is faster and more accurate than performing separate multiply and add operations.
However, on occasion you may wish to disable the merging of multiplies and adds into fused multiply-add instructions.
To inhibit this optimization one can write the multiplies and additions using intrinsics with explicit rounding mode as shown in the previous tables.
Operations written directly as intrinsics are guaranteed to remain independent and will not be merged into fused multiply-add instructions.
It is also possible to disable FMA merging via a compiler flag. 4.4. Compiler Flags  Compiler flags relevant to IEEE 754 operations are -ftz={true|false} , -prec-div={true|false} , and -prec-sqrt={true|false} .
These flags control single precision operations on devices of compute capability of 2.0 or later.
mode flags IEEE 754 mode (default) -ftz=false -prec-div=true -prec-sqrt=true fast mode -ftz=true -prec-div=false -prec-sqrt=false The default IEEE 754 mode means that single precision operations are correctly rounded and support denormals, as per the IEEE 754 standard.
In the fast mode denormal numbers are flushed to zero, and the operations division and square root are not computed to the nearest floating point value.
The flags have no effect on double precision or on devices of compute capability below 2.0. 4.5. Differences from x86  NVIDIA GPUs differ from the x86 architecture in that rounding modes are encoded within each floating point instruction instead of dynamically using a floating point control word.
On the GPU there is no status flag to indicate when calculations have overflowed, underflowed, or have involved inexact arithmetic.
Like SSE , the precision of each GPU operation is encoded in the instruction (for x87 the precision is controlled dynamically by the floating point control word). 5. Considerations for a Heterogeneous World  5.1.
Mathematical Function Accuracy  So far we have only considered simple math operations such as addition, multiplication, division, and square root.
These operations are simple enough that computing the best floating point result (e.g., the closest in round-to-nearest) is reasonable.
To guarantee the correctly rounded result, it is not generally enough to compute the function to a fixed high accuracy.
There might still be rare cases where the error in the high accuracy result affects the rounding step at the lower accuracy.
It is possible to solve the dilemma for particular functions by doing mathematical analysis and formal proofs [4] , but most math libraries choose instead to give up the guarantee of correct rounding.
Instead they provide implementations of math functions and document bounds on the relative error of the functions over the input range.
For example, the double precision sin function in CUDA is guaranteed to be accurate to within 2 units in the last place (ulp) of the correctly rounded result.
In other words, the difference between the computed result and the mathematical result is at most ±2 with respect to the least significant bit position of the fraction part of the floating point result.
We compiled the code sequence on a 64-bit x86 platform using gcc version 4.4.3 (Ubuntu 4.3.3-4ubuntu5).
This shows that the result of computing cos(5992555.0) using a common library differs depending on whether the code is compiled in 32-bit mode or 64-bit mode.
The consequence is that different math libraries cannot be expected to compute exactly the same result for a given input.
Functions compiled for the GPU will use the NVIDIA CUDA math library implementation while functions compiled for the CPU will use the host compiler math library implementation (e.g., glibc on Linux).
Because these implementations are independent and neither is guaranteed to be correctly rounded, the results will often differ slightly. 5.2. x87 and SSE  One of the unfortunate realities of C compilers is that they are often poor at preserving IEEE 754 semantics of floating point operations [6] .
Just like CUDA operations, SSE operations are performed on single or double precision values, while x87 operations often use an additional internal 80-bit precision format.
Sometimes the results of a computation using x87 can depend on whether an intermediate result was allocated to a register or stored to memory.
Values stored to memory are rounded to the declared precision (e.g., single precision for float and double precision for double ).
Also, x87 instructions will often be used by default for 32-bit compiles but SSE instructions will be used by default for 64-bit compiles.
Because of these issues, guaranteeing a specific precision level on the CPU can sometimes be tricky.
When comparing CPU results to results computed on the GPU, it is generally best to compare using SSE instructions.
On 32-bit x86 targets without SSE it can be helpful to declare variables using volatile and force floating point values to be stored to memory ( /Op in Visual Studio and -ffloat-store in gcc ).
This moves results from extended precision registers into memory, where the precision is precisely single or double precision.
Alternately, the x87 control word can be updated to set the precision to 24 or 53 bits using the assembly instruction fldcw or a compiler option such as -mpc32 or -mpc64 in gcc . 5.3. Core Counts  As we have shown in Section 3 , the final values computed using IEEE 754 arithmetic can depend on implementation choices such as whether to use fused multiply-add or whether additions are organized in series or parallel.
One way such differences can arise is from differences between the number of concurrent threads involved in a computation.
On the GPU, a common design pattern is to have all threads in a block coordinate to do a parallel reduction on data within the block, followed by a serial reduction of the results from each block.
Changing the number of threads per block reorganizes the reduction; if the reduction is addition, then the change rearranges parentheses in the long string of additions.
Even if the same general strategy such as parallel reduction is used on the CPU and GPU, it is common to have widely different numbers of threads on the GPU compared to the CPU.
For example, the GPU implementation might launch blocks with 128 threads per block, while the CPU implementation might use 4 threads in total. 5.4. Verifying GPU Results  The same inputs will give the same results for individual IEEE 754 operations to a given precision on the CPU and GPU.
As we have explained, there are many reasons why the same sequence of operations may not be performed on the CPU and GPU.
Finally, many common mathematical functions are not required by the IEEE 754 standard to be correctly rounded so should not be expected to yield identical results between implementations.
When porting numeric code from the CPU to the GPU of course it makes sense to use the x86 CPU results as a reference.
Differences are not automatically evidence that the result computed by the GPU is wrong or that there is a problem on the GPU.
Computing results in a high precision and then comparing to results computed in a lower precision can be helpful to see if the lower precision is adequate for a particular application.
However, rounding high precision results to a lower precision is not equivalent to performing the entire computation in lower precision.
The results of the CPU may be computed to an unexpectedly high extended precision for some or all of the operations.
The GPU result will be computed using single or double precision only. 6. Concrete Recommendations  The key points we have covered are the following: Use the fused multiply-add operator.
The fused multiply-add operator on the GPU has high performance and increases the accuracy of computations.
Understand that a hardware fused multiply-add operation is not yet available on the CPU, which can cause differences in numerical results.
Even in the strict world of IEEE 754 operations, minor details such as organization of parentheses or thread counts can affect the final result.
Devices of compute capability 2.0 and later are capable of single and double precision arithmetic following the IEEE 754 standard, and have hardware units for performing fused multiply-add in both single and double precision.
The math library includes all the math functions listed in the C99 standard [3] plus some additional useful functions.
These functions have been tuned for a reasonable compromise between performance and accuracy.
Please let us know about any functions that you require that we do not provide, or if the accuracy or performance of any of our functions does not meet your needs.
Leave comments in the NVIDIA CUDA forum 1 or join the Registered Developer Program 2 and file a bug with your feedback. 7. Acknowledgements  This paper was authored by Nathan Whitehead and Alex Fit-Florea for NVIDIA Corporation.
Thanks to Ujval Kapasi, Kurt Wall, Paul Sidenblad, Massimiliano Fatica, Everett Phillips, Norbert Juffa, and Will Ramey for their helpful comments and suggestions.
Permission to make digital or hard copies of all or part of this work for any use is granted without fee provided that copies bear this notice and the full citation on the first page. 8. References  [1] ANSI/IEEE 754-1985.
[4] Catherine Daramy-Loirat, David Defour, Florent de Dinechin, Matthieu Gallet, Nicolas Gast, and Jean-Michel Muller.
CR-LIBM: A library of correctly rounded elementary functions in double-precision, February 2005.
Edited reprint available at: http: download.oracle.com/docs/cd/E19957-01/806-3568/ncg_goldberg.html .
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 9.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 9.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
1 https: forums.nvidia.com/index.php?showforum=62 2 https: developer.nvidia.com/ join-nvidia-registered-developer-program Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2011-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Overview v12.5 | PDF | Archive CUDA Binary Utilities The application notes for cuobjdump, nvdisasm, cu++filt, and nvprune.
Overview  This document introduces cuobjdump , nvdisasm , cu++filt and nvprune , four CUDA binary tools for Linux (x86, ARM and P9), Windows, Mac OS and Android. 1.1.  A CUDA binary (also referred to as cubin) file is an ELF-formatted file which consists of CUDA executable code sections as well as other sections containing symbols, relocators, debug info, etc.
By default, the CUDA compiler driver nvcc embeds cubin files into the host executable file.
Note For more details on cubin files or the CUDA compilation trajectory, refer to NVIDIA CUDA Compiler Driver NVCC .
Differences between cuobjdump and nvdisasm  CUDA provides two binary utilities for examining and disassembling cubin files and host executables: cuobjdump and nvdisasm .
Basically, cuobjdump accepts both cubin files and host binaries while nvdisasm only accepts cubin files; but nvdisasm provides richer output options.
Comparison of cuobjdump and nvdisasm  cuobjdump nvdisasm Disassemble cubin Yes Yes Extract ptx and extract and disassemble cubin from the following input files: Host binaries Executables Object files Static libraries External fatbinary files Yes No Control flow analysis and output No Yes Advanced display options No Yes 1.3.
Command Option Types and Notation  This section of the document provides common details about the command line options for the following tools: cuobjdump nvdisasm nvprune Each command-line option has a long name and a short name, which are interchangeable with each other.
These two variants are distinguished by the number of hyphens that must precede the option name, i.e.
long names must be preceded by two hyphens and short names must be preceded by a single hyphen.
Long options are intended for use in build scripts, where size of the option is less important than descriptive value and short options are intended for interactive use.
The tools mentioned above recognize three types of command options: boolean options, single value options and list options.
Boolean options do not have an argument, they are either specified on a command line or not.
Examples of each of these option types are, respectively: Boolean option : nvdisams --print-raw Single value : nvdisasm --binary SM70 List options : cuobjdump --function "foo,bar,foobar" Single value options and list options must have arguments, which must follow the name of the option by either one or more spaces or an equals character.
When a one-character short name such as -I , -l , and -L is used, the value of the option may also immediately follow the option itself without being seperated by spaces or an equal character.
The individual values of list options may be separated by commas in a single instance of the option or the option may be repeated, or any combination of these two cases.
Hence, for the two sample options mentioned above that may take values, the following notations are legal: -o file -o=file -Idir1,dir2 -I=dir3 -I dir4,dir5 For options taking a single value, if specified multiple times, the rightmost value in the command line will be considered for that option.
In the below example, test.bin binary will be disassembled assuming SM75 as the architecture.
nvdisasm.exe -b SM70 -b SM75 test.bin nvdisasm warning : incompatible redefinition for option 'binary', the last value of this option was used For options taking a list of values, if specified multiple times, the values get appended to the list.
In the below example, functions foo and bar are considered as valid values for option --function and the duplicate value foo is ignored.
cuobjdump  cuobjdump extracts information from CUDA binary files (both standalone and those embedded in host binaries) and presents them in human readable format.
The output of cuobjdump includes CUDA assembly code for each kernel, CUDA ELF section headers, string tables, relocators and other CUDA specific sections.
For a list of CUDA assembly instruction set of each GPU architecture, see Instruction Set Reference . 2.1. Usage  cuobjdump accepts a single input file each time it’s run.
Fatbin ptx code: = arch = sm_70 code version = [7,0] producer = cuda host = linux compile_size = 64bit compressed identifier = add.cu .version 7.0 .target sm_70 .address_size 64 .visible .entry _Z3addPiS_S_( .param .u64 _Z3addPiS_S__param_0, .param .u64 _Z3addPiS_S__param_1, .param .u64 _Z3addPiS_S__param_2 ) { .reg .s32 %r; .reg .s64 %rd; ld.param.u64 %rd1, [_Z3addPiS_S__param_0]; ld.param.u64 %rd2, [_Z3addPiS_S__param_1]; ld.param.u64 %rd3, [_Z3addPiS_S__param_2]; cvta.to.global.u64 %rd4, %rd3; cvta.to.global.u64 %rd5, %rd2; cvta.to.global.u64 %rd6, %rd1; ld.global.u32 %r1, [%rd6]; ld.global.u32 %r2, [%rd5]; add.s32 %r3, %r2, %r1; st.global.u32 [%rd4], %r3; ret; } As shown in the output, the a.out host binary contains cubin and ptx code for sm_70.
To dump common and per function resource usage information: $ cuobjdump test.cubin -res-usage Resource usage: Common: GLOBAL:56 CONSTANT[3]:28 Function calculate: REG:24 STACK:8 SHARED:0 LOCAL:0 CONSTANT[0]:472 CONSTANT[2]:24 TEXTURE:0 SURFACE:0 SAMPLER:0 Function mysurf_func: REG:38 STACK:8 SHARED:4 LOCAL:0 CONSTANT[0]:532 TEXTURE:8 SURFACE:7 SAMPLER:0 Function mytexsampler_func: REG:42 STACK:0 SHARED:0 LOCAL:0 CONSTANT[0]:472 TEXTURE:4 SURFACE:0 SAMPLER:1 Note that value for REG, TEXTURE, SURFACE and SAMPLER denotes the count and for other resources it denotes no.
of byte(s) used. 2.2. Command-line Options  Table 2 contains supported command-line options of cuobjdump , along with a description of what each option does.
cuobjdump Command-line Options  Option (long) Option (short) Description --all-fatbin -all Dump all fatbin sections.
By default will only dump contents of executable fatbin (if exists), else relocatable fatbin if no executable fatbin.
--dump-sass -sass Dump CUDA assembly for a single cubin file or all cubin files embedded in the binary.
-findex Specify symbol table index of the function whose fat binary structures must be dumped.
Allowed values for this option: sm_50 , sm_52 , sm_53 , sm_60 , sm_61 , sm_62 , sm_70 , sm_72 , sm_75 , sm_80 , sm_86 , sm_87 , sm_89 , sm_90 , sm_90a .
--version -V Print version information on this tool. 3. nvdisasm  nvdisasm extracts information from standalone cubin files and presents them in human readable format.
The output of nvdisasm includes CUDA assembly code for each kernel, listing of ELF data sections and other CUDA specific sections.
nvdisasm also does control flow analysis to annotate jump/branch targets and makes the output easier to read.
If this information is missing from the CUDA binary, either use the nvdisasm option -ndf to turn off control flow analysis, or use the ptxas and nvlink option -preserve-relocs to re-generate the cubin file. 3.1. Usage  nvdisasm accepts a single input file each time it’s run.
The basic usage is as following: nvdisasm [options] Here’s a sample output of nvdisasm : .headerflags @"EF_CUDA_TEXMODE_UNIFIED EF_CUDA_64BIT_ADDRESS EF_CUDA_SM70 EF_CUDA_VIRTUAL_SM(EF_CUDA_SM70)" .elftype @"ET_EXEC"  - .nv.info - .section .nv.info,"",@"SHT_CUDA_INFO" .align 4 .
The output of the control flow from nvdisasm can be directly imported to a DOT graph visualization tool such as Graphviz .
Here’s how you can generate a PNG image ( cfg.png ) of the control flow of the above cubin ( a.cubin ) with nvdisasm and Graphviz: nvdisasm -cfg a.cubin | dot -ocfg.png -Tpng Here’s the generated graph: Control Flow Graph  To generate a PNG image ( bbcfg.png ) of the basic block control flow of the above cubin ( a.cubin ) with nvdisasm and Graphviz: nvdisasm -bbcfg a.cubin | dot -obbcfg.png -Tpng Here’s the generated graph: Basic Block Control Flow Graph  nvdisasm is capable of showing the register (general and predicate) liveness range information.
For each line of CUDA assembly, nvdisasm displays whether a given device register was assigned, accessed, live or re-assigned.
This is useful if the user is interested in the life range of any particular register, or register usage in general.
In absence of any function inlining the output is same as the one with nvdisasm -g command.
Command-line Options  Table 3 contains the supported command-line options of nvdisasm , along with a description of what each option does.
nvdisasm Command-line Options  Option (long) Option (short) Description --base-address -base Specify the logical base address of the image to disassemble.
This option is only valid when disassembling a raw instruction binary (see option --binary ), and is ignored when disassembling an Elf file.
--binary -b When this option is specified, the input file is assumed to contain a raw instruction binary, that is, a sequence of binary instruction encodings as they occur in instruction memory.
Allowed values for this option: SM50 , SM52 , SM53 , SM60 , SM61 , SM62 , SM70 , SM72 , SM75 , SM80 , SM86 , SM87 , SM89 , SM90 , SM90a .
-fun Restrict the output to the CUDA functions represented by symbols with the given indices.
--life-range-mode -lrm This option implies option --print-life-ranges , and determines how register live range info should be printed.
count : Not at all, leaving only the # column (number of live registers); wide : Columns spaced out for readability (default); narrow : A one-character column for each register, economizing on table width Allowed values for this option: count , narrow , wide .
Dataflow analysis is normally enabled to perform branch stack analysis and annotate all instructions that jump via the GPU branch stack with inferred branch target labels.
However, it may occasionally fail when certain restrictions on the input nvelf/cubin are not met.
--no-vliw -novliw Conventional mode; disassemble paired instructions in normal syntax, instead of VLIW syntax.
--output-control-flow-graph -cfg When specified output the control flow graph, where each node is a hyperblock, in a format consumable by graphviz tools (such as dot).
--output-control-flow-graph-with-basic-blocks -bbcfg When specified output the control flow graph, where each node is a basicblock, in a format consumable by graphviz tools (such as dot).
--print-instr-offsets-cfg -poff When specified, print instruction offsets in the control flow graph.
This should be used along with the option –output-control-flow-graph or –output-control-flow-graph-with-basic-blocks.
--print-instruction-encoding -hex When specified, print the encoding bytes after each disassembled operation.
--print-life-ranges -plr Print register life range information in a trailing column in the produced disassembly.
--print-line-info -g Annotate disassembly with source line information obtained from .debug_line section, if present.
--print-line-info-inline -gi Annotate disassembly with source line information obtained from .debug_line section along with function inlining info, if present.
--print-line-info-ptx -gp Annotate disassembly with source line information obtained from .nv_debug_line_sass section, if present.
--separate-functions -sf Separate the code corresponding with function symbols by some new lines to let them stand out in the printed disassembly. 4. Instruction Set Reference  This section contains instruction set reference for NVIDIA NVIDIA ® GPU architectures.
4.1. Maxwell and Pascal Instruction Set  The Maxwell (Compute Capability 5.x) and the Pascal (Compute Capability 6.x) architectures have the following instruction set format: (instruction) (destination) (source1), (source2) ...
Valid destination and source locations include: RX for registers SRX for special system-controlled registers PX for condition registers c[X][Y] for constant memory Table 4 lists valid instructions for the Maxwell and Pascal GPUs.
Volta Instruction Set  The Volta architecture (Compute Capability 7.x) has the following instruction set format: (instruction) (destination) (source1), (source2) ...
Valid destination and source locations include: RX for registers SRX for special system-controlled registers PX for predicate registers c[X][Y] for constant memory Table 5 lists valid instructions for the Volta GPUs.
Turing Instruction Set  The Turing architecture (Compute Capability 7.3 and 7.5) have the following instruction set format: (instruction) (destination) (source1), (source2) ...
Valid destination and source locations include: RX for registers URX for uniform registers SRX for special system-controlled registers PX for predicate registers c[X][Y] for constant memory Table 6 lists valid instructions for the Turing GPUs.
NVIDIA Ampere GPU and Ada Instruction Set  The NVIDIA Ampere GPU and Ada architectures (Compute Capability 8.0 and 8.6) have the following instruction set format: (instruction) (destination) (source1), (source2) ...
Valid destination and source locations include: RX for registers URX for uniform registers SRX for special system-controlled registers PX for predicate registers UPX for uniform predicate registers c[X][Y] for constant memory Table 7 lists valid instructions for the NVIDIA Ampere architecrture and Ada GPUs.
Hopper Instruction Set  The Hopper architecture (Compute Capability 9.0) has the following instruction set format: (instruction) (destination) (source1), (source2) ...
Valid destination and source locations include: RX for registers URX for uniform registers SRX for special system-controlled registers PX for predicate registers UPX for uniform predicate registers c[X][Y] for constant memory desc[URX][RY] for memory descriptors Table 8 lists valid instructions for the Hopper GPUs.
cu++filt  cu++filt decodes (demangles) low-level identifiers that have been mangled by CUDA C++ into user readable names.
For every input alphanumeric word, the output of cu++filt is either the demangled name if the name decodes to a CUDA C++ name, or the original name itself. 5.1. Usage  cu++filt accepts one or more alphanumeric words (consisting of letters, digits, underscores, dollars, or periods) and attepts to decipher them.
The basic usage is as following: cu++filt [options] To demangle an entire file, like a binary, pipe the contents of the file to cu++filt, such as in the following command: nm | cu++filt To demangle function names without printing their parameter types, use the following command : cu++filt -p To skip a leading underscore from mangled symbols, use the following command: cu++filt -_ Here’s a sample output of cu++filt : $ cu++filt _Z1fIiEbl bool f(long) As shown in the output, the symbol _Z1fIiEbl was successfully demangled.
To strip all types in the function signature and parameters, use the -p option: $ cu++filt -p _Z1fIiEbl f To skip a leading underscore from a mangled symbol, use the -_ option: $ cu++filt -_ __Z1fIiEbl bool f(long) To demangle an entire file, pipe the contents of the file to cu++filt: $ nm test.sm_70.cubin | cu++filt 0000000000000000 t hello(char *) 0000000000000070 t hello(char *)::display() 0000000000000000 T hello(int *) Symbols that cannot be demangled are printed back to stdout as is: $ cu++filt _ZD2 _ZD2 Multiple symbols can be demangled from the command line: $ cu++filt _ZN6Scope15Func1Enez _Z3fooIiPFYneEiEvv _ZD2 Scope1::Func1(__int128, long double, ...) void foo() _ZD2 5.2.
Command-line Options  Table 9 contains supported command-line options of cu++filt , along with a description of what each option does.
-p When demangling the name of a function, do not display the types of the function’s parameters.
-v Print the version information of this tool. 5.3. Library Availability  cu++filt is also available as a static library (libcufilt) that can be linked against an existing project.
The following interface describes it’s usage: char* __cu_demangle(const char *id, char *output_buffer, size_t *length, int *status) This interface can be found in the file “nv_decode.h” located in the SDK.
If output-buffer is NULL, memory will be malloc’d to store the demangled name and returned through the function return value.
length It is necessary to provide the size of the output buffer if the user is providing pre-allocated memory.
status *status is set to one of the following values: 0 - The demangling operation succeeded -1 - A memory allocation failure occurred -2 - Not a valid mangled id -3 - An input validation failure has occurred (one or more arguments are invalid) Return Value A pointer to the start of the NUL-terminated demangled name, or NULL if the demangling fails.
Example Usage #include #include #include "nv_decode.h" int main() { int status; const char *real_mangled_name="_ZN8clstmp01I5cls01E13clstmp01_mf01Ev"; const char *fake_mangled_name="B@d_iDentiFier"; char* realname = __cu_demangle(fake_mangled_name, 0, 0, &status); printf("fake_mangled_name:\t result => %s\t status => %d ", realname, status); free(realname); size_t size = sizeof(char)*1000; realname = (char*)malloc(size); __cu_demangle(real_mangled_name, realname, &size, &status); printf("real_mangled_name:\t result => %s\t status => %d ", realname, status); free(realname); return 0; } This prints: fake_mangled_name: result => (null) status => -2 real_mangled_name: result => clstmp01::clstmp01_mf01() status => 0 6.
nvprune  nvprune prunes host object files and libraries to only contain device code for the specified targets. 6.1. Usage  nvprune accepts a single input file each time it’s run, emitting a new output file.
The basic usage is as following: nvprune [options] -o The input file must be either a relocatable host object or static library (not a host executable), and the output file will be the same format.
For example, the following will prune libcublas_static.a to only contain sm_70 cubin rather than all the targets which normally exist: nvprune -arch sm_70 libcublas_static.a -o libcublas_static70.a Note that this means that libcublas_static70.a will not run on any other architecture, so should only be used when you are building for a single architecture. 6.2. Command-line Options  Table 10 contains supported command-line options of nvprune , along with a description of what each option does.
-arch Specify the name of the NVIDIA GPU architecture which will remain in the object or library.
--generate-code -gencode This option is same format as nvcc –generate-code option, and provides a way to specify multiple architectures which should remain in the object or library.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 7.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 7.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2013-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Preparing An Application For Profiling v12.5 | PDF | Archive Profiler User’s Guide The user manual for NVIDIA profiling tools for optimizing performance of CUDA applications.
Profiling Overview This document describes NVIDIA profiling tools that enable you to understand and optimize the performance of your CUDA, OpenACC or OpenMP applications.
The Visual Profiler is a graphical profiling tool that displays a timeline of your application’s CPU and GPU activity, and that includes an automated analysis engine to identify optimization opportunities.
The nvprof profiling tool enables you to collect and view profiling data from the command-line.
The NVIDIA Volta platform is the last architecture on which these tools are fully supported.
It is recommended to use next-generation tools NVIDIA Nsight Systems for GPU and CPU sampling and tracing and NVIDIA Nsight Compute for GPU kernel profiling.
Refer the Migrating to Nsight Tools from Visual Profiler and nvprof section for more details.
It corresponds to a single hardware counter value which is collected during kernel execution.
To see a list of all available events on a particular NVIDIA GPU, type nvprof --query-events .
A metric is a characteristic of an application that is calculated from one or more event values.
To see a list of all available metrics on a particular NVIDIA GPU, type nvprof --query-metrics .
Preparing An Application For Profiling  The CUDA profiling tools do not require any application changes to enable profiling; however, by making some simple modifications and additions, you can greatly increase the usability and effectiveness profiling.
This section describes these modifications and how they can improve your profiling results. 1.1. Focused Profiling  By default, the profiling tools collect profile data over the entire run of your application.
But, as explained below, you typically only want to profile the region(s) of your application containing some or all of the performance-critical code.
Limiting profiling to performance-critical regions reduces the amount of profile data that both you and the tools must process, and focuses attention on the code where optimization will result in the greatest performance gains.
There are several common situations where profiling a region of the application is helpful.
The application is a test harness that contains a CUDA implementation of all or part of your algorithm.
The test harness initializes the data, invokes the CUDA functions to perform the algorithm, and then checks the results for correctness.
Using a test harness is a common and productive way to quickly iterate and test algorithm changes.
When profiling, you want to collect profile data for the CUDA functions implementing the algorithm, but not for the test harness code that initializes the data or checks the results.
The application operates in phases, where a different set of algorithms is active in each phase.
When the performance of each phase of the application can be optimized independently of the others, you want to profile each phase separately to focus your optimization efforts.
The application contains algorithms that operate over a large number of iterations, but the performance of the algorithm does not vary significantly across those iterations.
To limit profiling to a region of your application, CUDA provides functions to start and stop profile data collection.
cudaProfilerStart() is used to start profiling and cudaProfilerStop() is used to stop profiling (using the CUDA driver API, you get the same functionality with cuProfilerStart() and cuProfilerStop() ).
To use these functions you must include cuda_profiler_api.h (or cudaProfiler.h for the driver API).
When using the start and stop functions, you also need to instruct the profiling tool to disable profiling at the start of the application.
For the Visual Profiler you use the Start execution with profiling enabled checkbox in the Settings View . 1.2. Marking Regions of CPU Activity  The Visual Profiler can collect a trace of the CUDA function calls made by your application.
The Visual Profiler shows these calls in the Timeline View , allowing you to see where each CPU thread in the application is invoking CUDA functions.
To understand what the application’s CPU threads are doing outside of CUDA function calls, you can use the NVIDIA Tools Extension API (NVTX).
When you add NVTX markers and ranges to your application, the Timeline View shows when your CPU threads are executing within those regions.
In summary mode, each range is shown with CUDA activities associated with that range. 1.3. Naming CPU and CUDA Resources  The Visual Profiler Timeline View shows default naming for CPU thread and GPU devices, context and streams.
Using custom names for these resources can improve understanding of the application behavior, especially for CUDA applications that have many host threads, devices, contexts, or streams.
You can use the NVIDIA Tools Extension API to assign custom names for your CPU and GPU resources.
Thread names are displayed in summary mode. 1.4. Flush Profile Data  To reduce profiling overhead, the profiling tools collect and record profile information into internal buffers.
These buffers are then flushed asynchronously to disk with low priority to avoid perturbing application behavior.
To avoid losing profile information that has not yet been flushed, the application being profiled should make sure, before exiting, that all GPU work is done (using CUDA synchronization calls), and then call cudaProfilerStop() or cuProfilerStop() .
If your CUDA application includes graphics that operate using a display or main loop, care must be taken to call cudaProfilerStop() or cuProfilerStop() before the thread executing that loop calls exit() .
Failure to call one of these APIs may result in the loss of some or all of the collected profile data.
For some graphics applications like the ones use OpenGL, the application exits when the escape key is pressed.
In those cases where calling the above functions before exit is not feasible, use nvprof option --timeout or set the “Execution timeout” in the Visual Profiler.
The profiler will force a data flush just before the timeout. 1.5. Profiling CUDA Fortran Applications  CUDA Fortran applications compiled with the PGI CUDA Fortran compiler can be profiled by nvprof and the Visual Profiler.
In cases where the profiler needs source file and line information (kernel profile analysis, global memory access pattern analysis, divergent execution analysis, etc.
This option is supported on Linux 64-bit targets in PGI 2019 version 19.1 or later. 2. ​Visual Profiler  The NVIDIA Visual Profiler allows you to visualize and optimize the performance of your application.
The Visual Profiler displays a timeline of your application’s activity on both the CPU and GPU so that you can identify opportunities for performance improvement.
In addition, the Visual Profiler will analyze your application to detect potential performance bottlenecks and direct you on how to take action to eliminate or reduce those bottlenecks.
The Visual Profiler is available as both a standalone application and as part of Nsight Eclipse Edition.
The standalone version of the Visual Profiler, nvvp , is included in the CUDA Toolkit for all supported OSes except for macOS.
Starting with the CUDA 11.0, Visual Profiler and nvprof don’t support macOS as the target platform.
Visual Profiler is provided in a separate installer package to maintain the remote profiling workflow for CUDA developers on macOS.
Within Nsight Eclipse Edition, the Visual Profiler is located in the Profile Perspective and is activated when an application is run in profile mode. 2.1. Getting Started  This section describes steps you might take as you begin profiling.
2.1.1. Setting up Java Runtime Environment  Visual Profiler requires Java Runtime Environment (JRE) 1.8 to be available on the local system.
However, starting with CUDA Toolkit version 10.1 Update 2, the JRE is no longer included in the CUDA Toolkit due to Oracle upgrade licensing changes.
To run Visual Profiler on OpenSUSE15 or SLES15: Make sure that you invoke Visual Profiler with the command-line option included as shown below: nvvp -vm /usr/lib64/jvm/jre-1.8.0/bin/java Note The -vm option is only required when JRE 1.8 is not in the default path.
To run Visual Profiler on Ubuntu 18.04 or Ubuntu 18.10: Make sure that you invoke Visual Profiler with the command-line option included as shown below: nvvp -vm /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java Note The -vm option is only required when JRE 1.8 is not in the default path.
On Ubuntu 18.10, if you get error “ no swt-pi-gtk in java.library.path ” when running Visual Profiler, then you need to install GTK2.
apt-get install libgtk2.0-0 To run Visual Profiler on Fedora 29: Make sure that you invoke Visual Profiler with the command-line option included as shown below: nvvp -vm /usr/bin/java Note The -vm option is only required when JRE 1.8 is not in the default path.
To run Visual Profiler on macOS: Visual Profiler requires Java Runtime Environment (JRE) 1.8 update 151.
Make sure that you invoke Visual Profiler with the command-line option included as shown below: nvvp -vm /usr/bin/java Note The -vm option is only required when JRE 1.8 update 151 is not in the default path.
To run Visual Profiler on Windows: Make sure that you invoke Visual Profiler with the command-line option included as shown below: nvvp -vm "C:\Program Files\Java\jdk1.8.0_77\jre\bin\java" Note The -vm option is only required when JRE 1.8 is not in the default path. 2.1.2. Installing JRE  Visual Profiler require Java Runtime Environment (JRE) 1.8 to be available on the local system.
However, as of CUDA Toolkit version 10.1 Update 2, the JRE is no longer included in the CUDA Toolkit due to Oracle upgrade licensing changes.
Windows Oracle JRE 1.8 (may require paid updates) OpenJDK JRE 1.8 Linux Oracle JRE 1.8 (may require paid updates) OpenJDK JRE 1.8 Mac Oracle JRE 1.8 (may require paid updates) Note JRE 1.8u152 or later is not supported for Visual Profiler.
You can find the JRE update 151 on the Oracle Download Archive site here: https: www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html?printOnly=1 . 2.1.3. Modify Your Application For Profiling  The Visual Profiler does not require any application changes; however, by making some simple modifications and additions, you can greatly increase its usability and effectiveness.
Section Preparing An Application For Profiling describes how you can focus your profiling efforts and add extra annotations to your application that will greatly improve your profiling experience. 2.1.4. Creating a Session  The first step in using the Visual Profiler to profile your application is to create a new profiling session.
You can create a new session by selecting the Profile An Application link on the Welcome page, or by selecting New Session from the File menu.
Optionally, you can also specify the working directory, arguments, multi-process profiling option and environment.
The muti-process profiling options are: Profile child processes - If selected, profile all processes launched by the specified application.
Profile all processes - If selected, profile every CUDA process launched on the same system by the same user who launched nvprof.
In this mode the Visual Profiler will launch nvprof and user needs to run his application in another terminal outside the Visual Profiler.
User can exit this mode by pressing “Cancel” button on progress dialog in Visual Profiler to load the profile data Profile current process only - If selected, only profile specified application.
CUDA options: Start execution with profiling enabled - If selected profile data is collected from the start of application execution.
If not selected profile data is not collected until cudaProfilerStart() is called in the application.
Enable concurrent kernel profiling - This option should be selected for an application that uses CUDA streams to launch kernels that can execute concurrently.
If the application uses only a single stream (and therefore cannot have concurrent kernel execution), deselecting this option may decrease profiling overhead.
Enable CUDA API tracing in the timeline - If selected, the CUDA driver and runtime API call trace is collected and displayed on timeline.
Enable power, clock, and thermal profiling - If selected, power, clock, and thermal conditions on the GPUs will be sampled and displayed on the timeline.
Enable unified memory profiling - If selected for the GPU that supports Unified Memory, the Unified Memory related memory traffic to and from each GPU is collected on your system and displayed on timeline.
Replay application to collect events and metrics - If selected, the whole application is re-run instead of replaying each kernel, in order to collect all events/metrics.
Run guided analysis - If selected, the guided analysis is run immediately after the creation of a new session.
CPU (host) options: Profile execution on the CPU - If selected the CPU threads are sampled and data collected about the CPU performance is shown in the CPU Details View .
Enable OpenACC profiling - If selected and an OpenACC application is profiled, OpenACC activities will be recorded and displayed on a new OpenACC timeline.
Enable CPU thread tracing - If enabled, selected CPU thread API calls will be recorded and displayed on a new thread API timeline.
For performance reasons, only those API calls that influence concurrent execution are recorded and collection of this data is not supported on Windows.
This option should be selected for dependency analysis of applications with multiple CPU threads using CUDA.
Timeline Options: Load data for time range - If selected the start and end time stamps for the range of data to be loaded can be specified.
If a timeline is un-checked, the data associated with that timeline will not be loaded and it will not be displayed.
Note If some timelines are disabled by un-checking the option the analyses results which use this timeline data will be incorrect.
Press Finish. 2.1.5. Analyzing Your Application  If the Don’t run guided analysis option was not selected when you created your session, the Visual Profiler will immediately run your application to collect the data needed for the first stage of guided analysis.
As described in the Analysis View section, you can use the guided analysis system to get recommendations on performance limiting behavior in your application. 2.1.6. Exploring the Timeline  In addition to the guided analysis results, you will see a timeline for your application showing the CPU and GPU activity that occurred as your application executed.
Read Timeline View and Properties View to learn how to explore the profiling information that is available in the timeline.
Navigating the Timeline describes how you can zoom and scroll the timeline to focus on specific areas of your application. 2.1.7. Looking at the Details  In addition to the results provided in the Analysis View , you can also look at the specific metric and event values collected as part of the analysis.
You can collect specific metric and event values that reveal how the kernels in your application are behaving.
You collect metrics and events as described in the GPU Details View section. 2.1.8. Improve Loading of Large Profiles  Some applications launch many tiny kernels, making them prone to very large (100s of megabytes or larger) output, even for application runs of only a few seconds.
The Visual Profiler needs roughly the same amount of memory as the size of the profile it is opening/importing.
The Java virtual machine may use a fraction of the main memory if no “max heap size” setting is specified.
So depending on the size of main memory, the Visual Profiler may fail to load some large files.
If the Visual Profiler fails to load a large profile, try setting the max heap size that JVM is allowed to use according to main memory size.
On macOS the nvvp.ini file is present in folder /Developer/{cuda_install_dir}/libnvvp/nvvp.app/Contents/MacOS/ .
The nvvp.ini configuration file looks like this: -startup plugins/org.eclipse.equinox.launcher_1.3.0.v20140415-2008.jar --launcher.library plugins/org.eclipse.equinox.launcher.gtk.linux.x86_64_1.1.200.v20140603-1326 -data @user.home/nvvp_workspace -vm ../jre/bin/java -vmargs -Dorg.eclipse.swt.browser.DefaultType=mozilla To force the JVM to use 3 gigabytes of memory, for example, add a new line with ‑Xmx3G after ‑vmargs .
For example, if your system has 24GB of system memory, and you happen to know that you won’t need to run any other memory-intensive applications at the same time as the Visual Profiler, so it’s okay for the profiler to take up the vast majority of that space.
So you might pick, say, 22GB as the maximum heap size, leaving a few gigabytes for the OS, GUI, and any other programs that might be running.
Some other nvvp.ini configuration settings can also be modified: Increase the default heap size (the one Java automatically starts up with) to, say, 2GB.
( -Xms ) Tell Java to run in 64-bit mode instead of the default 32-bit mode (only works on 64-bit systems); this is required if you want heap sizes >4GB.
( -d64 ) Enable Javas parallel garbage collection system, which helps both to decrease the required memory space for a given input size as well as to catch out of memory errors more gracefully.
( -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode ) Note: most installations require administrator/root-level access to modify this file.
The modified nvvp.ini file as per examples given above is as follows: -data @user.home/nvvp_workspace -vm ../jre/bin/java -d64 -vmargs -Xms2g -Xmx22g -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode -Dorg.eclipse.swt.browser.DefaultType=Mozilla For more details on JVM settings, consult the Java virtual machine manual.
In addition to this you can use timeline options Load data for time range and Enable timelines in the session mentioned in the Creating a Session section to limit the data which is loaded and displayed. 2.2. Sessions  A session contains the settings, data, and profiling results associated with your application.
Each session is saved in a separate file; so you can delete, move, copy, or share a session by simply deleting, moving, copying, or sharing the session file.
There are two types of sessions: an executable session that is associated with an application that is executed and profiled from within the Visual Profiler, and an import session that is created by importing data generated by nvprof . 2.2.1. Executable Session  You can create a new executable session for your application by selecting the Profile An Application link on the Welcome page, or by selecting New Session from the File menu.
Once a session is created, you can edit the session’s settings as described in the Settings View .
To analyze your application and to collect metric and event values, the Visual Profiler will execute your application multiple times.
To get accurate profiling results, it is important that your application conform to the requirements detailed in Application Requirements . 2.2.2. Import Session  You create an import session from the output of nvprof by using the Import… option in the File menu.
Selecting this option opens the import dialog which guides you through the import process.
Because an executable application is not associated with an import session, the Visual Profiler cannot execute the application to collect additional profile data.
Also, the GPU Details View will show any imported event and metrics values but new metrics and events cannot be selected and collected for the import session. 2.2.2.1. Import Single-Process nvprof Session  Using the import dialog you can select one or more nvprof data files for import into the new session.
You must have one nvprof data file that contains the timeline information for the session.
You can optionally enable other options such as --system-profiling on , but you should not collect any events or metrics as that will distort the timeline so that it is not representative of the applications true behavior.
You may optionally specify one or more event/metric data files that contain event and metric values for the application.
These data files should be collected by running nvprof with one or both of the --events and --metrics options.
To collect all the events and metrics that are needed for the analysis system, you can simply use the --analysis-metrics option along with the --kernels option to select the kernel(s) to collect events and metrics for.
If you are importing multiple nvprof output files into the session, it is important that your application conform to the requirements detailed in Application Requirements . 2.2.2.2. Import Multi-Process nvprof Session  Using the import wizard you can select multiple nvprof data files for import into the new multi-process session.
Select the Multiple Processes option in the Import nvprof Data dialog as shown in the figure below.
When importing timeline data from multiple processes you may not specify any event/metric data files for those processes.
Multi-processes profiling is only supported for timeline data. 2.2.2.3. Import Command-Line Profiler Session  Support for command-line profiler (using the environment variable COMPUTE_PROFILE) has been dropped, but CSV files generated using earlier versions can still be imported.
Using the import wizard you can select one or more command-line profiler generated CSV files for import into the new session.
When you import multiple CSV files, their contents are combined and displayed in a single timeline.
The command-line profiler CSV file must be generated with the gpustarttimestamp and streamid configuration parameters.
It is fine to include other configuration parameters, including events. 2.3. Application Requirements  To collect performance data about your application, the Visual Profiler must be able to execute your application repeatedly in a deterministic manner.
Due to software and hardware limitations, it is not possible to collect all the necessary profile data in a single execution of your application.
Each time your application is run, it must operate on the same data and perform the same kernel and memory copy invocations in the same order.
Specifically, For a device, the order of context creation must be the same each time the application executes.
For a multi-threaded application where each thread creates its own context(s), care must be taken to ensure that the order of those context creations is consistent across multiple runs.
For example, it may be necessary to create the contexts on a single thread and then pass the contexts to the other threads.
Alternatively, the NVIDIA Tools Extension API can be used to provide a custom name for each context.
As long as the same custom name is applied to the same context on each execution of the application, the Visual Profiler will be able to correctly associate those contexts across multiple runs.
For a context, the order of stream creation must be the same each time the application executes.
Alternatively, the NVIDIA Tools Extension API can be used to provide a custom name for each stream.
As long as the same custom name is applied to the same stream on each execution of the application, the Visual Profiler will be able to correctly associate those streams across multiple runs.
Within a stream, the order of kernel and memcpy invocations must be the same each time the application executes. 2.4. Visual Profiler Views  The Visual Profiler is organized into views.
Together, the views allow you to analyze and visualize the performance of your application.
This section describes each view and how you use it while profiling your application. 2.4.1. Timeline View  The Timeline View shows CPU and GPU activity that occurred while your application was being profiled.
Multiple timelines can be opened in the Visual Profiler at the same time in different tabs.
Along the top of the view is a horizontal ruler that shows elapsed time from the start of application profiling.
Along the left of the view is a vertical ruler that describes what is being shown for each horizontal row of the timeline, and that contains various controls for the timeline.
These controls are described in Timeline Controls The timeline view is composed of timeline rows.
Each row shows intervals that represent the start and end times of the activities that correspond to the type of the row.
For example, timeline rows representing kernels have intervals representing the start and end times of executions of that kernel.
These sub-rows are created dynamically as necessary depending on how much activity overlap there is.
The placement of intervals within certain sub-rows does not convey any particular meaning.
Intervals are just packed into sub-rows using a heuristic that attempts to minimize the number of needed sub-rows.
The types of timeline rows that are displayed in the Timeline View are: Process A timeline will contain a Process row for each application profiled.
Thread A timeline will contain a Thread row for each CPU thread in the profiled application that performed either a CUDA driver or CUDA runtime API call.
Runtime API A timeline will contain a Runtime API row for each CPU thread that performs a CUDA Runtime API call.
Driver API A timeline will contain a Driver API row for each CPU thread that performs a CUDA Driver API call.
OpenACC A timeline will contain one or multiple OpenACC rows for each CPU thread that calls OpenACC directives.
Within one timeline, OpenACC activities on rows further down are called from within activities on the rows above.
Each interval in the row represents how long the application spends in a given OpenMP region or state.
The application may be in multiple states at the same time, this is shown by drawing multiple rows where some intervals overlap.
Pthread A timeline will contain one Pthread row for each CPU thread that performs Pthread API calls, given that host thread API calls have been recorded during measurement.
Note that for performance reasons, only selected Pthread API calls may have been recorded.
Markers and Ranges A timeline will contain a single Markers and Ranges row for each CPU thread that uses the NVIDIA Tools Extension API to annotate a time range or marker.
Each interval in the row represents the duration of a time range, or the instantaneous point of a marker.
Profiling Overhead A timeline will contain a single Profiling Overhead row for each process.
Each interval in the row represents the duration of execution of some activity required for profiling.
These intervals represent activity that does not occur when the application is not being profiled.
Device A timeline will contain a Device row for each GPU device utilized by the application being profiled.
The name of the timeline row indicates the device ID in square brackets followed by the name of the device.
After running the Compute Utilization analysis, the row will contain an estimate of the compute utilization of the device over time.
If power, clock, and thermal profiling are enabled, the row will also contain points representing those readings.
Unified Memory A timeline will contain a Unified Memory row for each CPU thread and device that uses unified memory.
The Unified memory may contain CPU Page Faults, GPU Page Faults, Data Migration (DtoH) and Data Migration (HtoD) rows.
When creating a session user can select segment mode or non-segment mode for Unified Memory timelines.
In the segment mode the timeline is split into equal width segments and only aggregated data values for each time segment are shown.
In non-segment mode each interval on the timeline will represent the actual data collected and the properties for each interval can be viewed.
Under properties for the timeline the property which is used for selecting the color is given and also a legend displays the mapping of colors to different range of property values.
In the non-segment mode each interval on the timeline corresponds to one data migration from device to host.
In the non-segment mode each interval on the timeline corresponds to one GPU page fault group.
In the non-segment mode each interval on the timeline corresponds to one data migration from host to device.
The name of the timeline row indicates the context ID or the custom context name if the NVIDIA Tools Extension API was used to name the context.
A context may contain up to four memcpy rows for device-to-host, host-to-device, device-to-device, and peer-to-peer memory copies.
Compute A timeline will contain a Compute row for each context that performs computation on the GPU.
All kernel activity, including kernels launched using CUDA Dynamic Parallelism, is shown on the Compute row.
The Kernel rows following the Compute row show activity of each individual application kernel.
Each interval in a row represents the duration of execution of an instance of that kernel in the containing context.
Each row is labeled with a percentage that indicates the total execution time of all instances of that kernel compared to the total execution time of all kernels.
For each context, the kernels are ordered top to bottom by this execution time percentage.
For CUDA Dynamic Parallelism applications, the kernels are organized in a hierarchy that represents the parent/child relationship between the kernels.
Kernels that use CUDA Dynamic Parallelism to launch other kernels can be expanded using the ‘+’ icon to show the kernel rows representing those child kernels.
For kernels that don’t launch child kernels, the kernel execution is represented by a solid interval, showing the time that that instance of the kernel was executing on the GPU.
For kernels that launch child kernels, the interval can also include a hollow part at the end.
The hollow part represents the time after the kernel has finished executing where it is waiting for child kernels to finish executing.
The CUDA Dynamic Parallelism execution model requires that a parent kernel not complete until all child kernels complete and this is what the hollow part is showing.
The Focus control described in Timeline Controls can be used to control display of the parent/child timelines.
Stream A timeline will contain a Stream row for each stream used by the application (including both the default stream and any application created streams).
Each interval in a Stream row represents the duration of a memcpy or kernel execution performed on that stream. 2.4.1.1. Timeline Controls  The Timeline View has several controls that you use to control how the timeline is displayed.
Some of these controls also influence the presentation of data in the GPU Details View and the Analysis View .
Resizing the Vertical Timeline Ruler The width of the vertical ruler can be adjusted by placing the mouse pointer over the right edge of the ruler.
When the double arrow pointer appears, click and hold the left mouse button while dragging.
You may want to reorder these rows to aid in visualizing related kernels and streams, or to move unimportant kernels and streams to the bottom of the timeline.
Filtering Timelines Memcpy and Kernel rows can be filtered to exclude their activities from presentation in the GPU Details View and the Analysis View .
When a row is filtered, any intervals on that row are dimmed to indicate their filtered status.
Expanding and Collapsing Timelines Groups of timeline rows can be expanded and collapsed using the [+] and [-] controls just to the left of the row labels.
There are three expand/collapse states: Collapsed No timeline rows contained in the collapsed row are shown.
Intervals associated with collapsed rows may not be shown in the GPU Details View and the Analysis View , depending on the filtering mode set for those views (see view documentation for more information).
For example, if you collapse a device row, then all memcpys, memsets, and kernels associated with that device are excluded from the results shown in those views.
The coloring mode can be selected in the View menu, in the timeline context menu (accessed by right-clicking in the timeline view), and on the profiler toolbar.
In kernel coloring mode, each type of kernel is assigned a unique color (that is, all activity intervals in a kernel row have the same color).
In stream coloring mode, each stream is assigned a unique color (that is, all memcpy and kernel activity occurring on a stream are assigned the same color).
In process coloring mode, each process is assigned a unique color (that is, all memcpy and kernel activity occurring in a process are assigned the same color).
Focusing Kernel Timelines For applications using CUDA Dynamic Parallelism, the Timeline View displays a hierarchy of kernel activity that shows the parent/child relationship between kernels.
The focus timeline control can be used to focus the displayed parent/child relationships to a specific, limited set of “family trees”.
The focus timeline mode can be selected and deselected in the timeline context menu (accessed by right-clicking in the timeline view), and on the profiler toolbar.
To see the “family tree” of a particular kernel, select a kernel and then enable Focus mode.
All kernels except those that are ancestors or descendants of the selected kernel will be hidden.
Use the “Don’t Focus” option to disable focus mode and restore all kernels to the Timeline view.
Dependency Analysis Controls There are two modes for visualizing dependency analysis results in the timeline: Focus Critical Path and Highlight Execution Dependencies.
These modes can be selected in the View menu, in the timeline context menu (accessed by right-clicking in the timeline view), and on the Visual Profiler toolbar.
These options become available after the Dependency Analysis application analysis stage has been run (see Unguided Application Analysis ).
Navigating the Timeline  The timeline can be scrolled, zoomed, and focused in several ways to help you better understand and visualize your application’s performance.
Zooming The zoom controls are available in the View menu, in the timeline context menu (accessed by right-clicking in the timeline view), and on the profiler toolbar.
Zoom-in reduces the timespan displayed in the view, zoom-out increases the timespan displayed in the view, and zoom-to-fit scales the view so that the entire timeline is visible.
You can also zoom-in and zoom-out with the mouse wheel while holding the Ctrl key (for macOS use the Command key).
Select a region of the timeline by holding Ctrl (for macOS use the Command key) while left-clicking and dragging the mouse.
The highlighted region will be expanded to occupy the entire view when the mouse button is released.
The timeline can be scrolled horizontally with the scrollbar or by using the mouse wheel while holding the Shift key.
Highlighting/Correlation When you move the mouse pointer over an activity interval on the timeline, that interval is highlighted in all places where the corresponding activity is shown.
For example, if you move the mouse pointer over an interval representing a kernel execution, that kernel execution is also highlighted in the Stream and in the Compute timeline row.
When a kernel or memcpy interval is highlighted, the corresponding driver or runtime API interval will also highlight.
This allows you to see the correlation between the invocation of a driver or runtime API or OpenACC directive on the CPU and the corresponding activity on the GPU.
When a single interval or row is selected, the information about that interval or row is pinned in the Properties View .
In the GPU Details View , the detailed information for the selected interval is shown in the table.
Measuring Time Deltas Measurement rulers can be created by left-click dragging in the horizontal ruler at the top of the timeline.
After a ruler is created, it can be resized by dragging the vertical guide lines that appear over the timeline.
If the mouse is dragged over a timeline interval, the guideline will snap to the nearest edge of that interval. 2.4.1.3. Timeline Refreshing  The profiler loads the timeline gradually as it reads the data.
This is more apparent if the data file being loaded is big, or the application has generated a lot of data.
At the same time, a spinning circle replaces the icon of the current session tab, indicating the timeline is not fully loaded.
To reduce its memory footprint, the profiler may skip loading some timeline contents if they are not visible at the current zoom level.
These contents will be automatically loaded when they become visible on a new zoom level. 2.4.1.4. Dependency Analysis Controls  The profiler allows the visualization of dependency analysis results in the timeline once the respective analysis stage has been run.
Focus Critical Path visualizes the critical path through the application by focusing on all intervals on the critical path and fading others.
When the mode is enabled and any timeline interval is selected (by left-clicking it), the selected interval will have focus.
This allows you to “follow” the critical path through the execution and to inspect individual intervals.
Highlight Execution Dependencies allows you to analyze the execution dependencies for each interval (Note that for certain intervals, no dependency information is collected).
When this mode is enabled, the highlighting color changes from yellow (representing correlated intervals) to red (representing dependencies).
Both the selected interval as well as all incoming and outgoing dependencies are highlighted. 2.4.2. Analysis View  The Analysis View is used to control application analysis and to display the analysis results.
In guided mode the analysis system will guide you through multiple analysis stages to help you understand the likely performance limiters and optimization opportunities in your application.
In unguided mode you can manually explore all the analysis results collected for your application.
The left part of the view provides step-by-step directions to help you analyze and optimize your application.
The right part of the view shows detailed analysis results appropriate for each part of the analysis. 2.4.2.1. Guided Application Analysis  In guided mode, the analysis view will guide you step-by-step through analysis of your entire application with specific analysis guidance provided for each kernel within your application.
Guided analysis starts with CUDA Application Analysis and from there will guide you to optimization opportunities within your application. 2.4.2.2. Unguided Application Analysis  In unguided analysis mode each application analysis stage has a Run analysis button that can be used to generate the analysis results for that stage.
When the Run analysis button is selected, the profiler will execute the application to collect the profiling data needed to perform the analysis.
The green check-mark next to an analysis stage indicates that the analysis results for that stage are available.
Each analysis result contains a brief description of the analysis and a More… link to detailed documentation on the analysis.
When you select an analysis result, the timeline rows or intervals associated with that result are highlighted in the Timeline View .
When a single kernel instance is selected in the timeline, additional kernel-specific analysis stages are available.
Each kernel-specific analysis stage has a Run analysis button that operates in the same manner as for the application analysis stages.
The following figure shows the analysis results for the Divergent Execution analysis stage.
Some kernel instance analysis results, like Divergent Execution, are associated with specific source-lines within the kernel.
The source file associated with that entry will open. 2.4.2.3. PC Sampling View  Devices with compute capability 5.2 and higher, excluding mobile devices, have a feature for PC sampling.
In this feature PC and state of warp are sampled at regular interval for one of the active warps per SM.
The warp state indicates if that warp issued an instruction in a cycle or why it was stalled and could not issue an instruction.
When a warp that is sampled is stalled, there is a possibility that in the same cycle some other warp is issuing an instruction.
Hence the stall for the sampled warp need not necessarily indicate that there is a hole in the instruction issue pipeline.
Devices with compute capability 6.0 and higher have a new feature that gives latency reasons.
While collecting these samples, there is no instruction issued in the respective warp scheduler and hence these give the latency reasons.
The latency reasons will be one of the stall reasons in Warp State section except ‘not selected’ stall reason.
The profiler collects this information and presents it in the Kernel Profile - PC Sampling view.
After clicking on the source file or device function the Kernel Profile - PC Sampling view is opened.
The hotspots shown next to the vertical scroll bar are determined by the number of samples collected for each source and assembly line.
The distribution of the stall reasons is shown as a stacked bar for each source and assembly line.
For devices with compute capability 6.0 and higher, Visual Profiler show two views: ‘Kernel Profile - PC Sampling’ which gives the warp state view and ‘Kernel Profile - PC Sampling - Latency’ which gives the latency reasons.
The tables in result section give percentage distribution for total latency samples, issue pipeline busy samples and instruction issued samples.
The blog post Pinpoint Performance Problems with Instruction-Level Profiling shows how PC Sampling can be used to optimize a CUDA kernel. 2.4.2.4. Memory Statistics  Devices with compute capability 5.0 and higher have a feature to show usage of the memory sub-system during kernel execution.
The green nodes in the diagram depict logical memory space whereas blue nodes depicts actual hardware unit on the chip.
For the various caches the reported percentage number states the cache hit rate; that is the ratio of requests that could be served with data locally available to the cache over all requests made.
The links between the nodes in the diagram depict the data paths between the SMs to the memory spaces into the memory system.
The data paths from the SMs to the memory spaces (Global, Local, Texture, Surface and Shared) report the total number of memory instructions executed, it includes both read and write operations.
The data path between memory spaces and “Unified Cache” or “Shared Memory” reports the total amount of memory requests made.
The arrow pointing to right direction indicates WRITE operation whereas the arrow pointing to left direction indicates the READ operations. 2.4.2.5. NVLink view  NVIDIA NVLink is a high-bandwidth, energy-efficient interconnect that enables fast communication between the CPU and GPU, and between GPUs.
Visual Profiler collects NVLink topology and NVLink transmit/receive throughput metrics and maps the metrics on to the topology.
NVLink information is presented in the Results section of Examine GPU Usage in CUDA Application Analysis in Guided Analysis.
NVLink Analysis shows topology that shows the logical NVLink connections between different devices.
A logical link comprises of 1 to 4 physical NVLinks of same properties connected between two devices.
Visual profiler lists the properties and achieved utilization for logical NVLinks in ‘Logical NVLink Properties’ table.
It also lists the transmit and receive throughputs for logical NVLink in ‘Logical NVLink Throughput’ table. 2.4.3. Source-Disassembly View  The Source-Disassembly View is used to display the analysis results for a kernel at the source and assembly instruction level.
To be able to view the kernel source you need to compile the code using the -lineinfo option.
This view is displayed for the following types of analysis: Global Memory Access Pattern Analysis Shared Memory Access Pattern Analysis Divergent Execution Analysis Kernel Profile - Instruction Execution Analysis Kernel Profile - PC Sampling Analysis As part of the Guided Analysis or Unguided Analysis for a kernel the analysis results are displayed under the Analysis view.
After clicking on the source file or device function the Source-Disassembly view is opened.
If the source file is not found a dialog is opened to select and point to the new location of the source file.
The Source-Disassembly view contains: High level source Assembly instructions Hotspots at the source level Hotspots at the assembly instruction level Columns for profiling data aggregated to the source level Columns for profiling data collected at the assembly instruction level The information shown in the Source-Disassembly view can be customized by the following toolbar options: View menu - Select one or more out of the available profiler data columns to display.
Maximize the source view Maximize the disassembly view Hotspots are colored based on level of importance - low, medium or high.
Hovering the mouse over the hotspot displays the value of the profiler data, the level of importance and the source or disassembly line.
You can click on a hotspot at the source level or assembly instruction level to view the source or disassembly line corresponding to the hotspot.
In the disassembly view the assembly instructions corresponding to the selected source line are highlighted.
You can click on the up and down arrow buttons displayed at the right of the disassembly column header to navigate to the next or previous instruction block. 2.4.4. GPU Details View  The GPU Details View displays a table of information for each memory copy and kernel execution in the profiled application.
For kernels, the table will also contain a column for each metric or event value collected for that kernel.
In the figure, the Achieved Occupancy column shows the value of that metric for each of the kernel executions.
You can sort the data by column by left clicking on the column header, and you can rearrange the columns by left clicking on a column header and dragging it to its new location.
If you select a row in the table, the corresponding interval will be selected in the Timeline View .
Similarly, if you select a kernel or memcpy interval in the Timeline View the table will be scrolled to show the corresponding data.
If you hover the mouse over a column header, a tooltip will display the data shown in that column.
For a column containing event or metric data, the tooltip will describe the corresponding event or metric.
The information shown in the GPU Details View can be filtered in various ways using the menu accessible from the Details View toolbar.
The following modes are available: Filter By Selection - If selected, the GPU Details View shows data only for the selected kernel and memcpy intervals.
Show Hidden Timeline Data - If not selected, data is shown only for kernels and memcpys that are visible in the timeline.
Kernels and memcpys that are not visible because they are inside collapsed parts of the timeline are not shown.
Show Filtered Timeline Data - If not selected, data is shown only for kernels and memcpys that are in timeline rows that are not filtered.
Collecting Events and Metrics Specific event and metric values can be collected for each kernel and displayed in the details table.
Use the toolbar icon in the upper right corner of the view to configure the events and metrics to collect for each device, and to run the application to collect those events and metrics.
Show Summary Data By default the table shows one row for each memcpy and kernel invocation.
Use the toolbar icon in the upper right corner of the view to select or deselect summary format.
Formatting Table Contents The numbers in the table can be displayed either with or without grouping separators.
Use the toolbar icon in the upper right corner of the view to select or deselect grouping separators.
Exporting Details The contents of the table can be exported in CSV format using the toolbar icon in the upper right corner of the view. 2.4.5. CPU Details View  CPU Details view This view details the amount of time your application spends executing functions on the CPU.
Each thread is sampled periodically to capture its callstack and the summary of these measurements are displayed in this view.
You can manipulate the view by selecting different orientations for organizing the callstack: Top-down, Bottom-up, Code Structure (3), choosing which thread to view (1), and by sorting or highlighting a specific thread (7, 8).
All the threads profiled are shown in one view when the ‘all threads’ option is selected (default).
This column displays a tree of events representing the structure of the application’s execution on the CPU.
The following modes are available: Top-down (callers first) call tree view - The CPU details tree is organized as a call tree with each function shown as a child of its caller.
Bottom-up (callees first) call tree view - The CPU details tree is organized in such a way that each function is shown as a child of any functions it calls.
In this mode you can quickly identify the call path that contributes the most time to the application’s execution.
Code structure (file and line) tree view - The CPU details tree shows which functions belong to each source file and library as well as how much of the application’s execution is attributed to a given line of source code.
In every mode the time listed for each function is ‘inclusive’ and includes time spent both in this function and any functions that it calls.
This column displays the total amount of time spent by all threads in this event as a percentage of the total amount of time spent in all events.
This column displays a bar denoting a range where the amount of time spent in an event by any thread is always within this this range.
Also, if there is space, a small ‘diamond’ is drawn in the middle of the bar where the mean time is spent in this event across all threads.
On the left is a vertical scale showing the same minimum and maximum values as shown on the range chart.
If the cell for the given event / thread combination is greyed out then no time was spent by this thread in this event (for this example both threads 1 and 2 spent no time in the event ‘x_solve’).
Furthermore, the thread(s) with the minimum or maximum amount of time spent in the event across all threads are annotated with the ‘triangle / line’.
In this example thread 3 spent the most and thread 6 the least amount of time in the event ‘x_solve’.
To reorder the rows by the time spent on a given thread click on the thread column header.
Having highlighted thread 3 we now see a vertical line on the range chart showing the amount of time this thread spent in this event compared to the range across all thread.
CPU Threads CPU Source Code You can open the CPU Source View for any function by double-clicking on it in the tree.
Sometimes a file within a specific directory is being sought, in this case you should give the path to where this directory resides.
Tip The CPU profile is gathered by periodically sampling the state of the running application.
For this reason a function will only appear in this view if it was sampled during execution.
If a function was not sampled the time it was running is accounted to the function that called it.
In order to gather a CPU profile that is representative of the application’s performance the code of interest must execute for enough to gather enough samples.
Tip The file and line information is gathered from the application’s debug information obtained by the compiler.
To ensure that this information is available it is recommended that you compile with ‘-g’ or a similar option. 2.4.6. OpenACC Details View  OpenACC table view The OpenACC Details View displays each OpenACC runtime activity executed by the profiled application.
Each activity is grouped by source location: each activity which occurs at the same file and line number in the application’s source code is placed under a node labeled with the source location.
Each activity shows the amount of time spent by the profiled application as both a unit of time and as a percentage of the total time this application was executing any OpenACC activity.
There are two ways to count how much time is spent in a particular OpenACC activity: Show the Inclusive durations (counting any other OpenACC activities running at the same time) in the OpenACC details view - The OpenACC details view shows the total time spent in each activity including any activities that were executed as the result of this activity.
In this case the amount of time spent in each activity occurring at a given application source location is totaled and displayed on the row displaying the source location.
Show the Exclusive durations (excluding any other OpenACC activities running at the same time) in the OpenACC details view - The OpenACC details view shows the time spent only in a given activity.
In this case the amount of time spent at a given source location is always zero—time is attributed solely to each activity occurring at this source location. 2.4.7. OpenMP Details View  OpenMP table view The OpenMP Details view displays the activity of the OpenMP runtime on the CPU.
The time your application spends in a parallel region or idling is shown both on the timeline and is summarized in this view.
The reference for the percentage of time spent in each type of activity is the time from the start of the first parallel region to the end of the last parallel region.
The sum of the percentages of each activity type often exceeds 100% because the OpenMP runtime can be in multiple states at the same time. 2.4.8. Properties View  The Properties View shows information about the row or interval highlighted or selected in the Timeline View .
If a row or interval is not selected, the displayed information tracks the motion of the mouse pointer.
If a row or interval is selected, the displayed information is pinned to that row or interval.
When an OpenACC interval with an associated source file is selected, this filename is shown in the Source File table entry.
Double-clicking on the filename opens the respective source file if it is available on the file-system. 2.4.9. Console View  The Console View shows stdout and stderr output of the application each time it executes.
If you need to provide stdin input to your application, do so by typing into the console view. 2.4.10. Settings View  The Settings View allows you to specify execution settings for the application being profiled.
As shown in the following figure, the Executable settings tab allows you to specify the executable file, the working directory, the command-line arguments, and the environment for the application.
Exection Timeout The Executable settings tab also allows you to specify an optional execution timeout.
If the execution timeout is specified, the application execution will be terminated after that number of seconds.
If the execution timeout is not specified, the application will be allowed to continue execution until it terminates normally.
Start execution with profiling enabled The Start execution with profiling enabled checkbox is set by default to indicate that application profiling begins at the start of application execution.
If you are using cudaProfilerStart() and cudaProfilerStop() to control profiling within your application as described in Focused Profiling , then you should uncheck this box.
Enable concurrent kernel profiling The Enable concurrent kernel profiling checkbox is set by default to enable profiling of applications that exploit concurrent kernel execution.
Disabling concurrent kernel execution can reduce profiling overhead in some cases and so may be appropriate for applications that do not exploit concurrent kernels.
Enable power, clock, and thermal profiling The Enable power, clock, and thermal profiling checkbox can be set to enable low frequency sampling of the power, clock, and thermal behavior of each GPU used by the application. 2.4.11. CPU Source View  The CPU source code view allows you to inspect the files that comprise the profiled application’s CPU source.
This view can be opened in the CPU Details View by double-clicking on a function in the tree–the source file that corresponds to this function is then opened.
When compiling using the PGI® compilers annotations can be added to this view (see Common Compiler Feedback Format for more information).
PGI compilers save information about how your program was optimized, or why a particular optimization was not made.
This can be combined with the CPU Details View to help identify why certain lines of code performed the way they did.
For example, the message may tell you about the following: vector instructions generated by the compiler.
compute-intensity of a loop, a ratio computation to memory operations–higher numbers mean that there is more computation than memory loads and stores.
information about parallelization, with a hint for how it might be possible to make the loop run in parallel if the compiler could not auto-parallelize it. 2.5. Customizing the Profiler  When you first start the Visual Profiler , and after closing the Welcome page, you will be presented with a default placement of the views.
By moving and resizing the views, you can customize the profiler to meet your development needs.
Any changes you make are restored the next time you start the profiler. 2.5.1. Resizing a View  To resize a view, simply left click and drag on the dividing area between the views.
All views stacked together in one area are resized at the same time. 2.5.2. Reordering a View  To reorder a view in a stacked set of views, left click and drag the view tab to the new location within the view stack.
2.5.3. Moving a View  to move a view, left click the view tab and drag it to its new location.
You can place the view in a new location, or stack it in the same location as other views. 2.5.4. Undocking a View  You can undock a view from the profiler window so that the view occupies its own stand-alone window.
You may want to do this to take advantage of multiple monitors or to maximum the size of an individual view.
To dock a view, left click the view tab (not the window decoration) and drag it into the profiler window. 2.5.5. Opening and Closing a View  Use the X icon on a view tab to close a view.
To open a view, use the View menu. 2.6. Command Line Arguments  When the Visual Profiler is started from the command line, it is possible, using command line arguments, to specify executable to start new session with or import profile files exported from nvprof using one of the following patterns: Start new executable session by launching nvvp with name of executable followed, optionally, by its arguments: nvvp executableName [[ executableArguments ]...] Import single-process nvprof session by launching nvvp with single .nvprof file as argument(see nvprof’s export/import options section for more details): nvvp data .
nvprof Import multi-process nvprof session, by launching nvvp with multiple .nvprof files as arguments: nvvp data1 .
nvprof ... 3. ​nvprof  The nvprof profiling tool enables you to collect and view profiling data from the command-line.
nvprof enables the collection of a timeline of CUDA-related activities on both CPU and GPU, including kernel execution, memory transfers, memory set and CUDA API calls and events or metrics for CUDA kernels.
Profiling results are displayed in the console after the profiling data is collected, and may also be saved for laterviewing by either nvprof or the Visual Profiler.
To profile an application from the command-line: nvprof [ options ] [ application ] [ application - arguments ] To view the full help page, type nvprof --help . 3.1. Command Line Options  3.1.1.
CUDA Profiling Options  Option Values Default Description aggregate-mode on, off on Turn on/off aggregate mode for events and metrics specified by subsequent --events and --metrics options.
Those event/metric values will be collected for each domain instance, instead of the whole device.
analysis-metrics N/A N/A Collect profiling data that can be imported to Visual Profiler’s “analysis” mode.
If concurrent kernel execution is off, all kernels running on one device will be serialized.
continuous-sampling-interval {interval in milliseconds} 2 milliseconds Set the continuous mode sampling interval in milliseconds.
dependency-analysis N/A N/A Generate event dependency graph for host and device activities and run dependency analysis.
device-buffer-size {size in MBs} 8 MB Set the device memory size (in MBs) reserved for storing profiling data for non-CDP operations, especially for concurrent kernel tracing, for each buffer on a context.
device-cdp-buffer-size {size in MBs} 8 MB Set the device memory size (in MBs) reserved for storing profiling data for CDP operations for each buffer on a context.
devices {comma-separated device IDs}, all N/A Change the scope of subsequent --events , --metrics , --query-events and --query-metrics options.
event-collection-mode kernel, continuous kernel Choose event collection mode for all events/metrics.
kernel: Events/metrics are collected only for durations of kernel executions continuous: Events/metrics are collected for duration of application.
This mode is incompatible with --profile-all-processes or --profile-child-processes or --replay-mode kernel or --replay-mode application .
events (e) {comma-separated event names}, all N/A Specify the events to be profiled on certain device(s).
kernel-latency-timestamps on, off off Turn on/off collection of kernel latency timestamps, namely queued and submitted.
The queued timestamp is captured when a kernel launch command was queued into the CPU command buffer.
The submitted timestamp denotes when the CPU command buffer containing this kernel launch was submitted to the GPU.
kernels {kernel name}, {[context id/name]:[stream id/name]:[kernel name]:[invocation]} N/A Change the scope of subsequent --events , --metrics options.
{[context id/name]:[stream id/name]:[kernel name]:[invocation]}: The context/stream IDs, names, kernel name and invocation can be regular expressions.
If [context id/name] or [stream id/name] is a positive number, it’s strictly matched against the CUDA context/stream ID.
Otherwise it’s treated as a regular expression and matched against the context/stream name specified by the NVTX library.
If the invocation count is a positive number, it’s strictly matched against the invocation of the kernel.
Example: --kernels "1:foo:bar:2" will profile any kernel whose name contains “bar” and is the 2nd instance on context 1 and on stream named “foo”.
metrics (m) {comma-separated metric names}, all N/A Specify the metrics to be profiled on certain device(s).
Note: --metrics all does not include some metrics which are needed for Visual Profiler’s source level analysis.
pc-sampling-period {period in cycles} Between 5 and 12 based on the setup Specify PC Sampling period in cycles, at which the sampling records will be dumped.
profile-all-processes N/A N/A Profile all processes launched by the same user who launched this nvprof instance.
profile-api-trace none, runtime, driver, all all Turn on/off CUDA runtime/driver API tracing.
none: turn off API tracing runtime: only turn on CUDA runtime API tracing driver: only turn on CUDA driver API tracing all: turn on all API tracing profile-child-processes N/A N/A Profile the application and all child processes launched by it.
If it’s disabled, the application can use {cu,cuda}Profiler{Start,Stop} to turn on/off profiling.
profiling-semaphore-pool-size {count} 65536 Set the profiling semaphore pool size reserved for storing profiling data for serialized kernels and memory operations for each context.
replay-mode disabled, kernel, application kernel Choose replay mode used when not all events/metrics can be collected in a single run.
disabled: replay is disabled, events/metrics couldn’t be profiled will be dropped kernel: each kernel invocation is replayed application: the entire application is replayed.
skip-kernel-replay-save-restore on, off off If enabled, this option can vastly improve kernel replay speed, as save and restore of the mutable state for each kernel pass will be skipped.
Skipping of save/restore of input/output buffers allows you to specify that all profiled kernels on the context do not change the contents of their input buffers during execution, or call device malloc/free or new/delete, that leave the device heap in a different state.
Specifically, a kernel can malloc and free a buffer in the same launch, but it cannot call an unmatched malloc or an unmatched free.
Note: incorrectly using this mode while one of the kernels does modify the input buffer or uses unmatched malloc/free will result in undefined behavior, including kernel execution failure and/or corrupted device data.
on: skip save/restore of the input/output buffers off: save/restore input/output buffers for each kernel replay pass source-level-analysis (a) global_access, shared_access, branch, instruction_execution, pc_sampling N/A Specify the source level metrics to be profiled on a certain kernel invocation.
One or more of these may be specified, separated by commas global_access: global access shared_access: shared access branch: divergent branch instruction_execution: instruction execution pc_sampling: pc sampling, available only for GM20X+ Note: Use --export-profile to specify an export file.
track-memory-allocations on, off off Turn on/off tracking of memory operations, which involves recording timestamps, memory size, memory type and program counters of the memory allocations and frees.
unified-memory-profiling per-process-device, off per-process-device Configure unified memory profiling.
per-process-device: collect counts for each process and each device off: turn off unified memory profiling See Unified Memory Profiling for more information. 3.1.2. CPU Profiling Options  Option Values Default Description cpu-profiling on, off off Turn on CPU profiling.
cpu-profiling-explain-ccff {filename} N/A Set the path to a PGI pgexplain.xml file that should be used to interpret Common Compiler Feedback Format (CCFF) messages.
cpu-profiling-frequency {frequency} 100Hz Set the CPU profiling frequency in samples per second.
cpu-profiling-mode flat, top-down, bottom-up bottom-up Set the output mode of CPU profiling.
flat: Show flat profile top-down: Show parent functions at the top bottom-up: Show parent functions at the bottom cpu-profiling-percentage-threshold {threshold} 0 (i.e.
function: Each level in the stack trace represents a distinct function instruction: Each level in the stack trace represents a distinct instruction address cpu-profiling-show-ccff on, off off Choose whether to print Common Compiler Feedback Format (CCFF) messages embedded in the binary.
cpu-profiling-show-library on, off off Choose whether to print the library name for each sample.
cpu-profiling-thread-mode separated, aggregated aggregated Set the thread mode of CPU profiling.
separated: Show separate profile for each thread aggregated: Aggregate data from all threads cpu-profiling-unwind-stack on, off on Choose whether to unwind the CPU call-stack at each sample point.
openacc-profiling on, off on Enable/disable recording information from the OpenACC profiling interface.
openmp-profiling on, off off Enable/disable recording information from the OpenMP profiling interface.
See OpenMP for more information. 3.1.3. Print Options  Option Values Default Description context-name {name} N/A Name of the CUDA context.
%p in the context name string is replaced with the process ID of the application being profiled.
normalized-time-unit (u) s, ms, us, ns, col, auto auto Specify the unit of time that will be used in the output.
s: second ms: millisecond us: microsecond ns: nanosecond col: a fixed unit for each column auto: the scale is chosen for each value based on its length.
openacc-summary-mode exclusive, inclusive exclusive Set how durations are computed in the OpenACC summary.
api - only turn on CUDA runtime and driver API tracing gpu - only turn on CUDA GPU tracing print-api-summary N/A N/A Print a summary of CUDA runtime/driver API calls.
print-gpu-summary N/A N/A Print a summary of the activities on the GPU (including CUDA kernels and memcpy’s/memset’s).
print-gpu-trace N/A N/A Print individual kernel invocations (including CUDA memcpy’s/memset’s) and sort them in chronological order.
%p in the process name string is replaced with the process ID of the application being profiled.
%p in the stream name string is replaced with the process ID of the application being profiled.
%% in the stream name string is replaced with % . 3.1.4. IO Options  Option Values Default Description export-profile (o) {filename} N/A Export the result file which can be imported later or opened by the NVIDIA Visual Profiler.
%p in the file name string is replaced with the process ID of the application being profiled.
Note: If the application being profiled creates child processes, or if --profile-all-processes is used, the %p format is needed to get correct export files for each process.
force-overwrite (f) N/A N/A Force overwriting all output files (any existing files will be overwritten).
log-file {filename} N/A Make nvprof send all its output to the specified file, or one of the standard channels.
print-nvlink-topology N/A N/A Print nvlink topology print-pci-topology N/A N/A Print PCI topology help (h) N/A N/A Print help information.
version (V) N/A N/A Print version information of this tool. 3.2. Profiling Modes  nvprof operates in one of the modes listed below.
In this mode, nvprof outputs a single result line for each kernel function and each type of CUDA memory copy/set performed by the application.
For each kernel, nvprof outputs the total time of all instances of the kernel or type of memory copy as well as the average, minimum, and maximum time.
Output of nvprof (except for tables) are prefixed with = , being the process ID of the application being profiled.
Here’s a simple example of running nvprof on the CUDA sample matrixMul : $ nvprof matrixMul [ Matrix Multiply Using CUDA ] - Starting ...
== 27694 == NVPROF is profiling process 27694 , command : matrixMul GPU Device 0 : "GeForce GT 640M LE" with compute capability 3.0 MatrixA ( 320 , 320 ), MatrixB ( 640 , 320 ) Computing result using CUDA Kernel ...
done Performance = 35.35 GFlop / s , Time = 3.708 msec , Size = 131072000 Ops , WorkgroupSize = 1024 threads / block Checking computed result for correctness : OK Note : For peak performance , please refer to the matrixMulCUBLAS example .
If multiple CUDA capable devices are profiled, nvprof --print-summary-per-gpu can be used to print one summary per GPU.
If your application uses Dynamic Parallelism, the output will contain one column for the number of host-launched kernels and one for the number of device-launched kernels.
Here’s an example of running nvprof on the CUDA Dynamic Parallelism sample cdpSimpleQuicksort : $ nvprof cdpSimpleQuicksort == 27325 == NVPROF is profiling process 27325 , command : cdpSimpleQuicksort Running on GPU 0 ( Tesla K20c ) Initializing data : Running quicksort on 128 elements Launching kernel on the GPU Validating results : OK == 27325 == Profiling application : cdpSimpleQuicksort == 27325 == Profiling result : Time ( % ) Time Calls ( host ) Calls ( device ) Avg Min Max Name 99.71 % 1.2114 ms 1 14 80.761 us 5.1200 us 145.66 us cdp_simple_quicksort ( unsigned int * , int , int , int ) 0.18 % 2.2080 us 1 - 2.2080 us 2.2080 us 2.2080 us [ CUDA memcpy DtoH ] 0.11 % 1.2800 us 1 - 1.2800 us 1.2800 us 1.2800 us [ CUDA memcpy HtoD ] 3.2.2.
GPU-Trace and API-Trace Modes  GPU-Trace and API-Trace modes can be enabled individually or together.
GPU-Trace mode provides a timeline of all activities taking place on the GPU in chronological order.
For each kernel or memory copy, detailed information such as kernel parameters, shared memory usage and memory transfer throughput are shown.
The number shown in the square brackets after the kernel name correlates to the CUDA API that launched that kernel.
Here’s an example: $ nvprof -- print - gpu - trace matrixMul == 27706 == NVPROF is profiling process 27706 , command : matrixMul == 27706 == Profiling application : matrixMul [ Matrix Multiply Using CUDA ] - Starting ...
GPU Device 0 : "GeForce GT 640M LE" with compute capability 3.0 MatrixA ( 320 , 320 ), MatrixB ( 640 , 320 ) Computing result using CUDA Kernel ...
done Performance = 35.36 GFlop / s , Time = 3.707 msec , Size = 131072000 Ops , WorkgroupSize = 1024 threads / block Checking computed result for correctness : OK Note : For peak performance , please refer to the matrixMulCUBLAS example .
This number includes registers used internally by the CUDA driver and / or tools and can be more than what the compiler shows .
API-trace mode shows the timeline of all CUDA runtime and driver API calls invoked on the host in chronological order.
Here’s an example: $nvprof -- print - api - trace matrixMul == 27722 == NVPROF is profiling process 27722 , command : matrixMul == 27722 == Profiling application : matrixMul [ Matrix Multiply Using CUDA ] - Starting ...
GPU Device 0 : "GeForce GT 640M LE" with compute capability 3.0 MatrixA ( 320 , 320 ), MatrixB ( 640 , 320 ) Computing result using CUDA Kernel ...
done Performance = 35.35 GFlop / s , Time = 3.708 msec , Size = 131072000 Ops , WorkgroupSize = 1024 threads / block Checking computed result for correctness : OK Note : For peak performance , please refer to the matrixMulCUBLAS example .
== 27722 == Profiling result : Start Duration Name 108.38 ms 6.2130 us cuDeviceGetCount 108.42 ms 840 ns cuDeviceGet 108.42 ms 22.459 us cuDeviceGetName 108.45 ms 11.782 us cuDeviceTotalMem 108.46 ms 945 ns cuDeviceGetAttribute 149.37 ms 23.737 us cudaLaunch ( void matrixMulCUDA ( float * , float * , float * , int , int ) [ 2198 ]) 149.39 ms 6.6290 us cudaEventRecord 149.40 ms 1.10156 s cudaEventSynchronize 1.25096 s 21.543 us cudaEventElapsedTime 1.25103 s 1.5462 ms cudaMemcpy 1.25467 s 153.93 us cudaFree 1.25483 s 75.373 us cudaFree 1.25491 s 75.564 us cudaFree 1.25693 s 10.901 ms cudaDeviceReset Note Due to the way the profiler is setup, the first “cuInit()” driver API call is never traced. 3.2.3. Event/metric Summary Mode  To see a list of all available events on a particular NVIDIA GPU, use the --query-events option.
To see a list of all available metrics on a particular NVIDIA GPU, use the --query-metrics option.
Here’s an example: $ nvprof -- events warps_launched , local_load -- metrics ipc matrixMul [ Matrix Multiply Using CUDA ] - Starting ...
== 6461 == NVPROF is profiling process 6461 , command : matrixMul GPU Device 0 : "GeForce GTX TITAN" with compute capability 3.5 MatrixA ( 320 , 320 ), MatrixB ( 640 , 320 ) Computing result using CUDA Kernel ...
== 6461 == Warning : Some kernel ( s ) will be replayed on device 0 in order to collect all events / metrics .
done Performance = 6.39 GFlop / s , Time = 20.511 msec , Size = 131072000 Ops , WorkgroupSize = 1024 threads / block Checking computed result for correctness : Result = PASS NOTE : The CUDA Samples are not meant for performance measurements .
== 6461 == Profiling application : matrixMul == 6461 == Profiling result : == 6461 == Event result : Invocations Event Name Min Max Avg Device "GeForce GTX TITAN (0)" Kernel : void matrixMulCUDA ( float * , float * , float * , int , int ) 301 warps_launched 6400 6400 6400 301 local_load 0 0 0 == 6461 == Metric result : Invocations Metric Name Metric Description Min Max Avg Device "GeForce GTX TITAN (0)" Kernel : void matrixMulCUDA ( float * , float * , float * , int , int ) 301 ipc Executed IPC 1.282576 1.299736 1.291500 If the specified events/metrics can’t be profiled in a single run of the application, nvprof by default replays each kernel multiple times until all the events/metrics are collected.
In “application replay” mode, nvprof re-runs the whole application instead of replaying each kernel, in order to collect all events/metrics.
In some cases this mode can be faster than kernel replay mode if the application allocates large amount of device memory.
Replay can also be turned off entirely, in which case the profiler will not collect some events/metrics.
Note Events or metrics collection may significantly change the overall performance characteristics of the application because all kernel executions are serialized on the GPU.
Note If a large number of events or metrics are requested, no matter which replay mode is chosen, the overall application execution time may increase significantly. 3.2.4. Event/metric Trace Mode  In event/metric trace mode, event and metric values are shown for each kernel execution.
For example, multiprocessor specific events are aggregated across all multiprocessors on the GPU.
For example, in the following example, the “branch” event value is shown for each multiprocessor on the GPU: $ nvprof -- aggregate - mode off -- events local_load -- print - gpu - trace matrixMul [ Matrix Multiply Using CUDA ] - Starting ...
== 6740 == NVPROF is profiling process 6740 , command : matrixMul GPU Device 0 : "GeForce GTX TITAN" with compute capability 3.5 MatrixA ( 320 , 320 ), MatrixB ( 640 , 320 ) Computing result using CUDA Kernel ...
done Performance = 16.76 GFlop / s , Time = 7.822 msec , Size = 131072000 Ops , WorkgroupSize = 1024 threads / block Checking computed result for correctness : Result = PASS NOTE : The CUDA Samples are not meant for performance measurements .
== 6740 == Profiling application : matrixMul == 6740 == Profiling result : Device Context Stream Kernel local_load ( 0 ) local_load ( 1 ) ...
GeForce GTX TIT 1 7 void matrixMulCUDA Note Although --aggregate-mode applies to metrics, some metrics are only available in aggregate mode and some are only available in non-aggregate mode. 3.3. Profiling Controls  3.3.1.
Note Timeout starts counting from the moment the CUDA driver is initialized. 3.3.2. Concurrent Kernels  Concurrent-kernel profiling is supported, and is turned on by default.
This forces concurrent kernel executions to be serialized when a CUDA application is run with nvprof . 3.3.3. Profiling Scope  When collecting events/metrics, nvprof profiles all kernels launched on all visible CUDA devices by default.
--devices applies to --events , --metrics , --query-events and --query-metrics options that follows it .
It limits these options to collect events/metrics only on the devices specified by , which can be a list of device ID numbers separated by comma.
It limits these options to collect events/metrics only on the kernels specified by , which has the following syntax: or ::: Each string in the angle brackets can be a standard Perl regular expression.
If invocation is a positive number, it’s strictly matched against the invocation of the kernel.
If the context/stream string is a positive number, it’s strictly matched against the cuda context/stream ID.
Otherwise it’s treated as a regular expression and matched against the context/stream name provided by the NVIDIA Tools Extension.
Both --devices and --kernels can be specified multiple times, with distinct events/metrics associated.
--events , --metrics , --query-events and --query-metrics are controlled by the nearest scope options before them.
As an example, the following command, nvprof --devices 0 --metrics ipc --kernels "1:foo:bar:2" --events local_load a.out collects metric ipc on all kernels launched on device 0.
It also collects event local_load for any kernel whose name contains bar and is the 2nd instance launched on context 1 and on stream named foo on device 0. 3.3.4. Multiprocess Profiling  By default, nvprof only profiles the application specified by the command-line argument.
To profile all processes launched by an application, use the --profile-child-processes option.
nvprof also has a “profile all processes” mode, in which it profiles every CUDA process launched on the same system by the same user who launched nvprof .
Note CPU profiling is not supported in multi-process mode. 3.3.5. System Profiling  For devices that support system profiling, nvprof can enable low frequency sampling of the power, clock, and thermal behavior of each GPU used by the application.
To see the detail of each sample point, combine the above option with --print-gpu-trace . 3.3.6. Unified Memory Profiling  For GPUs that support Unified Memory, nvprof collects the Unified Memory related memory traffic to and from each GPU on your system.
To see the detail of each memory transfer while this feature is enabled, use --print-gpu-trace .
On multi-GPU configurations without P2P support between any pair of devices that support Unified Memory, managed memory allocations are placed in zero-copy memory.
In certain cases, the environment variable CUDA_MANAGED_FORCE_DEVICE_ALLOC can be set to force managed allocations to be in device memory and to enable migration on these hardware configurations.
Normally, using the environment variable CUDA_VISIBLE_DEVICES is recommended to restrict CUDA to only use those GPUs that have P2P support.
Please refer to the environment variables section in the CUDA C++ Programming Guide for further details. 3.3.7. CPU Thread Tracing  In order to allow a correct Dependency Analysis , nvprof can collect information about CPU-side threading APIs.
Recording this information is necessary if the application uses multiple CPU threads and at least two of these threads call the CUDA API.
nvprof tries to detect which calls are necessary to model the execution behavior and filters others.
Filtered calls include pthread_mutex_lock and pthread_mutex_unlock when those do not cause any concurrent thread to block.
Note CPU thread tracing starts after the first CUDA API call, from the thread issuing this call.
cuInit from its main thread before spawning any other user threads that call the CUDA API. 3.4. Output  3.4.1.
Adjust Units  By default, nvprof adjusts the time units automatically to get the most precise time values.
The --normalized-time-unit options can be used to get fixed time units throughout the results. 3.4.2. CSV  For each profiling mode, option --csv can be used to generate output in comma-separated values (CSV) format.
The result can be directly imported to spreadsheet software such as Excel. 3.4.3. Export/Import  For each profiling mode, option --export-profile can be used to generate a result file.
This file is not human-readable, but can be imported back to nvprof using the option --import-profile , or into the Visual Profiler.
Thus, exporting profiles to slower devices such as a network drive may slow down the execution of the application. 3.4.4. Demangling  By default, nvprof demangles C++ function names.
Use option --demangling off to turn this feature off. 3.4.5. Redirecting Output  By default, nvprof sends most of its output to stderr .
Use %p in the filename to be replaced by the process ID of nvprof , %h by the hostname , %q{ENV} by the value of environment variable ENV , and %% by % . 3.4.6. Dependency Analysis  nvprof can run a Dependency Analysis after the application has been profiled, using the --dependency-analysis option.
For applications using CUDA from multiple CPU threads, CPU Thread Tracing should be enabled, too.
The option --print-dependency-analysis-trace can be specified to change from a summary output to a trace output, showing computed metrics such as time on the critical path per function instance rather than per function type.
An example for dependency analysis summary output with all computed metrics aggregated per function type is shown below.
The summary contains an entry named Other , referring to all CPU activity that is not tracked by nvprof (e.g.
== 20704 == Dependency Analysis : == 20704 == Analysis progress : 100 % Critical path ( % ) Critical path Waiting time Name % s s 92.06 4.061817 0.000000 clock_block ( long * , long ) 4.54 0.200511 0.000000 cudaMalloc 3.25 0.143326 0.000000 cudaDeviceReset 0.13 5.7273280e-03 0.000000 0.01 2.7200900e-04 0.000000 cudaFree 0.00 0.000000 4.062506 pthread_join 0.00 0.000000 4.061790 cudaStreamSynchronize 0.00 0.000000 1.015485 pthread_mutex_lock 0.00 0.000000 1.013711 pthread_cond_wait 0.00 0.000000 0.000000 pthread_mutex_unlock 0.00 0.000000 0.000000 pthread_exit 0.00 0.000000 0.000000 pthread_enter 0.00 0.000000 0.000000 pthread_create 0.00 0.000000 0.000000 pthread_cond_signal 0.00 0.000000 0.000000 cudaLaunch 3.5.
CPU Sampling  Sometimes it’s useful to profile the CPU portion of your application, in order to better understand the bottlenecks and identify potential hotspots for the entire CUDA application.
For the CPU portion of the application, nvprof is able to sample the program counter and call stacks at a certain frequency.
For instance, the bottom-up view (shown above) can be useful in identifying the “hot” functions in which the application is spending most of its time.
The top-down view gives a break-down of the application execution time, starting from the main function, allowing you to find “call paths” which are executed frequently.
Note When using the CPU profiling feature on POSIX systems, the profiler samples the application by sending periodic signals.
Applications should therefore ensure that system calls are handled appropriately when interrupted.
Note On Windows, nvprof requires Visual Studio installation (2010 or later) and compiler-generated .PDB (program database) files to resolve symbol information.
When building your application, ensure that .PDB files are created and placed next to the profiled executable and libraries. 3.5.1. CPU Sampling Limitations  The following are known issues with the current release.
The result stack traces might not be complete under some compiler optimizations, notably frame pointer omission and function inlining.
The CPU sampling result does not support CSV mode. 3.6. OpenACC  On 64bit Linux platforms, nvprof supports recording OpenACC activities using the CUPTI Activity API.
This allows to investigate the performance on the level of OpenACC constructs in addition to the underlying, compiler-generated CUDA API calls.
OpenACC profiling in nvprof requires the targeted application to use PGI OpenACC runtime 19.1 or later.
Even though recording OpenACC activities is only supported on x86_64 Linux systems, importing and viewing previously generated profile data is available on all platforms supported by nvprof .
The CUPTI OpenACC activities are mapped to the original OpenACC constructs using their source file and line information.
For acc_enqueue_launch activities, it will furthermore show the launched CUDA kernel name which is generated by the OpenACC compiler.
OpenACC Options  Table 1 contains OpenACC profiling related command-line options of nvprof .
--print-openacc-trace Print a detailed trace of all recorded OpenACC activities, including each activity’s timestamp and duration.
--print-openacc-constructs Include the name of the OpenACC parent construct that caused an OpenACC activity to be emitted.
Note that for applications using PGI OpenACC runtime before 19.1, this value will always be unknown .
--openacc-summary-mode Specify how activity durations are presented in the OpenACC summary.
“inclusive” - inclusive durations. 3.6.2. OpenACC Summary Modes  nvprof supports two modes for presenting OpenACC activity durations in the OpenACC summary mode (enabled with --print-openacc-summary ): “exclusive” and “inclusive”.
This includes the time spent in this activity but excludes the runtime of all of its children (callees).
As an example, consider the OpenACC acc_compute_construct which itself calls acc_enqueue_launch to launch a kernel to the device and acc_implicit_wait , which waits on the completion of this kernel.
In “inclusive” mode, the duration for acc_compute_construct will include the time spent in acc_enqueue_launch and acc_implicit_wait .
In the summary profile, this is helpful to identify if a long acc_compute_construct represents a high launch overhead or rather a long wait (synchronization) time. 3.7. OpenMP  On 64bit Linux platforms, nvprof supports recording OpenMP activities OpenMP profiling in nvprof requires the targeted application to use a runtime supporting the OpenMP Tools interface (OMPT).
Even though recording OpenMP activities is only supported on x86_64 Linux systems, importing and viewing previously generated profile data is available on all platforms supported by nvprof .
An example for the OpenMP summary output is shown below: == 20854 == NVPROF is profiling process 20854 , command : .
Type Time ( % ) Time Calls Avg Min Max Name OpenMP ( incl ) : 99.97 % 277.10 ms 20 13.855 ms 13.131 ms 18.151 ms omp_parallel 0.03 % 72.728 us 19 3.8270 us 2.9840 us 9.5610 us omp_idle 0.00 % 7.9170 us 7 1.1310 us 1.0360 us 1.5330 us omp_wait_barrier 3.7.1.
OpenMP Options  Table 2 contains OpenMP profiling related command-line options of nvprof .
OpenMP Options  Option Description --print-openmp-summary Print a summary of all recorded OpenMP activities. 4. Remote Profiling  Remote profiling is the process of collecting profile data from a remote system that is different than the host system at which that profile data will be viewed and analyzed.
Or you can use nvprof to collect the profile data on the remote system and then use nvvp on the host system to view and analyze the data. 4.1. Remote Profiling With Visual Profiler  This section describes how to perform remote profiling by using the remote capabilities of nsight and the Visual Profiler.
Nsight Eclipse Edition supports full remote development including remote building, debugging, and profiling.
Using these capabilities you can create a project and launch configuration that allows you to remotely profile your application.
As shown in the following figure, when creating a new session or editing an existing session you can specify that the application being profiled resides on a remote system.
Once you have configured your session to use a remote application, you can perform all profiler functions in the same way as you would with a local application, including timeline generation, guided analysis, and event and metric collection.
To use the Visual Profiler remote profiling you must install the same version of the CUDA Toolkit on both the host and remote systems.
It is not necessary for the host system to have an NVIDIA GPU, but ensure that the CUDA Toolkit installed on the host system supports the target device.
The host and remote systems may run different operating systems or have different CPU architectures.
The remote system must be accessible via SSH. 4.1.1. One-hop remote profiling  In certain remote profiling setups, the machine running the actual CUDA program is not accessible from the machine running the Visual Profiler.
These two machines are connected via an intermediate machine, which we refer to as the login node.
The profiling data generated will be copied over to the login node, so that it can be used by the Visual Profiler on the host.
To configure one-hop profiling, you need to do the following one-time setup: Copy the one-hop profiling Perl script onto the login node.
In Visual Profiler’s New Session wizard, use the Configure button to open the toolkit configuration window.
Here, use the radio button to select the custom script option, and browse to point to the Perl script on the login node.
Once this setup is complete, you can profile the application as you would on any remote machine.
Copying all data to and from the login and compute nodes happens transparently and automatically. 4.2. Remote Profiling With nvprof  This section describes how to perform remote profiling by running nvprof manually on the remote system and then importing the collected profile data into the Visual Profiler .
4.2.1. Collect Data On Remote System  There are three common remote profiling use cases that can be addressed by using nvprof and the Visual Profiler.
Timeline The first use case is to collect a timeline of the application executing on the remote system.
The timeline should be collected in a way that most accurately reflects the behavior of the application.
You should copy this file back to the host system and then import it into the Visual Profiler as described in the next section.
Metrics And Events The second use case is to collect events or metrics for all kernels in an application for which you have already collected a timeline.
Collecting events or metrics for all kernels will significantly change the overall performance characteristics of the application because all kernel executions will be serialized on the GPU.
Even though overall application performance is changed, the event or metric values for individual kernels will be correct and so you can merge the collected event and metric values onto a previously collected timeline to get an accurate picture of the applications behavior.
prof You can collect any number of events and metrics for each nvprof invocation, and you can invoke nvprof multiple times to collect multiple metrics.prof files.
You should copy these files back to the host system and then import it into the Visual Profiler as described in the next section.
Analysis For Individual Kernel The third common remote profiling use case is to collect the metrics needed by the analysis system for an individual kernel.
When imported into the Visual Profiler this data will enable the analysis system to analyze the kernel and report optimization opportunities for that kernel.
It is important that the --kernels option appear before the --analysis-metrics option so that metrics are collected only for the kernel(s) specified by kernel specifier .
prof The profile data will be collected in analysis.prof. 4.2.2. View And Analyze Data  The collected profile data is viewed and analyzed by importing it into the Visual Profiler on the host system.
Timeline, Metrics And Events To view collected timeline data, the timeline.prof file can be imported into the Visual Profiler as described in Import Single-Process nvprof Session .
If metric or event data was also collected for the application, the corresponding metrics.prof file(s) can be imported into the Visual Profiler along with the timeline so that the events and metrics collected for each kernel are associated with the corresponding kernel in the timeline.
Guided Analysis For Individual Kernel To view collected analysis data for an individual kernel, the analysis.prof file can be imported into the Visual Profiler as described in Import Single-Process nvprof Session .
The timeline will show just the individual kernel that we specified during data collection.
After importing, the guided analysis system can be used to explore the optimization opportunities for the kernel. 5. NVIDIA Tools Extension  NVIDIA Tools Extension (NVTX) is a C-based Application Programming Interface (API) for annotating events, code ranges, and resources in your applications.
Applications which integrate NVTX can use the Visual Profiler to capture and visualize these events and ranges.
The sample program below shows the use of marker events, range events, and resource naming.
void Wait(int waitMilliseconds) { nvtxNameOsThread(“MAIN”); nvtxRangePush(__FUNCTION__); nvtxMark("Waiting..."); Sleep(waitMilliseconds); nvtxRangePop(); } int main(void) { nvtxNameOsThread("MAIN"); nvtxRangePush(__FUNCTION__); Wait(); nvtxRangePop(); } 5.1.
NVTX API Overview  Files The core NVTX API is defined in file nvToolsExt.h, whereas CUDA-specific extensions to the NVTX interface are defined in nvToolsExtCuda.h and nvToolsExtCudaRt.h.
On Linux the NVTX shared library is called libnvToolsExt.so and on macOS the shared library is called libnvToolsExt.dylib .
On Windows the library (.lib) and runtime components (.dll) are named nvToolsExt[bitness=32|64]_[version].
Function Calls All NVTX API functions start with an nvtx name prefix and may end with one of the three suffixes: A, W, or Ex.
NVTX functions with these suffixes exist in multiple variants, performing the same core functionality with different parameter encodings.
Depending on the version of the NVTX library, available encodings may include ASCII (A), Unicode (W), or event structure (Ex).
The CUDA implementation of NVTX only implements the ASCII (A) and event structure (Ex) variants of the API, the Unicode (W) versions are not supported and have no effect when called.
For example, the nvtxRangeStart() function returns a unique range identifier and nvtxRangePush() function outputs the current stack level.
It is recommended not to use the returned values as part of conditional code in the instrumented application.
The returned values can differ between various implementations of the NVTX library and, consequently, having added dependencies on the return values might work with one tool, but may fail with another. 5.2. NVTX API Events  Markers are used to describe events that occur at a specific time during the execution of an application, while ranges detail the time span in which they occur.
This information is presented alongside all of the other captured data, which makes it easier to understand the collected information.
The Ex version of the marker and range APIs also allows category, color, and payload attributes to be associated with the event using the event attributes structure. 5.2.1. NVTX Markers  A marker is used to describe an instantaneous event.
A marker can contain a text message or specify additional information using the event attributes structure .
Use nvtxMarkEx() to create a marker containing additional attributes specified by the event attribute structure.
The nvtxMarkW() function is not supported in the CUDA implementation of NVTX and has no effect if called.
Code Example nvtxMarkA ( "My mark" ); nvtxEventAttributes_t eventAttrib = { 0 }; eventAttrib .
NVTX Range Start/Stop  A start/end range is used to denote an arbitrary, potentially non-nested, time span.
A range can contain a text message or specify additional information using the event attributes structure .
Use nvtxRangeStartEx() to create a range containing additional attributes specified by the event attribute structure.
The nvtxRangeStartW() function is not supported in the CUDA implementation of NVTX and has no effect if called.
For the correlation of a start/end pair, a unique correlation ID is created that is returned from nvtxRangeStartA() or nvtxRangeStartEx() , and is then passed into nvtxRangeEnd() .
Code Example   non-overlapping range nvtxRangeId_t id1 = nvtxRangeStartA ( "My range" ); nvtxRangeEnd ( id1 ); nvtxEventAttributes_t eventAttrib = { 0 }; eventAttrib .
ascii = "my start/stop range" ; nvtxRangeId_t id2 = nvtxRangeStartEx ( & eventAttrib ); nvtxRangeEnd ( id2 );   overlapping ranges nvtxRangeId_t r1 = nvtxRangeStartA ( "My range 0" ); nvtxRangeId_t r2 = nvtxRangeStartA ( "My range 1" ); nvtxRangeEnd ( r1 ); nvtxRangeEnd ( r2 ); 5.2.3.
Use nvtxRangePushEx() to create a range containing additional attributes specified by the event attribute structure.
The nvtxRangePushW() function is not supported in the CUDA implementation of NVTX and has no effect if called.
If the pop does not have a matching push, a negative value is returned to indicate an error.
Code Example nvtxRangePushA ( "outer" ); nvtxRangePushA ( "inner" ); nvtxRangePop ();   end "inner" range nvtxRangePop ();   end "outer" range nvtxEventAttributes_t eventAttrib = { 0 }; eventAttrib .
Event Attributes Structure  The events attributes structure, nvtxEventAttributes_t , is used to describe the attributes of an event.
The layout of the structure is defined by a specific version of NVTX and can change between different versions of the Tools Extension library.
Attributes Markers and ranges can use attributes to provide additional information for an event or to guide the tool’s visualization of the data.
Each of the attributes is optional and if left unspecified, the attributes fall back to a default value.
Payload The payload attribute can be used to provide additional data for markers and ranges.
Initialization The caller should always perform the following three tasks when using attributes: Zero the structure Set the version field Set the size field Zeroing the structure sets all the event attributes types and values to the default value.
The version and size field are used by NVTX to handle multiple versions of the attributes structure.
It is recommended that the caller use the following method to initialize the event attributes structure.
NVTX Synchronization Markers  The NVTX synchronization module provides functions to support tracking additional synchronization details of the target application.
Naming OS synchronization primitives may allow users to better understand the data collected by traced synchronization APIs.
Additionally, annotating a user-defined synchronization object can allow the user to tell the tools when the user is building their own synchronization system that does not rely on the OS to provide behaviors, and instead uses techniques like atomic operations and spinlocks.
Code Example class MyMutex { volatile long bLocked ; nvtxSyncUser_t hSync ; public : MyMutex ( const char * name , nvtxDomainHandle_t d ) { bLocked = 0 ; nvtxSyncUserAttributes_t attribs = { 0 }; attribs .
ascii = name ; hSync = nvtxDomainSyncUserCreate ( d , & attribs ); } ~ MyMutex () { nvtxDomainSyncUserDestroy ( hSync ); } bool Lock () { nvtxDomainSyncUserAcquireStart ( hSync );  atomic compiler intrinsic bool acquired = __sync_bool_compare_and_swap ( & bLocked , 0 , 1 ); if ( acquired ) { nvtxDomainSyncUserAcquireSuccess ( hSync ); } else { nvtxDomainSyncUserAcquireFailed ( hSync ); } return acquired ; } void Unlock () { nvtxDomainSyncUserReleasing ( hSync ); bLocked = false ; } }; 5.3.
Each domain maintains its own categories thread range stacks registered strings The function nvtxDomainDestroy() marks the end of the domain.
Destroying a domain unregisters and destroys all objects associated with it such as registered strings, resource objects, named categories, and started ranges.
Code Example nvtxDomainHandle_t domain = nvtxDomainCreateA ( "Domain_A" ); nvtxMarkA ( "Mark_A" ); nvtxEventAttributes_t attrib = { 0 }; attrib .
ascii = "Mark A Message" ; nvtxDomainMarkEx ( NULL , & attrib ); nvtxDomainDestroy ( domain ); 5.4.
NVTX Resource Naming  NVTX resource naming allows custom names to be associated with host OS threads and CUDA resources such as devices, contexts, and streams.
The nvtxNameOsThreadW() function is not supported in the CUDA implementation of NVTX and has no effect if called.
Windows nvtxNameOsThread ( GetCurrentThreadId (), "MAIN_THREAD" );   Linux/Mac nvtxNameOsThread ( pthread_self (), "MAIN_THREAD" ); CUDA Runtime Resources The nvtxNameCudaDeviceA() and nvtxNameCudaStreamA() functions are used to name CUDA device and stream objects, respectively.
The nvtxNameCudaDeviceW() and nvtxNameCudaStreamW() functions are not supported in the CUDA implementation of NVTX and have no effect if called.
nvtxNameCudaDeviceA ( 0 , "my cuda device 0" ); cudaStream_t cudastream ; cudaStreamCreate ( & cudastream ); nvtxNameCudaStreamA ( cudastream , "my cuda stream" ); CUDA Driver Resources The nvtxNameCuDeviceA() , nvtxNameCuContextA() and nvtxNameCuStreamA() functions are used to name CUDA driver device, context and stream objects, respectively.
The nvtxNameCuDeviceW() , nvtxNameCuContextW() and nvtxNameCuStreamW() functions are not supported in the CUDA implementation of NVTX and have no effect if called.
CUdevice device ; cuDeviceGet ( & device , 0 ); nvtxNameCuDeviceA ( device , "my device 0" ); CUcontext context ; cuCtxCreate ( & context , 0 , device ); nvtxNameCuContextA ( context , "my context" ); cuStream stream ; cuStreamCreate ( & stream , 0 ); nvtxNameCuStreamA ( stream , "my stream" ); 5.5.
NVTX String Registration  Registered strings are intended to increase performance by lowering instrumentation overhead.
String may be registered once and the handle may be passed in place of a string where an the APIs may allow.
The nvtxDomainRegisterStringW() function is not supported in the CUDA implementation of NVTX and has no effect if called.
nvtxDomainHandle_t domain = nvtxDomainCreateA ( "Domain_A" ); nvtxStringHandle_t message = nvtxDomainRegisterStringA ( domain , "registered string" ); nvtxEventAttributes_t eventAttrib = { 0 }; eventAttrib .
Automatic MPI Annotation with NVTX  You can annotate MPI calls with NVTX markers to profile, trace and visualize them.
It can get tedious to wrap every MPI call with NVTX markers, but there are two ways to do this automatically: Built-in annotation nvprof has a built-in option that supports two MPI implementations - OpenMPI and MPICH.
If you have either of these installed on your system, you can use the --annotate-mpi option and specify your installed MPI implementation.
If you use this option, nvprof will generate NVTX markers every time your application makes MPI calls.
Additionally, we use NVTX to rename the current thread and current device object to indicate the MPI rank.
For example if you have OpenMPI installed, you can annotate your application using the command: $ mpirun - np 2 nvprof -- annotate - mpi openmpi .
/ my_mpi_app This will give you output that looks something like this: NVTX result : Thread "MPI Rank 0" ( id = 583411584 ) Domain "" Range "MPI_Reduce" Type Time ( % ) Time Calls Avg Min Max Name Range : 100.00 % 16.652 us 1 16.652 us 16.652 us 16.652 us MPI_Reduce ...
Range "MPI_Scatter" Type Time ( % ) Time Calls Avg Min Max Name Range : 100.00 % 3.0320 ms 1 3.0320 ms 3.0320 ms 3.0320 ms MPI_Scatter ...
NVTX result : Thread "MPI Rank 1" ( id = 199923584 ) Domain "" Range "MPI_Reduce" Type Time ( % ) Time Calls Avg Min Max Name Range : 100.00 % 21.062 us 1 21.062 us 21.062 us 21.062 us MPI_Reduce ...
Range "MPI_Scatter" Type Time ( % ) Time Calls Avg Min Max Name Range : 100.00 % 85.296 ms 1 85.296 ms 85.296 ms 85.296 ms MPI_Scatter ...
Custom annotation If your system has a version of MPI that is not supported by nvprof, or if you want more control over which MPI functions are annotated and how the NVTX markers are generated, you can create your own annotation library, and use the environment variable LD_PRELOAD to intercept MPI calls and wrap them with NVTX markers.
You can create this annotation library conveniently using the documentation and open-source scripts located here . 6.2. Manual MPI Profiling  To use nvprof to collect the profiles of the individual MPI processes, you must tell nvprof to send its output to unique files.
However, you can now easily do it utilizing the %h , %p and %q{ENV} features of the --export-profile argument to the nvprof command.
/ my_mpi_app Alternatively, one can make use of the new feature to turn on profiling on the nodes of interest using the --profile-all-processes argument to nvprof .
/ my_mpi_app Any processes that run on the node where the --profile-all-processes is running will automatically get profiled.
Note that the %q{OMPI_COMM_WORLD_RANK} option will not work here, because this environment variable will not be available in the shell where nvprof is running.
Starting CUDA 7.5, you can name threads and CUDA contexts just as you name output files with the options –process-name and –context-name, by passing a string like "MPI Rank %q{OMPI_COMM_WORLD_RANK}" as a parameter.
This feature is useful to spot resources associated with a specific rank when user imports multiple files into the same time-line in the Visual Profiler.
$ mpirun - np 2 - host c0 -0 , c0 -1 nvprof -- process - name "MPI Rank %q{OMPI_COMM_WORLD_RANK}" -- context - name "MPI Rank %q{OMPI_COMM_WORLD_RANK}" - o output .
Further Reading  Details about what types of additional arguments to use with nvprof can be found in the Multiprocess Profiling and Redirecting Output section.
Additional information about how to view the data with the Visual Profiler can be found in the Import Single-Process nvprof Session and Import Multi-Process nvprof Session sections.
The blog post Profiling MPI Applications shows how to use new output file naming of nvprof introduced in CUDA 6.5 and NVTX library to name various resources to analyze the performance of a MPI application.
The blog post Track MPI Calls in the Visual Profiler shows how Visual Profiler, combined with PMPI and NVTX can give interesting insights into how the MPI calls in your application interact with the GPU. 7. MPS Profiling  You can collect profiling data for a CUDA application using Multi-Process Service(MPS) with nvprof and then view the timeline by importing the data in the Visual Profiler.
7.1. MPS profiling with Visual Profiler  Visual Profiler can be run on a particular MPS client or for all MPS clients.
Event or metric profiling results in serialization - only one MPS client will execute at a time.
nvidia-cuda-mps-control -d In Visual Profiler open “New Session” wizard using main menu “File->New Session”.
Run the application in a separate terminal To end profiling press the “Cancel” button on progress dialog in Visual Profiler.
Note that the profiling output also includes data for the CUDA MPS server processes which have process name nvidia-cuda-mps-server . 7.2. MPS profiling with nvprof  nvprof can be run on a particular MPS client or for all MPS clients.
nvidia-cuda-mps-control -d Run nvprof with --profile-all-processes argument and to generate separate output files for each process use the %p feature of the --export-profile argument.
nvprof --profile-all-processes -o output_%p Run the application in a separate terminal Exit nvprof by typing “Ctrl-c”. 7.3. Viewing nvprof MPS timeline in Visual Profiler  Import the nvprof generated data files for each process using the multi-process import option.
Note that the Compute and kernel timeline row shows three kernels overlapping. 8. Dependency Analysis  The dependency analysis feature enables optimization of the program runtime and concurrency of applications utilizing multiple CPU threads and CUDA streams.
It allows to compute the critical path of a specific execution, detect waiting time and inspect dependencies between functions executing in different threads or streams. 8.1. Background  The dependency analysis in nvprof and the Visual Profiler is based on execution traces of applications.
A trace captures all relevant activities such as API function calls or CUDA kernels along with their timestamps and durations.
Given this execution trace and a model of the dependencies between those activities on different threads/streams, a dependency graph can be constructed.
Typical dependencies modelled in this graph would be that a CUDA kernel can not start before its respective launch API call or that a blocking CUDA stream synchronization call can not return before all previously enqueued work in this stream has been completed.
A wait state is the duration for which an activity such as an API function call is blocked waiting on an event in another thread or stream.
Given the previous stream synchronization example, the synchronizing API call is blocked for the time it has to wait on any GPU activity in the respective CUDA stream.
Knowledge about where wait states occur and how long functions are blocked is helpful to identify optimization opportunities for more high-level concurrency in the application.
In addition to individual wait states, the critical path through the captured event graph enables to pinpoint those function calls, kernel and memory copies that are responsible for the total application runtime.
The critical path is the longest path through an event graph that does not contain wait states, i.e.
optimizing activities on this path can directly improve the execution time. 8.2. Metrics  Waiting Time A wait state is the duration for which an activity such as an API function call is blocked waiting on an event in another thread or stream.
In the example below, the blocking CUDA synchronization API calls are waiting on their respective kernels to finish executing on the GPU.
Instead of waiting immediately, one should attempt to overlap the kernel executions with concurrent CPU work with a similar runtime, thereby reducing the time that any computing device (CPU or GPU) is blocked.
Time on Critical Path The critical path is the longest path through an event graph that does not contain wait states, i.e.
Activities with a high time on the critical path have a high direct impact on the application runtime.
In the example pictured below, copy_kernel is on the critical path since the CPU is blocked waiting for it to finish in cudeDeviceSynchronize .
Reducing the kernel runtime allows the CPU to return earlier from the API call and continue program execution.
Since no execution stream is waiting on this kernel to finish, reducing its duration will likely not improve the overall application runtime. 8.3. Support  The following programming APIs are currently supported for dependency analysis CUDA runtime and driver API POSIX threads (Pthreads), POSIX mutexes and condition variables Dependency analysis is available in Visual Profiler and nvprof .
A Dependency Analysis stage can be selected in the Unguided Application Analysis and new Dependency Analysis Controls are available for the timeline.
See section Dependency Analysis on how to use this feature in nvprof . 8.4. Limitations  The dependency and wait time analysis between different threads and CUDA streams only takes into account execution dependencies stated in the respective supported API contracts.
For example, asynchronous memory copies enqueued into independent CUDA streams will not be marked dependent even if the concrete GPU has only a single copy engine.
For example, a CPU thread actively polling for a value at some memory location (busy-waiting) will not be considered blocked on another concurrent activity.
The dependency analysis has only limited support for applications using CUDA Dynamic Parallelism (CDP).
CDP kernels can use CUDA API calls from the GPU which are not tracked via the CUPTI Activity API.
Therefore, the analysis cannot determine the full dependencies and waiting time for CDP kernels.
As a result the critical path will always include the last CDP kernel of each host-launched kernel.
The dependency analysis does not support API functions cudaLaunchCooperativeKernelMultiDevice or cuLaunchCooperativeKernelMultiDevice .
Kernel launched by either of these API functions might not be tracked correctly. 9. Metrics Reference  This section contains detailed descriptions of the metrics that can be collected by nvprof and the Visual Profiler.
A scope value of “Single-context” indicates that the metric can only be accurately collected when a single context (CUDA or graphic) is executing on the GPU.
A scope value of “Multi-context” indicates that the metric can be accurately collected when multiple contexts are executing on the GPU.
A scope value of “Device” indicates that the metric will be collected at device level, that is it will include values for all the contexts executing on the GPU.
Note that, NVLink metrics collected for kernel mode exhibit the behavior of “Single-context”. 9.1. Metrics for Capability 5.x  Devices with compute capability 5.x implement the metrics shown in the following table.
Note that for some metrics the “Multi-context” scope is supported only for specific devices.
Multi-context * dram_utilization The utilization level of the device memory relative to the peak utilization on a scale of 0 to 10 Multi-context * dram_write_bytes Total bytes written from L2 cache to DRAM.
Multi-context * eligible_warps_per_cycle Average number of warps that are eligible to issue per active cycle Multi-context flop_count_dp Number of double-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate).
Multi-context flop_count_dp_add Number of double-precision floating-point add operations executed by non-predicated threads.
Multi-context flop_count_dp_fma Number of double-precision floating-point multiply-accumulate operations executed by non-predicated threads.
Multi-context flop_count_dp_mul Number of double-precision floating-point multiply operations executed by non-predicated threads.
Multi-context flop_count_hp Number of half-precision floating-point operations executed by non-predicated threads (add, multiply and multiply-accumulate).
Multi-context * flop_count_hp_add Number of half-precision floating-point add operations executed by non-predicated threads.
Multi-context * flop_count_hp_fma Number of half-precision floating-point multiply-accumulate operations executed by non-predicated threads.
Multi-context * flop_count_hp_mul Number of half-precision floating-point multiply operations executed by non-predicated threads.
Multi-context * flop_count_sp Number of single-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate).
Multi-context flop_count_sp_add Number of single-precision floating-point add operations executed by non-predicated threads.
Multi-context flop_count_sp_fma Number of single-precision floating-point multiply-accumulate operations executed by non-predicated threads.
Multi-context flop_count_sp_mul Number of single-precision floating-point multiply operations executed by non-predicated threads.
Multi-context flop_count_sp_special Number of single-precision floating-point special operations executed by non-predicated threads.
Multi-context flop_dp_efficiency Ratio of achieved to peak double-precision floating-point operations Multi-context flop_hp_efficiency Ratio of achieved to peak half-precision floating-point operations.
Multi-context * flop_sp_efficiency Ratio of achieved to peak single-precision floating-point operations Multi-context gld_efficiency Ratio of requested global memory load throughput to required global memory load throughput expressed as percentage.
Multi-context * gld_requested_throughput Requested global memory load throughput Multi-context gld_throughput Global memory load throughput Multi-context * gld_transactions Number of global memory load transactions Multi-context * gld_transactions_per_request Average number of global memory load transactions performed for each global memory load.
Multi-context * global_atomic_requests Total number of global atomic(Atom and Atom CAS) requests from Multiprocessor Multi-context global_hit_rate Hit rate for global loads in unified l1/tex cache.
Multi-context * global_load_requests Total number of global load requests from Multiprocessor Multi-context global_reduction_requests Total number of global reduction requests from Multiprocessor Multi-context global_store_requests Total number of global store requests from Multiprocessor.
Multi-context gst_efficiency Ratio of requested global memory store throughput to required global memory store throughput expressed as percentage.
Multi-context * gst_requested_throughput Requested global memory store throughput Multi-context gst_throughput Global memory store throughput Multi-context * gst_transactions Number of global memory store transactions Multi-context * gst_transactions_per_request Average number of global memory store transactions performed for each global memory store Multi-context * half_precision_fu_utilization The utilization level of the multiprocessor function units that execute 16 bit floating-point instructions and integer instructions on a scale of 0 to 10.
Multi-context * l2_local_load_bytes Bytes read from L2 for misses in Unified Cache for local loads Multi-context * l2_read_throughput Memory read throughput seen at L2 cache for all read requests Multi-context * l2_read_transactions Memory read transactions seen at L2 cache for all read requests Multi-context * l2_surface_atomic_store_bytes Bytes transferred between Unified Cache and L2 for surface atomics (ATOM and ATOM CAS) Multi-context * l2_surface_load_bytes Bytes read from L2 for misses in Unified Cache for surface loads Multi-context * l2_surface_reduction_bytes Bytes written to L2 from Unified Cache for surface reductions Multi-context * l2_surface_store_bytes Bytes written to L2 from Unified Cache for surface stores.
Multi-context * l2_tex_hit_rate Hit rate at L2 cache for all requests from texture cache Multi-context * l2_tex_read_hit_rate Hit rate at L2 cache for all read requests from texture cache.
Multi-context * l2_tex_read_throughput Memory read throughput seen at L2 cache for read requests from the texture cache Multi-context * l2_tex_read_transactions Memory read transactions seen at L2 cache for read requests from the texture cache Multi-context * l2_tex_write_hit_rate Hit Rate at L2 cache for all write requests from texture cache.
Multi-context sysmem_utilization The utilization level of the system memory relative to the peak utilization on a scale of 0 to 10.
Multi-context * sysmem_write_bytes Number of bytes written to system memory Multi-context * sysmem_write_throughput System memory write throughput Multi-context * sysmem_write_transactions Number of system memory write transactions Multi-context * sysmem_write_utilization The write utilization level of the system memory relative to the peak utilization on a scale of 0 to 10.
Multi-context dram_utilization The utilization level of the device memory relative to the peak utilization on a scale of 0 to 10 Multi-context dram_write_bytes Total bytes written from L2 cache to DRAM Multi-context dram_write_throughput Device memory write throughput.
Multi-context eligible_warps_per_cycle Average number of warps that are eligible to issue per active cycle Multi-context flop_count_dp Number of double-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate).
Multi-context flop_count_hp Number of half-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate).
Multi-context flop_count_hp_add Number of half-precision floating-point add operations executed by non-predicated threads.
Multi-context flop_count_hp_fma Number of half-precision floating-point multiply-accumulate operations executed by non-predicated threads.
Multi-context flop_count_hp_mul Number of half-precision floating-point multiply operations executed by non-predicated threads.
Multi-context flop_count_sp Number of single-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate).
Multi-context flop_dp_efficiency Ratio of achieved to peak double-precision floating-point operations Multi-context flop_hp_efficiency Ratio of achieved to peak half-precision floating-point operations Multi-context flop_sp_efficiency Ratio of achieved to peak single-precision floating-point operations Multi-context gld_efficiency Ratio of requested global memory load throughput to required global memory load throughput expressed as percentage.
Multi-context gld_requested_throughput Requested global memory load throughput Multi-context gld_throughput Global memory load throughput Multi-context gld_transactions Number of global memory load transactions Multi-context gld_transactions_per_request Average number of global memory load transactions performed for each global memory load.
Multi-context global_atomic_requests Total number of global atomic(Atom and Atom CAS) requests from Multiprocessor Multi-context global_hit_rate Hit rate for global loads in unified l1/tex cache.
Multi-context global_load_requests Total number of global load requests from Multiprocessor Multi-context global_reduction_requests Total number of global reduction requests from Multiprocessor Multi-context global_store_requests Total number of global store requests from Multiprocessor.
Multi-context l2_local_load_bytes Bytes read from L2 for misses in Unified Cache for local loads Multi-context l2_read_throughput Memory read throughput seen at L2 cache for all read requests Multi-context l2_read_transactions Memory read transactions seen at L2 cache for all read requests Multi-context l2_surface_atomic_store_bytes Bytes transferred between Unified Cache and L2 for surface atomics (ATOM and ATOM CAS) Multi-context l2_surface_load_bytes Bytes read from L2 for misses in Unified Cache for surface loads Multi-context l2_surface_reduction_bytes Bytes written to L2 from Unified Cache for surface reductions Multi-context l2_surface_store_bytes Bytes written to L2 from Unified Cache for surface stores.
Multi-context l2_tex_hit_rate Hit rate at L2 cache for all requests from texture cache Multi-context l2_tex_read_hit_rate Hit rate at L2 cache for all read requests from texture cache.
Multi-context l2_tex_read_throughput Memory read throughput seen at L2 cache for read requests from the texture cache Multi-context l2_tex_read_transactions Memory read transactions seen at L2 cache for read requests from the texture cache Multi-context l2_tex_write_hit_rate Hit Rate at L2 cache for all write requests from texture cache.
Device nvlink_overhead_data_transmitted Ratio of overhead data to the total data, transmitted through NVLink.
Device nvlink_total_data_received Total data bytes received through NVLinks including headers.
Device nvlink_total_data_transmitted Total data bytes transmitted through NVLinks including headers.
Device nvlink_total_nratom_data_transmitted Total non-reduction atomic data bytes transmitted through NVLinks.
Device nvlink_total_ratom_data_transmitted Total reduction atomic data bytes transmitted through NVLinks This is available for compute capability 6.0.
Device nvlink_total_response_data_received Total response data bytes received through NVLink, response data includes data for read requests and result of non-reduction atomic requests.
Device nvlink_total_write_data_transmitted Total write data bytes transmitted through NVLinks.
Device nvlink_user_data_received User data bytes received through NVLinks, doesn’t include headers.
Device nvlink_user_data_transmitted User data bytes transmitted through NVLinks, doesn’t include headers.
Device nvlink_user_nratom_data_transmitted Total non-reduction atomic user data bytes transmitted through NVLinks.
Device nvlink_user_ratom_data_transmitted Total reduction atomic user data bytes transmitted through NVLinks.
Device nvlink_user_response_data_received Total user response data bytes received through NVLink, response data includes data for read requests and result of non-reduction atomic requests.
Device nvlink_user_write_data_transmitted User write data bytes transmitted through NVLinks.
Multi-context sysmem_write_bytes Number of bytes written to system memory Multi-context sysmem_write_throughput System memory write throughput Multi-context sysmem_write_transactions Number of system memory write transactions Multi-context sysmem_write_utilization The write utilization level of the system memory relative to the peak utilization on a scale of 0 to 10.
Multi-context tex_cache_hit_rate Unified cache hit rate Multi-context tex_cache_throughput Unified cache throughput Multi-context tex_cache_transactions Unified cache read transactions Multi-context tex_fu_utilization The utilization level of the multiprocessor function units that execute global, local and texture memory instructions on a scale of 0 to 10 Multi-context tex_utilization The utilization level of the unified cache relative to the peak utilization on a scale of 0 to 10 Multi-context texture_load_requests Total number of texture Load requests from Multiprocessor Multi-context unique_warps_launched Number of warps launched.
Multi-context warp_execution_efficiency Ratio of the average active threads per warp to the maximum number of threads per warp supported on a multiprocessor Multi-context warp_nonpred_execution_efficiency Ratio of the average active threads per warp executing non-predicated instructions to the maximum number of threads per warp supported on a multiprocessor Multi-context 9.3.
Metrics for Capability 7.x  Devices with compute capability 7.x implement the metrics shown in the following table.
Multi-context global_atomic_requests Total number of global atomic(Atom and Atom CAS) requests from Multiprocessor Multi-context global_hit_rate Hit rate for global load and store in unified l1/tex cache Multi-context global_load_requests Total number of global load requests from Multiprocessor Multi-context global_reduction_requests Total number of global reduction requests from Multiprocessor Multi-context global_store_requests Total number of global store requests from Multiprocessor.
Multi-context gst_requested_throughput Requested global memory store throughput Multi-context gst_throughput Global memory store throughput Multi-context gst_transactions Number of global memory store transactions Multi-context gst_transactions_per_request Average number of global memory store transactions performed for each global memory store Multi-context half_precision_fu_utilization The utilization level of the multiprocessor function units that execute 16 bit floating-point instructions on a scale of 0 to 10.
Device nvlink_total_ratom_data_transmitted Total reduction atomic data bytes transmitted through NVLinks.
The warp can have following states: Instruction issued - An instruction or a pair of independent instructions was issued from a warp.
The stall reason distribution can be seen at source level in PC Sampling View or at kernel level in Latency analysis using ‘Examine Stall Reasons’ Stalled for instruction fetch - The next instruction was not yet available.
To reduce instruction fetch stalls: If large loop have been unrolled in kernel, try reducing them.
If the kernel contains many calls to small function, try inlining more of them with the __inline__ or __forceinline__ qualifiers.
Conversely, if inlining many functions or large functions, try __noinline__ to disable inlining of those functions.
Occasional calls to __syncthreads() will then keep the warps in sync which may improve instruction cache hit rate.
Stalled for execution dependency - The next instruction is waiting for one or more of its inputs to be computed by earlier instruction(s).
To reduce execution dependency stalls, try to increase instruction-level parallelism (ILP).
This can be done by, for example, increasing loop unrolling or processing several elements per thread.
Stalled for memory dependency - The next instruction is waiting for a previous memory accesses to complete.
To reduce the memory dependency stalls Try to improve memory coalescing and/or efficiency of bytes fetched (alignment, etc.).
Look at the source level analysis ‘Global Memory Access Pattern’ and/or the metrics gld_efficiency and gst_efficiency.
Try to increase memory-level parallelism (MLP): the number of independent memory operations in flight per thread.
Loop unrolling, loading vector types such as float4, and processing multiple elements per thread are all ways to increase memory-level parallelism.
Consider moving frequently-accessed data closer to SM, such as by use of shared memory or read-only data cache.
If local memory accesses are high, consider increasing register count per thread to reduce spilling, even at the expense of occupancy since local memory accesses are cached only in L2 for GPUs with compute capability major = 5.
Stalled for memory throttle - A large number of outstanding memory requests prevents forward progress.
On GPUs with compute capability major = 3, memory throttle indicates high number of memory replays.
To reduce memory throttle stalls: Try to find ways to combine several memory transactions into one (e.g., use 64-bit memory requests instead of two 32-bit requests).
Check for un-coalesced memory accesses using the source level analysis ‘Global Memory Access Pattern’ and/or the profiler metrics gld_efficiency and gst_efficiency; minimize them wherever possible.
On GPUs with compute capability major >= 3, consider using read-only data cache using LDG for un-coalesced global reads Stalled for texture - The texture sub-system is fully utilized or has too many outstanding requests.
To reduce texture stalls: Consider combining several texture fetch operations into one (e.g., packing data in texture and unpacking in SM or using vector loads).
On GPUs with compute capability major = 3: If __syncthreads() is being used because of data exchange through shared memory within a threadblock, consider whether warp shuffle operations can be used in place of some of these exchange/synchronize sequences.
Stalled for constant memory dependency - The warp is stalled on a miss in the cache for __constant__ memory and immediate.
This may be high the first time each constant is accessed (e.g., at the beginning of a kernel).
To reduce these stalls, Consider reducing use of __constant__ or increase kernel runtime by increasing block count Consider increasing number of items processed per thread Consider merging several kernels that use the same __constant__ data to amortize the cost of misses in the constant cache.
Stalled for pipe busy - The warp is stalled because the functional unit required to execute the next instruction is busy.
To reduce stalls due to pipe busy: Prefer high-throughput operations over low-throughput operations.
Look for arithmetic improvements (e.g., order-of-operations changes) that may be mathematically valid but unsafe for the compiler to do automatically.
Stalled for not selected - Warp was ready but did not get a chance to issue as some other warp was selected for issue.
This reason generally indicates that kernel is possibly optimized well but in some cases, you may be able to decrease occupancy without impacting latency hiding, and doing so may help improve cache hit rates.
Stalled for other - Warp is blocked for an uncommon reason like compiler or hardware reasons.
Developers do not have control over these stalls. 11. Migrating to Nsight Tools from Visual Profiler and nvprof  Visual Profiler and nvprof will be deprecated in a future CUDA release.
The new tools are powerful, fast, and feature rich, allowing you to find solutions even more quickly.
NVIDIA Nsight Systems is a system-wide performance analysis tool designed to visualize an application’s algorithms, help you identify the largest opportunities to optimize, and tune to scale efficiently across any quantity or size of CPUs and GPUs; from large servers to our smallest SoC.
Refer to the Migrating from NVIDIA nvprof section section in the NVIDIA Nsight Systems User Guide NVIDIA Nsight Compute is an interactive kernel profiler for CUDA applications.
It provides detailed performance metrics and API debugging via a user interface and command line tool.
Nsight Compute provides a customizable and data-driven user interface and metric collection and can be extended with analysis scripts for post-processing results.
Also refer to the blog posts on how to move your development to the next-generation tools: Migrating to Nsight Tools from Visual Profiler and nvprof Transitioning to Nsight Systems from Visual Profiler and nvprof Using Nsight Compute to Inspect your Kernels Table 7.
Which tools are available on which GPU architectures  GPU architecture Visual Profiler and nvprof Nsight Systems Nsight Compute Maxwell Yes No No Pascal Yes Yes No Volta Yes Yes Yes Turing Yes* Yes Yes Ampere and later GPU architectures No Yes Yes * Only Tracing functionality is supported - Timeline, Activity, API.
The following table maps the key features of Visual Profiler and nvprof to the NVIDIA Nsight tools Table 8.
Mapping of key Visual Profiler and nvprof features  Visual Profiler/nvprof feature categories Nsight Systems Nsight Compute Timeline/Activity/API Tracing Yes CPU Sampling Yes OpenACC Yes OpenMP Yes MPI Yes MPS Yes Application Dependency Analysis Unified Memory Transfers Yes Unified Memory Page Faults Yes Application Unified Memory Analysis Application NVLink Analysis Yes (per kernel) Events and Metrics (per kernel) Yes Guided and Unguided Kernel Analysis Yes Kernel Source-Disassembly View Yes Kernel PC Sampling Yes NVTX Yes Yes Remote Profiling Yes Yes 12.
Starting with the CUDA 11.0, Visual Profiler and nvprof won’t support macOS as the target platform.
Thus it’s required to set the path to the CUPTI library before launching Visual Profiler and nvprof on Windows.
CUPTI library can be found at "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\\extras\CUPTI\lib64" for Windows.
A security vulnerability issue required profiling tools to disable features using GPU performance counters for non-root or non-admin users when using a Windows 419.17 or Linux 418.43 or later driver.
By default, NVIDIA drivers require elevated permissions to access GPU performance counters.
On other platforms, you can either start profiling as root or using sudo, or by enabling non-admin profiling.
More details about the issue and the solutions can be found on the ERR_NVGPUCTRPERM web page .
Note Visual Profiler and nvprof allow tracing features for non-root and non-admin users on desktop platforms only, Tegra platforms require root or sudo access.
Use of the environment variable LD_PRELOAD to load some versions of MPI libraries may result in a crash on Linux platforms.
To ensure that all profile data is collected and flushed to a file, cudaDeviceSynchronize() followed by either cudaProfilerStop() or cuProfilerStop() should be called before the application exits.
Concurrent kernel mode can add significant overhead if used on kernels that execute a large number of blocks and that have short execution durations.
If the kernel launch rate is very high, the device memory used to collect profiling data can run out.
When profiling an application that uses CUDA Dynamic Parallelism (CDP) there are several limitations to the profiling tools.
CDP kernel launch tracing has a limitation for devices with compute capability 7.0 and higher.
Profiler traces all the host launched kernels until it encounters a host launched kernel which launches child kernels.
The Visual Profiler timeline does not display CUDA API calls invoked from within device-launched kernels.
The Visual Profiler does not display detailed event, metric, and source-level results for device-launched kernels.
Event, metric, and source-level results collected for CPU-launched kernels will include event, metric, and source-level results for the entire call-tree of kernels launched from within that kernel.
Events/metrics collected for CPU-launched kernels will include events/metrics for the entire call-tree of kernels launched from within that kernel.
When profiling an application in which a device kernel was stopped due to an assertion the profiling data will be incomplete and a warning or error message is displayed.
For dependency analysis, in cases where activity timestamps in the trace are slightly distorted such that they violate the programming model constraints, no dependencies or waiting times can be analyzed.
Devices with compute capability 6.0 and higher introduce a new feature, compute preemption, to give fair chance for all compute contexts while running long tasks.
With compute preemption feature- If multiple contexts are running in parallel it is possible that long kernels will get preempted.
If kernel has been preempted, the time the kernel spends preempted is still counted towards kernel duration.
This can affect the kernel optimization priorities given by Visual Profiler as there is randomness introduced due to preemption.
The following are known issues with the current release: Events and metrics collection for a MPS client can result in higher counts than expected on devices with compute capability 7.0 and higher, since MPS client may get preempted due to termination of another MPS client.
Events warps_launched and sm_cta_launched and metric inst_per_warp might provide higher counts than expected on devices with compute capability 6.0 and 6.1.
Metric unique_warps_launched can be used in place of warps_launched to get correct count of actual warps launched as it is not affected by compute preemption.
To avoid compute preemption affecting profiler results try to isolate the context being profiled: Run the application on secondary GPU where display is not connected.
On Linux if the application is running on the primary GPU where the display driver is connected then unload the display driver.
When the kernel is scheduled for the first time, all the pages allocated using cudaMallocManaged and that are required for execution of the kernel are fetched in the global memory when GPU faults are generated.
Profiler requires multiple passes to collect all the metrics required for kernel analysis.
For devices with compute capability 6.0 and higher and platforms supporting Unified memory, in the first kernel iteration the GPU faults will be generated and all pages will be fetched in the global memory.
The time taken from trace will include the time required to fetch the pages but most of the metrics profiled in multiple iterations will not include time/cycles required to fetch the pages.
CUDA device enumeration and order, typically controlled through environment variables CUDA_VISIBLE_DEVICES and CUDA_DEVICE_ORDER , should remain the same for the profiler and the application.
CUDA profiling might not work on systems that contain a mixture of supported and unsupported GPUs.
On such systems, either set option --devices to supported devices in nvprof , or set environment variable CUDA_VISIBLE_DEVICES before launching nvprof or the Visual Profiler.
Because of the low resolution of the timer on Windows, the start and end timestamps can be same for activities having short execution duration on Windows.
As a result, the nvprof and Visual Profiler report the following warning: “Found N invalid records in the result.” Profiler cannot interoperate with other Nvidia tools such as cuda-gdb, cuda-memcheck, Nsight Systems and Nsight Compute.
OpenACC profiling might fail when OpenACC library is linked statically in the user application.
This happens due to the missing definition of the OpenACC API routines needed for the OpenACC profiling, as compiler might ignore definitions for the functions not used in the application.
Visual Profiler and nvprof versions shipped in the CUDA Toolkit 11.7 and CUDA Toolkit 11.8 don’t support Kepler (sm_35 and sm_37) devices.
Refer to the webpages CUPTI 11.7 and CUPTI 11.8 for location of the CUPTI packages having the support for these Kepler devices.
Profiler is not supported on below system configurations: 64-bit ARM Server CPU architecture (arm64 SBSA).
Visual Profiler The following are known issues related to Visual Profiler: Visual Profiler requires Java Runtime Environment (JRE) 1.8 to be available on the local system.
When these analyses are attempted on a device where the metric is not available the analysis results will show that the required data is “not available”.
Using the mouse wheel button to scroll does not work within the Visual Profiler on Windows.
Since Visual Profiler uses nvprof for collecting profiling data, nvprof limitations also apply to Visual Profiler.
Visual Profiler cannot load profiler data larger than the memory size limited by JVM or available memory on the system.
Visual Profiler global menus do not show properly or are empty on some versions of Ubuntu.
One workaround is to set environment variable “UBUNTU_MENUPROXY=0” before running Visual Profiler In the Visual Profiler the NVLink Analysis diagram can be incorrect after scrolling the diagram.
Visual Profiler might not be able to show NVLink events on the timeline when large number of samples are collected.
For unified memory profiling on a remote setup having different version of GCC than host machine, Visual Profiler might not be able to show the source code location for CPU page fault events.
For unified memory profiling on a remote setup having different architecture than the host machine (x86 versus POWER), Visual Profiler might not be able to show the source code location for CPU page fault and allocation tracking events.
The workaround is to run nvprof on the target and load the nvprof output in the Visual Profiler.
For remote profiling, the CUDA Toolkit installed on the host system must support the target device on the remote system.
Visual Profiler might show strange symbol fonts on platforms which don’t have required fonts installed.
When using remote profiling if there is a connection failure due to key exchange failure, then you will get an error message “Unable to establish shell connection to ‘user @ xxx ’”.
Check the SSH daemon config file (default path is /etc/ssh/sshd_config) on the target Comment out lines starting with: KexAlgorithms HostbasedAcceptedKeyTypes Ciphers HostKey AuthorizedKeysFile Re-generate keys sudo ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key Restart sshd service sudo services sshd restart Accessing the local help document from Visual Profiler leads to HTTP Error 500.
nvprof The following are known issues related to nvprof : nvprof cannot profile processes that fork() but do not then exec() .
nvprof assumes it has access to the temporary directory on the system, which it uses to store temporary profiling data.
When multiple nvprof processes are run simultaneously on the same node, there is an issue of contention for files under the temporary directory.
Multiple nvprof processes running concurrently using application replay may generate incorrect results or no results at all.
To profile application on Android $TMPDIR environment variable has to be defined and point to a user-writable folder.
nvprof tries to disable auto boost by default, it might fail to do so in some conditions, but profiling will continue.
Profiling a C++ application which overloads the new operator at the global scope and uses any CUDA APIs like cudaMalloc() or cudaMallocManaged() inside the overloaded new operator will result in a hang.
NVTX annotations will not work when profiling all processes using the nvprof option --profile-all-processes .
It is advised to set the environment variable NVTX_INJECTION64_PATH to point to the profiler injection library, libcuinj64.so on Linux and cuinj64_*.dll on Windows, before launching the application.
Events and Metrics The following are known issues related to Events and Metrics profiling: Profiling features for devices with compute capability 7.5 and higher are supported in the NVIDIA Nsight Compute .
Visual Profiler does not support Guided Analysis, some stages under Unguided Analysis and events and metrics collection for devices with compute capability 7.5 and higher.
One can launch the NVIDIA Nsight Compute UI for devices with compute capability 7.5 and higher from Visual Profiler.
Also nvprof does not support query and collection of events and metrics, source level analysis and other options used for profiling on devices with compute capability 7.5 and higher.
Events or metrics collection may significantly change the overall performance characteristics of the application because all kernel executions are serialized on the GPU.
This includes synchronization between the host and the device build upon value-based CUDA stream synchronization APIs such as cuStreamWaitValue32() and cuStreamWriteValue32() .
Event and metric collection requiring multiple passes will not work with the nvprof kernel replay option for any kernel performing IPC or data communication between the kernel and CPU, kernel and regular CPU allocated memory, kernel and Peer GPU, or kernel and other Peer devices (e.g.
For an application that uses multiple CUDA contexts, these metrics will only be collected for one of the contexts.
The metrics that can be collected only for a single CUDA context are indicated in the metric reference tables .
Some metric values are calculated assuming a kernel is large enough to occupy all device multiprocessors with approximately the same amount of work.
If a kernel launch does not have this characteristic, then those metric values may not be accurate.
The profilers may fail to collect events or metrics when “application replay” mode is turned on.
For applications that allocate large amount of device memory, the profiler may take significant time to collect all events or metrics when “kernel replay” mode is used.
Here are a couple of reasons why Visual Profiler may fail to gather metric or event information.
Tools include Nsight Compute, Nsight Systems, Nsight Graphics, and applications that use either CUPTI or PerfKit API (NVPM) to read event values.
More than one application is using the GPU at the same time Visual Profiler is profiling a CUDA application.
To fix this issue please close all applications and just run the one with Visual Profiler.
Interacting with the active desktop should be avoided while the application is generating event information.
Please note that for some types of event Visual Profiler gathers events for only one context if the application is using multiple contexts within the same application.
When collecting events or metrics with the --events , --metrics , or --analysis-metrics options, nvprof will use kernel replay to execute each kernel multiple times as needed to collect all the requested data.
If a large number of events or metrics are requested then a large number of replays may be required, resulting in a significant increase in application execution time.
To see a list of all available events on a particular device, type nvprof --query-events .
Enabling certain events can cause GPU kernels to run longer than the driver’s watchdog time-out limit.
In these cases the driver will terminate the GPU kernel resulting in an application error and profiling data will not be available.
Please disable the driver watchdog time out before profiling such long running CUDA kernels On Linux, setting the X Config option Interactive to false is recommended.
For Windows, detailed information about TDR (Timeout Detection and Recovery) and how to disable it is available at https: docs.microsoft.com/en-us/windows-hardware/drivers/display/timeout-detection-and-recovery nvprof can give out of memory error for event and metrics profiling, it could be due to large number of instructions in the kernel.
Profiling results might be incorrect for CUDA applications compiled with nvcc version older than 9.0 for devices with compute capability 6.0 and 6.1.
Profiling is not supported for multidevice cooperative kernels, that is, kernels launched by using the API functions cudaLaunchCooperativeKernelMultiDevice or cuLaunchCooperativeKernelMultiDevice.
Profiling is not supported for CUDA kernel nodes launched by a CUDA Graph. 13. Changelog  Profiler changes in CUDA 12.5 List of changes done as part of the CUDA Toolkit 12.5 release.
Profiler changes in CUDA 12.4 List of changes done as part of the CUDA Toolkit 12.4 release.
Profiler changes in CUDA 12.3 List of changes done as part of the CUDA Toolkit 12.3 release.
Profiler changes in CUDA 12.2 List of changes done as part of the CUDA Toolkit 12.2 release.
Profiler changes in CUDA 12.1 List of changes done as part of the CUDA Toolkit 12.1 release.
Profiler changes in CUDA 12.0 List of changes done as part of the CUDA Toolkit 12.0 release.
Profiler changes in CUDA 11.8 List of changes done as part of the CUDA Toolkit 11.8 release.
Profiler changes in CUDA 11.7 List of changes done as part of the CUDA Toolkit 11.7 release.
Profiler changes in CUDA 11.6 List of changes done as part of the CUDA Toolkit 11.6 release.
Profiler changes in CUDA 11.5 List of changes done as part of the CUDA Toolkit 11.5 release.
Profiler changes in CUDA 11.4 List of changes done as part of the CUDA Toolkit 11.4 release.
Profiler changes in CUDA 11.3 List of changes done as part of the CUDA Toolkit 11.3 release.
Visual Profiler extends remote profiling support to macOS host running version 11 (Big Sur) on Intel x86_64 architecture.
Profiler changes in CUDA 11.2 List of changes done as part of the CUDA Toolkit 11.2 release.
Profiler changes in CUDA 11.1 List of changes done as part of the CUDA Toolkit 11.1 release.
Profiler changes in CUDA 11.0 List of changes done as part of the CUDA Toolkit 11.0 release.
Starting with the CUDA 11.0, Visual Profiler and nvprof won’t support Mac as the target platform.
Visual Profiler will be provided in a separate installer package to maintain the remote profiling workflow for CUDA developers on Mac.
Profiler changes in CUDA 10.2 List of changes done as part of the CUDA Toolkit 10.2 release.
Visual Profiler and nvprof allow tracing features for non-root and non-admin users on desktop platforms.
Note that events and metrics profiling is still restricted for non-root and non-admin users.
Thus it’s required to set the path to the CUPTI library before launching Visual Profiler and nvprof.
CUPTI library can be found at /usr/local extras/CUPTI/lib64 or /usr/local targets lib for POSIX platforms and "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\\extras\CUPTI\lib64" for Windows.
Profilers no longer turn off the performance characteristics of CUDA Graph when tracing the application.
Profiler changes in CUDA 10.1 Update 2 List of changes done as part of the CUDA Toolkit 10.1 Update 2 release.
A security vulnerability issue required profiling tools to disable all the features for non-root or non-admin users.
As a result, Visual Profiler and nvprof cannot profile the application when using a Windows 419.17 or Linux 418.43 or later driver.
Visual Profiler requires Java Runtime Environment (JRE) 1.8 to be available on the local system.
Profiler changes in CUDA 10.1 List of changes done as part of the CUDA Toolkit 10.1 release.
Profiler changes in CUDA 10.0 List of changes done as part of the CUDA Toolkit 10.0 release.
Profiling features for devices with compute capability 7.5 and higher are supported in the NVIDIA Nsight Compute .
Profiler changes in CUDA 9.2 List of changes done as part of the CUDA Toolkit 9.2 release.
The Visual Profiler allows to switch multiple segments to non-segment mode for Unified Memory profiling on the timeline.
The Visual Profiler shows a summary view of the memory hierarchy of the CUDA programming model.
The Visual Profiler can correctly import profiler data generated by nvprof when the option --kernels kernel-filter is used.
nvprof supports display of basic PCIe topolgy including PCI bridges between NVIDIA GPUs and Host Bridge.
To view and analyze bandwidth of memory transfers over PCIe topologies, new set of metrics to collect total data bytes transmitted and recieved through PCIe are added.
The Visual Profiler and nvprof added support for new metrics: Instruction executed for different types of load and store Total number of cached global/local load requests from SM to texture cache Global atomic/non-atomic/reduction bytes written to L2 cache from texture cache Surface atomic/non-atomic/reduction bytes written to L2 cache from texture cache Hit rate at L2 cache for all requests from texture cache Device memory (DRAM) read and write bytes The utilization level of the multiprocessor function units that execute tensor core instructions for devices with compute capability 7.0 nvprof allows to collect tracing infromation along with the profiling information in the same pass.
Profiler changes in CUDA 9.1 List of changes done as part of the CUDA Toolkit 9.1 release.
The Visual Profiler shows the breakdown of the time spent on the CPU for each thread in the CPU Details View .
Profiler changes in CUDA 9.0 List of changes done as part of the CUDA Toolkit 9.0 release.
Tools and extensions for profiling are hosted on Github at https: github.com/NVIDIA/cuda-profiler There are several enhancements to Unified Memory profiling: The Visual Profiler now associates unified memory events with the source code at which the memory is allocated.
The Visual Profiler now correlates a CPU page fault to the source code resulting in the page fault.
New Unified Memory profiling events for page thrashing, throttling and remote map are added.
The Visual Profiler provides an option to switch between segment and non-segment mode on the timeline.
The Visual Profiler supports filtering of Unified Memory profiling events based on the virtual address, migration reason or the page fault access type.
The Visual Profiler supports new options to make it easier to do multi-hop remote profiling.
The Visual Profiler supports remote profiling to systems supporting ssh key exchange algorithms with a key length of 2048 bits.
Profiler changes in CUDA 8.0 List of changes done as part of the CUDA Toolkit 8.0 release.
Visual Profiler and nvprof now support NVLink analysis for devices with compute capability 6.0.
Visual Profiler and nvprof now support dependency analysis which enables optimization of the program runtime and concurrency of applications utilizing multiple CPU threads and CUDA streams.
It allows computing the critical path of a specific execution, detect waiting time and inspect dependencies between functions executing in different threads or streams.
Unified Memory profiling now provides GPU page fault information on devices with compute capability 6.0 and 64 bit Linux platforms.
Unified Memory profiling now provides CPU page fault information on 64 bit Linux platforms.
There is now a single integrated view for the different source level analysis results collected for a kernel instance.
The PC sampling feature is enhanced to point out the true latency issues for devices with compute capability 6.0 and higher.
If the new NVIDIA Tools Extension API(NVTX) feature of domains is used then Visual Profiler and nvprof will show the NVTX markers and ranges grouped by domain.
The Visual Profiler now adds a default file extension .nvvp if an extension is not specified when saving or opening a session file.
The Visual Profiler now supports timeline filtering options in create new session and import dialogs.
Profiler changes in CUDA 7.5 List of changes done as part of the CUDA Toolkit 7.5 release.
Visual Profiler now supports profiling child processes and profiling all processes launched on the same system.
For profiling CUDA applications using Multi-Process Service(MPS) see MPS profiling with Visual Profiler Visual Profiler import now supports browsing and selecting files on a remote system.
All events and metrics for devices with compute capability 5.2 can now be collected accurately in presence of multiple contexts on the GPU.
Profiler changes in CUDA 7.0 The profiling tools contain a number of changes and new features as part of the CUDA Toolkit 7.0 release.
The Visual Profiler has been updated with several enhancements: Performance is improved when loading large data file.
Unified memory profiling is enhanced by providing fine grain data transfers to and from the GPU, coupled with more accurate timestamps with each transfer.
nvprof has been updated with several enhancements: All events and metrics for devices with compute capability 3.x and 5.0 can now be collected accurately in presence of multiple contexts on the GPU.
Profiler changes in CUDA 6.5 List of changes done as part of the CUDA Toolkit 6.5 release.
The Visual Profiler kernel memory analysis has been updated with several enhancements: ECC overhead is added which provides a count of memory transactions required for ECC Under L2 cache a split up of transactions for L1 Reads, L1 Writes, Texture Reads, Atomic and Noncoherent reads is shown Under L1 cache a count of Atomic transactions is shown The Visual Profiler kernel profile analysis view has been updated with several enhancements: Initially the instruction with maximum execution count is highlighted A bar is shown in the background of the counter value for the “Exec Count” column to make it easier to identify instruction with high execution counts The current assembly instruction block is highlighted using two horizontal lines around the block.
Also “next” and “previous” buttons are added to move to the next or previous block of assembly instructions.
nvprof now supports a new application replay mode for collecting multiple events and metrics.
This is useful for cases when the kernel uses a large amount of device memory and use of kernel replay can be slow due to a high overhead of saving and restoring device memory for each kernel replay run.
Visual Profiler also supports this new application replay mode and it can enabled in the Visual Profiler “New Session” dialog.
Visual Profiler now displays peak single precision flops and double precision flops for a GPU under device properties.
Improved source-to-assembly code correlation for CUDA Fortran applications compiled by the PGI CUDA Fortran compiler.
Profiler changes in CUDA 6.0 List of changes done as part of the CUDA Toolkit 6.0 release.
Both profilers allow you to see the Unified Memory related memory traffic to and from each GPU on your system.
You can import multiple timeline data sets collected with nvprof into nvvp and view them on the same timeline to see how they are sharing the GPU(s).
This multi-process import capability also includes support for CUDA applications using MPS.
The Visual Profiler now supports a remote profiling mode that allows you to collect a profile on a remote Linux system and view the timeline, analysis results, and detailed results on your local Linux, Mac, or Windows system.
The Visual Profiler analysis system now includes a side-by-side source and disassembly view annotated with instruction execution counts, inactive thread counts, and predicated instruction counts.
This new view enables you to find hotspots and inefficient code sequences within your kernels.
The Visual Profiler analysis system has been updated with several new analysis passes: 1) kernel instructions are categorized into classes so that you can see if instruction mix matches your expectations, 2) inefficient shared memory access patterns are detected and reported, and 3) per-SM activity level is presented to help you detect detect load-balancing issues across the blocks of your kernel.
The report is a PDF version of the per-kernel information presented by the guided analysis system.
You can import profile data collected from another system and view and analyze it on your GPU-less system.
Profiling overheads for both nvvp and nvprof have been significantly reduced. 14. Notices  14.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 14.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 14.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
Introduction Release Notes Installation and Setup CUDA Debugger Getting Started with the CUDA Debugger Build and Run Control GPU Execution Inspect State Advanced Topics Reference Reference Release Information Archives Copyright and License Notices EULA nsight-visual-studio-edition » NVIDIA Nsight Visual Studio Edition v2024.2.1 | Archive NVIDIA Nsight Visual Studio Edition  Introduction NVIDIA® Nsight™ Visual Studio Edition is an application development environment which brings GPU computing into Microsoft Visual Studio.
allows you to build and debug integrated GPU kernels and native CPU code as well as inspect the state of the CPU, GPU, and memory.
Release Notes See the latest features and updates for this version of NVIDIA Nsight Visual Studio Edition.
Installation and Setup This chapter walks you through the system requirements for NVIDIA Nsight Visual Studio Edition, and the steps you’ll need to install and get started using the software.
CUDA Debugger  Getting Started with the CUDA Debugger This section provides a walkthrough and tutorial for using the CUDA Debugger with NVIDIA Nsight Visual Studio Edition.
Build and Run This section details how to configure the properties of a CUDA project, launching the CUDA Debugger, and how to attach debugging to a running CUDA Process.
Control GPU Execution In this section, learn more about how to control GPU execution, set GPU breakpoints, and use global freeze.
Inspect State In this section, learn more about how to use various state inspection features of the CUDA Debugger, such as specifying the debugger context, viewing memory and variables, using the CUDA Info View, and using the CUDA Warp Watch.
Advanced Topics In this section, learn more about advanced CUDA topics, such as PTX and SASS assembly debugging, as well as how to use the CUDA Memory Checker.
Reference  Reference Additional resources for learning more about working with NVIDIA Nsight Visual Studio Edition.
Release Information  Archives Find documentation for previous versions of NVIDIA Nsight Visual Studio Edition.
Copyright And License Notices  EULA This document is the End User License Agreement (EULA) for NVIDIA Nsight Visual Studio Edition.
This document contains specific license terms and conditions for NVIDIA Nsight Visual Studio Edition.
By accepting this agreement, you agree to comply with all the terms and conditions applicable to the specific product(s) included herein.
NvRules API Training Training Release Information Archives Copyright and Licenses Copyright and Licenses NsightCompute » Nsight Compute Documentation v2024.2.1 | Archive Nsight Compute Documentation  Nsight Compute  Release Notes Release notes, including new features and important bug fixes.
Kernel Profiling Guide Kernel Profiling Guide with metric types and meaning, data collection modes and FAQ for common problems.
Information on workflows and options for the command line, including multi-process profiling and NVTX filtering.
Developer Interfaces  Customization Guide User manual on customizing NVIDIA Nsight Compute tools or integrating them with custom workflows.
Information on writing section files, rules for automatic result analysis and scripting access to report files.
Release Information  Archives Find documentation for previous versions of NVIDIA Nsight Compute.
Copyright And Licenses  Copyright and Licenses Information on the NVIDIA Software License Agreement as well as third party software and tools used by Nsight Compute.
jQuery(function () { SphinxRtdTheme.Navigation.enable(true); });
Release Notes Installation Guide User Guide Copyright and Licenses Archives Archives nsight-systems » Nsight Systems v2024.4 | Archive Nsight Systems  Release Notes Release notes and known issues.
Copyright and Licenses Information on the NVIDIA Software License Agreement as well as third party software and tools used by Nsight Systems.
Developer Interfaces  Archives Documentation for previous versions of the NVIDIA Nsight Systems.
Introduction v12.5 | PDF | Archive Nsight Eclipse Plugins Edition Getting Started Guide The user guide for using Nsight Eclipse Plugins Edition.
Introduction  This guide introduces Nsight Eclipse Plugins Edition and provides instructions necessary to start using this tool.
For a detailed description of Eclipse CDT features consult the integrated help “C/C++ Development User Guide” available from inside Nsight (through Help->Help Contents menu). 1.1. About Nsight Eclipse Plugins Edition  NVIDIA ® Nsight™ Eclipse Edition is a unified CPU plus GPU integrated development environment (IDE) for developing CUDA ® applications on Linux and Mac OS X for the x86, POWER and ARM platforms.
Nsight Eclipse Plugins can be installed on vanilla Eclipse using the standard Help->Install New Software..
The principal features are as follows: Edit, build, debug and profile CUDA-C applications CUDA aware source code editor – syntax highlighting, code completion and inline help Graphical user interface for debugging heterogeneous applications Profiler integration – Launch visual profiler as an external application with the CUDA application built in this IDE to easily identify performance bottlenecks For more information about Eclipse Platform, visit http: eclipse.org 2.
Installing Nsight Eclipse Edition  Nsight Eclipse Plugins archive is part of the CUDA Toolkit.
Nsight Eclipse Plugins archive can be installed using the Help -> Install New Software… Menu on Eclipse 2.1.1.
Installing CUDA Toolkit  To install CUDA Toolkit: Visit the NVIDIA CUDA Toolkit download page: https: developer.nvidia.com/cuda-downloads Select appropriate operating system.
Follow instructions to configure CUDA Driver and Toolkit on your system. 2.1.2. Configure CUDA Toolkit Path  When Eclipse is first launched with Nsight Eclipse plugins in the new workspace, NVIDIA usage data collection dialog will be displayed as below.
Usage data collection page  To get started, CUDA Toolkit path must be configured in Eclipse with Nsight Plugins: Open the Preferences page, Window > Preferences.
CUDA toolkit path can be also specified in the project properties page in order to use different toolkit for a project.
For QNX: When QNX is selected as Target OS, a dialog will be displayed to set the QNX_HOST and QNX_TARGET environment variables if they were not already set.
QNX_HOST environment variable identifies the directory that holds the host-related components: QNX_TARGET environment variable identifies the directory that holds the target-related components: 2.2.
Nsight Eclipse Main Window  On the first run Eclipse will ask to pick a workspace location.
The workspace is a folder where Nsight will store its settings, local files history and caches.
The main window is divided into the following areas: Editor - displays source files that are opened for editing.
Project Explorer - displays project files Outline - displays structure of the source file in the current editor.
Problems - displays errors and warnings detected by static code analysis in IDE or by a compiler during the build.
Console - displays make output during the build or output from the running application. 2.3. Creating a New Project  From the main menu, open the new project wizard - File > New… > CUDA C/C++ Project Specify the project name and project files location.
Importing CUDA Samples  The CUDA samples are an optional component of the CUDA Toolkit installation.
Nsight provides a mechanism to import these samples and work with them easily: Note Samples that use the CUDA driver API (suffixed with “Drv”) are not supported by Nsight.
From the main menu, open the new project wizard - File > New… > CUDA C/C++ Project Specify the project name and project files location.
Press Next… Specify the project parameters on the next wizard page. 2.4.1. cuHook Sample  cuHook sample builds both the library and the executable.
From the main menu, open the new project wizard - File > New… > CUDA C/C++ Project Select project type “Makefile project” and choose “Empty Project” Specify the project name and project files location.
Right click on the project - Import… > General > File System On the next wizard page, select the location of cuHook sample(Samples/7_CUDALibraries/cuHook) Select all the source files and makefile and Finish the wizard Build the project by clicking on the hammer button on the main toolbar.
To run the sample, from the main menu - Run > Run Configurations… > Select the executable > Go to Environment tab > New… > enter Name=LD_PRELOAD, Value=./libcuhook.so.1 > Run will execute the sample 2.5.
Configure Build Settings  To define build settings: In the C/C++ Projects view, right-click your project, and select Properties.
The following are the categories of Nvcc linker settings that can be configured for the selected project.
Note All options field in the main page is not editable and it’s the collection of options set in the child categories.
When you are cross compiling for different target os, the library search path should point to the appropriate location where the target os libraries are present.
The following are the categories of Nvcc Compiler settings that can be configured for the selected project.
Additionally, set the number of threads that the compiler will use during the compiliation process (“split compilation”).
CUDA - Generate code for different real architectures with the PTX for the same vitrual architectures. 2.6. Debugging CUDA Applications  Nsight must be running and at least one project must exist.
Make sure the project executable is compiled and no error markers are shown on the project.
You can now explore your CUDA device state, step through your GPU code or resume the application.
Debugging CUDA application  Additional debugger options can be set in the debug configuration dialog through Run > Debug Configurations ..
Remote development of CUDA Applications  Nsight Eclipse Edition also supports remote development of CUDA application starting with CUDA Toolkit 6.0.
The picture below shows how Nsight Eclipse Edition can be used for local as well as remote development: For remote development you do not need any NVIDIA GPU on your host system.
The remote target system can be a Linux x86 or POWER system with an NVIDIA GPU or an Tegra-based ARM system.
In the cross compilation mode the project resides on the host system and the cross compilation is also done on the host system.
To cross compile select the target cross compile architecture in CPU architecture drop down in the project properties page: 2.8.
Debugging Remote CUDA Applications  Remote debugging is available starting with CUDA Toolkit 5.5.
Debug host and target may run different operating systems or have different CPU architectures.
The remote machine must be accessible via SSH and CUDA Toolkit must be installed on both machines.
Note If there is a firewall between the host and the target, it must be set up to let RSP messages through, or SSH port-forwarding must be used.
Select the project and right click then go to Debug As…>NVIDIA CUDA GDB Debugger(Remote) menu item.
Select a remote connection from a drop-down list or press the Add connection… button to create a new one.
If you are creating a new remote connection, enter the host name(or IP address) as well as the user name.
For Android devices: To configure the remote connection using Android debug bridge, select the Android debug bridge from the Remote Connection drop-down list, Android device must be connected to the host system using USB port.
Type the full path to cuda-gdbserver on the remote system or select one using the Browse… button.
Click on “Add new path” or on the Browse… button to specify the path to the shared libraries the remote application depends on.
Click on the Finish button to finish the new debug configuration wizard and start debugging the application.
Improving Remote Debugging Performance  When doing remote debugging, it can be useful to host copies of the target libraries in a local sysroot.
Starting with CUDA 11.5, this feature is available on the Debugger Configurations -> Debugger tab.
You can modify ‘CUDA GDB sysroot (for remote debugging)’ to point to a local sysroot directory to improve debugging performance. 2.9. Profiling CUDA applications  Nsight must be running and at least one project must exist.
Nsight Eclipse Edition profiling features are based on the NVIDIA Visual Profiler ( nvvp ) code.
Nsight Eclipse Plugins Edition will launch the Visual Profiler as an external tool with the executable and other information from the selected project.
Nsight Eclipse will launch the Visual Profiler to specify extra profiler options with the executable information already passed from the selected project.
Build CUDA Projects inside a Docker Container  You can build and debug C/C++ and CUDA projects in a Docker container using Nsight Eclipse Edition.
To get started, you need to first pull and install the Docker image that encapsulates the CUDA toolkit and cross platform tool chains.
You can get the Docker images from NVIDIA GPU Cloud.Then you can use Nsight Eclipse Edition to build CUDA projects in a Docker container.
Open the Preferences page, Window > Preferences and go to: CUDA > Container Settings Select the option if you want to build the projects inside the Docker container.
Make sure the CUDA toolkit path that is specified in the CUDA preferences is the path of the CUDA toolkit inside a Docker container.
Select the Connection and the Image dropdown will display all the Docker images that are currently installed.
The preferences that are set here will be automatically displayed in the project setup wizard.
You can choose to modify the container settings for the individual projects from the project setup wizard.
To create a project, From the main menu, open the new project wizard: File > New… > CUDA C/C++ Project Specify the project name and project files location.
If you need to mount any other directories that contains the include files/libraries and etc to the docker container, you can mount those directories from the project property page.
The project is now built in the chosen Docker container the executable will be available on the host. 2.11. Remote debugging using CUDA GDB inside Docker container  From Nsight Eclipse, you can remote debug the applications running on the target using the CUDA GDB inside the Docker container running on the host.
The remote machine must be accessible via SSH and CUDA Toolkit must be installed on target machine.
Create a new debug configuration under CUDA GDB Container Launcher either double clicking or using right click menu.
If you are creating a new remote connection, click on the manage button in Remote Connection enter the host name(or IP address) as well as the user name.
Also select the CUDA toolkit location on the target and choose the location to where to upload the executable.
From the “Container” tab, select the connection and Docker image that contains the CUDA GDB.
Click on the Apply button to save the changes and click on the Debug button to start the debug session.
This action will upload the local executable to the target system and will start CUDA GDB Server on the target.
And the Docker container will be started on the host and CUDA GDB running inside the docker container will establish the remote debug session with the target.
Importing Nsight Eclipse Projects  The projects that are created with Nsight Eclipse Edition can be imported into the Eclipse workbench with Nsight Eclipse plugins.
Right click on the Nsight Eclipse project and go to - Export > C/C++ > C/C++ Project Settings > Next menu.
Create a CUDA C/C++ Project from the main menu File > New > CUDA C/C++ Project Specify the project name and choose Empty project type with CUDA toolchains.
Import > General > File System >(From directory) or copy the source files from the existing project.
Import the project settings like include paths and symbols using the following right click menu Import > C/C++ > C/C++ Project Settings >Next… Select the location of the project settigns file and select the project and configuration on the next wizard page.
The project settings will be imported from the file exported from Nsight Eclipse Edition. 2.13. Enabling Dark Theme in Eclipse  To work comfortably in dark environments, Eclipse offers a dark theme mode that’s easy to activate: Open Eclipse Preferences: Click on “Windows” in the top menu.
More Information  More information about the Eclipse CDT features and other topics is available in the Help contents.
More information about CUDA, CUDA Toolkit and other tools is available on CUDA web page at http: developer.nvidia.com/cuda 3.
Known Issues  Executable must exist in order to start debug session for the first time Nsight will not automatically perform build when starting debug session for a given project for the first time.
Note To manually build the project, select it (or any file within the project) in a Project Explorer view and click hammer icon on the main window toolbar.
Source editors may show error markers on a valid code for the files in newly created projects.
Mac OS X users may be prompted to install Java Runtime Environment (JRE) when running Nsight Eclipse Edition for the first time.
Nsight Eclipse Plugin Edition requires functioning Java Runtime Environment to be present on the local system to run.
Nsight Eclipse Plugin Edition does not provide compilation support for using the QNX qcc and q++ compilers.
The workaround to compile using qcc and q++ is Specify the q++ path in CCBIN field on toolkit configuration page on project properties dialog as shown below.
You can access toolkit configuration page by clicking main menu Project > Properties > C/C++ Build > CUDA Toolkit Change default CONF to gcc_ntoaarch64le in the file ${QNX_HOST}/etc/qcc/gcc/5.4.0/default as below CONF = gcc_ntoaarch64le 4.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 4.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 4.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2016-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Introduction v12.5 | PDF | Archive Nsight Eclipse Plugins Installation Guide The user guide for installing Nsight Eclipse Plugins.
Introduction  This guide provides the procedures to install the Nsight Eclipse Edition Plugins in users own eclipse environment.
Nsight Eclipse Plugins offers full-featured IDE that provides an all-in-one integrated environment to edit, build, debug and profile CUDA-C applications. 1.1. Install plugins using Eclipse IDE  You can install Nsight Eclipse plugins in your own Eclipse environment or download and install Eclipse IDE for C/C++ developers .
Choose the the zip file(com.nvidia.cuda.repo.zip) that contains the plugins using Archive button or Enter the full path of zip file.
Nsight EE plugns zip file can be found in /usr/local/cuda-11.8/nsightee_plugins directory.
Click OK on the “Security Warning” dialog to ignore the warning message about unsigned content (This warning message is displayed for all the plugins that are not signed by Eclipse.org).
Menu to verify the “Cuda Developer Tools” and “Cuda Remote Launch” plugins are installed 1.2.
Uninstall plugins using Eclipse IDE  Launch Eclipse and go to Help > Installation Details menu.
Select “Cuda Developer Tools” and “Cuda Remote Launch” options from the dialog Click on the Uninstall button.
menu to verify. 1.3. Install Using Script  To install or uninstall the Nsight Eclipse Plugins using the script, run the installation script provided in the bin directory of the toolkit.
By default, it is located in /usr/local/cuda-11.8/bin : The usage of the script is as follows: Usage: ./nsight_ee_plugins_manage.sh : 'install' or 'uninstall' : eclipse installation directory To install the Nsight Eclipse Plugins, run the following command: $ /usr/local/cuda-11.8/bin/nsight_ee_plugins_manage.sh install To uninstall the Nsight Eclipse Plugins, run the following command: $ /usr/local/cuda-11.8/bin/nsight_ee_plugins_manage.sh uninstall 2.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 2.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 2.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2017-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
NVIDIA Compute Sanitizer Documentation Search In: Entire Site Just This Document clear search search Compute Sanitizer Release Notes Tools Compute Sanitizer User Manual Developer Interfaces Compute Sanitizer API Reference Manual Sanitizer Api NVTX API for Compute Sanitizer Reference Manual Copyright And Licenses Copyright and Licenses Search Results Compute Sanitizer Release Notes Release notes and known issues.
Tools Compute Sanitizer User Manual Developer Interfaces Compute Sanitizer API Reference Manual Sanitizer API The NVIDIA Compute Sanitizer API.
NVTX API for Compute Sanitizer Reference Manual Copyright And Licenses Copyright and Licenses Information on the NVIDIA Software License Agreement as well as third party software and tools used by Compute Sanitizer.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2019-2024 NVIDIA Corporation _satellite.pageBottom(); var switchTo5x=true; stLight.options({publisher: "998dc202-a267-4d8e-bce9-14debadb8d92", doNotHash: false, doNotCopy: false, hashAddressBar: false});
1.
Introduction v12.5 | PDF | Archive CUDA-GDB The user manual for CUDA-GDB, the NVIDIA tool for debugging CUDA applications on Linux and QNX systems.
Introduction  This document introduces CUDA-GDB, the NVIDIA ® CUDA ® debugger for Linux and QNX targets. 1.1.  CUDA-GDB is the NVIDIA tool for debugging CUDA applications running on Linux and QNX.
The tool provides developers with a mechanism for debugging CUDA applications running on actual hardware.
This enables developers to debug applications without the potential variations introduced by simulation and emulation environments.
Supported Features  CUDA-GDB is designed to present the user with a seamless debugging environment that allows simultaneous debugging of both GPU and CPU code within the same application.
Just as programming in CUDA C is an extension to C programming, debugging with CUDA-GDB is a natural extension to debugging with GDB.
The existing GDB debugging features are inherently present for debugging the host code, and additional features have been provided to support debugging CUDA device code.
CUDA-GDB allows the user to set breakpoints, to single-step CUDA applications, and also to inspect and modify the memory and variables of any given thread running on the hardware.
CUDA-GDB supports debugging all CUDA applications, whether they use the CUDA driver API, the CUDA runtime API, or both.
CUDA-GDB supports debugging kernels that have been compiled for specific CUDA architectures, such as sm_75 or sm_80 , but also supports debugging kernels compiled at runtime, referred to as just-in-time compilation, or JIT compilation for short. 1.3. About This Document  This document is the main documentation for CUDA-GDB and is organized more as a user manual than a reference manual.
The rest of the document will describe how to install and use CUDA-GDB to debug CUDA kernels and how to use the new CUDA commands that have been added to GDB.
It is assumed that the user already knows the basic GDB commands used to debug host applications. 2. Release Notes  12.5 Release Updated GDB version Moved from GDB 13.1 to 13.2.
See GDB 13.2 changes Support removal notice Support for the macOS host client of CUDA-GDB has been removed.
Features Multi build feature that supports native Python and TUI mode across all supported platforms.
If no supported Python or libncurses is detected, the wrapper will fallback to a cuda-gdb binary with Python and TUI support disabled.
Fixed issues resulting in crashes/errors when reading/writing from/to CUDA generic memory.
Fixed issue where break_on_launch breakpoints were missed for back to back launches of the same kernel.
Fixed issue with incorrectly reporting breakpoint hit events as SIGTRAP when breakpoint is hit in divergent thread.
Python 3.6 and 3.7 deprecation notice Support for end-of-life Python 3.6 and 3.7 versions is deprecated.
Features Performance enhancement which reduces the number of overall CUDA Debugger API calls.
Performance enhancement when loading large cubins with device functions using a large number of GPU registers.
12.3 Release macOS host client deprecation notice Support for the macOS host client of CUDA-GDB is deprecated.
New $_cuda_const_bank(bank, offset) convenience function to obtain address of offset in constant bank.
Performance enhancements added which reduce overhead when running applications with many CUDA threads.
Added support for opening of GPU core dumps when no valid warps are present on the device.
Fixed issue where CUDA Cluster coordinates were being displayed when no CUDA Cluster was present.
12.2 Release Features Enabled printing of extended error messages when a CUDA Debugger API error is encountered.
Fixed issue with attaching to an application using CUDA Lazy Loading when debugging remotely with cuda-gdbserver.
12.1 Release CUDA Driver API added for controlling core dump behavior CTK 12.1 and the r530 driver adds new APIs that allow developers to enable/configure core dump settings programmatically inside their application instead of using environment variables.
See GDB 12.1 changes Texture and surface reference support removed CTK 12.0 removed support for the Texture and Surface Reference API.
CUDA Memory Checker integration removed cuda-memcheck has been deprecated in CUDA 11.x and replaced by Compute Sanitizer.
This will support coredumps when issues are detected which can then be opened and inspected with CUDA-GDB, similar to other coredumps.
Debugging of applications using CUDA Dynamic Parallelism Support for debugging applications using CUDA Dynamic Parallelism with the classic debugger backend or on Maxwell GPUs has been removed by default for applications compiled with the CTK 12.0 or newer.
Debugging can be accomplished in these situations by recompiling the application while passing the -DCUDA_FORCE_CDP1_IF_SUPPORTED flag.
Fixed Issues Addressed a hang that could be encountered when stepping through device system calls.
Changed internal CUDA Dynamic Parallelsim detection breakpoint to be set only when break_on_launch is enabled.
For Maxwell debugging, or to force the old classic debugging backend, set CUDBG_USE_LEGACY_DEBUGGER to 1 in your environment.
11.7 Release Features Major break_on_launch performance enhancements to use new KERNEL_READY notification mechanism instead of setting manual breakpoints.
Fixed Issues Fixed follow-fork child to avoid hanging behavior when both parent and child processes use CUDA.
Added a missing dlsym of a libpython function that was causing errors with some versions of libpython.
11.5 Release Python 3 support on Jetson and Drive Tegra devices Support for Python 2 has been removed.
11.4 Update 1 Release Known Issues with Fedora 34 CUDA-GDB has known issues with debugging on Fedora 34 and may not be reliable.
Changed python behavior to dlopen libpython libraries that match the version of the python3 interpreter in PATH.
Coredump support Added support for writing coredumps to named pipe using CUDA_COREDUMP_FILE .
Fixed cuda_register_name and cuda_special_register_name to avoid returning old cached result on error.
11.2 Update 1 Release GDB TUI deprecation notice Support for GDB TUI mode is being deprecated.
This will avoid cross platform dependency mismatches for OSes that lack ncurses-5.5 support.
See gdb 8.3.1 changes Support for SM 8.6 CUDA-GDB now supports Devices with Compute Capability 8.6.
Updated DWARF parser Old binaries might need to be recompiled in order to ensure CUDA-specific DWARF info are up to date.
However, macOS can still be used as the host system (where CUDA-GDB runs under macOS, using cuda-gdbserver to debug a remote target).
The download for the macOS version of CUDA-GDB can be found at the following location: Download Here 10.1 Release Enhanced debugging with only linenumber information Several enhancements were made to CUDA-GDB support for debugging programs compiled with -lineinfo but not with -G .
See also Compilation With Linenumber Information 10.0 Release Turing Uniform Register Support Support added for examining and modifying uniform registers on Turing GPUs.
9.2 Release User induced core dump support For the devices that support compute preemption, user induced core dump support is added.
New environment variable: CUDA_ENABLE_USER_TRIGGERED_COREDUMP can be used to enable this feature.
9.1 Release Volta-MPS core dump support GPU core dump generation is supported on Volta-MPS.
Lightweight GPU core dump support CUDA-GDB supports reading lightweight GPU core dump files.
New environment variable: CUDA_ENABLE_LIGHTWEIGHT_COREDUMP can be used to enable this feature.
7.0 Release GPU core dump support CUDA-GDB supports reading GPU and GPU+CPU core dump files.
New environment variables: CUDA_ENABLE_COREDUMP_ON_EXCEPTION , CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION and CUDA_COREDUMP_FILE can be used to enable and configure this feature.
6.5 Release CUDA Fortran Support CUDA-GDB supports CUDA Fortran debugging on 64-bit Linux operating systems.
6.0 Release Unified Memory Support Managed variables can be read and written from either a host thread or a device thread.
The debugger also annotates memory addresses that reside in managed memory with @managed .
The list of statically allocated managed variables can be accessed through a new info cuda managed command.
Android Support CUDA-GDB can now be used to debug Android native applications either locally or remotely.
Single-Stepping Optimizations CUDA-GDB can now use optimized methods to single-step the program, which accelerate single-stepping most of the time.
Faster Remote Debugging A lot of effort has gone into making remote debugging considerably faster, up to 2 orders of magnitude.
Kernel Entry Breakpoints The set cuda break_on_launch option will now break on kernels launched from the GPU.
Precise Error Attribution On Maxwell architecture (SM 5.0), the instruction that triggers an exception will be reported accurately.
The application keeps making forward progress and the PC at which the debugger stops may not match that address but an extra output message identifies the origin of the exception.
Live Range Optimizations To mitigate the issue of variables not being accessible at some code addresses, the debugger offers two new options.
With set cuda value_extrapolation , the latest known value is displayed with (possibly) prefix.
With set cuda ptx_cache , the latest known value of the PTX register associated with a source variable is displayed with the (cached) prefix.
New kernel events verbosity options have been added: set cuda kernel_events , set cuda kernel_events_depth .
Also set cuda defer_kernel_launch_notifications has been deprecated and has no effect any more.
5.5 Release Kernel Launch Trace Two new commands, info cuda launch trace and info cuda launch children , are introduced to display the kernel launch trace and the children kernel of a given kernel when Dynamic Parallelism is used.
Single-GPU Debugging (BETA) CUDA-GDB can now be used to debug a CUDA application on the same GPU that is rendering the desktop GUI.
This feature also enables debugging of long-running or indefinite CUDA kernels that would otherwise encounter a launch timeout.
In addition, multiple CUDA-GDB sessions can debug CUDA applications context-switching on the same GPU.
For information on enabling this, please see Single-GPU Debugging with the Desktop Manager Running and Multiple Debuggers .
Remote GPU Debugging CUDA-GDB in conjunction with CUDA-GDBSERVER can now be used to debug a CUDA application running on the remote host.
5.0 Release Dynamic Parallelism Support CUDA-GDB fully supports Dynamic Parallelism, a new feature introduced with the 5.0 toolkit.
The debugger is able to track the kernels launched from another kernel and to inspect and modify variables like any other CPU-launched kernel.
When attached, all the usual features of the debugger are available to the user, as if the application had been launched from the debugger.
Attach on exception Using the environment variable CUDA_DEVICE_WAITS_ON_EXCEPTION , the application will run normally until a device exception occurs.
Then the application will wait for the debugger to attach itself to it for further debugging.
API Error Reporting Checking the error code of all the CUDA driver API and CUDA runtime API function calls is vital to ensure the correctness of a CUDA application.
Inlined Subroutine Support Inlined subroutines are now accessible from the debugger on SM 2.0 and above.
The user can inspect the local variables of those subroutines and visit the call frame stack as if the routines were not inlined.
4.2 Release Kepler Support The primary change in Release 4.2 of CUDA-GDB is the addition of support for the new Kepler architecture.
4.1 Release Source Base Upgraded to GDB 7.2 Until now, CUDA-GDB was based on GDB 6.6 on Linux, and GDB 6.3.5 on Darwin (the Apple branch).
Now CUDA-GDB supports newer versions of GCC (tested up to GCC 4.5), has better support for DWARF3 debug information, and better C++ debugging support.
Simultaneous Sessions Support With the 4.1 release, the single CUDA-GDB process restriction is lifted.
Now, multiple CUDA-GDB sessions are allowed to co-exist as long as the GPUs are not shared between the applications being processed.
For instance, one CUDA-GDB process can debug process foo using GPU 0 while another CUDA-GDB process debugs process bar using GPU 1.
The command increases the precision of CUDA exceptions by automatically single-stepping through portions of code.
Under normal execution, the thread and instruction where an exception occurred may be imprecisely reported.
However, the exact instruction that generates the exception can be determined if the program is being single-stepped when the exception occurs.
Therefore ‘autostep’ aides the user by allowing them to specify sections of code where they suspect an exception could occur.
These sections are automatically single-stepped through when the program is running, and any exception that occurs within these sections is precisely reported.
Multiple Context Support On GPUs with compute capability of SM20 or higher, debugging multiple contexts on the same GPU is now supported.
Device Assertions Support The R285 driver released with the 4.1 version of the toolkit supports device assertions.
CUDA_GDB supports the assertion call and stops the execution of the application when the assertion is hit.
Use the ‘set cuda hide_internal_frames’ option to expose/hide the system call frames (hidden by default).
Temporary Directory By default, the debugger API will use /tmp as the directory to store temporary files.
To select a different directory, the $TMPDIR environment variable and the API CUDBG_APICLIENT_PID variable must be set. 3. Getting Started  The CUDA toolkit can be installed by following instructions in the Quick Start Guide .
Further steps should be taken to set up the debugger environment, build the application, and run the debugger. 3.1. Setting Up the Debugger Environment  3.1.1.
Temporary Directory  By default, CUDA-GDB uses /tmp as the directory to store temporary files.
Note The user must have write and execute permission to the temporary directory used by CUDA-GDB.
Note The value of $TMPDIR must be the same in the environment of the application and CUDA-GDB.
Note Since /tmp folder does not exist on Android device, the $TMPDIR environment variable must be set and point to a user-writeable folder before launching cuda-gdb. 3.1.2. Using the CUDA-GDB debugger on Jetson and Drive Tegra devices  By default, on Jetson and Drive Tegra devices, GPU debugging is supported only if cuda-gdb and cuda-gdbserver are launched by a user who is a member of the debug group.
To add the current user to the debug group run this command: sudo usermod -a -G debug $USER 3.2.
Debug Compilation  NVCC, the NVIDIA CUDA compiler driver, provides a mechanism for generating the debugging information necessary for CUDA-GDB to work properly.
The -g -G option pair must be passed to NVCC when an application is compiled for ease of debugging with CUDA-GDB; for example, nvcc -g -G foo.cu -o foo Using this line to compile the CUDA application foo.cu forces -O0 compilation, with the exception of very limited dead-code eliminations and register-spilling optimizations.
makes the compiler include debug information in the executable To compile your CUDA Fortran code with debgging information necessary for CUDA-GDB to work properly, pgfortran, the PGI CUDA Fortran compiler, must be invoked with -g option.
Also, for the ease of debugging and forward compatibility with the future GPU architectures, it is recommended to compile the code with -Mcuda=nordc option; for example, pgfortran -g -Mcuda=nordc foo.cuf -o foo For more information about the available compilation flags, please consult the PGI compiler documentation. 3.2.2. Compilation With Linenumber Information  Several enhancements were made to cuda-gdb’s support for debugging programs compiled with -lineinfo but not with -G .
The user may step into code that has no linenumber information, leading to an inability to determine which source-file/linenumber the code at the PC belongs to.
When debugging OptiX/RTCore code, the following should be kept in mind: NVIDIA internal code cannot be debugged or examined by the user.
OptiX/RTCode debugging is limited to -lineinfo , and building this code with full debug infomation ( -G ) is not supported.
OptiX/RTCode code is highly optimized, and as such the notes above about debugging optimized code apply. 3.2.3. Compiling For Specific GPU architectures  By default, the compiler will only generate code for the compute_52 PTX and sm_52 cubins.
For later GPUs, the kernels are recompiled at runtime from the PTX for the architecture of the target GPU(s).
Compiling for a specific virtual architecture guarantees that the application will work for any GPU architecture after that, for a trade-off in performance.
It is highly recommended to compile the application once and for all for the GPU architectures targeted by the application, and to generate the PTX code for the latest virtual architecture for forward compatibility.
The list of GPUs and their respective compute capability, see https: developer.nvidia.com/cuda-gpus .
For instance, to compile an application for a GPU with compute capability 7.0, add the following flag to the compilation command: -gencode arch=compute_70,code=sm_70 To compile PTX code for any future architecture past the compute capability 7.0, add the following flag to the compilation command: -gencode arch=compute_70,code=compute_70 For additional information, please consult the compiler documentation at https: docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#extended-notation 3.3.
Single-GPU Debugging with the Desktop Manager Running  For devices with compute capability 6.0 and higher CUDA-GDB can be used to debug CUDA applications on the same GPU that is running the desktop GUI.
Additionally for devices with compute capability less than 6.0 software preemption can be used to debug CUDA applications on the same GPU that is running the desktop GUI.
There are two ways to enable this functionality: Note This is a BETA feature available on Linux and is only supported on Maxwell.
Use the following command: set cuda software_preemption on Export the following environment variable: CUDA_DEBUGGER_SOFTWARE_PREEMPTION=1 Either of the options above will activate software preemption.
When the GPU hits a breakpoint or any other event that would normally cause the GPU to freeze, CUDA-GDB releases the GPU for use by the desktop or other applications.
This enables CUDA-GDB to debug a CUDA application on the same GPU that is running the desktop GUI, and also enables debugging of multiple CUDA applications context-switching on the same GPU. 3.3.2. Multi-GPU Debugging  Multi-GPU debugging designates the scenario where the application is running on more than one CUDA-capable device.
Multi-GPU debugging is not much different than single-GPU debugging except for a few additional CUDA-GDB commands that let you switch between the GPUs.
Once paused, you can use info cuda kernels to view all the active kernels and the GPUs they are running on.
Note If the CUDA_VISIBLE_DEVICES environment is used, only the specified devices are suspended and resumed.
To switch to an active kernel, use cuda kernel , where n is the ID of the kernel retrieved from info cuda kernels .
Note The same kernel can be loaded and used by different contexts and devices at the same time.
When a breakpoint is set in such a kernel, by either name or file name and line number, it will be resolved arbitrarily to only one instance of that kernel.
With the runtime API, the exact instance to which the breakpoint will be resolved cannot be controlled.
With the driver API, the user can control the instance to which the breakpoint will be resolved to by setting the breakpoint right after its module is loaded. 3.3.3. Remote Debugging  There are multiple methods to remote debug an application with CUDA-GDB.
In addition to using SSH or VNC from the host system to connect to the target system, it is also possible to use the target remote GDB feature.
Using this option, the local cuda-gdb (client) connects to the cuda-gdbserver process (the server) running on the target system.
Setting remote debugging that way is a 2-step process: Launch the cuda-gdbserver on the remote host cuda-gdbserver can be launched on the remote host in different operation modes.
To launch a new application in debug mode, invoke cuda-gdb server as follows: $ cuda-gdbserver :1234 app_invocation Where 1234 is the TCP port number that cuda-gdbserver will listen to for incoming connections from cuda-gdb , and app-invocation is the invocation command to launch the application, arguments included.
Option 2: Attach cuda-gdbserver to the running process To attach cuda-gdbserver to an already running process, the --attach option followed by process identification number (PID) must be used: $ cuda-gdbserver :1234 --attach 5678 Where 1234 is the TCP port number and 5678 is process identifier of the application cuda-gdbserver must be attached to.
Launch cuda-gdb on the client Configure cuda-gdb to connect to the remote target using either: (cuda-gdb) target remote or (cuda-gdb) target extended-remote It is recommended to use set sysroot command if libraries installed on the debug target might differ from the ones installed on the debug host.
For example, cuda-gdb could be configured to connect to remote target as follows: (cuda-gdb) set sysroot remote:  (cuda-gdb) target remote 192.168.0.2:1234 Where 192.168.0.2 is the IP address or domain name of the remote target, and 1234 is the TCP port previously previously opened by cuda-gdbserver . 3.3.4. Multiple Debuggers  For devices with compute capability 6.0 and higher several debugging sessions may take place simultaneously.
For devices with compute capability less than 6.0, several debugging sessions may take place simultaneously as long as the CUDA devices are used exclusively.
For instance, one instance of CUDA-GDB can debug a first application that uses the first GPU while another instance of CUDA-GDB debugs a second application that uses the second GPU.
The exclusive use of a GPU is achieved by specifying which GPU is visible to the application by using the CUDA_VISIBLE_DEVICES environment variable.
$ CUDA_VISIBLE_DEVICES=1 cuda-gdb my_app Additionally for devices with compute capability less than 6.0, with software preemption enabled ( set cuda software_preemption on ), multiple CUDA-GDB instances can be used to debug CUDA applications context-switching on the same GPU. 3.3.5. Attaching/Detaching  CUDA-GDB can attach to and detach from a CUDA application running on GPUs with compute capability 2.0 and beyond, using GDB’s built-in commands for attaching to or detaching from a process.
Additionally, if the environment variable CUDA_DEVICE_WAITS_ON_EXCEPTION is set to 1 prior to running the CUDA application, the application will run normally until a device exception occurs.
Note By default on some Linux distributions, the debugger cannot attach to an already running processes due to security settings.
In order to enable the attach feature of the CUDA debugger, either cuda-gdb should be launched as root, or /proc/sys/kernel/yama/ptrace_scope should be set to zero, using the following command: $ sudo sh -c "echo 0 >/proc/sys/kernel/yama/ptrace_scope" To make the change permanent, edit /etc/sysctl.d/10-ptrace.conf . 4. CUDA-GDB Extensions  4.1.
As much as possible, CUDA-GDB command names will be similar to the equivalent GDB commands used for debugging host code.
For instance, the GDB command to display the host threads and switch to host thread 1 are, respectively: (cuda-gdb) info threads (cuda-gdb) thread 1 To display the CUDA threads and switch to cuda thread 1, the user only has to type: (cuda-gdb) info cuda threads (cuda-gdb) cuda thread 1 4.2.
Getting Help  As with GDB commands, the built-in help for the CUDA commands is accessible from the cuda-gdb command line by using the help command: (cuda-gdb) help cuda name_of_the_cuda_command (cuda-gdb) help set cuda name_of_the_cuda_option (cuda-gdb) help info cuda name_of_the_info_cuda_command Moreover, all the CUDA commands can be auto-completed by pressing the TAB key, as with any other GDB command.
CUDA commands can also be queried using the apropos command. 4.3. Initialization File  The initialization file for CUDA-GDB is named .cuda-gdbinit and follows the same rules as the standard .gdbinit file used by GDB.
Those commands will be processed in order when CUDA-GDB is launched. 4.4. GUI Integration  Emacs CUDA-GDB works with GUD in Emacs and XEmacs.
To use DDD with CUDA-GDB, launch DDD with the following command: ddd --debugger cuda-gdb cuda-gdb must be in your $PATH . 4.5. GPU core dump support  There are two ways to configure the core dump options for CUDA applications.
Environment variables set in the application environment or programmatically from the application with the CUDA Driver API .
Compilation for GPU core dump generation GPU core dumps will be generated regardless of compilation flags used to generate the GPU application.
For the best debugging experience, it is recommended to compile the application with the -g -G or the -lineinfo option with NVCC.
See Compiling the Application for more information on passing compilation flags for debugging.
Enabling GPU core dump generation on exception with environment variables Set the CUDA_ENABLE_COREDUMP_ON_EXCEPTION environment variable to 1 in order to enable generating a GPU core dump when a GPU exception is encountered.
Set the CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION environment variable to 0 in order to disable generating a CPU core dump when a GPU exception is encountered.
Set the CUDA_ENABLE_LIGHTWEIGHT_COREDUMP environment variable to 1 in order to enable generating lightweight corefiles instead of full corefiles.
When enabled, GPU core dumps will not contain the memory dumps (local, shared, global) of the application.
Controlling behavior of GPU core dump generation The CUDA_COREDUMP_GENERATION_FLAGS environment variable can be used when generating GPU core dumps to deviate from default generation behavior.
These flags can be used to accomplish tasks such as reducing the size of the generated GPU core dump or other desired behaviors that deviate from the defaults.
GPU core dump CUDA_COREDUMP_GENERATION_FLAGS  Environment Variable flag Description skip_nonrelocated_elf_images Disables including copies of nonrelocated elf images in the GPU core dump.
Note Setting the CUDA_ENABLE_LIGHTWEIGHT_COREDUMP environment variable to 1 is equivalent to CUDA_COREDUMP_GENERATION_FLAGS="skip_nonrelocated_elf_images,skip_global_memory,skip_shared_memory,skip_local_memory" .
Note Setting the CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION environment variable to 0 is equivalent to CUDA_COREDUMP_GENERATION_FLAGS="skip_abort" .
Limitations and notes for core dump generation The following limitations apply to core dump support: For Windows WDDM, GPU core dump is only supported on a GPU with compute capability 6.0 or higher.
GPU core dump is unsupported for the Windows Subsystem for Linux on GPUs running in SLI mode.
Multi-GPU setups are supported, but SLI mode cannot be enabled in the Driver Control Panel.
GPU core dump is supported for the Windows Subsystem for Linux only when the hardware scheduling mode is enabled.
Generating a CPU core dump with CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION is currently unsupported on the QNX platform.
GPUs with compute capability less than 6.0 will return CUDA_ERROR_NOT_SUPPORTED when using the Coredump Attributes Control API.
If an MPS client triggers a core dump, every other client running on the same MPS server will fault.
The indirectly faulting clients will also generate a core dump if they have core dump generation enabled.
GPU core dump is unsupported when other developer tools, including CUDA-GDB, are interacting with the application.
Unless explicitly documented as a supported use case (e.g generate-cuda-core-file command).
When generating a coredump on exception, if the kernel exits before the exception has been recognized it may result in failure to generate the corefile.
Note The user should not send the application process a signal and ensure that the application process does not automatically terminate while the coredump generation is in process.
Note Starting from CUDA 11.6, the compute-sanitizer tool can generate a GPU core dump when an error is detected by using the --generate-coredump yes option.
See the compute-sanitizer documentation for more information: https: docs.nvidia.com/compute-sanitizer/ComputeSanitizer/index.html#coredump Note CPU core dumps will be located in a distribution specific location.
Examining the /proc/sys/kernel/core_pattern file will typically hint at the name/location of the CPU core dump.
Note NVIDIA vGPU platforms must explicitly enable debugging support to perform GPU core dump generation.
Please reference the Virtual GPU Software User Guide for information on how to enable debugging on vGPU.
Note NVIDIA Jetson and Drive Tegra devices must explicitly enable debugging support to perform GPU core dump generation.
Note When generating core dumps on NVIDIA Drive Tegra devices running QNX, core dump generation may hang when generating CPU core dumps.
Note If core dumps are not generated when running programs built with OptiX/RTCore, try setting the environment variable OPTIX_FORCE_DEPRECATED_LAUNCHER to 1.
Note If core dumps are not generated when running programs on Windows Subsystem for Linux, ensure the debug interface is enabled via setting the registry key >HKEY_LOCAL_MACHINE\SOFTWARE\NVIDIA Corporation\GPUDebugger\EnableInterface to (DWORD) 1 .
Note GPU core dump is supported on GPUs running with Confidential Compute mode only with devtools mode.
Naming of GPU core dump files By default, a GPU core dump is created in the current working directory.
It is named core_TIME_HOSTNAME_PID.nvcudmp where TIME is the number of seconds since the Epoch, HOSTNAME is the host name of the machine running the CUDA application and PID is the process identifier of the CUDA application.
The CUDA_COREDUMP_FILE environment variable can be used to define a template that is used to change the name of a GPU core dump file.
The template can either be an absolute path or a relative path to the current working directory.
The template can contain % specifiers which are substituted by the following patterns when a GPU core dump is created: Specifier Description %h Host name of the machine running the CUDA application %p Process identifier of the CUDA application %t Time as the number of seconds since the Epoch, 1970-01-01 00:00:00 +0000 (UTC) As an example, setting CUDA_COREDUMP_FILE to: export CUDA_COREDUMP_FILE=newName.%h.%p Would result in GPU core dumps being written to newName.myhost.1234 relative to the current working directory.
Setting CUDA_COREDUMP_FILE to: export CUDA_COREDUMP_FILE="/home/$USER/newName.%h.%p" Would result in GPU core dumps being written to the user’s home directory with the same name logic as in the above example.
If CUDA_COREDUMP_FILE points to an existing file of FIFO type (e.g named pipe), the core dump will be streamed to it.
Coredumps may be piped to shell commands via CUDA_COREDUMP_FILE with the following format: export CUDA_COREDUMP_FILE='| cmd > file' For example, to pipe a coredump to gzip use: export CUDA_COREDUMP_FILE='| gzip -9 > cuda-coredump.gz' Note When piping a coredump, the % specifiers will not be recognized.
Enabling user induced GPU core dump generation For the devices that support compute preemption, the user can interrupt a running CUDA process to generate the GPU core dump.
Set the CUDA_ENABLE_USER_TRIGGERED_COREDUMP environment variable to 1 in order to enable generating a user induced GPU core dump.
Setting this environment variable will open a communication pipe for each subsequently running CUDA process.
To change the default pipe file name, set the CUDA_COREDUMP_PIPE environment variable to a specific pipe name.
The default pipe name is in the following format: corepipe.cuda.HOSTNAME.PID where HOSTNAME is the host name of machine running the CUDA application and PID is the process identifier of the CUDA application.
Displaying core dump generation progress By default, when an application crashes and generates a GPU core dump, the application may appear to be unresponsive or frozen until fully generated.
Set the CUDA_COREDUMP_SHOW_PROGRESS environment variable to 1 in order to print core dump generation progress messages to stderr .
coredump: Writing out device table coredump: Finalizing coredump: All done Enabling GPU core dump generation with the CUDA Driver API The Driver API has equivalent settings for all of the environment variables, with the added feature of being able to set different core dump settings per-context instead of globally.
Use cuCoredumpGetAttributeGlobal and cuCoredumpSetAttributeGlobal to fetch or set the global attribute.
Use cuCoredumpGetAttribute and cuCoredumpSetAttribute to fetch or set the per context attribute.
The table below lists the environment variables and the equivalent CUcoredumpSettings flags that are available to manage core dump settings with the Coredump Attributes Control API.
Note The CU_COREDUMP_ENABLE_USER_TRIGGER setting can only be set globally in the driver API and CU_COREDUMP_PIPE must be set (if desired) before user-triggered core dumps are enabled.
GPU core dump configuration parameters  Environment Variable Description Environment Variable: CUDA_ENABLE_COREDUMP_ON_EXCEPTION CUcoredumpSettings Flag: CU_COREDUMP_ENABLE_ON_EXCEPTION Enables GPU core dump generation for exceptions.
Environment Variable: CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION CUcoredumpSettings Flag: CU_COREDUMP_TRIGGER_HOST Triggers host (CPU) core dump after GPU core dump is complete.
Environment Variable: CUDA_ENABLE_LIGHTWEIGHT_COREDUMP CUcoredumpSettings Flag: CU_COREDUMP_LIGHTWEIGHT When enabled, GPU core dumps will not contain the memory dumps (local, shared, global) of the application.
Environment Variable: CUDA_ENABLE_USER_TRIGGERED_COREDUMP CUcoredumpSettings Flag: CU_COREDUMP_ENABLE_USER_TRIGGER Enables user triggerable core dumps by writing to a pipe defined in the COREDUMP_PIPE setting.
Environment Variable: CUDA_COREDUMP_FILE CUcoredumpSettings Flag: CU_COREDUMP_FILE Filename template for the GPU core dump.
Environment Variable: CUDA_COREDUMP_PIPE CUcoredumpSettings Flag: CU_COREDUMP_PIPE Filename template for the user pipe trigger.
Inspecting GPU and GPU+CPU core dumps in cuda-gdb Use the following command to load the GPU core dump into the debugger (cuda-gdb) target cudacore core.cuda.localhost.1234 This will open the core dump file and print the exception encountered during program execution.
Then, issue standard cuda-gdb commands to further investigate application state on the device at the moment it was aborted.
Use the following command to load CPU and GPU core dumps into the debugger (cuda-gdb) target core core.cpu core.cuda This will open the core dump file and print the exception encountered during program execution.
Then, issue standard cuda-gdb commands to further investigate application state on the host and the device at the moment it was aborted.
Kernel Focus  A CUDA application may be running several host threads and many device threads.
To simplify the visualization of information about the state of application, commands are applied to the entity in focus.
When the focus is set to a host thread, the commands will apply only to that host thread (unless the application is fully resumed, for instance).
On the device side, the focus is always set to the lowest granularity level–the device thread. 5.1. Software Coordinates vs.
Hardware Coordinates  A device thread belongs to a block, which in turn belongs to a kernel.
Software and hardware coordinates can be used interchangeably and simultaneously as long as they remain coherent.
Note If software preemption is enabled ( set cuda software_preemption on ), hardware coordinates corresponding to a device thread are likely to change upon resuming execution on the device.
However, software coordinates will remain intact and will not change for the lifetime of the device thread. 5.2. Current Focus  To inspect the current focus, use the cuda command followed by the coordinates of interest: (cuda-gdb) cuda device sm warp lane block thread block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0 (cuda-gdb) cuda kernel block thread kernel 1, block (0,0,0), thread (0,0,0) (cuda-gdb) cuda kernel kernel 1 5.3.
Switching Focus  To switch the current focus, use the cuda command followed by the coordinates to be changed: (cuda-gdb) cuda device 0 sm 1 warp 2 lane 3 [Switching focus to CUDA kernel 1, grid 2, block (8,0,0), thread (67,0,0), device 0, sm 1, warp 2, lane 3] 374 int totalThreads = gridDim.x * blockDim.x; If the specified focus is not fully defined by the command, the debugger will assume that the omitted coordinates are set to the coordinates in the current focus, including the subcoordinates of the block and thread.
(cuda-gdb) cuda thread (15) [Switching focus to CUDA kernel 1, grid 2, block (8,0,0), thread (15,0,0), device 0, sm 1, warp 0, lane 15] 374 int totalThreads = gridDim.x * blockDim.x; The parentheses for the block and thread arguments are optional.
(cuda-gdb) cuda block 1 thread 3 [Switching focus to CUDA kernel 1, grid 2, block (1,0,0), thread (3,0,0), device 0, sm 3, warp 0, lane 3] 374 int totalThreads = gridDim.x * blockDim. 6. Program Execution  Applications are launched the same way in CUDA-GDB as they are with GDB by using the run command.
Interrupting the Application  If the CUDA application appears to be hanging or stuck in an infinite loop, it is possible to manually interrupt the application by pressing CTRL+C.
At that point, the program can be inspected, modified, single-stepped, resumed, or terminated at the user’s discretion.
It is not possible to break into and debug applications that have been launched outside the debugger. 6.2. Single Stepping  Single-stepping device code is supported.
However, unlike host code single-stepping, device code single-stepping works at the warp level.
This means that single-stepping a device kernel advances all the active threads in the warp currently in focus.
In order to advance the execution of more than one warp, a breakpoint must be set at the desired location and then the application must be fully resumed.
A special case is single-stepping over thread barrier calls like: __syncthreads() or cluster-wide barriers.
In this case, an implicit temporary breakpoint is set immediately after the barrier and all threads are resumed until the temporary breakpoint is hit.
To force a function to not be inlined by the compiler, the __noinline__ keyword must be added to the function declaration.
Asynchronous SASS instructions executed on the device, such as the warpgroup instructions, at prior PCs are not guaranteed to be complete.
The following list defines single-step behavior when encountering these APIs: When encountering device side kernel launches (denoted by the >> launch syntax), the step and next commands will have the same behavior, and both will step over the launch call.
On devices prior to Hopper (SM 9.0), stepping into the deprecated cudaDeviceSynchronize() results in undefined behavior.
When stepping a device grid launch to completion, focus will automatically switch back to the CPU.
The cuda kernel focus switching command must be used to switch to another grid of interest (if one is still resident).
Note It is not possible to step into a device launch call (nor the routine launched by the call). 7. Breakpoints and Watchpoints  There are multiple ways to set a breakpoint on a CUDA application.
The commands used to set a breakpoint on device code are the same as the commands used to set a breakpoint on host code.
If a breakpoint is set on device code, the breakpoint will be marked pending until the ELF image of the kernel is loaded.
When a breakpoint is set, it forces all resident GPU threads to stop at this location when it reaches the corresponding PC.
When a breakpoint is hit by one thread, there is no guarantee that the other threads will hit the breakpoint at the same time.
Therefore the same breakpoint may be hit several times, and the user must be careful with checking which thread(s) actually hit(s) the breakpoint.
The disable command can be used to prevent hitting the breakpoint by additional threads. 7.1. Symbolic Breakpoints  To set a breakpoint at the entry of a function, use the break command followed by the name of the function or method: (cuda-gdb) break my_function (cuda-gdb) break my_class::my_method For templatized functions and methods, the full signature must be given: (cuda-gdb) break int my_templatized_function(int) The mangled name of the function can also be used.
To find the mangled name of a function, you can use the following command: (cuda-gdb) set demangle-style none (cuda-gdb) info function my_function_name (cuda-gdb) set demangle-style auto 7.2.
Line Breakpoints  To set a breakpoint on a specific line number, use the following syntax: (cuda-gdb) break my_file.cu:185 If the specified line corresponds to an instruction within templatized code, multiple breakpoints will be created, one for each instance of the templatized code. 7.3. Address Breakpoints  To set a breakpoint at a specific address, use the break command with the address as argument: (cuda-gdb) break *0x1afe34d0 The address can be any address on the device or the host.
7.4. Kernel Entry Breakpoints  To break on the first instruction of every launched kernel, set the break_on_launch option to application: (cuda-gdb) set cuda break_on_launch application See set cuda break_on_launch for more information.
7.5. Conditional Breakpoints  To make the breakpoint conditional, use the optional if keyword or the cond command.
Info CUDA Commands  These are commands that display information about the GPU and the application’s CUDA state.
The available options are: devices information about all the devices sms information about all the active SMs in the current device warps information about all the active warps in the current SM lanes information about all the active lanes in the current warp kernels information about all the active kernels blocks information about all the active blocks in the current kernel threads information about all the active threads in the current kernel launch trace information about the parent kernels of the kernel in focus launch children information about the kernels launched by the kernels in focus contexts information about all the contexts A filter can be applied to every info cuda command.
A restriction can be any of the following: device n sm n warp n lane n kernel n grid n block x[,y] or block (x[,y]) thread x[,y[,z]] or thread (x[,y[,z]]) breakpoint all and breakpoint n where n , x , y , z are integers, or one of the following special keywords: current , any , and all .
Note The breakpoint all and breakpoint n filter are only effective for the info cuda threads command. 8.3.1. info cuda devices  This command enumerates all the GPUs in the system sorted by device index.
(cuda-gdb) info cuda devices Dev PCI Bus/Dev ID Name Description SM Type SMs Warps/SM Lanes/Warp Max Regs/Lane Active SMs Mask 0 06:00.0 GeForce GTX TITAN Z GK110B sm_35 15 64 32 256 0x00000000 1 07:00.0 GeForce GTX TITAN Z GK110B sm_35 15 64 32 256 0x00000000 8.3.2.
info cuda sms  This command shows all the SMs for the device and the associated active warps on the SMs.
(cuda-gdb) info cuda sms SM Active Warps Mask Device 0 * 0 0xffffffffffffffff 1 0xffffffffffffffff 2 0xffffffffffffffff 3 0xffffffffffffffff 4 0xffffffffffffffff 5 0xffffffffffffffff 6 0xffffffffffffffff 7 0xffffffffffffffff 8 0xffffffffffffffff ... 8.3.3. info cuda warps  This command takes you one level deeper and prints all the warps information for the SM in focus.
(cuda-gdb) info cuda warps Wp /Active Lanes Mask/ Divergent Lanes Mask/Active Physical PC/Kernel/BlockIdx Device 0 SM 0 * 0 0xffffffff 0x00000000 0x000000000000001c 0 (0,0,0) 1 0xffffffff 0x00000000 0x0000000000000000 0 (0,0,0) 2 0xffffffff 0x00000000 0x0000000000000000 0 (0,0,0) 3 0xffffffff 0x00000000 0x0000000000000000 0 (0,0,0) 4 0xffffffff 0x00000000 0x0000000000000000 0 (0,0,0) 5 0xffffffff 0x00000000 0x0000000000000000 0 (0,0,0) 6 0xffffffff 0x00000000 0x0000000000000000 0 (0,0,0) 7 0xffffffff 0x00000000 0x0000000000000000 0 (0,0,0) ... 8.3.4. info cuda lanes  This command displays all the lanes (threads) for the warp in focus.
This command supports filters and the default is device current sm current warp current lane all .
(cuda-gdb) info cuda lanes Ln State Physical PC ThreadIdx Device 0 SM 0 Warp 0 * 0 active 0x000000000000008c (0,0,0) 1 active 0x000000000000008c (1,0,0) 2 active 0x000000000000008c (2,0,0) 3 active 0x000000000000008c (3,0,0) 4 active 0x000000000000008c (4,0,0) 5 active 0x000000000000008c (5,0,0) 6 active 0x000000000000008c (6,0,0) 7 active 0x000000000000008c (7,0,0) 8 active 0x000000000000008c (8,0,0) 9 active 0x000000000000008c (9,0,0) 10 active 0x000000000000008c (10,0,0) 11 active 0x000000000000008c (11,0,0) 12 active 0x000000000000008c (12,0,0) 13 active 0x000000000000008c (13,0,0) 14 active 0x000000000000008c (14,0,0) 15 active 0x000000000000008c (15,0,0) 16 active 0x000000000000008c (16,0,0) ... 8.3.5. info cuda kernels  This command displays on all the active kernels on the GPU in focus.
It prints the SM mask, kernel ID, and the grid ID for each kernel with the associated dimensions and arguments.
(cuda-gdb) info cuda kernels Kernel Parent Dev Grid Status SMs Mask GridDim BlockDim Name Args * 1 - 0 2 Active 0x00ffffff (240,1,1) (128,1,1) acos_main parms=...
This command will also show grids that have been launched on the GPU with Dynamic Parallelism.
Kernels with a negative grid ID have been launched from the GPU, while kernels with a positive grid ID have been launched from the CPU. 8.3.6. info cuda blocks  This command displays all the active or running blocks for the kernel in focus.
(cuda-gdb) info cuda blocks BlockIdx To BlockIdx Count State Kernel 1 * (0,0,0) (191,0,0) 192 running Coalescing can be turned off as follows in which case more information on the Device and the SM get displayed: (cuda-gdb) set cuda coalescing off The following is the output of the same command when coalescing is turned off.
(cuda-gdb) info cuda blocks BlockIdx State Dev SM Kernel 1 * (0,0,0) running 0 0 (1,0,0) running 0 3 (2,0,0) running 0 6 (3,0,0) running 0 9 (4,0,0) running 0 12 (5,0,0) running 0 15 (6,0,0) running 0 18 (7,0,0) running 0 21 (8,0,0) running 0 1 ... 8.3.7. info cuda threads  This command displays the application’s active CUDA blocks and threads with the total count of threads in those blocks.
Also displayed are the virtual PC and the associated source file and the line number information.
The outputs are coalesced by default as follows: (cuda-gdb) info cuda threads BlockIdx ThreadIdx To BlockIdx ThreadIdx Count Virtual PC Filename Line Device 0 SM 0 * (0,0,0 (0,0,0) (0,0,0) (31,0,0) 32 0x000000000088f88c acos.cu 376 (0,0,0)(32,0,0) (191,0,0) (127,0,0) 24544 0x000000000088f800 acos.cu 374 ...
Coalescing can be turned off as follows in which case more information is displayed with the output.
(cuda-gdb) info cuda threads BlockIdx ThreadIdx Virtual PC Dev SM Wp Ln Filename Line Kernel 1 * (0,0,0) (0,0,0) 0x000000000088f88c 0 0 0 0 acos.cu 376 (0,0,0) (1,0,0) 0x000000000088f88c 0 0 0 1 acos.cu 376 (0,0,0) (2,0,0) 0x000000000088f88c 0 0 0 2 acos.cu 376 (0,0,0) (3,0,0) 0x000000000088f88c 0 0 0 3 acos.cu 376 (0,0,0) (4,0,0) 0x000000000088f88c 0 0 0 4 acos.cu 376 (0,0,0) (5,0,0) 0x000000000088f88c 0 0 0 5 acos.cu 376 (0,0,0) (6,0,0) 0x000000000088f88c 0 0 0 6 acos.cu 376 (0,0,0) (7,0,0) 0x000000000088f88c 0 0 0 7 acos.cu 376 (0,0,0) (8,0,0) 0x000000000088f88c 0 0 0 8 acos.cu 376 (0,0,0) (9,0,0) 0x000000000088f88c 0 0 0 9 acos.cu 376 ...
If some threads are not currently running on the hardware, they will create holes in the thread ranges.
For instance, if a kernel consist of 2 blocks of 16 threads, and only the 8 lowest threads are active, then 2 coalesced ranges will be printed: one range for block 0 thread 0 to 7, and one range for block 1 thread 0 to 7.
(cuda-gdb) info cuda threads breakpoint all BlockIdx ThreadIdx Virtual PC Dev SM Wp Ln Filename Line Kernel 0 (1,0,0) (0,0,0) 0x0000000000948e58 0 11 0 0 infoCommands.cu 12 (1,0,0) (1,0,0) 0x0000000000948e58 0 11 0 1 infoCommands.cu 12 (1,0,0) (2,0,0) 0x0000000000948e58 0 11 0 2 infoCommands.cu 12 (1,0,0) (3,0,0) 0x0000000000948e58 0 11 0 3 infoCommands.cu 12 (1,0,0) (4,0,0) 0x0000000000948e58 0 11 0 4 infoCommands.cu 12 (1,0,0) (5,0,0) 0x0000000000948e58 0 11 0 5 infoCommands.cu 12 (cuda-gdb) info cuda threads breakpoint 2 lane 1 BlockIdx ThreadIdx Virtual PC Dev SM Wp Ln Filename Line Kernel 0 (1,0,0) (1,0,0) 0x0000000000948e58 0 11 0 1 infoCommands.cu 12 8.3.8.
info cuda launch trace  This command displays the kernel launch trace for the kernel in focus.
For each kernel in the trace, the command prints the level of the kernel in the trace, the kernel ID, the device ID, the grid Id, the status, the kernel dimensions, the kernel name, and the kernel arguments.
(cuda-gdb) info cuda launch trace Lvl Kernel Dev Grid Status GridDim BlockDim Invocation * 0 3 0 -7 Active (32,1,1) (16,1,1) kernel3(c=5) 1 2 0 -5 Terminated (240,1,1) (128,1,1) kernel2(b=3) 2 1 0 2 Active (240,1,1) (128,1,1) kernel1(a=1) A kernel that has been launched but that is not running on the GPU will have a Pending status.
For the few cases, when the debugger cannot determine if a kernel is pending or terminated, the status is set to Undetermined .
Note With set cuda software_preemption on , no kernel will be reported as active. 8.3.9. info cuda launch children  This command displays the list of non-terminated kernels launched by the kernel in focus.
For each kernel, the kernel ID, the device ID, the grid Id, the kernel dimensions, the kernel name, and the kernel parameters are displayed.
(cuda-gdb) info cuda launch children Kernel Dev Grid GridDim BlockDim Invocation * 3 0 -7 (1,1,1) (1,1,1) kernel5(a=3) 18 0 -8 (1,1,1) (32,1,1) kernel4(b=5) This command supports filters and the default is kernel all . 8.3.10. info cuda contexts  This command enumerates all the CUDA contexts running on all GPUs.
(cuda-gdb) info cuda contexts Context Dev State 0x080b9518 0 inactive * 0x08067948 0 active 8.3.11.
info cuda managed  This command shows all the static managed variables on the device or on the host depending on the focus.
(cuda-gdb) info cuda managed Static managed variables on device 0 are: managed_var = 3 managed_consts = {one = 1, e = 2.71000004, pi = 3.1400000000000001} 8.4.
Disassembly  The device SASS code can be disassembled using the standard GDB disassembly instructions such as x/i and display/i .
(cuda-gdb) x/4i $pc-32 0xa689a8 : MOV R0, c[0x0][0x34] 0xa689b8 : MOV R3, c[0x0][0x28] 0xa689c0 : IMUL R2, R0, R3 => 0xa689c8 : MOV R0, c[0x0][0x28] Note For disassembly instruction to work properly, cuobjdump must be installed and present in your $PATH .
For Maxwell (SM 5.0) and newer architectures, if an instruction triggers an exception it will be prefixed with *> .
For example, consider the following exception: CUDA Exception: Warp Illegal Address The exception was triggered at PC 0x555555c08620 (memexceptions_kernel.cu:17) Thread 1 "memexceptions" received signal CUDA_EXCEPTION_14, Warp Illegal Address.
[Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0] 0x0000555555c08fb0 in exception_kernel>> (data=0x7fffccc00000, exception=MMU_FAULT) at memexceptions_kernel.cu:50 50 } (cuda-gdb) The disas command can be used to view both the PC and the error PC that triggered the exception.
(cuda-gdb) disas $pc,+16 Dump of assembler code from 0x555555c08fb0 to 0x555555c08fc0: => 0x0000555555c08fb0 : ERRBAR End of assembler dump.
(cuda-gdb) disas $errorpc,+16 Dump of assembler code from 0x555555c08620 to 0x555555c08630: *> 0x0000555555c08620 : ST.E.U8.STRONG.SYS [R6.64], R5 End of assembler dump. 8.5. Registers  The device registers code can be inspected/modified using the standard GDB commands such as info registers .
(cuda-gdb) info registers $R0 $R1 $R2 $R3 R0 0xf0 240 R1 0xfffc48 16776264 R2 0x7800 30720 R3 0x80 128 The registers are also accessible as $R built-in variables, for example: (cuda-gdb) printf "%d %d ", $R0*$R3, $R2 30720 30720 Values of predicate and CC registers can be inspecting by printing system registers group or by using their respective pseudo-names: $P0 ..
(cuda-gdb) info registers system P0 0x1 1 P1 0x1 1 P2 0x0 0 P3 0x0 0 P4 0x0 0 P5 0x0 0 P6 0x1 1 CC 0x0 0 8.6.
Const banks  Memory allocated in the constant address space of GPU memory resides in two dimensional arrays called constant banks.
The memory address of a given bank/offset pair is obtained via the convenience function $_cuda_const_bank(bank, offset) .
(cuda-gdb) disass $pc,+16 Dump of assembler code from 0x7fffd5043d40 to 0x7fffd5043d50: => 0x00007fffd5043d40 : MOV R0, c[0x0][0xc] End of assembler dump.
Event Notifications  As the application is making forward progress, CUDA-GDB notifies the users about kernel events and context events.
Within CUDA-GDB, kernel refers to the device code that executes on the GPU, while context refers to the virtual address space on the GPU for the kernel.
You can enable output of CUDA context and kernel events to review the flow of the active contexts and kernels.
By default, only context event messages are displayed. 9.1. Context Events  Any time a CUDA context is created, pushed, popped, or destroyed by the application, CUDA-GDB can optionally display a notification message.
[Context Create of context 0xad2fe60 on Device 0] [Context Destroy of context 0xad2fe60 on Device 0] By default, context event notification is disabled.
(cuda-gdb) set cuda context_events off CUDA-GDB does not display the context event notification messages (default).
(cuda-gdb) set cuda context_events on CUDA-GDB displays the context event notification messages. 9.2. Kernel Events  Any time CUDA-GDB is made aware of the launch or the termination of a CUDA kernel, a notification message can be displayed.
The message includes the kernel id, the kernel name, and the device to which the kernel belongs.
[Launch of CUDA Kernel 1 (kernel3) on Device 0] [Termination of CUDA Kernel 1 (kernel3) on Device 0] The kernel event notification policy is controlled with kernel_events and kernel_events_depth options.
(cuda-gdb) set cuda kernel_events none Possible options are: none no kernel, application or system (default) application kernel launched by the user application system any kernel launched by the driver, such as memset all any kernel, application and system (cuda-gdb) set cuda kernel_events_depth 0 Controls the maximum depth of the kernels after which no kernel event notifications will be displayed.
A value of zero means that there is no maximum and that all the kernel notifications are displayed.
A value of one means that the debugger will display kernel event notifications only for kernels launched from the CPU (default). 10. Automatic Error Checking  10.1.
Checking API Errors  CUDA-GDB can automatically check the return code of any driver API or runtime API call.
Three modes are supported: hide CUDA API call failures are not reported ignore Warning message is printed for every fatal CUDA API call failure (default) stop The application is stopped when a CUDA API call returns a fatal error ignore_all Warning message is printed for every CUDA API call failure stop_all The application is stopped when a CUDA API call returns any error Note The success return code and other non-error return codes are ignored.
For the runtime API, they are cudaSuccess and cudaErrorNotReady . 10.2. GPU Error Reporting  With improved GPU error reporting in CUDA-GDB, application bugs are now easier to identify and easy to fix.
The following table shows the new errors that are reported on GPUs with compute capability sm_20 and higher.
Note Continuing the execution of your application after these errors are found can lead to application termination or indeterminate results.
Note Warp errors may result in instructions to continue executing before the exception is recognized and reported.
The reported $errorpc shall contain the precise address of the instruction that caused the exception.
If the warp exits after the instruction causing exception has executed, but before the exception has been recognized and reported, it may result in the exception not being reported.
To help avoid this scenario of unreported exceptions: For Volta+ architectures, compile the application with -G .
Rely on the compute-sanitizer memcheck tool to catch accesses that can lead to an exception.
CUDA Exception Codes  Exception Code Precision of the Error Scope of the Error Description CUDA_EXCEPTION_0 : “Device Unknown Exception” Unknown Global error on the GPU This is a global GPU error caused by the application which does not match any of the listed error codes below.
Potentially, this may be due to Device Hardware Stack overflows or a kernel generating an exception very close to its termination.
CUDA_EXCEPTION_1 : “Deprecated” Deprecated Deprecated This exception is deprecated and should be treated as CUDA_EXCEPTION_0 .
CUDA_EXCEPTION_2 : “Lane User Stack Overflow” Precise Per lane/thread error This occurs when a thread exceeds its stack memory limit.
CUDA_EXCEPTION_3 : “Device Hardware Stack Overflow” Precise Global error on the GPU This occurs when the application triggers a global hardware stack overflow.
The main cause of this error is large amounts of divergence in the presence of function calls.
CUDA_EXCEPTION_4 : “Warp Illegal Instruction” Precise Warp error This occurs when any thread within a warp has executed an illegal instruction.
CUDA_EXCEPTION_5 : “Warp Out-of-range Address” Precise Warp error This occurs when any thread within a warp accesses an address that is outside the valid range of local or shared memory regions.
CUDA_EXCEPTION_6 : “Warp Misaligned Address” Precise Warp error This occurs when any thread within a warp accesses an address in the local or shared memory segments that is not correctly aligned.
CUDA_EXCEPTION_7 : “Warp Invalid Address Space” Precise Warp error This occurs when any thread within a warp executes an instruction that accesses a memory space not permitted for that instruction.
CUDA_EXCEPTION_8 : “Warp Invalid PC” Precise Warp error This occurs when any thread within a warp advances its PC beyond the 40-bit address space.
CUDA_EXCEPTION_9 : “Warp Hardware Stack Overflow” Precise Warp error This occurs when any thread in a warp triggers a hardware stack overflow.
CUDA_EXCEPTION_10 : “Device Illegal Address” Precise Global error This occurs when a thread accesses an illegal(out of bounds) global address.
CUDA_EXCEPTION_11 : “Deprecated” Deprecated Deprecated This exception is deprecated and should be treated as CUDA_EXCEPTION_0 .
CUDA_EXCEPTION_12 : “Warp Assert” Precise Per warp This occurs when any thread in the warp hits a device side assertion.
CUDA_EXCEPTION_13 : “Deprecated” Deprecated Deprecated This exception is deprecated and should be treated as CUDA_EXCEPTION_0 .
CUDA_EXCEPTION_14 : “Warp Illegal Address” Precise Per warp This occurs when a thread accesses an illegal(out of bounds) global/local/shared address.
CUDA_EXCEPTION_15 : “Invalid Managed Memory Access” Precise Per host thread This occurs when a host thread attempts to access managed memory currently used by the GPU.
CUDA_EXCEPTION_17 : “Cluster Out-of-range Address” Not precise Per Cuda Cluster This occurs when any thread within a block accesses an address that is outside the valid range of shared memory regions belonging to the cluster.
CUDA_EXCEPTION_18 : “Cluster Target Block Not Present” Not precise Per Cuda Cluster This occurs when any thread within a block accesses another block that is outside the valid range of blocks belonging to the cluster. 10.3. Autostep  Autostep is a command to increase the precision of CUDA exceptions to the exact lane and instruction, when they would not have been otherwise.
Under normal execution, an exception may be reported several instructions after the exception occurred, or the exact thread where an exception occurred may not be known unless the exception is a lane error.
However, the precise origin of the exception can be determined if the program is being single-stepped when the exception occurs.
Single- stepping manually is a slow and tedious process; stepping takes much longer than normal execution and the user has to single-step each warp individually.
Autostep aides the user by allowing them to specify sections of code where they suspect an exception could occur, and these sections are automatically and transparently single- stepped the program is running.
The rest of the program is executed normally to minimize the slow-down caused by single-stepping.
The precise origin of an exception will be reported if the exception occurs within these sections.
Thus the exact instruction and thread where an exception occurred can be found quickly and with much less effort by using autostep.
Autostep Usage autostep [LOCATION] autostep [LOCATION] for LENGTH [lines|instructions] LOCATION may be anything that you use to specify the location of a breakpoint, such as a line number, function name, or an instruction address preceded by an asterisk.
LENGTH specifies the size of the autostep window in number of lines or instructions ( lines and instructions can be shortened, e.g., l or i ).
In case of divergence, the length of the autostep window is determined by the number of lines or instructions the first active lane in each warp executes.
Divergent lanes are also single stepped, but the instructions they execute do not count towards the length of the autostep window.
If a breakpoint occurs while inside an autostep window, the warp where the breakpoint was hit will not continue autostepping when the program is resumed.
If an autostep is encountered while another autostep is being executed, then the second autostep is ignored.
If an autostep is set before the location of a memory error and no memory error is hit, then it is possible that the chosen window is too small.
This may be caused by the presence of function calls between the address of the autostep location and the instruction that triggers the memory error.
In that situation, either increase the size of the window to make sure that the faulty instruction is included, or move to the autostep location to an instruction that will be executed closer in time to the faulty instruction.
Related Commands Autosteps and breakpoints share the same numbering so most commands that work with breakpoints will also work with autosteps.
(cuda-gdb) info autosteps Num Type Disp Enb Address What 1 autostep keep y 0x0000000000401234 in merge at sort.cu:30 for 49 instructions 3 autostep keep y 0x0000000000489913 in bubble at sort.cu:94 for 11 lines disable autosteps disables an autostep.
ignore n i tells the debugger to not single-step the next i times the debugger enters the window for autostep n .
This command already exists for breakpoints. 11. Walk-Through Examples  The chapter contains two CUDA-GDB walk-through examples: Example: bitreverse Example: autostep Example: MPI CUDA Application 11.1.
Example: bitreverse  This section presents a walk-through of CUDA-GDB by debugging a sample application–called bitreverse –that performs a simple 8 bit reversal on a data set.
Source Code 1 #include 2 #include 3 4   Simple 8-bit bit reversal Compute test 5 6 #define N 256 7 8 __global__ void bitreverse(void *data) { 9 unsigned int *idata = (unsigned int*)data; 10 extern __shared__ int array[]; 11 12 array[threadIdx.x] = idata[threadIdx.x]; 13 14 array[threadIdx.x] = ((0xf0f0f0f0 & array[threadIdx.x]) >> 4) | 15 ((0x0f0f0f0f & array[threadIdx.x]) > 2) | 17 ((0x33333333 & array[threadIdx.x]) > 1) | 19 ((0x55555555 & array[threadIdx.x]) >>(d); 36 37 cudaMemcpy(odata, d, sizeof(int)*N, 38 cudaMemcpyDeviceToHost); 39 40 for (i = 0; i %u ", idata[i], odata[i]); 42 43 cudaFree((void*)d); 44 return 0; 45 } 11.1.1.
Walking through the Code  Begin by compiling the bitreverse.cu CUDA application for debugging by entering the following command at a shell prompt: $ nvcc -g -G bitreverse.cu -o bitreverse This command assumes that the source file name is bitreverse.cu and that no additional compiler flags are required for compilation.
See also Debug Compilation Start the CUDA debugger by entering the following command at a shell prompt: $ cuda-gdb bitreverse Set breakpoints.
Run the CUDA application, and it executes until it reaches the first breakpoint ( main ) set in 3 .
(cuda-gdb) run Starting program: /Users/CUDA_User1/docs/bitreverse Reading symbols for shared libraries ..++.
done Breakpoint 1, main () at bitreverse.cu:25 25 void *d = NULL; int i; At this point, commands can be entered to advance execution or to print the program state.
done [Context Create of context 0x80f200 on Device 0] [Launch of CUDA Kernel 0 (bitreverse>>) on Device 0] Breakpoint 3 at 0x8667b8: file bitreverse.cu, line 21.
[Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0] Breakpoint 2, bitreverse>> (data=0x110000) at bitreverse.cu:9 9 unsigned int *idata = (unsigned int*)data; CUDA−GDB has detected that a CUDA device kernel has been reached.
Since thread ( 0,0,0 ) reverses the value of 0 , switch to a different thread to show more interesting data: (cuda-gdb) cuda thread 170 [Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread (170,0,0), device 0, sm 0, warp 5, lane 10] Delete the breakpoints and continue the program to completion: (cuda-gdb) delete breakpoints Delete all breakpoints? (y or n) y (cuda-gdb) continue Continuing.
Example: autostep  This section shows how to use the autostep command and demonstrates how it helps increase the precision of memory error reporting.
This will cause CUDA_EXCEPTION_10 "Device Illegal Address" to be thrown when we try to access the integer that corresponds with block 3, thread 39.
This exception should occur at line 16 when we try to write to that value. 11.2.1. Debugging with Autosteps  Compile the example and start CUDA−GDB as normal.
We begin by running the program: (cuda-gdb) run Starting program: /home/jitud/cudagdb_test/autostep_ex/example [Thread debugging using libthread_db enabled] [New Thread 0x7ffff5688700 (LWP 9083)] [Context Create of context 0x617270 on Device 0] [Launch of CUDA Kernel 0 (example>>) on Device 0] Program received signal CUDA_EXCEPTION_10, Device Illegal Address.
[Switching focus to CUDA kernel 0, grid 1, block (1,0,0), thread (0,0,0), device 0, sm 1, warp 0, lane 0] 0x0000000000796f60 in example (data=0x200300000) at example.cu:17 17 *(data[idx1]) = value3; As expected, we received a CUDA_EXCEPTION_10 .
Since CUDA_EXCEPTION_10 is a Global error, there is no thread information that is reported, so we would manually have to inspect all 512 threads.
To get more accurate information, we reason that since CUDA_EXCEPTION_10 is a memory access error, it must occur on code that accesses memory.
This happens on lines 11, 12, 16, 17, and 18, so we set two autostep windows for those areas: (cuda-gdb) autostep 11 for 2 lines Breakpoint 1 at 0x796d18: file example.cu, line 11.
Created autostep of length 2 lines (cuda-gdb) autostep 16 for 3 lines Breakpoint 2 at 0x796e90: file example.cu, line 16.
Created autostep of length 3 lines Finally, we run the program again with these autosteps: (cuda-gdb) run The program being debugged has been started already.
Start it from the beginning? (y or n) y [Termination of CUDA Kernel 0 (example>>) on Device 0] Starting program: /home/jitud/cudagdb_test/autostep_ex/example [Thread debugging using libthread_db enabled] [New Thread 0x7ffff5688700 (LWP 9089)] [Context Create of context 0x617270 on Device 0] [Launch of CUDA Kernel 1 (example>>) on Device 0] [Switching focus to CUDA kernel 1, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0] Program received signal CUDA_EXCEPTION_10, Device Illegal Address.
[Current focus set to CUDA kernel 1, grid 1, block (3,0,0), thread (32,0,0), device 0, sm 1, warp 3, lane 0] Autostep precisely caught exception at example.cu:16 (0x796e90) This time we correctly caught the exception at line 16.
Even though CUDA_EXCEPTION_10 is a global error, we have now narrowed it down to a warp error, so we now know that the thread that threw the exception must have been in the same warp as block 3, thread 32.
In this example, we have narrowed down the scope of the error from 512 threads down to 32 threads just by setting two autosteps and re−running the program. 11.3. Example: MPI CUDA Application  For large scale MPI CUDA application debugging, NVIDIA recommends using parallel debuggers supplied by our partners Allinea and Totalview.
However, for debugging smaller applications, or for debugging just a few processes in a large application, CUDA-GDB can be used.
If the cluster nodes have xterm support, launch CUDA-GDB in the same way you would launch gdb with your job launcher.
For example: $ mpirun -np 4 -host nv1,nv2 xterm -e cuda-gdb a.out You may have to export the DISPLAY variable to make sure that the xterm finds its way back to your display.
For example: $ mpirun -np 4 -host nv1,nv2 -x DISPLAY=host.nvidia.com:0 xterm -e cuda-gdb a.out Job launchers have different ways of exporting environment variables to the cluster nodes.
When xterm is not supported by your cluster environment, you can insert a spin loop inside your program, ssh to the compute node(s), and attach onto the MPI processes.
Somewhere near the start of your program, add a code snippet similar to the following: { int i = 0 ; char host [ 256 ]; printf ( "PID %d on node %s is ready for attach   " , getpid (), host ); fflush ( stdout ); while ( 0 == i ) { sleep ( 5 ); } } Recompile and launch the application.
Set the variable i to 1 to break out of the loop: $ mpirun -np 2 -host nv1,nv2 a.out PID 20060 on node nv1 is ready for attach PID 5488 on node nv2 is ready for attach $ ssh nv1 [nv1]$ cuda-gdb --pid 5488 $ ssh nv2 [nv2]$ cuda-gdb --pid 20060 For larger applications, you can conditionalize the spin loop based on the MPI rank using the MPI_Comm_rank function.
For devices with compute capability below 6.0, the software preemption workaround described in Multiple Debuggers does not work with MPI applications.
If CUDA_VISIBLE_DEVICES is set, it may cause problems with the GPU selection logic in the MPI application.
It may also prevent CUDA IPC working between GPUs on a node. 12. Tips and Tricks  This section serves as reference to advanced settings and various tips and tricks users of CUDA-GDB can utilize which are not documented elsewhere.
12.1. set cuda break_on_launch  To break on the first instruction of every launched kernel, set the break_on_launch option to application: (cuda-gdb) set cuda break_on_launch application Possible options are: none no kernel, application or system (default) application kernel launched by the user application system any kernel launched by the driver, such as memset all any kernel, application and system Those automatic breakpoints are not displayed by the info breakpoints command and are managed separately from individual breakpoints.
Turning off the option will not delete other individual breakpoints set to the same address and vice-versa. 12.2. set cuda launch_blocking  When enabled, the kernel launches are synchronous as if the environment variable CUDA_LAUNCH_BLOCKING had been set to 1.
(cuda-gdb) set cuda launch_blocking off The kernel launches are launched synchronously or asynchronously as dictacted by the application.
If the application has already started, the change will only take affect after the current session has terminated. 12.3. set cuda notify  Any time a CUDA event occurs, the debugger needs to be notified.
The host thread to receive that special signal is determined with the set cuda notify option.
(cuda-gdb) set cuda notify youngest The host thread with the smallest thread id will receive the notification signal (default).
(cuda-gdb) set cuda notify random An arbitrary host thread will receive the notification signal. 12.4. set cuda ptx_cache  Before accessing the value of a variable, the debugger checks whether the variable is live or not at the current PC.
On CUDA devices, the variables may not be live all the time and will be reported as “Optimized Out”.
CUDA-GDB offers an option to circumvent this limitation by caching the value of the variable at the PTX register level.
Each source variable is compiled into a PTX register, which is later mapped to one or more hardware registers.
Using the debug information emitted by the compiler, the debugger may be able cache the value of a PTX register based on the latest hardware register it was mapped to at an earlier time.
When enabled, the cached value will be displayed as the normal value read from an actual hardware register and indicated with the (cached) prefix.
This setting is the default and is always safe. 12.5. set cuda single_stepping_optimizations  Single-stepping can take a lot of time.
When enabled, this option tells the debugger to use safe tricks to accelerate single-stepping.
(cuda-gdb) set cuda single_stepping_optimizations off The debugger will not try to accelerate single-stepping.
(cuda-gdb) set cuda single_stepping_optimizations on The debugger will use safe techniques to accelerate single-stepping.
This is the default starting with the 6.0 release. 12.6. set cuda thread_selection  When the debugger must choose an active thread to focus on, the decision is guided by a heuristics.
(cuda-gdb) set cuda thread_selection logical The thread with the lowest blockIdx/threadIdx coordinates is selected.
(cuda-gdb) set cuda thread_selection physical The thread with the lowest dev/sm/warp/lane coordinates is selected. 12.7. set cuda value_extrapolation  Before accessing the value of a variable, the debugger checks whether the variable is live or not at the current PC.
CUDA-GDB offers an option to opportunistically circumvent this limitation by extrapolating the value of a variable when the debugger would otherwise mark it as optimized out.
If the register that was used to store the value of a variable has been reused since the last time the variable was seen as live, then the reported value will be wrong.
(cuda-gdb) set cuda value_extrapolation off The debugger only read the value of live variables.
(cuda-gdb) set cuda value_extrapolation on The debugger will attempt to extrapolate the value of variables beyound their respecitve live ranges.
This setting may report erroneous values. 12.8. Debugging Docker Containers  When debugging an application within a Docker container, the PTRACE capability needs to be enabled.
The user needs to also ensure that the root file system has both read/write permissions set.
To enable the PTRACE capability, add the following to your Docker run command: --cap-add=SYS_PTRACE 12.9.
Switching to Classic Debugger Backend  A new debugger backend named the Unified Debugger (UD) has been introduced on Linux platforms with the CTK 11.8 release.
UD allows for a unified debugger backend shared with debugging tools such as cuda-gdb and NVIDIA® Nsight™ VSE.
The previous debugger backend, known as the classic debugger backend, can still be used by setting CUDBG_USE_LEGACY_DEBUGGER to 1 in the environment before starting CUDA-GDB.
Users must switch to the classic debugger backend to debug their applications on Maxwell GPUs. 12.10. Thread Block Clusters  CUDA applications that make use of Thread Block Clusters will see the cluster index displayed in the CUDA focus.
Both cluster index and cluster dimension can be queried by printing the convenience variables clusterIdx and clusterDim . 12.11. Debugging OptiX/RTCore applications  When debugging programs built with OptiX/RTCore, it may be necessary to set the environment variable OPTIX_FORCE_DEPRECATED_LAUNCHER to 1.
If breakpoints are unable to be hit, try setting this environment variable before starting your application. 12.12. Debugging on Windows Subsystem for Linux  If you are unable to use the debugger on Windows Subsystem for Linux, make sure the debug interface is enabled by setting the registry key >HKEY_LOCAL_MACHINE\SOFTWARE\NVIDIA Corporation\GPUDebugger\EnableInterface to (DWORD) 1 13.
Supported Platforms  Host Platform Requirements CUDA-GDB is supported on all the platforms supported by the CUDA toolkit with which it is shipped.
GPU Requirements Debugging is supported on all CUDA-capable GPUs supported by the current CUDA release.
GDB Python integration GDB Python integration is supported in cuda-gdb with a multiple builds mechanism in order to support multiple python3 interpreters across different platforms.
The cuda-gdb program is a shell script that selects the associated supported cuda-gdb binary based on the version of python available on the system.
Support exists for the following Python versions: Python 3.8 , Python 3.9 , Python 3.10 , Python 3.11 , and Python 3.12 Windows Subsystem for Linux (WSL) cuda-gdb supports debugging CUDA application on WSL2.
Make sure this capability is enabled via the registry key >HKEY_LOCAL_MACHINE\SOFTWARE\NVIDIA Corporation\GPUDebugger\EnableInterface set to (DWORD) 1 .
Debugging compute-intensive apps may require to increase or disable TDR . 14. Common Issues on Supported Operating Systems  The following are known issues with the current release on supported operating systems and how to fix them.
Python not initialized This happens due to a missing Python 3.x library on the machine, installing it fixes the issue.
This can also be caused by having a mismatched major.minor version of libpython installed with the default python3 interpreter in PATH.
For example, the following command would tell us that a libpython3.8.so* needs to be installed in a default library search path: $ python3 --version Python 3.8.10 Specific commands to install the proper libpython are below.
RHEL 8/9 $ sudo yum -y install python3-libs Debian 10/11/12 $ sudo apt-get -y install libpython3-stdlib Fedora 39 $ sudo yum -y install python3-libs OpenSUSE 15 $ sudo zypper install -y libpython3 Ubuntu 20.04/22.04 $ sudo apt-get -y install python3.8 $ sudo apt-get -y install libpython3.8 15.
On Windows Subsystem for Linux (WSL), the r555 WDDM driver released with the CUDA Toolkit 12.5 has a known issue when CUDA_ENABLE_COREDUMP_ON_EXCEPTION=1 that will prevent coredump file generation and may cause a segmentation fault.
Setting a breakpoint on a line within a __device__ or __global__ function before its module is loaded may result in the breakpoint being temporarily set on the first line of a function below in the source code.
As soon as the module for the targeted function is loaded, the breakpoint will be reset properly.
In those situations, the breakpoint can be safely ignored, and the application can be resumed.
When remotely debugging 32-bit applications on a 64-bit server, the cuda-gdbserver binary used must be 32-bit.
Attaching to a CUDA application with Software Preemption enabled in cuda-gdb is not supported.
Attaching to the MPS server process (nvidia-cuda-mps-server) using cuda-gdb, or starting the MPS server with cuda-gdb is not supported.
If a CUDA application is started in the MPS client mode with cuda-gdb, the MPS client will wait until all other MPS clients have terminated, and will then run as non-MPS application.
Significant performance degradation will occur when the debugger steps over inlined routines.
Because inlined code blocks may have multiple exit points, under the hood, the debugger steps every single instruction until an exit point is reached, which incurs considerable cost for large routines.
The following actions are recommended to avoid this problem: Avoid using __forceinline__ when declaring a function.
(For code is compiled with debug information, only routines declared with the __forceinline__ keyword are actually inlined) Use the until command to step over inlined subroutines.
On Jetson, calls to the cuda API might result in the debugger jumping to _dl_catch_exception().
On Jetson and Drive devices GPU debugging works correctly only if the debugger is run with the root permissions.
Changes to devfs node permissions are required for the debugger to work without running as root.
Debugger can miss reporting an induced trap( __trap() ) in case it is the next instruction executed after the device resumes from a breakpoint.
Debugger can miss reporting breakpoints or exceptions during resume in case new warps are launched on a previously empty SM.
Use of Python scripting functionality will expose cuda-gdb to the same vulnerabilities as those in the system libpython version.
Debugger doesn’t support accesses to shared memory allocations that are imported from other processes using the CUDA IPC APIs.
Attempts to access these shared memory allocations by the debugger will result in an error stating access to memory allocations shared via IPC is not supported.
break_on_launch will not function with OptiX/RTCore programs unless OPTIX_FORCE_DEPRECATED_LAUNCHER is set to 1 .
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 16.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 16.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
--suppress-async-bulk-multicast-advisory-warning ( -suppress-async-bulk-multicast-advisory-warning ) 4.2.9.1.22.
Introduction v12.5 | PDF | Archive NVIDIA CUDA Compiler Driver NVCC The documentation for nvcc , the CUDA compiler driver.
CUDA Programming Model  The CUDA Toolkit targets a class of applications whose control part runs as a process on a general purpose computing device, and which use one or more NVIDIA GPUs as coprocessors for accelerating single program, multiple data (SPMD) parallel jobs.
Such jobs are self-contained, in the sense that they can be executed and completed by a batch of GPU threads entirely without intervention by the host process, thereby gaining optimal benefit from the parallel graphics hardware.
The GPU code is implemented as a collection of functions in a language that is essentially C++, but with some annotations for distinguishing them from the host code, plus annotations for distinguishing different types of data memory that exists on the GPU.
Such functions may have parameters, and they can be called using a syntax that is very similar to regular C function calling, but slightly extended for being able to specify the matrix of GPU threads that must execute the called function.
For more information on the CUDA programming model, consult the CUDA C++ Programming Guide . 1.1.2. CUDA Sources  Source files for CUDA applications consist of a mixture of conventional C++ host code, plus GPU device functions.
The CUDA compilation trajectory separates the device functions from the host code, compiles the device functions using the proprietary NVIDIA compilers and assembler, compiles the host code using a C++ host compiler that is available, and afterwards embeds the compiled GPU functions as fatbinary images in the host object file.
In the linking stage, specific CUDA runtime libraries are added for supporting remote SPMD procedure calling and for providing explicit GPU manipulation such as allocation of GPU memory buffers and host-GPU data transfer. 1.1.3. Purpose of NVCC  The compilation trajectory involves several splitting, compilation, preprocessing, and merging steps for each CUDA source file.
It is the purpose of nvcc , the CUDA compiler driver, to hide the intricate details of CUDA compilation from developers.
It accepts a range of conventional compiler options, such as for defining macros and include/library paths, and for steering the compilation process.
All non-CUDA compilation steps are forwarded to a C++ host compiler that is supported by nvcc , and nvcc translates its options to appropriate host compiler command line options. 1.2. Supported Host Compilers  A general purpose C++ host compiler is needed by nvcc in the following situations: During non-CUDA phases (except the run phase), because these phases will be forwarded by nvcc to this compiler.
During CUDA phases, for several preprocessing stages and host code compilation (see also The CUDA Compilation Trajectory ).
nvcc assumes that the host compiler is installed with the standard method designed by the compiler provider.
If the host compiler installation is non-standard, the user must make sure that the environment is set appropriately and use relevant nvcc compile options.
The following documents provide detailed information about supported host compilers: NVIDIA CUDA Installation Guide for Linux NVIDIA CUDA Installation Guide for Microsoft Windows On all platforms, the default host compiler executable ( gcc and g++ on Linux and cl.exe on Windows) found in the current execution search path will be used, unless specified otherwise with appropriate options (see File and Path Specifications ).
Note, nvcc does not support the compilation of file paths that exceed the maximum path length limitations of the host system.
To support the compilation of long file paths, please refer to the documentation for your system. 2. Compilation Phases  2.1.
NVCC Identification Macro  nvcc predefines the following macros: __NVCC__ Defined when compiling C/C++/CUDA source files.
__CUDACC_RDC__ Defined when compiling CUDA source files in relocatable device code mode (see NVCC Options for Separate Compilation ).
__CUDACC_EWP__ Defined when compiling CUDA source files in extensible whole program mode (see Options for Specifying Behavior of Compiler/Linker ).
__CUDACC_DEBUG__ Defined when compiling CUDA source files in the device-debug mode (see Options for Specifying Behavior of Compiler/Linker ).
__CUDACC_RELAXED_CONSTEXPR__ Defined when the --expt-relaxed-constexpr flag is specified on the command line.
__CUDACC_EXTENDED_LAMBDA__ Defined when the --expt-extended-lambda or --extended-lambda flag is specified on the command line.
__NVCC_DIAG_PRAGMA_SUPPORT__ Defined when the CUDA frontend compiler supports diagnostic control with the nv_diag_suppress , nv_diag_error , nv_diag_warning , nv_diag_default , nv_diag_once , and nv_diagnostic pragmas. 2.2. NVCC Phases  A compilation phase is a logical translation step that can be selected by command line options to nvcc .
A single compilation phase can still be broken up by nvcc into smaller steps, but these smaller steps are just implementations of the phase: they depend on seemingly arbitrary capabilities of the internal tools that nvcc uses, and all of these internals may change with a new release of the CUDA Toolkit.
Hence, only compilation phases are stable across releases, and although nvcc provides options to display the compilation steps that it executes, these are for debugging purposes only and must not be copied and used in build scripts.
nvcc phases are selected by a combination of command line options and input file name suffixes, and the execution of these phases may be modified by other command line options.
In phase selection, the input file suffix defines the phase input, while the command line option defines the required output of the phase.
The following paragraphs list the recognized file name suffixes and the supported compilation phases.
A full explanation of the nvcc command line options can be found in NVCC Command Options . 2.3. Supported Input File Suffixes  The following table defines how nvcc interprets its input files: Input File Suffix Description .cu CUDA source file, containing host code and device functions .c C source file .cc , .cxx , .cpp C++ source file .ptx PTX intermediate assembly file (see Figure 1 ) .cubin CUDA device code binary file (CUBIN) for a single GPU architecture (see Figure 1 ) .fatbin CUDA fat binary file that may contain multiple PTX and CUBIN files (see Figure 1 ) .o , .obj Object file .a , .lib Library file .res Resource file .so Shared object file Note that nvcc does not make any distinction between object, library or resource files.
It just passes files of these types to the linker when the linking phase is executed. 2.4. Supported Phases  The following table specifies the supported compilation phases, plus the option to nvcc that enables the execution of each phase.
It also lists the default name of the output file generated by each phase, which takes effect when no explicit output file name is specified using the option --output-file : Phase nvcc Option Default Output File Name Long Name Short Name CUDA compilation to C/C++ source file --cuda -cuda .cpp.ii appended to source file name, as in x.cu.cpp.ii .
This output file can be compiled by the host compiler that was used by nvcc to preprocess the .cu file.
C/C++ preprocessing --preprocess -E C/C++ compilation to object file --compile -c Source file name with suffix replaced by o on Linux or obj on Windows Cubin generation from CUDA source files --cubin -cubin Source file name with suffix replaced by cubin Cubin generation from PTX intermediate files.
--cubin -cubin Source file name with suffix replaced by cubin PTX generation from CUDA source files --ptx -ptx Source file name with suffix replaced by ptx Fatbinary generation from source, PTX or cubin files --fatbin -fatbin Source file name with suffix replaced by fatbin Linking relocatable device code.
--device-link -dlink a_dlink.obj on Windows or a_dlink.o on other platforms Cubin generation from linked relocatable device code.
--device-link --cubin -dlink -cubin a_dlink.cubin Fatbinary generation from linked relocatable device code --device-link --fatbin -dlink -fatbin a_dlink.fatbin Linking an executable a.exe on Windows or a.out on other platforms Constructing an object file archive, or library --lib -lib a.lib on Windows or a.a on other platforms make dependency generation --generate-dependencies -M make dependency generation without headers in system paths.
--optix-ir -optix-ir Source file name with suffix replaced by optixir Running an executable --run -run Notes: The last phase in this list is more of a convenience phase.
It allows running the compiled and linked executable without having to explicitly set the library path to the CUDA dynamic libraries.
Unless a phase option is specified, nvcc will compile and link all its input files. 3. The CUDA Compilation Trajectory  CUDA compilation works as follows: the input program is preprocessed for device compilation and is compiled to CUDA binary ( cubin ) and/or PTX intermediate code, which are placed in a fatbinary.
The input program is preprocessed once again for host compilation and is synthesized to embed the fatbinary and transform CUDA specific C++ extensions into standard C++ constructs.
Then the C++ host compiler compiles the synthesized host code with the embedded fatbinary into a host object.
The embedded fatbinary is inspected by the CUDA runtime system whenever the device code is launched by the host program to obtain an appropriate fatbinary image for the current GPU.
CUDA programs are compiled in the whole program compilation mode by default, i.e., the device code cannot reference an entity from a separate file.
For more information on the separate compilation and the whole program compilation, see Using Separate Compilation in CUDA .
Command Option Types and Notation  Each nvcc option has a long name and a short name, which are interchangeable with each other.
These two variants are distinguished by the number of hyphens that must precede the option name: long names must be preceded by two hyphens, while short names must be preceded by a single hyphen.
Long options are intended for use in build scripts, where the size of the option is less important than the descriptive value.
nvcc recognizes three types of command options: boolean options, single value options, and list options.
Boolean options do not have an argument; they are either specified on the command line or not.
Examples of each of these option types are, respectively: --verbose (switch to verbose mode), --output-file (specify output file), and --include-path (specify include path).
Single value options and list options must have arguments, which must follow the name of the option itself by either one of more spaces or an equals character.
When a one-character short name such as -I , -l , and -L is used, the value of the option may also immediately follow the option itself without being seperated by spaces or an equal character.
The individual values of list options may be separated by commas in a single instance of the option, or the option may be repeated, or any combination of these two cases.
Hence, for the two sample options mentioned above that may take values, the following notations are legal: -o file -o=file -Idir1,dir2 -I=dir3 -I dir4,dir5 Unless otherwise specified, long option names are used throughout this document.
However, short names can be used instead of long names for the same effect. 4.2. Command Option Description  This section presents tables of nvcc options.
The option type in the tables can be recognized as follows: Boolean options do not have arguments specified in the first column, while the other two types do.
Long options are described in the first column of the options table, and short options occupy the second column. 4.2.1. File and Path Specifications  4.2.1.1.
--output-file file ( -o )  Specify name and location of the output file. 4.2.1.2. --objdir-as-tempdir ( -objtemp )  Create all intermediate files in the same directory as the object file.
Using this option will ensure that the intermediate file name that is embedded in the object file will not change in multiple compiles of the same file.
If the same file is compiled with two different options, ex., ‘nvcc -c t.cu’ and ‘nvcc -c -ptx t.cu’, then the files should be compiled in different directories.
Compiling them in the same directory can either cause the compilation to fail or produce incorrect results. 4.2.1.3. --pre-include file,...
( -include )  Specify header files that must be pre-included during preprocessing. 4.2.1.4. --library library,...
( -l )  Specify libraries to be used in the linking stage without the library file extension.
The libraries are searched for on the library search paths that have been specified using option --library-path (see Libraries ). 4.2.1.5. --define-macro def,...
name = definition - The contents of definition are tokenized and preprocessed as if they appear during translation phase three in a #define directive.
The definition will be truncated by embedded new line characters. 4.2.1.6. --undefine-macro def,...
( -U )  Undefine an existing macro during preprocessing or compilation. 4.2.1.7. --include-path path,...
( -L )  Specify library search paths (see Libraries ). 4.2.1.10. --output-directory directory ( -odir )  Specify the directory of the output file.
This option is intended for letting the dependency generation step (see --generate-dependencies ) generate a rule that defines the target object file in the proper directory. 4.2.1.11. --dependency-output file ( -MF )  Specify the dependency output file.
This option specifies the output file for the dependency generation step (see --generate-dependencies ).
The option --generate-dependencies or --generate-nonystem-dependencies must be specified if a dependency output file is set. 4.2.1.12. --generate-dependency-targets ( -MP )  Add an empty target for each dependency.
This option adds phony targets to the dependency generation step (see --generate-dependencies ) intended to avoid makefile errors if old dependencies are deleted.
The input files are not emitted as phony targets. 4.2.1.13. --compiler-bindir directory ( -ccbin )  Specify the directory in which the default host compiler executable resides.
The host compiler executable name can be also specified to ensure that the correct host compiler is selected.
In addition, driver prefix options ( --input-drive-prefix , --dependency-drive-prefix , or --drive-prefix ) may need to be specified, if nvcc is executed in a Cygwin shell or a MinGW shell on Windows. 4.2.1.14. --allow-unsupported-compiler ( -allow-unsupported-compiler )  Disable nvcc check for supported host compiler versions.
Using an unsupported host compiler may cause compilation failure or incorrect run time execution.
This option has no effect on MacOS. 4.2.1.15. --archiver-binary executable ( -arbin )  Specify the path of the archiver tool used create static library with --lib .
4.2.1.16. --cudart { none | shared | static } ( -cudart )  Specify the type of CUDA runtime library to be used: no CUDA runtime library, shared/dynamic CUDA runtime library, or static CUDA runtime library.
Allowed Values none shared static Default The static CUDA runtime library is used by default. 4.2.1.17. --cudadevrt { none | static } ( -cudadevrt )  Specify the type of CUDA device runtime library to be used: no CUDA device runtime library, or static CUDA device runtime library.
Allowed Values none static Default The static CUDA device runtime library is used by default. 4.2.1.18. --libdevice-directory directory ( -ldir )  Specify the directory that contains the libdevice library files.
Libdevice library files are located in the nvvm/libdevice directory in the CUDA Toolkit. 4.2.1.19. --target-directory string ( -target-dir )  Specify the subfolder name in the targets directory where the default include and library paths are located.
4.2.2. Options for Specifying the Compilation Phase  Options of this category specify up to which stage the input files must be compiled.
4.2.2.1. --link ( -link )  Specify the default behavior: compile and link all input files.
Default Output File Name a.exe on Windows or a.out on other platforms is used as the default output file name. 4.2.2.2. --lib ( -lib )  Compile all input files into object files, if necessary, and add the results to the specified library output file.
Default Output File Name a.lib on Windows or a.a on other platforms is used as the default output file name. 4.2.2.3. --device-link ( -dlink )  Link object files with relocatable device code and .ptx , .cubin , and .fatbin files into an object file with executable device code, which can be passed to the host linker.
Default Output File Name a_dlink.obj on Windows or a_dlink.o on other platforms is used as the default output file name.
When this option is used in conjunction with --fatbin , a_dlink.fatbin is used as the default output file name.
When this option is used in conjunction with --cubin , a_dlink.cubin is used as the default output file name. 4.2.2.4. --device-c ( -dc )  Compile each .c , .cc , .cpp , .cxx , and .cu input file into an object file that contains relocatable device code.
Default Output File Name The source file name extension is replaced by .obj on Windows and .o on other platforms to create the default output file name.
For example, the default output file name for x.cu is x.obj on Windows and x.o on other platforms. 4.2.2.5. --device-w ( -dw )  Compile each .c , .cc , .cpp , .cxx , and .cu input file into an object file that contains executable device code.
It is equivalent to --relocatable-device-code=false --compile . 4.2.2.6. --cuda ( -cuda )  Compile each .cu input file to a .cu.cpp.ii file.
Default Output File Name .cu.cpp.ii is appended to the basename of the source file name to create the default output file name.
For example, the default output file name for x.cu is x.cu.cpp.ii . 4.2.2.7. --compile ( -c )  Compile each .c , .cc , .cpp , .cxx , and .cu input file into an object file.
4.2.2.8. --fatbin ( -fatbin )  Compile all .cu , .ptx , and .cubin input files to device-only .fatbin files.
Default Output File Name The source file name extension is replaced by .fatbin to create the default output file name.
For example, the default output file name for x.cu is x.fatbin . 4.2.2.9. --cubin ( -cubin )  Compile all .cu and .ptx input files to device-only .cubin files.
Default Output File Name The source file name extension is replaced by .cubin to create the default output file name.
For example, the default output file name for x.cu is x.cubin . 4.2.2.10. --ptx ( -ptx )  Compile all .cu input files to device-only .ptx files.
Default Output File Name The source file name extension is replaced by .ptx to create the default output file name.
For example, the default output file name for x.cu is x.ptx . 4.2.2.11. --preprocess ( -E )  Preprocess all .c , .cc , .cpp , .cxx , and .cu input files.
Default Output File Name The output is generated in stdout by default. 4.2.2.12. --generate-dependencies ( -M )  Generate a dependency file that can be included in a Makefile for the .c , .cc , .cpp , .cxx , and .cu input file.
nvcc uses a fixed prefix to identify dependencies in the preprocessed file ( ‘ #line 1 ’ on Linux and ‘ # 1 ’ on Windows).
The files mentioned in source location directives starting with this prefix will be included in the dependency list. 4.2.2.13. --generate-nonsystem-dependencies ( -MM )  Same as --generate-dependencies but skip header files found in system directories (Linux only).
4.2.2.14. --generate-dependencies-with-compile ( -MD )  Generate a dependency file and compile the input file.
The dependency file can be included in a Makefile for the .c , .cc , .cpp , .cxx , and .cu input file.
The dependency file name is computed as follows: If -MF is specified, then the specified file is used as the dependency file name.
If -o is specified, the dependency file name is computed from the specified file name by replacing the suffix with ‘.d’.
Otherwise, the dependency file name is computed by replacing the input file names’s suffix with ‘.d’.
If the dependency file name is computed based on either -MF or -o , then multiple input files are not supported. 4.2.2.15. --generate-nonsystem-dependencies-with-compile ( -MMD )  Same as --generate-dependencies-with-compile but skip header files found in system directories (Linux only).
This feature is not supported with link-time-optimization ( -dlto ), the lto_NN -arch target, or with -gencode .
Default Output File Name The source file name extension is replaced by .optixir to create the default output file name.
For example, the default output file name for x.cu is x.optixir . 4.2.2.17. --run ( -run )  Compile and link all input files into an executable, and executes it.
This step is intended for developers who do not want to be bothered with setting the necessary environment variables; these are set temporarily by nvcc . 4.2.3. Options for Specifying Behavior of Compiler/Linker  4.2.3.1.
--profile ( -pg )  Instrument generated code/executable for use by gprof . 4.2.3.2. --debug ( -g )  Generate debug information for host code.
It is not intended for profiling; use --generate-line-info instead for profiling. 4.2.3.4. --extensible-whole-program ( -ewp )  Generate extensible whole program device code, which allows some calls to not be resolved until linking with libcudadevrt.
4.2.3.6. --generate-line-info ( -lineinfo )  Generate line-number information for device code.
Inlining pass may be invoked multiple times by the compiler and a function not inlined in an earlier pass may be inlined in a subsequent pass. 4.2.3.8. --optimize level ( -O )  Specify optimization level for host code.
When specified along with -G , enables limited debug information generation for optimized device code (currently, only line number information).
Allowed Values on : enable device code optimization. 4.2.3.10. --dlink-time-opt ( -dlto )  Perform link-time optimization of device code.
Link-time optimization must be specified at both compile and link time; at compile time it stores high-level intermediate code, then at link time it links together and optimizes the intermediate code.
The options -dlto -arch=sm_NN will add a lto_NN target; if you want to only add a lto_NN target and not the compute_NN that -arch=sm_NN usually generates, use -arch=lto_NN . 4.2.3.11. --gen-opt-lto ( -gen-opt-lto )  Run the optimizer passes before generating the LTO IR.
4.2.3.12. --split-compile number ( -split-compile )  [Experimental] Perform compiler optimizations in parallel.
Split compilation attempts to reduce compile time by enabling the compiler to run certain optimization passes concurrently.
It does this by splitting the device code into smaller translation units, each containing one or more device functions, and running optimization passes on each unit concurrently across multiple threads.
The option accepts a numerical value that specifies the maximum number of threads the compiler can use.
One can also allow the compiler to use the maximum threads available on the system by setting --split-compile=0 .
This option can work in conjunction with device Link Time Optimization ( -dlto ) as well as --threads . 4.2.3.13. --ftemplate-backtrace-limit limit ( -ftemplate-backtrace-limit )  Set the maximum number of template instantiation notes for a single warning or error to limit.
This value is also passed to the host compiler if it provides an equivalent flag. 4.2.3.14. --ftemplate-depth limit ( -ftemplate-depth )  Set the maximum instantiation depth for template classes to limit.
Disable exception handling for host code, by passing “-EHs-c-” (for cl.exe) and “–fno-exceptions” (for other host compilers) during host compiler invocation.
These flags are added to the host compiler invocation before any flags passed directly to the host compiler with “-Xcompiler” Default (on Windows) On Windows, nvcc passes /EHsc to the host compiler by default.
Use option --linker-options when other linker options are required for more control. 4.2.3.17. --x { c | c++ | cu } ( -x )  Explicitly specify the language for the input files, rather than letting the compiler choose a default based on the file name suffix.
Allowed Values c c++ cu Default The language of the source code is determined based on the file name suffix. 4.2.3.18. --std { c++03 | c++11 | c++14 | c++17 | c++20 } ( -std )  Select a particular C++ dialect.
Allowed Values c++03 c++11 c++14 c++17 c++20 Default The default C++ dialect depends on the host compiler.
nvcc matches the default C++ dialect that the host compiler uses. 4.2.3.19. --no-host-device-initializer-list ( -nohdinitlist )  Do not consider member functions of std::initializer_list as __host__ __device__ functions implicitly.
4.2.3.20. --expt-relaxed-constexpr ( -expt-relaxed-constexpr )  Experimental flag : Allow host code to invoke ``__device__ constexpr`` functions, and device code to invoke ``__host__ constexpr`` functions.
Note that the behavior of this flag may change in future compiler releases. 4.2.3.21. --extended-lambda ( -extended-lambda )  Allow __host__ , __device__ annotations in lambda declarations.
4.2.3.22. --expt-extended-lambda ( -expt-extended-lambda )  Alias for --extended-lambda .
Allowed Values 64 Default This option is set based on the host platform on which nvcc is executed. 4.2.3.24. --m64 ( -m64 )  Alias for --machine=64 4.2.3.25.
--host-linker-script { use-lcs | gen-lcs } ( -hls )  Use the host linker script (GNU/Linux only) to enable support for certain CUDA specific requirements, while building executable files or shared libraries.
Allowed Values use-lcs Prepares a host linker script and enables host linker to support relocatable device object files that are larger in size, that would otherwise, in certain cases, cause the host linker to fail with relocation truncation error.
gen-lcs Generates a host linker script that can be passed to host linker manually, in the case where host linker is invoked separately outside of nvcc.
This option can be combined with -shared or -r option to generate linker scripts that can be used while generating host shared libraries or host relocatable links respectively.
The file generated using this options must be provided as the last input file to the host linker.
A linker script may already be in used and passed to the host linker using the host linker option --script (or -T ), then the generated host linker script must augment the existing linker script.
In such cases, the option -aug-hls must be used to generate linker script that contains only the augmentation parts.
A host linker option, such as -z with a non-default argument, that can modify the default linker script internally, is incompatible with this option and the behavior of any such usage is undefined.
Default Value use-lcs is used as the default type. 4.2.3.26. --augment-host-linker-script ( -aug-hls )  Enables generation of host linker script that augments an existing host linker script (GNU/Linux only).
See option --host-linker-script for more details. 4.2.3.27. --host-relocatable-link ( -r )  When used in combination with -hls=gen-lcs , controls the behaviour of -hls=gen-lcs and sets it to generate host linker script that can be used in host relocatable link ( ld -r linkage).
This option currently is effective only when used with -hls=gen-lcs ; in all other cases, this option is ignored currently. 4.2.4. Options for Passing Specific Phase Options  These flags allow for passing specific options directly to the internal compilation tools that nvcc encapsulates, without burdening nvcc with too-detailed knowledge on these tools.
( -Xcompiler )  Specify options directly to the compiler/preprocessor. 4.2.4.2. --linker-options options,...
( -Xlinker )  Specify options directly to the host linker. 4.2.4.3. --archive-options options,...
( -Xarchive )  Specify options directly to the library manager. 4.2.4.4. --ptxas-options options,...
( -Xptxas )  Specify options directly to ptxas , the PTX optimizing assembler. 4.2.4.5. --nvlink-options options,...
( -Xnvlink )  Specify options directly to nvlink , the device linker. 4.2.5. Options for Guiding the Compiler Driver  4.2.5.1.
--forward-unknown-to-host-compiler ( -forward-unknown-to-host-compiler )  Forward unknown options to the host compiler.
An ‘unknown option’ is a command line argument that starts with - followed by another character, and is not a recognized nvcc flag or an argument for a recognized nvcc flag.
If the unknown option is followed by a separate command line argument, the argument will not be forwarded, unless it begins with the - character.
For example: nvcc -forward-unknown-to-host-compiler -foo=bar a.cu will forward -foo=bar to host compiler.
nvcc -forward-unknown-to-host-compiler -foo bar a.cu will report an error for bar argument.
nvcc -forward-unknown-to-host-compiler -foo -bar a.cu will forward -foo and -bar to host compiler. 4.2.5.2. --forward-unknown-to-host-linker ( -forward-unknown-to-host-linker )  Forward unknown options to the host linker.
For example: nvcc -forward-unknown-to-host-linker -foo=bar a.cu will forward -foo=bar to host linker.
nvcc -forward-unknown-to-host-linker -foo -bar a.cu will forward -foo and -bar to host linker. 4.2.5.3. --dont-use-profile ( -noprof )  Do not use configurations from the nvcc.profile file for compilation.
4.2.5.4. --threads number ( -t )  Specify the maximum number of threads to be used to execute the compilation steps in parallel.
This option can be used to improve the compilation speed when compiling for multiple architectures.
If number is 0, the number of threads used is the number of CPUs on the machine. 4.2.5.5. --dryrun ( -dryrun )  List the compilation sub-commands without executing them.
4.2.5.7. --keep ( -keep )  Keep all intermediate files that are generated during internal compilation steps.
4.2.5.8. --keep-dir directory ( -keep-dir )  Keep all intermediate files that are generated during internal compilation steps in this directory.
4.2.5.10. --clean-targets ( -clean )  Delete all the non-temporary files that the same nvcc command would generate without this option.
Instead, all of the non-temporary files that nvcc would otherwise create will be deleted. 4.2.5.11. --run-args arguments,...
( -run-args )  Specify command line arguments for the executable when used in conjunction with --run . 4.2.5.12. --use-local-env ( -use-local-env )  Skip MSVC environment initialization.
This is done by executing the appropriate command file available for the MSVC installation detected or specified.
If the environment used to invoke nvcc has already been configured, this option can be used to skip this step. 4.2.5.13. --input-drive-prefix prefix ( -idp )  Specify the input drive prefix.
On Windows, all command line arguments that refer to file names must be converted to the Windows native format before they are passed to pure Windows executables.
Use /cygwin/ as prefix for Cygwin build environments and / as prefix for MinGW. 4.2.5.14. --dependency-drive-prefix prefix ( -ddp )  Specify the dependency drive prefix.
On Windows, when generating dependency files (see --generate-dependencies ), all file names must be converted appropriately for the instance of make that is used.
Some instances of make have trouble with the colon in absolute paths in the native Windows format, which depends on the environment in which the make instance has been compiled.
Or leave these file names in the native Windows format by specifying nothing. 4.2.5.15. --drive-prefix prefix ( -dp )  Specify the drive prefix.
This option specifies prefix as both --input-drive-prefix and --dependency-drive-prefix . 4.2.5.16. --dependency-target-name target ( -MT )  Specify the target name of the generated rule when generating a dependency file (see --generate-dependencies ).
4.2.5.17. --no-align-double  Specify that -malign-double should not be passed as a compiler argument on 32-bit platforms.
WARNING: this makes the ABI incompatible with the CUDA’s kernel ABI for certain 64-bit types. 4.2.5.18. --no-device-link ( -nodlink )  Skip the device link step when linking object files.
--default-stream { legacy | null | per-thread } ( -default-stream )  Specify the stream that CUDA commands from the compiled program will be sent to by default.
Allowed Values legacy The CUDA legacy stream (per context, implicitly synchronizes with other streams) per-thread Normal CUDA stream (per thread, does not implicitly synchronize with other streams) null Deprecated alias for legacy Default legacy is used as the default stream. 4.2.7. Options for Steering GPU Code Generation  4.2.7.1.
--gpu-architecture ( -arch )  Specify the name of the class of NVIDIA virtual GPU architecture for which the CUDA input files must be compiled.
With the exception as described for the shorthand below, the architecture specified with this option must be a virtual architecture (such as compute_50).
Normally, this option alone does not trigger assembly of the generated PTX for a real architecture (that is the role of nvcc option --gpu-code , see below); rather, its purpose is to control preprocessing and compilation of the input to PTX.
For convenience, in case of simple nvcc compilations, the following shorthand is supported.
If no value for option --gpu-code is specified, then the value of this option defaults to the value of --gpu-architecture .
In this situation, as the only exception to the description above, the value specified for --gpu-architecture may be a real architecture (such as a sm_50), in which case nvcc uses the specified real architecture and its closest virtual architecture as the effective architecture values.
For example, nvcc --gpu-architecture=sm_50 is equivalent to nvcc --gpu-architecture=compute_50 --gpu-code=sm_50,compute_50 .
When -arch=native is specified, nvcc detects the visible GPUs on the system and generates codes for them, no PTX program will be generated for this option.
It is a warning if there are no visible supported GPU on the system, and the default architecture will be used.
If -arch=all is specified, nvcc embeds a compiled code image for all supported architectures (sm_*) , and a PTX program for the highest major virtual architecture.
For -arch=all-major , nvcc embeds a compiled code image for all supported major versions (sm_*0) , plus the earliest supported, and adds a PTX program for the highest major virtual architecture.
See Virtual Architecture Feature List for the list of supported virtual architectures and GPU Feature List for the list of supported real architectures.
Default sm_52 is used as the default value; PTX is generated for compute_52 then assembled and optimized for sm_52 . 4.2.7.2. --gpu-code code,...
nvcc embeds a compiled code image in the resulting executable for each specified code architecture, which is a true binary load image for each real architecture (such as sm_50), and PTX code for the virtual architecture (such as compute_50).
During runtime, such embedded PTX code is dynamically compiled by the CUDA runtime system if no binary load image is found for the current GPU.
Architectures specified for options --gpu-architecture and --gpu-code may be virtual as well as real , but the code architectures must be compatible with the arch architecture.
When the --gpu-code option is used, the value for the --gpu-architecture option must be a virtual PTX architecture.
For instance, --gpu-architecture=compute_60 is not compatible with --gpu-code=sm_52 , because the earlier compilation stages will assume the availability of compute_60 features that are not present on sm_52 . 4.2.7.3. --generate-code specification ( -gencode )  This option provides a generalization of the --gpu-architecture=arch --gpu-code=code,...
Where use of the previous options generates code for different real architectures with the PTX for the same virtual architecture, option --generate-code allows multiple PTX generations for different virtual architectures.
is equivalent to --generate-code=arch=arch,code=code,... . --generate-code options may be repeated for different virtual architectures.
4.2.7.4. --relocatable-device-code { true | false } ( -rdc )  Enable or disable the generation of relocatable device code.
Allowed Values true false Default The generation of relocatable device code is disabled. 4.2.7.5. --entries entry,...
PTX generated for all entry functions, but only the selected entry functions are assembled.
Default nvcc generates code for all entry functions. 4.2.7.6. --maxrregcount amount ( -maxrregcount )  Specify the maximum amount of registers that GPU functions can use.
Until a function-specific limit, a higher value will generally increase the performance of individual GPU threads that execute this function.
However, because thread registers are allocated from a global register pool on each GPU, a higher value of this option will also reduce the maximum thread block size, thereby reducing the amount of thread parallelism.
A value less than the minimum registers required by ABI will be bumped up by the compiler to ABI minimum limit.
User program may not be able to make use of all registers as some registers are reserved by compiler.
Default No maximum is assumed. 4.2.7.7. --use_fast_math ( -use_fast_math )  Make use of fast math library.
--use_fast_math implies --ftz=true --prec-div=false --prec-sqrt=false --fmad=true . 4.2.7.8. --ftz { true | false } ( -ftz )  Control single-precision denormals support.
Allowed Values true false Default This option is set to false and nvcc preserves denormal values. 4.2.7.9. --prec-div { true | false } ( -prec-div )  This option controls single-precision floating-point division and reciprocals.
--prec-div=true enables the IEEE round-to-nearest mode and --prec-div=false enables the fast approximation mode.
Allowed Values true false Default This option is set to true and nvcc enables the IEEE round-to-nearest mode. 4.2.7.10. --prec-sqrt { true | false } ( -prec-sqrt )  This option controls single-precision floating-point square root.
--prec-sqrt=true enables the IEEE round-to-nearest mode and --prec-sqrt=false enables the fast approximation mode.
--use_fast_math implies --prec-sqrt=false . 4.2.7.11. --fmad { true | false } ( -fmad )  This option enables (disables) the contraction of floating-point multiplies and adds/subtracts into floating-point multiply-add operations (FMAD, FFMA, or DFMA).
Allowed Values true false Default This option is set to true and nvcc enables the contraction of floating-point multiplies and adds/subtracts into floating-point multiply-add operations (FMAD, FFMA, or DFMA). 4.2.7.12. --extra-device-vectorization ( -extra-device-vectorization )  This option enables more aggressive device code vectorization.
Some PTX ISA features may not be usable in this compilation mode. 4.2.7.14. --keep-device-functions ( -keep-device-functions )  In whole program compilation mode, preserve user defined external linkage __device__ function definitions in generated PTX.
4.2.7.15. --jump-table-density percentage ( -jtd )  Specify the case density percentage in switch statements, and use it as a minimal threshold to determine whether jump table(brx.idx instruction) will be used to implement a switch statement.
Default This option is set to 101 and nvcc disables jump table generation for switch statements. 4.2.8. Generic Tool Options  4.2.8.1.
--disable-warnings ( -w )  Inhibit all warning messages. 4.2.8.2. --source-in-ptx ( -src-in-ptx )  Interleave source in PTX.
May only be used in conjunction with --device-debug or --generate-line-info . 4.2.8.3. --restrict ( -restrict )  Assert that all kernel pointer parameters are restrict pointers.
4.2.8.4. --Wno-deprecated-gpu-targets ( -Wno-deprecated-gpu-targets )  Suppress warnings about deprecated GPU target architectures.
4.2.8.5. --Wno-deprecated-declarations ( -Wno-deprecated-declarations )  Suppress warning on use of a deprecated entity.
4.2.8.6. --Wreorder ( -Wreorder )  Generate warnings when member initializers are reordered.
4.2.8.7. --Wdefault-stream-launch ( -Wdefault-stream-launch )  Generate warning when an explicit stream argument is not provided in the >> kernel launch syntax.
4.2.8.8. --Wmissing-launch-bounds ( -Wmissing-launch-bounds )  Generate warning when a __global__ function does not have an explicit __launch_bounds__ annotation.
4.2.8.9. --Wext-lambda-captures-this ( -Wext-lambda-captures-this )  Generate warning when an extended lambda implicitly captures this .
The following is the list of warning kinds accepted by this option: all-warnings Treat all warnings as errors.
The compiler will generate an error instead of a warning for a call from a __host__ __device__ to a __host__ function.
default-stream-launch Generate error when an explicit stream argument is not provided in the >> kernel launch syntax.
missing-launch-bounds Generate warning when a __global__ function does not have an explicit __launch_bounds__ annotation.
ext-lambda-captures-this Generate error when an extended lambda implicitly captures this .
deprecated-declarations Generate error on use of a deprecated entity. 4.2.8.11. --display-error-number ( -err-no )  This option displays a diagnostic number for any message generated by the CUDA frontend compiler (note: not the host compiler).
4.2.8.12. --no-display-error-number ( -no-err-no )  This option disables the display of a diagnostic number for any message generated by the CUDA frontend compiler (note: not the host compiler).
( -diag-error )  Emit error for specified diagnostic message(s) generated by the CUDA frontend compiler (note: does not affect diagnostics generated by the host compiler/preprocessor). 4.2.8.14. --diag-suppress errNum,...
( -diag-suppress )  Suppress specified diagnostic message(s) generated by the CUDA frontend compiler (note: does not affect diagnostics generated by the host compiler/preprocessor). 4.2.8.15. --diag-warn errNum,...
( -diag-warn )  Emit warning for specified diagnostic message(s) generated by the CUDA frontend compiler (note: does not affect diagnostics generated by the host compiler/preprocessor). 4.2.8.16. --resource-usage ( -res-usage )  Show resource usage such as registers and memory of the GPU code.
This option implies --nvlink-options=--verbose when --relocatable-device-code=true is set.
Otherwise, it implies --ptxas-options=--verbose . 4.2.8.17. --help ( -h )  Print help information on this tool.
( -optf )  Include command line options from specified file. 4.2.8.20. --time filename ( -time )  Generate a comma separated value table with the time taken by each compilation phase, and append it at the end of the file given as the option argument.
If the file name is - , the timing data is generated in stdout. 4.2.8.21. --qpp-config config ( -qpp-config )  Specify the configuration ([[compiler/]version,][target]) when using q++ host compiler.
The argument will be forwarded to the q++ compiler with its -V flag. 4.2.8.22. --list-gpu-code ( -code-ls )  List the non-accelerated gpu architectures (sm_XX) supported by the tool and exit.
If both –list-gpu-code and –list-gpu-arch are set, the list is displayed using the same format as the –generate-code value. 4.2.8.23. --list-gpu-arch ( -arch-ls )  List the non-accelerated virtual device architectures (compute_XX) supported by the tool and exit.
If both –list-gpu-arch and –list-gpu-code are set, the list is displayed using the same format as the –generate-code value. 4.2.9. Phase Options  The following sections lists some useful options to lower level compilation tools.
4.2.9.1. Ptxas Options  The following table lists some useful ptxas options which can be specified with nvcc option -Xptxas .
4.2.9.1.1. --allow-expensive-optimizations ( -allow-expensive-optimizations )  Enable (disable) to allow compiler to perform expensive optimizations using maximum available resources (memory and compile-time).
If unspecified, default behavior is to enable this feature for optimization level >= O2 . 4.2.9.1.2. --compile-only ( -c )  Generate relocatable object.
4.2.9.1.6. --disable-optimizer-constants ( -disable-optimizer-consts )  Disable use of optimizer constant bank.
( -e )  Semantics same as nvcc option --entries . 4.2.9.1.8. --fmad ( -fmad )  Semantics same as nvcc option --fmad .
4.2.9.1.9. --force-load-cache ( -flcm )  Force specified cache modifier on global/generic load.
4.2.9.1.10. --force-store-cache ( -fscm )  Force specified cache modifier on global/generic store.
4.2.9.1.11. --generate-line-info ( -lineinfo )  Semantics same as nvcc option --generate-line-info .
4.2.9.1.12. --gpu-name gpuname ( -arch )  Specify name of NVIDIA GPU to generate code for.
This option also takes virtual compute architectures, in which case code generation is suppressed.
Allowed values for this option: compute_50 , compute_52 , compute_53 , compute_60 , compute_61 , compute_62 , compute_70 , compute_72 , compute_75 , compute_80 , compute_86 , compute_87 , compute_89 , compute_90 , lto_50 , lto_52 , lto_53 , lto_60 , lto_61 , lto_62 , lto_70 , lto_72 , lto_75 , lto_80 , lto_86 , lto_87 , lto_89 , lto_90 , sm_50 , sm_52 , sm_53 , sm_60 , sm_61 , sm_62 , sm_70 , sm_72 , sm_75 , sm_80 , sm_86 , sm_87 , sm_89 , sm_90 Default value: sm_52 . 4.2.9.1.13. --help ( -h )  Semantics same as nvcc option --help .
4.2.9.1.15. --maxrregcount amount ( -maxrregcount )  Semantics same as nvcc option --maxrregcount .
( -optf )  Semantics same as nvcc option --options-file . 4.2.9.1.18. --position-independent-code ( -pic )  Generate position-independent code.
--preserve-relocs ( -preserve-relocs )  This option will make ptxas to generate relocatable references for variables and preserve relocations generated for them in linked executable. 4.2.9.1.20. --sp-bound-check ( -sp-bound-check )  Generate stack-pointer bounds-checking code sequence.
This option is turned on automatically when --device-debug or --opt-level=0 is specified. 4.2.9.1.21. --suppress-async-bulk-multicast-advisory-warning ( -suppress-async-bulk-multicast-advisory-warning )  Suppress the warning on use of .multicast::cluster modifier on cp.async.bulk{.tensor} instruction with sm_90 .
4.2.9.1.22. --verbose ( -v )  Enable verbose mode which prints code generation statistics.
4.2.9.1.25. --warn-on-double-precision-use ( -warn-double-usage )  Warning if double(s) are used in an instruction.
4.2.9.1.26. --warn-on-local-memory-usage ( -warn-lmem-usage )  Warning if local memory is used.
4.2.9.1.27. --warn-on-spills ( -warn-spills )  Warning if registers are spilled to local memory.
4.2.9.1.29. --maxntid ( -maxntid )  Specify the maximum number of threads that a thread block can have.
This option is also ignored for entry functions that have .maxntid directive specified. 4.2.9.1.30. --minnctapersm ( -minnctapersm )  Specify the minimum number of CTAs to be mapped to an SM.
This option is also ignored for entry functions that have .minnctapersm directive specified. 4.2.9.1.31. --override-directive-values ( -override-directive-values )  Override the PTX directives values by the corresponding option values.
This option is effective only for -minnctapersm , -maxntid and -maxrregcount options. 4.2.9.1.32. --make-errors-visible-at-exit ( -make-errors-visible-at-exit )  Generate required instructions at exit point to make memory faults and errors visible at exit.
4.2.9.2. NVLINK Options  The following is a list of some useful nvlink options which can be specified with nvcc option --nvlink-options .
4.2.9.2.1. 4.2.9.2.2. --preserve-relocs ( -preserve-relocs )  Preserve resolved relocations in linked executable.
4.2.9.2.3. 4.2.9.2.4. 4.2.9.2.5. --suppress-arch-warning ( -suppress-arch-warning )  Suppress the warning that otherwise is printed when object does not contain code for target arch.
4.2.9.2.6. --suppress-stack-size-warning ( -suppress-stack-size-warning )  Suppress the warning that otherwise is printed when stack size cannot be determined.
4.2.9.2.7. --dump-callgraph ( -dump-callgraph )  Dump information about the callgraph and register usage.
4.2.9.2.8. --dump-callgraph-no-demangle ( -dump-callgraph-no-demangle )  Dump callgraph information without demangling.
4.2.9.2.11. --extra-warnings ( -extrawarn )  Emit extra warnings about possible problems.
4.2.9.2.12. --gen-host-linker-script ( -ghls )  Specify the type of host linker script to be generated.
4.2.9.2.13. --ignore-host-info ( -ignore-host-info )  Ignore information about host references, so don’t remove device code that could potentially be referenced by host.
4.2.9.2.14. --keep-system-libraries ( -keep-system-libraries )  Don’t optimize away system library (e.g.
cudadevrt) code. 4.2.9.2.15. --kernels-used ( -kernels-used )  Specify kernels that are used.
If this option is used, then any other kernels are considered dead-code and removed. 4.2.9.2.16. --options-file ( -optf )  Include command line options from the specified file.
4.2.9.2.18. --suppress-debug-info ( -suppress-debug-info )  Do not preserve debug symbols in output.
This option is ignored if used without –debug option. 4.2.9.2.19. --variables-used ( -variables used )  Specify variables that are used.
If this option is used, then any other variables are considered dead-code and potentially removed unless have other accesses from device code. 4.3. NVCC Environment Variables  NVCC_PREPEND_FLAGS and NVCC_APPEND_FLAGS: The nvcc command line flags can be augmented using the following environment variables, if set: NVCC_PREPEND_FLAGS Flags to be injected before the normal nvcc command line.
For example, after setting: export NVCC_PREPEND_FLAGS='-G -keep -arch=sm_60' export NVCC_APPEND_FLAGS='-DNAME=" foo "' The following invocation: nvcc foo.cu -o foo Becomes equivalent to: nvcc -G -keep -arch=sm_60 foo.cu -o foo -DNAME=" foo " These environment variables can be useful for injecting nvcc flags globally without modifying build scripts.
The additional flags coming from either NVCC_PREPEND_FLAGS or NVCC_APPEND_FLAGS will be listed in the verbose log ( --verbose ).
NVCC_CCBIN: A default host compiler can be set using the environment variable NVCC_CCBIN .
For example, after setting: export NVCC_CCBIN='gcc' nvcc will choose gcc as the host compiler if --compiler-bindir is not set.
If NVCC_CCBIN and --compiler-bindir are both set, nvcc will choose the host compiler specified by --compiler-bindir .
For example: export NVCC_CCBIN='gcc' nvcc foo.cu -ccbin='clang' -o foo In this case, nvcc will choose clang as the host compiler. 5. GPU Compilation  This chapter describes the GPU compilation model that is maintained by nvcc , in cooperation with the CUDA driver.
It goes through some technical sections, with concrete examples at the end. 5.1. GPU Generations  In order to allow for architectural evolution, NVIDIA GPUs are released in different generations.
New generations introduce major improvements in functionality and/or chip architecture, while GPU models within the same generation show minor configuration differences that moderately affect functionality, performance, or both.
For example, a CUDA application that has been compiled for a Fermi GPU will very likely not run on a Kepler GPU (and vice versa).
This is because the instruction set and instruction encodings of a generation is different from those of other generations.
Binary compatibility within one GPU generation can be guaranteed under certain conditions because they share the basic instruction set.
This is the case when two GPU versions do not show functional differences (for instance when one version is a scaled down version of the other), or when one version is functionally included in the other.
An example of the latter is the base Maxwell version sm_50 whose functionality is a subset of all other Maxwell versions: any code compiled for sm_50 will run on all other Maxwell GPUs. 5.2. GPU Feature List  The following table lists the names of the current GPU architectures, annotated with the functional capabilities that they provide.
There are other differences, such as amounts of register and processor clusters, that only affect execution performance.
In the CUDA naming scheme, GPUs are named sm_xy , where x denotes the GPU generation number, and y the version in that generation.
Additionally, to facilitate comparing GPU capabilities, CUDA attempts to choose its GPU names such that if x1y1 can be used to implicitly call both the device and host linkers.
This works because if the device linker does not see any relocatable code it does not do anything.
Libraries  The device linker has the ability to read the static host library formats ( .a on Linux and Mac OS X, .lib on Windows).
The --library and --library-path options can be used to pass libraries to both the device and host linker.
The library name is specified without the library file extension when the --library option is used.
nvcc --gpu-architecture=sm_50 a.o b.o --library-path= --library=foo Alternatively, the library name, including the library file extension, can be used without the --library option on Windows.
nvcc --gpu-architecture=sm_50 a.obj b.obj foo.lib --library-path= Note that the device linker ignores any objects that do not have relocatable device code. 6.4. Examples  Suppose we have the following files:  - b.h - #define N 8 extern __device__ int g [ N ]; extern __device__ void bar ( void );  - b.cu - #include "b.h" __device__ int g [ N ]; __device__ void bar ( void ) { g [ threadIdx .
x ] ++ ; }  - a.cu - #include #include "b.h" __global__ void foo ( void ) { __shared__ int a [ N ]; a [ threadIdx .
x - 1 ]; bar (); } int main ( void ) { unsigned int i ; int * dg , hg [ N ]; int sum = 0 ; foo >> (); if ( cudaGetSymbolAddress (( void ** ) & dg , g )){ printf ( "couldn't get the symbol addr   " ); return 1 ; } if ( cudaMemcpy ( hg , dg , N * sizeof ( int ), cudaMemcpyDeviceToHost )){ printf ( "couldn't memcpy   " ); return 1 ; } for ( i = 0 ; i --library=cudart Note that all desired target architectures must be passed to the device linker, as that specifies what will be in the final executable (some objects or libraries may contain device code for multiple architectures, and the link step can then choose what to put in the final executable).
If you want to use the driver API to load a linked cubin, you can request just the cubin: nvcc --gpu-architecture=sm_50 --device-link a.o b.o \ --cubin --output-file link.cubin The objects could be put into a library and used with: nvcc --gpu-architecture=sm_50 --device-c a.cu b.cu nvcc --lib a.o b.o --output-file test.a nvcc --gpu-architecture=sm_50 test.a Note that only static libraries are supported by the device linker.
A PTX file can be compiled to a host object file and then linked by using: nvcc --gpu-architecture=sm_50 --device-c a.ptx An example that uses libraries, host linker, and dynamic parallelism would be: nvcc --gpu-architecture=sm_50 --device-c a.cu b.cu nvcc --gpu-architecture=sm_50 --device-link a.o b.o --output-file link.o nvcc --lib --output-file libgpu.a a.o b.o link.o g++ host.o --library=gpu --library-path= \ --library=cudadevrt --library=cudart It is possible to do multiple device links within a single host executable, as long as each device link is independent of the other.
This requirement of independence means that they cannot share code across device executables, nor can they share addresses (e.g., a device function address can be passed from host to device for a callback only if the device link sees both the caller and potential callback callee; you cannot pass an address from one device executable to another, as those are separate address spaces). 6.5. Optimization Of Separate Compilation  Separately compiled code may not have as high of performance as whole program code because of the inability to inline code across files.
A way to still get optimal performance is to use link-time optimization, which stores intermediate code which is then linked together to perform high level optimizations.
If only some of the files are compiled with -dlto , then those will be linked and optimized together while the rest uses the normal separate compilation.
A side effect is that this shifts some of the compile time to the link phase, and there may be some scalability issues with really large codes.
If you want to compile using -gencode to build for multiple arch, use -dc -gencode arch=compute_NN,code=lto_NN to specify the intermediate IR to be stored (where NN is the SM architecture version).
As of CUDA 12.0 there is support for runtime LTO via the nvJitLink library. 6.6. Potential Separate Compilation Issues  6.6.1.
Object Compatibility  Only relocatable device code with the same ABI version, link-compatible SM target architecture, and same pointer size (32 or 64) can be linked together.
Link-compatible SM architectures are ones that have compatible SASS binaries that can combine without translating, e.g.
An object could have been compiled for a different architecture but also have PTX available, in which case the device linker will JIT the PTX to cubin for the desired architecture and then link.
If Link Time Optimization is used with -dlto , the intermediate LTOIR is only guaranteed to be compatible within a major release (e.g.
If a kernel is limited to a certain number of registers with the launch_bounds attribute or the --maxrregcount option, then all functions that the kernel calls must not use more than that number of registers; if they exceed the limit, then a link error will be given. 6.6.2. JIT Linking Support  JIT linking means doing an implicit relink of the code at load time.
If the cubin does not match the target architecture at load time, the driver re-invokes the device linker to generate cubin for the target architecture, by first JIT’ing the PTX for each object to the appropriate cubin, and then linking together the new cubin.
If PTX or cubin for the target architecture is not found for an object, then the link will fail.
Implicit JIT linking of the LTO intermediates is not supported at this time, although they can be explicitly linked with the nvJitLink library. 6.6.3. Implicit CUDA Host Code  A file like b.cu above only contains CUDA device code, so one might think that the b.o object doesn’t need to be passed to the host linker.
But actually there is implicit host code generated whenever a device symbol can be accessed from the host side, either via a launch or an API call like cudaGetSymbolAddress() .
Plus, for JIT linking to work all device code must be passed to the host linker, else the host executable will not contain device code needed for the JIT link.
So a general rule is that the device linker and host linker must see the same host object files (if the object files have any device references in them—if a file is pure host then the device linker doesn’t need to see it).
If an object file containing device code is not passed to the host linker, then you will see an error message about the function __cudaRegisterLinkedBinary_name calling an undefined or unresolved symbol __fatbinwrap_name . 6.6.4. Using __CUDA_ARCH__  In separate compilation, __CUDA_ARCH__ must not be used in headers such that different objects could contain different behavior.
If a weak function or template function is defined in a header and its behavior depends on __CUDA_ARCH__ , then the instances of that function in the objects could conflict if the objects are compiled for different compute arch.
For example, if an a.h contains: template __device__ T * getptr ( void ) { #if __CUDA_ARCH__ == 500 return NULL ; /* no address */ #else __shared__ T arr [ 256 ]; return arr ; #endif } Then if a.cu and b.cu both include a.h and instantiate getptr for the same type, and b.cu expects a non-NULL address, and compile with: nvcc --gpu-architecture=compute_50 --device-c a.cu nvcc --gpu-architecture=compute_52 --device-c b.cu nvcc --gpu-architecture=sm_52 a.o b.o At link time only one version of the getptr is used, so the behavior would depend on which version is picked.
To avoid this, either a.cu and b.cu must be compiled for the same compute arch, or __CUDA_ARCH__ should not be used in the shared header function. 6.6.5. Device Code in Libraries  If a device function with non-weak external linkage is defined in a library as well as a non-library object (or another library), the device linker will complain about the multiple definitions (this differs from traditional host linkers that may ignore the function definition from the library object, if it was already found in an earlier object).
Cross Compilation  Cross compilation is controlled by using the following nvcc command line option: --compiler-bindir is used for cross compilation, where the underlying host compiler is capable of generating objects for the target platform.
On an x86 system, if a CUDA toolkit installation has been configured to support cross compilation to both Tegra and non-Tegra ARM targets, then nvcc will use the non-Tegra configuration by default, when an ARM host cross compiler has been specified.
To use the Tegra configuration instead, pass “ -target-dir aarch64-linux ” to nvcc. 7.2. Keeping Intermediate Phase Files  nvcc stores intermediate results by default into temporary files that are deleted immediately before it completes.
The location of the temporary file directories used are, depending on the current platform, as follows: Windows Value of environment variable TEMP is used.
Option --keep makes nvcc store these intermediate files in the current directory or in the directory specified by --keep-dir instead, with names as described in Supported Phases . 7.3. Cleaning Up Generated Files  All files generated by a particular nvcc command can be cleaned up by repeating the command, but with additional option --clean-targets .
This option is particularly useful after using --keep , because the --keep option usually leaves quite an amount of intermediate files around.
Because using --clean-targets will remove exactly what the original nvcc command created, it is important to exactly repeat all of the options in the original command.
For instance, in the following example, omitting --keep , or adding --compile will have different cleanup effects.
Printing Code Generation Statistics  A summary on the amount of used registers and the amount of memory needed per compiled device function can be printed by passing option --resource-usage to nvcc : $ nvcc --resource-usage acos.cu -arch sm_80 ptxas info : 1536 bytes gmem ptxas info : Compiling entry function 'acos_main' for 'sm_80' ptxas info : Function properties for acos_main 0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads ptxas info : Used 6 registers, 1536 bytes smem, 32 bytes cmem[0] As shown in the above example, the amount of statically allocated global memory (gmem) is listed.
Global memory and some of the constant banks are module scoped resources and not per kernel resources.
Spill stores and loads represent stores and loads done on stack memory which are being used for storing variables that couldn’t be allocated to physical registers.
Similarly number of registers, amount of shared memory and total space in constant bank allocated is shown. 8. Notices  8.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 8.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 8.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Overview v12.5 | PDF | Archive vGPUs and CUDA vGPUs that support CUDA This page describes the support for CUDA® on NVIDIA® virtual GPU software.
For details, follow the link in the table to the documentation for your version. 1.2. Notices  1.2.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 1.2.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 1.2.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates.
NVIDIA GPUDirect Storage Overview Guide The NVIDIA® Magnum IO GPUDirect® Storage Overview Guide provides a high-level overview of GDS.
cuFile API Reference Guide The NVIDIA® GPUDirect® Storage cuFile API Reference Guide provides information about the cuFile API reference that is used in applications and frameworks to leverage GDS technology and describes the intent, context, and operation of those APIs, which are part of the GDS technology.
NVIDIA GPUDirect Storage Release Notes Release information for NVIDIA® Magnum IO GPUDirect® Storage.
Getting Started with NVIDIA GPUDirect Storage Getting started information for NVIDIA® Magnum IO GPUDirect® Storage.
Understanding GPUDirect Storage NVIDIA GPUDirect Storage Best Practices Guide The Best Practices guide provides information about the lessons that were learned when building and massively scaling GPU accelerated I/O storage infrastructures.
NVIDIA GPUDirect Storage Benchmarking and Configuration Guide The Benchmarking and Configuration Guide helps you evaluate and test GDS functionality and performance by using sample applications.
NVIDIA GPUDirect Storage Installation and Troubleshooting Guide This guide describes how to install, debug, and isolate the performance and functional problems that are related to GDS and is intended for systems administrators and developers.
NVIDIA GPUDirect Storage O_DIRECT Requirements Guide The O_DIRECT Guide helps you understand how GDS provides significant benefit when it can leverage the O_DIRECT fcntl.h file mode for a direct data path between GPU memory and storage.
Aerospace Hardware / Semiconductor Architecture / Engineering / Construction Manufacturing Media & Entertainment Restaurant / Quick-Service Energy HPC / Scientific Computing IT Specialist Public Sector Financial Services Dev / IT Operations Consumer Internet Cloud Services Telecommunications Gaming Healthcare & Life Sciences Agriculture Academia / Higher Education Retail / Consumer Packaged Goods Data Center / Cloud Data Center / Cloud © Copyright 2024, NVIDIA.
Topics NVIDIA GPUDirect Storage NVIDIA GPUDirect Storage NVIDIA GPUDirect Storage Design Guide 1.
Troubleshooting GDS issues Understanding GPUDirect Storage NVIDIA GPUDirect Storage Best Practices Guide 1.
cuFileBufRegister, cuFileRead, cuFileWrite, cuFileBatchIOSubmit, cuFileBatchIOGetStatus, cuFileReadAsync, cuFileWriteAsync, and cuFileStreamRegister 3.3.1.
Benchmarking Storage Performance NVIDIA GPUDirect Storage Installation and Troubleshooting Guide 1.
Running Data Verification Tests Using GPUDirect Storage NVIDIA GPUDirect Storage Installation and Troubleshooting Guide 6.
Example: Tracing cuFileRead and cuFileWrite Failures, Print, Error Codes, and Time of Failure 16.14.
Cheat Sheet for Diagnosing Problems NVIDIA GPUDirect Storage O_DIRECT Requirements Guide 1.
Summary Corporate Info NVIDIA.com Home About NVIDIA ‎NVIDIA Developer Developer Home Blog Resources Contact Us Developer Program Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2024 NVIDIA Corporation _satellite.pageBottom(); const tables = document.querySelectorAll('.Page-twoColumn table'); const className = 'table-wrapper' tables.forEach(tbl => { const parent = tbl.parentNode; if(!parent.classList.contains(className)){ const wrapper = document.createElement('div'); wrapper.classList.add(className) parent.insertBefore( wrapper, tbl ); wrapper.appendChild(tbl); } })
1.
Overview v12.5 | PDF | Archive Developing a Linux Kernel Module using GPUDirect RDMA The API reference guide for enabling GPUDirect RDMA connections to NVIDIA GPUs.
Overview  GPUDirect RDMA is a technology introduced in Kepler-class GPUs and CUDA 5.0 that enables a direct path for data exchange between the GPU and a third-party peer device using standard features of PCI Express.
Examples of third-party devices are: network interfaces, video acquisition devices, storage adapters.
A number of limitations can apply, the most important being that the two devices must share the same upstream PCI Express root complex.
Some of the limitations depend on the platform used and could be lifted in current/future products.
A few straightforward changes must be made to device drivers to enable this functionality with a wide range of hardware devices.
This document introduces the technology and describes the steps necessary to enable an GPUDirect RDMA connection to NVIDIA GPUs on Linux.
How GPUDirect RDMA Works  When setting up GPUDirect RDMA communication between two peers, all physical addresses are the same from the PCI Express devices’ point of view.
Each device has six BAR registers at most, so it can have up to six active 32bit BAR regions.
The PCI Express device issues reads and writes to a peer device’s BAR addresses in the same way that they are issued to system memory.
Traditionally, resources like BAR windows are mapped to user or kernel address space using the CPU’s MMU as memory mapped I/O (MMIO) addresses.
However, because current operating systems don’t have sufficient mechanisms for exchanging MMIO regions between drivers, the NVIDIA kernel driver exports functions to perform the necessary address translations and mappings.
To add GPUDirect RDMA support to a device driver, a small amount of address mapping code within the kernel driver must be modified.
The APIs and control flow involved with GPUDirect RDMA are very similar to those used with standard DMA transfers.
See Supported Systems and PCI BAR sizes for more hardware details. 1.2. Standard DMA Transfer  First, we outline a standard DMA Transfer initiated from userspace.
In this scenario, the following components are present: Userspace program Userspace communication library Kernel driver for the device interested in doing DMA transfers The general sequence is as follows: The userspace program requests a transfer via the userspace communication library.
The communication library must make sure the memory region corresponding to the virtual address and size is ready for the transfer.
The kernel driver receives the virtual address and size from the userspace communication library.
It then asks the kernel to translate the virtual address range to a list of physical pages and make sure they are ready to be transferred to or from.
After the transfer is done, the communication library should eventually clean up any resources used to pin the memory.
We will refer to this operation as unpinning the memory. 1.3. GPUDirect RDMA Transfers  For the communication to support GPUDirect RDMA transfers some changes to the sequence above have to be introduced.
First of all, two new components are present: Userspace CUDA library NVIDIA kernel driver As described in Basics of UVA CUDA Memory Management , programs using the CUDA library have their address space split between GPU and CPU virtual addresses, and the communication library has to implement two separate paths for them.
The userspace CUDA library provides a function that lets the communication library distinguish between CPU and GPU addresses.
Moreover, for GPU addresses it returns additional metadata that is required to uniquely identify the GPU memory represented by the address.
The difference between the paths for CPU and GPU addresses is in how the memory is pinned and unpinned.
For CPU memory this is handled by built-in Linux Kernel functions ( get_user_pages() and put_page() ).
However, in the GPU memory case the pinning and unpinning has to be handled by functions provided by the NVIDIA Kernel driver.
Some hardware caveats are explained in Supported Systems and PCI BAR sizes . 1.4. Changes in CUDA 6.0  In this section we briefly list the changes that are available in CUDA 6.0: CUDA peer-to-peer tokens are no longer mandatory.
For memory buffers owned by the calling process (which is typical) tokens can be replaced by zero (0) in the kernel-mode function nvidia_p2p_get_pages() .
This new feature is meant to make it easier for existing third party software stacks to adopt RDMA for GPUDirect.
As a consequence of the change above, a new API cuPointerSetAttribute() has been introduced.
It is necessary to ensure correct synchronization behavior of the CUDA API when operation on memory which may be read by RDMA for GPUDirect.
cuPointerGetAttribute() has been extended to return a globally unique numeric identifier, which in turn can be used by lower-level libraries to detect buffer reallocations happening in user-level code (see Userspace API ).
It provides an alternative method to detect reallocations when intercepting CUDA allocation and deallocation APIs is not possible.
The kernel-mode memory pinning feature has been extended to work in combination with Multi-Process Service (MPS).
Caveats as of CUDA 6.0: CUDA Unified Memory is not explicitly supported in combination with GPUDirect RDMA.
While the page table returned by nvidia_p2p_get_pages() is valid for managed memory buffers and provides a mapping of GPU memory at any given moment in time, the GPU device copy of that memory may be incoherent with the writable copy of the page which is not on the GPU.
Using the page table in this circumstance may result in accessing stale data, or data loss, because of a DMA write access to device memory that is subsequently overwritten by the Unified Memory run-time.
cuPointerGetAttribute() may be used to determine if an address is being managed by the Unified Memory runtime.
Every time a device memory region is pinned, new GPU BAR space is allocated unconditionally, even when pinning overlapping or duplicate device memory ranges, i.e.
This behavior has been changed since CUDA 7.0. 1.5. Changes in CUDA 7.0  In this section we briefly list the changes that are available in CUDA 7.0: On the IBM POWER8 platform, GPUDirect RDMA is not supported, though it is not explicitly disabled.
Now when a device memory region is pinned, GPU BAR space might be shared with pre-existing mappings.
As a consequence, when unpinning a region, its whole BAR space will not be returned if even only a subset of its BAR space is shared.
cudaPointerGetAttributes() is now faster since it leverages cuPointerGetAttributes() internally.
It can be used as a template for implementing an interception framework for CUDA memory de/allocation APIs. 1.6. Changes in CUDA 8.0  In this section we briefly list the changes that are available in CUDA 8.0: The nvidia_p2p_page_table struct has been extended to include a new member, without breaking binary compatibility.
The minor version in the NVIDIA_P2P_PAGE_TABLE_VERSION macro has been updated accordingly.
The nvidia_p2p_dma_mapping structure, the nvidia_p2p_dma_map_pages() and nvidia_p2p_dma_unmap_pages() APIs, the NVIDIA_P2P_DMA_MAPPING_VERSION macro have been introduced.
These APIs can be used by third party device drivers to map and unmap the GPU BAR pages into their device’s I/O address space.
The main use case is on platforms where the I/O addresses of PCIe resources, used for PCIe peer-to-peer transactions, are different from the physical addresses used by the CPU to access those same resources.
The NVIDIA_P2P_PAGE_TABLE_VERSION_COMPATIBLE and NVIDIA_P2P_DMA_MAPPING_VERSION_COMPATIBLE macros have been introduced.
These are meant to be called by third-party device drivers to check for runtime binary compatibility, for example in case of changes to the data structure’s layout.
On the IBM POWER8 platform, when using the above APIs, GPUDirect RDMA is reported to work correctly restricted to the case where the GPU and the third party device are connected through a supported PCIe switch. 1.7. Changes in CUDA 10.1  GPUDirect RDMA is supported on Jetson AGX Xavier platform.
See Porting to Tegra section for details. 1.8. Changes in CUDA 11.2  GPUDirect RDMA is supported on Drive AGX Xavier Linux based platform.
1.9. Changes in CUDA 11.4  Added a new a kernel module, nvidia-peermem , which provides NVIDIA InfiniBand-based HCAs (Host Channel Adapters) direct peer-to-peer read and write access to the NVIDIA GPU’s video memory.
Users need to load the module manually. 1.10. Changes in CUDA 12.2  In drivers released from the R515 up to the R535 branches, except for newer R525 and R535 releases mentioned below, there is a race bug which may show up as a kernel null-pointer dereference.
This happens when the GPU invokes the (hereby I/O) kernel driver invalidation callback, the one which was registered during the call to nvidia_p2p_get_pages, concurrently with the I/O driver calling nvidia_p2p_put_pages.
The race bug does not affect the persistent mapping case, as in that case an invalidation callback is not supported nor needed.
The bug fix required the following API change: nvidia_p2p_get_pages no longer accepts a NULL callback pointer.
Instead, nvidia_p2p_put_pages_persistent and nvidia_p2p_get_pages_persistent have been introduced and should be used instead when requesting a persistent mapping.
The use of those new persistent APIs can be guarded by the NVIDIA_P2P_CAP_GET_PAGES_PERSISTENT_API preprocessor macro, for example when writing portable drivers.
Although deprecated when running GPU drivers from the R470 branch and newer, customers still using the off-tree nv_peer_mem module ( https: github.com/Mellanox/nv_peer_memory ) and needing the persistent mapping feature will have to switch to nvidia-peermem .
Note that I/O drivers, which do not need persistent mappings, do not require source code changes.
The API changes described above are deployed in the R535 branch, specifically in release 535.14 and later, and have also been back-ported to the R525 branch, for TeslaRD3 (525.105.17) and later. 2. Design Considerations  When designing a system to utilize GPUDirect RDMA, there a number of considerations which should be taken into account.
2.1. Lazy Unpinning Optimization  Pinning GPU device memory in BAR is an expensive operation, taking up to milliseconds.
The most straightforward implementation using GPUDirect RDMA would pin memory before each transfer and unpin it right after the transfer is complete.
Unfortunately, this would perform poorly in general, as pinning and unpinning memory are expensive operations.
The rest of the steps required to perform an RDMA transfer, however, can be performed quickly without entering the kernel (the DMA list can be cached and replayed using MMIO registers/command lists).
This takes advantage of the fact that it is likely that the same memory region will be used for future DMA transfers thus lazy unpinning saves pin/unpin operations.
An example implementation of lazy unpinning would keep a set of pinned memory regions and only unpin some of them (for example the least recently used one) if the total size of the regions reached some threshold, or if pinning a new region failed because of BAR space exhaustion (see PCI BAR sizes ). 2.2. Registration Cache  Communication middleware often employs an optimization called a registration cache, or pin-down cache, to minimize pinning overhead.
Typically it already exists for host memory, implementing lazy unpinning, LRU de-registration, etc.
For networking middleware, such caches are usually implemented in user-space, as they are used in combination with hardware capable of user-mode message injection.
CUDA UVA memory address layout enables GPU memory pinning to work with these caches by taking into account just a few design considerations.
In the CUDA environment, this is even more important as the amount of memory which can be pinned may be significantly more constrained than for host memory.
As the GPU BAR space is typically mapped using 64KB pages, it is more resource efficient to maintain a cache of regions rounded to the 64KB boundary.
Even more so, as two memory areas which are in the same 64KB boundary would allocate and return the same BAR mapping.
Registration caches usually rely on the ability to intercept deallocation events happening in the user application, so that they can unpin the memory and free important HW resources, e.g.
To implement a similar mechanism for GPU memory, an implementation has two options: Instrument all CUDA allocation and deallocation APIs.
There is a sample application, 7_CUDALibraries/cuHook , showing how to intercept calls to CUDA APIs at run-time, which can be used to detect GPU memory de/allocations.
While intercepting CUDA APIs is beyond the scope of this document, an approach to performing tag checks is available starting with CUDA 6.0.
It involves the usage of the CU_POINTER_ATTRIBUTE_BUFFER_ID attribute in cuPointerGetAttribute() (or cuPointerGetAttributes() if more attributes are needed) to detect memory buffer deallocations or reallocations.
The API will return a different ID value in case of reallocation or an error if the buffer address is no longer valid.
Note Using tag checks introduces an extra call into the CUDA API on each memory buffer use, so this approach is most appropriate when the additional latency is not a concern. 2.3. Unpin Callback  When a third party device driver pins the GPU pages with nvidia_p2p_get_pages() it must also provide a callback function that the NVIDIA driver will call if it needs to revoke access to the mapping.
This callback occurs synchronously , giving the third party driver the opportunity to clean up and remove any references to the pages in question (i.e., wait for outstanding DMAs to complete).
The user callback function may block for a few milliseconds , although it is recommended that the callback complete as quickly as possible.
Care has to be taken not to introduce deadlocks as waiting within the callback for the GPU to do anything is not safe.
The callback must call nvidia_p2p_free_page_table() (not nvidia_p2p_put_pages() ) to free the memory pointed to by page_table .
The corresponding mapped memory areas will only be unmapped by the NVIDIA driver after returning from the callback.
Note that the callback will be invoked in two scenarios: If the userspace program explicitly deallocates the corresponding GPU memory, e.g.
before the third party kernel driver has a chance to unpin the memory with nvidia_p2p_put_pages() .
In the latter case there can be tear-down ordering issues between closing the file descriptor of the third party kernel driver and that of the NVIDIA kernel driver.
In the case the file descriptor for the NVIDIA kernel driver is closed first, the nvidia_p2p_put_pages() callback will be invoked.
A proper software design is important as the NVIDIA kernel driver will protect itself from reentrancy issues with locks before invoking the callback.
The third party kernel driver will almost certainly take similar actions, so dead-locking or live-locking scenarios may arise if careful consideration is not taken. 2.4. Supported Systems  General remarks Even though the only theoretical requirement for GPUDirect RDMA to work between a third-party device and an NVIDIA GPU is that they share the same root complex, there exist bugs (mostly in chipsets) causing it to perform badly, or not work at all in certain setups.
We can distinguish between three situations, depending on what is on the path between the GPU and the third-party device: PCIe switches only single CPU/IOH CPU/IOH QPI/HT CPU/IOH The first situation, where there are only PCIe switches on the path, is optimal and yields the best performance.
The second one, where a single CPU/IOH is involved, works, but yields worse performance ( especially peer-to-peer read bandwidth has been shown to be severely limited on some processor architectures ).
Finally, the third situation, where the path traverses a QPI/HT link, may be extremely performance-limited or even not work reliably.
Tip lspci can be used to check the PCI topology: $ lspci - t Platform support For IBM POWER8 platform, GPUDirect RDMA and P2P are not supported, but are not explicitly disabled.
GPUDirect RDMA is supported on Jetson AGX Xavier platform starting from CUDA 10.1 and on Drive AGX Xavier Linux based platforms from CUDA 11.2.
On ARM64, the necessary peer-to-peer functionality depends on both the hardware and the software of the particular platform.
So while GPUDirect RDMA is not explicitly disabled on non-Jetson and non-Drive platforms, there are no guarantees that it will be fully functional.
IOMMUs GPUDirect RDMA currently relies upon all physical addresses being the same from the different PCI devices’ point of view.
This makes it incompatible with IOMMUs performing any form of translation other than 1:1, hence they must be disabled or configured for pass-through translation for GPUDirect RDMA to work. 2.5. PCI BAR sizes  PCI devices can ask the OS/BIOS to map a region of physical address space to them.
NVIDIA GPUs currently expose multiple BARs, and some of them can back arbitrary device memory, making GPUDirect RDMA possible.
Large BARs can pose a problem for the BIOS, especially on older motherbords, related to compatibility support for 32bit operating systems.
On those motherboards the bootstrap can stop during the early POST phase, or the GPU may be misconfigured and so unusable.
If this appears to be occuring it might be necessary to enable some special BIOS feature to deal with the large BAR issue.
Please consult your system vendor for more details regarding large BAR support. 2.6. Tokens Usage  Warning Starting in CUDA 6.0, tokens should be considered deprecated, though they are still supported.
As can be seen in Userspace API and Kernel API , one method for pinning and unpinning memory requires two tokens in addition to the GPU virtual address.
These tokens, p2pToken and vaSpaceToken , are necessary to uniquely identify a GPU VA space.
The tokens are consistent within a single CUDA context (i.e., all memory obtained through cudaMalloc() within the same CUDA context will have the same p2pToken and vaSpaceToken ).
However, a given GPU virtual address need not map to the same context/GPU for its entire lifetime.
As a concrete example: cudaSetDevice ( 0 ) ptr0 = cudaMalloc (); cuPointerGetAttribute ( & return_data , CU_POINTER_ATTRIBUTE_P2P_TOKENS , ptr0 );   Returns [p2pToken = 0xabcd, vaSpaceToken = 0x1] cudaFree ( ptr0 ); cudaSetDevice ( 1 ); ptr1 = cudaMalloc (); assert ( ptr0 == ptr1 );   The CUDA driver is free (although not guaranteed) to reuse the VA,   even on a different GPU cuPointerGetAttribute ( & return_data , CU_POINTER_ATTRIBUTE_P2P_TOKENS , ptr0 );   Returns [p2pToken = 0x0123, vaSpaceToken = 0x2] That is, the same address, when passed to cuPointerGetAttribute , may return different tokens at different times during the program’s execution.
Therefore, the third party communication library must call cuPointerGetAttribute() for every pointer it operates on.
Security implications The two tokens act as an authentication mechanism for the NVIDIA kernel driver.
If you know the tokens, you can map the address space corresponding to them, and the NVIDIA kernel driver doesn’t perform any additional checks.
When no tokens are used, the NVIDIA driver limits the Kernel API to the process which owns the memory allocation. 2.7. Synchronization and Memory Ordering  GPUDirect RDMA introduces a new independent GPU data flow path exposed to third party devices and it is important to understand how these devices interact with the GPU’s relaxed memory model.
Properly registering a BAR mapping of CUDA memory is required for that mapping to remain consistent with CUDA APIs operations on that memory.
Only CUDA synchronization and work submission APIs provide memory ordering of GPUDirect RDMA operations.
Registration for CUDA API Consistency Registration is necessary to ensure the CUDA API memory operations visible to a BAR mapping happen before the API call returns control to the calling CPU thread.
This provides a consistent view of memory to a device using GPUDirect RDMA mappings when invoked after a CUDA API in the thread.
This is a strictly more conservative mode of operation for the CUDA API and disables optimizations, thus it may negatively impact performance.
This behavior is enabled on a per-allocation granularity either by calling cuPointerSetAttribute() with the CU_POINTER_ATTRIBUTE_SYNC_MEMOPS attribute, or p2p tokens are retrieved for a buffer when using the legacy path.
An example situation would be Read-after-Write dependency between a cuMemcpyDtoD() and subsequent GPUDirect RDMA read operation on the destination of the copy.
As an optimization the device-to-device memory copy typically returns asynchronously to the calling thread after queuing the copy to the GPU scheduler.
However, in this circumstance that will lead to inconsistent data read via the BAR mapping, so this optimization is disabled an the copy completed before the CUDA API returns.
CUDA APIs for Memory Ordering Only CPU initiated CUDA APIs provide ordering of GPUDirect memory operations as observed by the GPU.
That is, despite a third party device having issued all PCIE transactions, a running GPU kernel or copy operation may observe stale data or data that arrives out-of-order until a subsequent CPU initiated CUDA work submission or synchronization API.
To ensure that memory updates are visible to CUDA kernels or copies, an implementation should ensure that all writes to the GPU BAR happen before control is returned to the CPU thread which will invoke the dependent CUDA API.
An example situation for a network communication scenario is when a network RDMA write operation is completed by the third party network device and the data is written to the GPU BAR mapping.
Though reading back the written data either through GPU BAR or a CUDA memory copy operation, will return the newly written data, a concurrently running GPU kernel to that network write might observe stale data, the data partially written, or the data written out-of-order.
In short, a GPU kernel is wholly inconsistent with concurrent RDMA for GPUDirect operations and accessing the memory overwritten by the third party device in such a situation would be considered a data race.
To resolve this inconsistency and remove the data race the DMA write operation must complete with respect to the CPU thread which will launch the dependent GPU kernel. 3. How to Perform Specific Tasks  3.1.
Displaying GPU BAR space  Starting in CUDA 6.0 the NVIDIA SMI utility provides the capability to dump BAR1 memory usage.
It can be used to understand the application usage of BAR space, the primary resource consumed by GPUDirect RDMA mappings.
GPU memory is pinned in fixed size chunks, so the amount of space reflected here might be unexpected.
In addition, a certain amount of BAR space is reserved by the driver for internal use, so not all available memory may be usable via GPUDirect RDMA.
Note that the same ability is offered programmatically through the nvmlDeviceGetBAR1MemoryInfo() NVML API. 3.2. Pinning GPU memory  Correct behavior requires using cuPointerSetAttribute() on the memory address to enable proper synchronization behavior in the CUDA driver.
void pin_buffer ( void * address , size_t size ) { unsigned int flag = 1 ; CUresult status = cuPointerSetAttribute ( & flag , CU_POINTER_ATTRIBUTE_SYNC_MEMOPS , address ); if ( CUDA_SUCCESS == status ) {   GPU path pass_to_kernel_driver ( address , size ); } else {   CPU path   ...
} } This is required so that the GPU memory buffer is treated in a special way by the CUDA driver, so that CUDA memory transfers are guaranteed to always be synchronous with respect to the host.
for boundary alignment requirement #define GPU_BOUND_SHIFT 16 #define GPU_BOUND_SIZE ((u64)1 page_table , free_callback , & my_state ); if ( ret == 0 ) {   Succesfully pinned, page_table can be accessed } else {   Pinning failed } } Note how the start address is aligned to a 64KB boundary before calling the pinning functions.
If the function succeeds the memory has been pinned and the page_table entries can be used to program the device’s DMA engine.
See Kernel API for details on nvidia_p2p_get_pages() . 3.3. Unpinning GPU memory  In the kernel driver, invoke nvidia_p2p_put_pages() .
void unpin_memory ( void * address , size_t size , nvidia_p2p_page_table_t * page_table ) { nvidia_p2p_put_pages ( 0 , 0 , address , size , page_table ); } See Kernel API for details on nvidia_p2p_put_pages() .
Note that nvidia_p2p_put_pages() must be called from within the same process context as the one from which the corresponding nvidia_p2p_get_pages() has been issued. 3.4. Handling the free callback  The NVIDIA kernel driver invokes free_callback(data) as specified in the nvidia_p2p_get_pages() call if it needs to revoke the mapping.
void free_callback ( void * data ) { my_state * state = data ; wait_for_pending_transfers ( state ); nvidia_p2p_free_pages ( state -> page_table ); } The NVIDIA kernel driver handles the unmapping so nvidia_p2p_put_pages() should not be called. 3.5. Buffer ID Tag Check for A Registration Cache  Remember that a solution built around Buffer ID tag checking is not recommended for latency sensitive implementations.
Instead, instrumentation of CUDA allocation and deallocation APIs to provide callbacks to the registration cache is recommended, removing tag checking overhead from the critical path.
The first time a device memory buffer is encountered and recognized as not yet pinned, the pinned mapping is created and the associated buffer ID is retrieved and stored together in the cache entry.
The cuMemGetAddressRange() function can be used to obtain the size and starting address for the whole allocation, which can then be used to pin it.
As nvidia_p2p_get_pages() will need a pointer aligned to 64K, it is useful to directly align the cached address.
Also, as the BAR space is currently mapped in chunks of 64KB, it is more resource efficient to round the whole pinning to 64KB.
struct buf represents an entry of the registration cache struct buf { CUdeviceptr pointer ; size_t size ; CUdeviceptr aligned_pointer ; size_t aligned_size ; int is_pinned ; uint64_t id ;   buffer id obtained right after pinning }; Once created, every time a registration cache entry will be used it must be first checked for validity.
One way to do this is to use the Buffer ID provided by CUDA as a tag to check for deallocation or reallocation.
int buf_is_gpu_pinning_valid ( struct buf * buf ) { uint64_t buffer_id ; int retcode ; assert ( buf -> is_pinned );   get the current buffer id retcode = cuPointerGetAttribute ( & buffer_id , CU_POINTER_ATTRIBUTE_BUFFER_ID , buf -> pointer ); if ( CUDA_ERROR_INVALID_VALUE == retcode ) {   the device pointer is no longer valid   it could have been deallocated return ERROR_INVALIDATED ; } else if ( CUDA_SUCCESS != retcode ) {   handle more serious errors here return ERROR_SERIOUS ; } if ( buf -> id != buffer_id )   the original buffer has been deallocated and the cached mapping should be invalidated and the buffer re-pinned return ERROR_INVALIDATED ; return 0 ; } When the buffer identifier changes the corresponding memory buffer has been reallocated so the corresponding kernel-space page table will not be valid anymore.
Thus the Buffer IDs provide a tag to keep the pin-down cache consistent with the kernel-space page table without requiring the kernel driver to up-call into the user-space.
If CUDA_ERROR_INVALID_VALUE is returned from cuPointerGetAttribute() , the program should assume that the memory buffer has been deallocated or is otherwise not a valid GPU memory buffer.
in the registration cache code if ( buf -> is_pinned && ! buf_is_gpu_pinning_valid ( buf )) { regcache_invalidate_entry ( buf ); pin_buffer ( buf ); } 3.6.
Linking a Kernel Module against nvidia.ko  Run the extraction script: ./NVIDIA-Linux-x86_64-.run -x This extracts the NVIDA driver and kernel wrapper.
Navigate to the output directory: cd /kernel/ Within this directory, build the NVIDIA module for your kernel: make module After this is done, the Module.symvers file under your kernel build directory contains symbol information for nvidia.ko .
Modify your kernel module build process with the following line: KBUILD_EXTRA_SYMBOLS := /Module.symvers 3.7.
Using nvidia-peermem  The NVIDIA GPU driver package provides a kernel module, nvidia-peermem , which provides NVIDIA InfiniBand based HCAs (Host Channel Adapters) direct peer-to-peer read and write access to the NVIDIA GPU’s video memory.
It allows GPUDirect RDMA-based applications to use GPU computing power with the RDMA interconnect without needing to copy data to host memory.
NVIDIA OFED (Open Fabrics Enterprise Distribution), or MLNX_OFED, introduces an API between the InfiniBand Core and peer memory clients such as NVIDIA GPUs.
The nvidia-peermem module registers the NVIDIA GPU with the InfiniBand subsystem by using peer-to-peer APIs provided by the NVIDIA GPU driver.
The kernel must have the required support for RDMA peer memory either through additional patches to the kernel or via MLNX_OFED as a prerequisite for loading and using nvidia-peermem .
It is possible that the nv_peer_mem module from the GitHub project may be installed and loaded on the system.
Installation of nvidia-peermem will not affect the functionality of the existing nv_peer_mem module.
Additionally, it is encouraged to uninstall the nv_peer_mem package to avoid any conflict with nvidia-peermem since only one module can be loaded at any time.
To stop the nv_peer_mem service: # service nv_peer_mem stop Check if nv_peer_mem.ko is still loaded after stopping the service: # lsmod | grep nv_peer_mem If nv_peer_mem.ko is still loaded, unload it using: # rmmod nv_peer_mem Uninstall the nv_peer_mem package: For DEB-based OS: # dpkg -P nvidia-peer-memory # dpkg -P nvidia-peer-memory-dkms For RPM-based OS: # rpm -e nvidia_peer_memory After ensuring kernel support and installing the GPU driver, nvidia-peermem can be loaded with the following command with root privileges in a terminal window: # modprobe nvidia-peermem Note Note: If the NVIDIA GPU driver is installed before MLNX_OFED, the GPU driver must be uninstalled and installed again to make sure nvidia-peermem is compiled with the RDMA APIs that are provided by MLNX_OFED. 4. References  4.1.
Basics of UVA CUDA Memory Management  Unified virtual addressing (UVA) is a memory address management system enabled by default in CUDA 4.0 and later releases on Fermi and Kepler GPUs running 64-bit processes.
On UVA-supported configurations, when the CUDA runtime initializes, the virtual address (VA) range of the application is partitioned into two areas: the CUDA-managed VA range and the OS-managed VA range.
All CUDA-managed pointers are within this VA range, and the range will always fall within the first 40 bits of the process’s VA space.
CUDA VA Space Addressing  Subsequently, within the CUDA VA space, addresses can be subdivided into three types: GPU A page backed by GPU memory.
This will not be accessible from the host and the VA in question will never have a physical backing on the host.
This partitioning allows the CUDA runtime to determine the physical location of a memory object by its pointer value within the reserved CUDA VA space.
Addresses are subdivided into these categories at page granularity; all memory within a page is of the same type.
GPUDirect RDMA operates exclusively on GPU pages (created by cudaMalloc() ) that are within this CUDA VA space. 4.2. Userspace API  Data structures typedef struct CUDA_POINTER_ATTRIBUTE_P2P_TOKENS_st { unsigned long long p2pToken ; unsigned int vaSpaceToken ; } CUDA_POINTER_ATTRIBUTE_P2P_TOKENS ; Function cuPointerSetAttribute() CUresult cuPointerSetAttribute ( void * data , CUpointer_attribute attribute , CUdeviceptr pointer ); In GPUDirect RDMA scope, the interesting usage is when CU_POINTER_ATTRIBUTE_SYNC_MEMOPS is passed as the attribute : unsigned int flag = 1 ; cuPointerSetAttribute ( & flag , CU_POINTER_ATTRIBUTE_SYNC_MEMOPS , pointer ); Parameters data [in] A pointer to a unsigned int variable containing a boolean value.
attribute [in] In GPUDirect RDMA scope should always be CU_POINTER_ATTRIBUTE_SYNC_MEMOPS .
Returns CUDA_SUCCESS if pointer points to GPU memory and the CUDA driver was able to set the new behavior for the whole device memory allocation.
It is used to explicitly enable a strictly synchronizing behavior on the whole memory allocation pointed to by pointer , and by doing so disabling all data transfer optimizations which might create problems with concurrent RDMA and CUDA memory copy operations.
This API has CUDA synchronizing behavior, so it should be considered expensive and possibly invoked only once per buffer.
Function cuPointerGetAttribute() CUresult cuPointerGetAttribute ( const void * data , CUpointer_attribute attribute , CUdeviceptr pointer ); This function has two different attributes related to GPUDirect RDMA: CU_POINTER_ATTRIBUTE_P2P_TOKENS and CU_POINTER_ATTRIBUTE_BUFFER_ID .
Warning CU_POINTER_ATTRIBUTE_P2P_TOKENS has been deprecated in CUDA 6.0 When CU_POINTER_ATTRIBUTE_P2P_TOKENS is passed as the attribute , data is a pointer to CUDA_POINTER_ATTRIBUTE_P2P_TOKENS : CUDA_POINTER_ATTRIBUTE_P2P_TOKENS tokens ; cuPointerGetAttribute ( & tokens , CU_POINTER_ATTRIBUTE_P2P_TOKENS , pointer ); In this case, the function returns two tokens for use with the Kernel API .
This function may be called at any time, including before CUDA initialization, and it has CUDA synchronizing behavior, as in CU_POINTER_ATTRIBUTE_SYNC_MEMOPS , so it should be considered expensive and should be invoked only once per buffer.
Note that values set in tokens can be different for the same pointer value during a lifetime of a user-space program.
Note that for security reasons the value set in p2pToken will be randomized, to prevent it from being guessed by an adversary.
In CUDA 6.0, a new attribute has been introduced that is useful to detect memory reallocations.
When CU_POINTER_ATTRIBUTE_BUFFER_ID is passed as the attribute , data is expected to point to a 64bit unsigned integer variable, like uint64_t .
uint64_t buf_id ; cuPointerGetAttribute ( & buf_id , CU_POINTER_ATTRIBUTE_BUFFER_ID , pointer ); Parameters data [out] A pointer to a 64 bits variable where the buffer id will be stored.
Some general remarks follow: cuPointerGetAttribute() and cuPointerSetAttribute() are CUDA driver API functions only.
In particular, cuPointerGetAttribute() is not equivalent to cudaPointerGetAttributes() , as the required functionality is only present in the former function.
This in no way limits the scope where GPUDirect RDMA may be used as cuPointerGetAttribute() is compatible with the CUDA Runtime API.
This is so as the additional overhead associated with the CUDA runtime API to driver API call sequence would introduce unneeded overhead and cuPointerGetAttribute() can be on the critical path, e.g.
Whenever possible, we suggest to combine multiple calls to cuPointerGetAttribute by using cuPointerGetAttributes .
Function ``cuPointerGetAttributes()`` CUresult cuPointerGetAttributes ( unsigned int numAttributes , CUpointer_attribute * attributes , void ** data , CUdeviceptr ptr ); This function can be used to inspect multiple attributes at once.
The one most probably related to GPUDirect RDMA are CU_POINTER_ATTRIBUTE_BUFFER_ID , CU_POINTER_ATTRIBUTE_MEMORY_TYPE and CU_POINTER_ATTRIBUTE_IS_MANAGED . 4.3. Kernel API  The following declarations can be found in the nv-p2p.h header that is distributed in the NVIDIA Driver package.
Please refer to the inline documentation contained in that header file for a detailed description of the parameters and the return values of the functions described below.
Preprocessor macros NVIDIA_P2P_PAGE_TABLE_VERSION_COMPATIBLE() and NVIDIA_P2P_DMA_MAPPING_VERSION_COMPATIBLE() preprocessor macros are meant to be called by third-party device drivers to check for runtime binary compatibility.
Structure nvidia_p2p_page typedef struct nvidia_p2p_page { uint64_t physical_address ; union nvidia_p2p_request_registers { struct { uint32_t wreqmb_h ; uint32_t rreqmb_h ; uint32_t rreqmb_0 ; uint32_t reserved [ 3 ]; } fermi ; } registers ; } nvidia_p2p_page_t ; In the nvidia_p2p_page structure only the physical_address field is relevant to GPUDirect RDMA.
Structure nvidia_p2p_page_table typedef struct nvidia_p2p_page_table { uint32_t version ; uint32_t page_size ; struct nvidia_p2p_page ** pages ; uint32_t entries ; uint8_t * gpu_uuid ; } nvidia_p2p_page_table_t ; The version field of the page table should be checked by using NVIDIA_P2P_PAGE_TABLE_VERSION_COMPATIBLE() before accessing the other fields.
Structure nvidia_p2p_dma_mapping typedef struct nvidia_p2p_dma_mapping { uint32_t version ; enum nvidia_p2p_page_size_type page_size_type ; uint32_t entries ; uint64_t * dma_addresses ; } nvidia_p2p_dma_mapping_t ; The version field of the dma mapping should be passed to NVIDIA_P2P_DMA_MAPPING_VERSION_COMPATIBLE() before accessing the other fields.
Function nvidia_p2p_get_pages() int nvidia_p2p_get_pages ( uint64_t p2p_token , uint32_t va_space_token , uint64_t virtual_address , uint64_t length , struct nvidia_p2p_page_table ** page_table , void ( * free_callback )( void * data ), void * data ); This function makes the pages underlying a range of GPU virtual memory accessible to a third-party device.
Warning This is an expensive operation and should be performed as infrequently as possible - see Lazy Unpinning Optimization .
Function nvidia_p2p_put_pages() int nvidia_p2p_put_pages ( uint64_t p2p_token , uint32_t va_space_token , uint64_t virtual_address , struct nvidia_p2p_page_table * page_table ); This function releases a set of pages previously made accessible to a third-party device.
Function nvidia_p2p_free_page_table() int nvidia_p2p_free_page_table ( struct nvidia_p2p_page_table * page_table ); This function frees a third-party P2P page table and is meant to be invoked during the execution of the nvidia_p2p_get_pages() callback.
Function nvidia_p2p_dma_map_pages() int nvidia_p2p_dma_map_pages ( struct pci_dev * peer , struct nvidia_p2p_page_table * page_table , struct nvidia_p2p_dma_mapping ** dma_mapping ); This function makes the physical pages retrieved using nvidia_p2p_get_pages() accessible to a third-party device.
It is required on platforms where the I/O addresses of PCIe resources, used for PCIe peer-to-peer transactions, are different from the physical addresses used by the CPU to access those same resources.
On some platforms, this function relies on a correct implementation of the dma_map_resource() Linux kernel function.
Function nvidia_p2p_dma_unmap_pages() int nvidia_p2p_dma_unmap_pages ( struct pci_dev * peer , struct nvidia_p2p_page_table * page_table , struct nvidia_p2p_dma_mapping * dma_mapping ); This function unmaps the physical pages previously mapped to the third-party device by nvidia_p2p_dma_map_pages().
It is not meant to be called from within the nvidia_p2p_get_pages() invalidation callback.
Function nvidia_p2p_free_dma_mapping() int nvidia_p2p_free_dma_mapping ( struct nvidia_p2p_dma_mapping * dma_mapping ); This function is meant to be called from within the nvidia_p2p_get_pages() invalidation callback.
Note that the deallocation of the I/O mappings may be deferred, for example after returning from the invalidation callback. 4.4. Porting to Tegra  GPUDirect RDMA is supported on Jetson AGX Xavier platform from CUDA 10.1, on DRIVE AGX Xavier Linux based platforms from CUDA 11.2 and on Jetson Orin platform from CUDA 11.4.
Owing to hardware and software specific divergence of Tegra vis-a-vis Linux-Desktop, already developed applications needs to be slightly modified in order to port them to Tegra.
The following sub-sections (4.4.1-4.4.3) briefs over the necessary changes. 4.4.1. Changing the allocator  GPUDirect RDMA on Desktop allows applications to operate exclusively on GPU pages allocated using cudaMalloc() .
On Tegra, applications will have to change the memory allocator from cudaMalloc() to cudaHostAlloc() .
Applications can either: Treat the returned pointer as if it is a device pointer, provided that the iGPU supports UVA or cudaDevAttrCanUseHostPointerForRegisteredMem device attribute is a non-zero value when queried using cudaDeviceGetAttribute() for iGPU.
Get the device pointer corresponding to the host memory allocated using cudaHostGetDevicePointer() .
Once the application has the device pointer, all the rules that are applicable to the standard GPUDirect solution also apply to Tegra. 4.4.2. Modification to Kernel API  The declarations under Tegra API column of the following table can be found in the nv-p2p.h header that is distributed in the NVIDIA Driver package.
Refer to the inline documentation contained in that header file for a detailed description of the parameters and the return values.
Other highlights  The length of the requested mapping and base address must be a multiple of 4KB, failing which leads to an error.
Unlike the Desktop version, callback registered at nvidia_p2p_get_pages() will always be triggered when nvidia_p2p_put_pages() is invoked.
It is the reponsibilty of the kernel driver to free the page_table allocated by calling nvidia_p2p_free_page_table() .
Note that, similar to the Desktop version, the callback will also triggered in scenarios explained in Unpin Callback .
Since cudaHostAlloc() can be allocated with cudaHostAllocWriteCombined flag or default flag, applications are expected to excercise caution when mapping the memory to userspace, for example using standard linux mmap() .
In this regard: When GPU memory is allocated as writecombined, the userspace mapping should also be done as writecombined by passing the vm_page_prot member of vm_area_struct to the standard linux interface: `pgprot_writecombine() `__.
When GPU memory is allocated as default, no modifcations to the vm_page_prot member of vm_area_struct should be done.
Incompatible combination of map and allocation attributes will lead to undefined behavior. 5. Notices  5.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 5.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 5.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit v12.5.1 Debugger API 1.
Deprecated List Search Results Debugger API ( PDF ) - v12.5.1 ( older ) - Last updated July 1, 2007-2024 - Send Feedback Debugger API The API reference guide for the CUDA debugger.
Deprecated List Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024 NVIDIA Corporation var switchTo5x=true; stLight.options({publisher: "998dc202-a267-4d8e-bce9-14debadb8d92", doNotHash: false, doNotCopy: false, hashAddressBar: false}); _satellite.pageBottom();
CUPTI Overview 1.
Namespaces Copyright and Licenses Notices Cupti » CUPTI v2024.2.0 | Archive CUPTI  The API reference for CUPTI, the CUDA Profiling Tools Interface.
Namespaces Copyright and Licenses NVIDIA Software License Agreement Third Party Licenses Boost Flatbuffers Font - Cascadia Mono Font - Open Sans Font - Roboto Microsoft Detours Notices Notice Trademarks Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact © Copyright 2018-2024, NVIDIA Corporation & Affiliates.
Why CUDA Compatibility v555 | PDF CUDA Compatibility CUDA Compatibility describes the use of new CUDA toolkit components on systems with older base installations.
Why CUDA Compatibility  The NVIDIA® CUDA® Toolkit enables developers to build NVIDIA GPU accelerated compute applications for desktop computers, enterprise, and data centers to hyperscalers.
It consists of the CUDA compiler toolchain including the CUDA runtime (cudart) and various CUDA libraries and tools.
To build an application, a developer has to install only the CUDA Toolkit and necessary libraries required for linking.
In order to run a CUDA application, the system should have a CUDA enabled GPU and an NVIDIA display driver that is compatible with the CUDA Toolkit that was used to build the application itself.
If the application relies on dynamic linking for libraries, then the system should have the right version of such libraries as well.
Figure 1 Components of CUDA  Every CUDA toolkit also ships with an NVIDIA display driver package for convenience.
The driver package includes both the user mode CUDA driver (libcuda.so) and kernel mode components necessary to run the application.
Typically, upgrading a CUDA Toolkit involves upgrading both the toolkit and the driver to get the bleeding edge toolkit and driver capabilities.
CUDA Compatibility guarantees allow for upgrading only certain components and that will be the focus of the rest of this document.
We will see how the upgrade to a new CUDA Toolkit can be simplified to not always require a full system upgrade. 2. Minor Version Compatibility  2.1.
CUDA 11 and Later Defaults to Minor Version Compatibility  From CUDA 11 onwards, applications compiled with a CUDA Toolkit release from within a CUDA major release family can run, with limited feature-set, on systems having at least the minimum required driver version as indicated below.
This minimum required driver can be different from the driver packaged with the CUDA Toolkit but should belong to the same major release.
Table 1 Example CUDA Toolkit 11.x and 12.x Minimum Required Driver Versions (Refer to CUDA Release Notes)  CUDA Toolkit Linux x86_64 Minimum Required Driver Version Windows Minimum Required Driver Version CUDA 12.x >=525.60.13 >=527.41 CUDA 11.x >= 450.80.02* >=452.39* CUDA 11.0 was released with an earlier driver version, but by upgrading to Tesla Recommended Drivers 450.80.02 (Linux) / 452.39 (Windows) as indicated, minor version compatibility is possible across the CUDA 11.x family of toolkits.
While applications built against any of the older CUDA Toolkits always continued to function on newer drivers due to binary backward compatibility, before CUDA 11, applications built against newer CUDA Toolkit releases were not supported on older drivers without forward compatibility package.
If you are using a new CUDA 10.x minor release, then the minimum required driver version is the same as the driver that’s packaged as part of that toolkit release.
Consequently, the minimum required driver version changed for every new CUDA Toolkit minor release until CUDA 11.1.
Therefore, system administrators always have to upgrade drivers in order to support applications built against CUDA Toolkits from 10.x releases.
Table 2 CUDA Toolkit 10.x Minimum Required Driver Versions  CUDA Toolkit Linux x86_64 Minimum Required Driver Version Windows Minimum Required Driver Version CUDA 10.2 >= 440.33 >=441.22 CUDA 10.1 >= 418.39 >=418.96 CUDA 10.0 >= 410.48 >=411.31 With minor version compatibility, upgrading to CUDA 11.1 is now possible on older drivers from within the same major release family such as 450.80.02 that was shipped with CUDA 11.0, as shown below: Minimum required driver version guidance can be found in the CUDA Toolkit Release Notes.
Note that if the minimum required driver version is not installed in the system, applications will return an error as shown below. 2.2. Application Considerations for Minor Version Compatibility  Developers and system admins should note two important caveats when relying on minor version compatibility.
If either of these caveats are limiting, then a new CUDA driver from the same minor version of the toolkit that the application was built with or later is required.
Limited feature set Sometimes features introduced in a CUDA Toolkit version may actually span both the toolkit and the driver.
In such cases an application that relies on features introduced in a newer version of the toolkit and driver may return the following error on older drivers: cudaErrorCallRequiresNewerDriver.
Application developers can avoid running into this problem by having the application explicitly check for the availability of features.
Applications using PTX will see runtime issues Applications that compile device code to PTX will not work on older drivers.
PTX Developers should refer to the CUDA Compatibility Developers Guide and PTX programming guide in the CUDA C++ Programming Guide for details on this limitation. 2.3. Deployment Considerations for Minor Version Compatibility  As described, applications that directly rely only on the CUDA runtime can be deployed in the following two scenarios: CUDA driver that’s installed on the system is newer than the runtime.
CUDA runtime is newer than the CUDA driver on the system but they are from the same major release of CUDA Toolkit.
In scenario 2, system admins should be aware of the aforementioned limitations and should be able to tell why an application may be failing if they run into any issues.
Minor version compatibility has another benefit that offers flexibility in the use and deployment of libraries.
Applications that use libraries that support minor version compatibility can be deployed on systems with a different version of the toolkit and libraries without recompiling the application for the difference in the library version.
This holds true for both older and newer versions of the libraries provided they are all from the same major release family.
Figure 3 NVRTC supports minor version compatibility from CUDA 11.3 onwards  However, if an application is unable to leverage the minor version compatibility due to any of the aforementioned reasons, then the Forward Compatibility model can be used as an alternative even though Forward Compatibility is mainly intended for compatibility across major toolkit versions. 3. Forward Compatibility  3.1.
Forward Compatibility Support Across Major Toolkit Versions  Increasingly, data centers and enterprises may not want to update the NVIDIA GPU Driver across major release versions due to the rigorous testing and validation that happens before any system level driver installations are done.
To support such scenarios, CUDA introduced a Forward Compatibility Upgrade path in CUDA 10.0.
Figure 4 Forward Compatibility Upgrade Path  Forward Compatibility is applicable only for systems with NVIDIA Data Center GPUs or select NGC Server Ready SKUs of RTX cards.
It’s mainly intended to support applications built on newer CUDA Toolkits to run on systems installed with an older NVIDIA Linux GPU driver from different major release families.
This new forward-compatible upgrade path requires the use of a special package called “CUDA compat package”. 3.2. Installing the Forward Compatibility Package  3.2.1.
From Network Repositories or Local Installers  The CUDA compat package is available in the local installers or the CUDA network repositories provided by NVIDIA as cuda-compat-12.4.
On Ubuntu, for example: The compat package will then be installed to the versioned toolkit location typically found in the toolkit directory.
* -GPU debugging support for CUDA Driver (CUDA 11.8 and later only) These files should be kept together as the CUDA driver is dependent on the libnvidia-ptxjitcompiler.so.
Example: CUDA Compatibility is installed and the application can now run successfully as shown below.
In this example, the user sets LD_LIBRARY_PATH to include the files installed by the cuda-compat-12-1 package.
Check the files installed under /usr/local/cuda/compat : The user can set LD_LIBRARY_PATH to include the files installed before running the CUDA 12.1 application: 3.2.2.
Manually Installing from Runfile  The cuda-compat package files can also be extracted from the appropriate datacenter driver ‘runfile’ installers ( .run ) available in NVIDIA driver downloads.
To do this: Download the latest NVIDIA Data Center GPU driver , and extract the .run file using option -x.
Copy the four CUDA compatibility upgrade files, listed at the start of this section, into a user- or root-created directory.
Follow your system’s guidelines for making sure that the system linker picks up the new libraries.
Note Symlinks under /usr/local/cuda/compat need to be created manually when using the runfile installer. 3.3. Deployment Considerations for Forward Compatibility  3.3.1.
Use the Right Compat Package  CUDA forward compat packages should be used only in the following situations when forward compatibility is required across major releases.
If you are on the R470 driver but require 12.5 application support, please install the cuda-compat package for 12.5.
But when performing a full system upgrade, when choosing to install both the toolkit and the driver, remove any forward compatible packages present in the system.
For example, if you are upgrading the driver to 525.60.13 which is the minimum required driver version for the 12.x toolkits, then the cuda-compat package is not required in most cases.
11.x and 12.x applications will be supported due to backward compatibility and future 12.x applications will be supported due to minor-version compatibility.
But there are feature restrictions that may make this option less desirable for your scenario - for example: Applications requiring PTX JIT compilation support.
Unlike the minor-version compatibility that is defined between CUDA runtime and CUDA driver, forward compatibility is defined between the kernel driver and the CUDA driver, and hence such restrictions do not apply.
In order to circumvent the limitation, a forward compatibility package may be used in such scenarios as well.
Table 3 CUDA Application Compatibility Support Matrix  NVIDIA Kernel Mode Driver - Production Branch CUDA Forward Compatible Upgrade 470.57.02+ (CUDA 11.4) 530.30.02+ (CUDA 12.1) 535.54.03+ (CUDA 12.2) 545.23.06+ (CUDA 12.3) 550.54.14+ (CUDA 12.4) 555.42.02+ (CUDA 12.5) 12-5 C X C X C Not required 12-4 C C Not required X 12-3 C C X X 12-2 C Not required X X 12-1 C X X X 12-0 C X X X 11-8 C X X X 11-7 C X X X 11-6 C X X X 11-5 C X X X 11-4 Not required X X X C - Compatible X - Not compatible Branches R525, R515, R510, R465, R460, R455, R450, R440, R418, R410, R396, R390 are end of life and are not supported targets for compatibility.
New Feature Branches (such as 495.xx) are not supported targets for CUDA Forward Compatibility.
Examples of how to read this table: The CUDA 12-4 compat package is “C”ompatible with driver versions 470, 535.
It is “Not required” for 550, as 12.4 was paired with 550 and therefore no extra packages are needed.
The CUDA “12-3” release is not-compatible (“X”) with driver version 550 as it was released prior to the driver.
Binaries created in 12.3 are still subject to the backwards compatibility guarantees described in this document. 3.3.2. Feature Exceptions  There are specific features in the CUDA driver that require kernel-mode support and will only work with a newer kernel mode driver.
Table 4 Forward-Compatible Feature-Driver Support Matrix  CUDA Forward Compatible Upgrade CUDA - OpenGL/Vulkan Interop cuMemMap* set of functionalities System Base Installation: 525 (>=.60.04) Driver 12-x No Yes [1] System Base Installation: 450 (>=.80.02) Driver 11-x No Yes [1] [1] This relies on CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED and CU_DEVICE_ATTRIBUTE_VIRTUAL_ADDRESS_MANAGEMENT_SUPPORTED , which should be queried if you intend to use the full range of this functionality.
[2] Supported on Red Hat Enterprise Linux operating system version 8.1 or higher. 3.3.3. Check for Compatibility Support  In addition to the CUDA driver and certain compiler components, there are other drivers in the system installation stack (for example, OpenCL) that remain on the old version.
A well-written application should use following error codes to determine if CUDA Forward Compatible Upgrade is supported.
System administrators should be aware of these error codes to determine if there are errors in the deployment.
This error indicates that there is a mismatch between the versions of the display driver and the CUDA driver.
This error indicates that the system was upgraded to run with forward compatibility but the visible hardware detected by CUDA does not support this configuration. 3.4. Deployment Model for Forward Compatibility  There are two models of deployment for the CUDA compat package.
Shared deployment: Allows sharing the same compat package across installed toolkits in the system.
Download and extract the latest forward compatibility package with the highest toolkit version in its name.
Using RPATH, or through LD_LIBRARY_PATH or through an automatic loader (for example, ld.so.conf ), point to that package.
The user can set LD_LIBRARY_PATH to include the files installed before running the CUDA 11.1 application: $ LD_LIBRARY_PATH =/ usr / local / cuda / compat : $LD_LIBRARY_PATH Per-application deployment: Individual applications can choose a package of their choice and place it as part of the Modules system tied to the toolkit and the libraries.
Using the Modules system, the admin, or the user, can set up ‘module’ scripts for each version of each toolkit package, and then load the module script for the toolkit as needed.
$ module load cuda / 11.0 There is an important consideration to the per-application deployment approach: older forward compatibility packages are not supported on new driver versions.
Therefore the module load scripts should proactively query the system for whether the compatibility package can be used before loading the files.
In the cases where the module script cannot use CUDA compatible upgrade, a fallback path to the default system’s installed CUDA driver can provide a more consistent experience and this can be achieved using RPATH . 4. Conclusion  The CUDA driver maintains backward compatibility to continue support of applications built on older toolkits.
Using a compatible minor driver version, applications build on CUDA Toolkit 11 and newer are supported on any driver from within the corresponding major release.
Using the CUDA Forward Compatibility package, system administrators can run applications built using a newer toolkit even when an older driver that does not satisfy the minimum required driver version is installed on the system.
This forward compatibility allows the CUDA deployments in data centers and enterprises to benefit from the faster release cadence and the latest features and performance of CUDA Toolkit.
CUDA compatibility helps users by: Faster upgrades to the latest CUDA releases: Enterprises or data centers with NVIDIA GPUs have complex workflows and require advance planning for NVIDIA driver rollouts.
Not having to update the driver for newer CUDA releases can mean that new versions of the software can be made available faster to users without any delays.
Faster upgrades of the CUDA libraries: Users can upgrade to the latest software libraries and applications built on top of CUDA (for example, math libraries or deep learning frameworks) without an upgrade to the entire CUDA Toolkit or driver.
This is possible as these libraries and frameworks do not have a direct dependency on the CUDA runtime, compiler or driver. 5. Frequently Asked Questions  This section includes some FAQs related to CUDA compatibility.
What is the difference between CUDA forward compatible upgrade and CUDA minor version compatibility? When should users use these features? Area CUDA Forward Compatible Upgrade CUDA Minor Version Compatibility Compatibility Across older drivers from different major release versions of CUDA.
When to use If you cannot upgrade the kernel driver but need to use the latest CUDA Toolkit.
If you want to support newer applications on older drivers within the same major release family.
GPUs supported 11.4 UMD (User Mode Driver) and later will extend forward compatibility support to select NGC Ready NVIDIA RTX boards.
All GPU products supported OS distributions supported Linux only Windows, Linux Features supported Some features such as (CUDA-GL interop, Power 9 ATS, cuMemMap APIs) are not supported.
Users may have to incorporate checks in their application when using new features in the minor release (that require a new driver) to ensure graceful errors.
CUDA releases supported All CUDA releases supported through the lifetime of the datacenter driver branch.
For example, R418 (CUDA 10.1) EOLs in March 2022 - so all CUDA versions released (including major releases) during this timeframe are supported.
Users can also set up LD_LIBRARY_PATH with the new libraries from the cuda-compat-* package.
Does CUDA forward compatible upgrades work intra-branch? Users can upgrade the kernel mode driver within the same branch.
This use-case is supported only for drivers on LLB and LTS branches of driver for select GPUs.
Which GPUs are supported by the driver? The CUDA compatible upgrade is meant to ease the management of large production systems for enterprise customers.
11.4 UMD (User Mode Driver) and later will extend forward compatibility support to select NGC Ready NVIDIA RTX boards.
It’s important to note that HW support is defined by the kernel mode driver and as such, newer CUDA drivers on their own will not enable new HW support.
Hardware Generation Compute Capability CTK Support Latest Forward Compatibility Package Support Driver Current Minimum Driver in Support Maximum Driver Supported* Hopper 9.x 11.8 - current current 525.60.13+ latest NVIDIA Ampere GPU Arch.
8.x 11.0 - current 470.57.02 latest Turing 7.5 10.0 - current latest Volta 7.x 9.0 - current latest Pascal 6.x 8.0 - current latest Maxwell 5.x 6.5 - current latest Refer to CUDA Driver Lifecycle to find the latest supported driver.
What should we do? PTX and application compatibility information can be found in Binary Compatibility .
If we build our CUDA application using CUDA 11.0, can it continue to be used with newer NVIDIA drivers (such as CUDA 11.1/R455, 11.x etc.)? Or is it only the other way around? Drivers have always been backwards compatible with CUDA.
This means that a CUDA 11.0 application will be compatible with R450 (11.0), R455 (11.1) and beyond.
CUDA applications typically statically include all the libraries (for example cudart, CUDA math libraries such as cuBLAS, cuFFT) they need, so they should work on new drivers or CUDA Toolkit installations.
In other words, since CUDA is backward compatible, existing CUDA applications can continue to be used with newer CUDA versions.
What is the minimum CUDA 11.x driver that supports the CUDA minor version compatibility? The minimum driver version required is 450.80.02.
What about new features introduced in minor releases of CUDA? How does a developer build an application using newer CUDA Toolkits (e.g.
11.x) work on a system with a CUDA 11.0 driver (R450)? By using new CUDA versions, users can benefit from new CUDA programming model APIs, compiler optimizations and math library features.
A subset of CUDA APIs don’t need a new driver and they can all be used without any driver dependencies.
To use other CUDA APIs introduced in a minor release (that require a new driver), one would have to implement fallbacks or fail gracefully.
This situation is not different from what is available today where developers use macros to compile out features based on CUDA versions.
Users should refer to the CUDA headers and documentation for new CUDA APIs introduced in a release.
There are some issues that admins can advise the application developers to accommodate in their code.
CUDA minor version compatibility and CUDA forward compatible upgrade both work when using either NGC Deep Learning Framework containers or using containers that are based on the official CUDA base images.
The images include the CUDA compatible upgrade libraries and the NVIDIA Container Toolkit (nvidia-docker2) has logic to correctly load the required libraries.
I’m running an NGC container and see this error: “This container was built for NVIDIA Driver Release 450.51 or later, but version 418.126.02 was detected and compatibility mode is UNAVAILABLE.”.
What could be wrong? It is possible you are either running a wrong version of the NVIDIA driver on the system or your system does not have an NVIDIA Data Center GPU. 6. Notices  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 6.1. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(true); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
NVIDIA NVIDIA Data Center GPU Driver Documentation Search In: Entire Site Just This Document clear search search NVIDIA Data Center GPU NVIDIA Multi-Instance GPU User Guide 1.
Changelog Search Results NVIDIA Multi-Instance GPU User Guide ( PDF ) - Last updated March 26, 2024 - NVIDIA Multi-Instance GPU User Guide User guide for Multi-Instance GPU on the NVIDIA® GPUs.
This edition of the user guide describes the Multi-Instance GPU feature first introduced with the NVIDIA® Ampere architecture.
Introduction The new Multi-Instance GPU (MIG) feature allows GPUs (starting with NVIDIA Ampere architecture) to be securely partitioned into up to seven separate GPU Instances for CUDA applications, providing multiple users with separate GPU resources for optimal GPU utilization.
This feature is particularly beneficial for workloads that do not fully saturate the GPU's compute capacity and therefore users may want to run different workloads in parallel to maximize utilization.
For Cloud Service Providers (CSPs), who have multi-tenant use cases, MIG ensures one client cannot impact the work or scheduling of other clients, in addition to providing enhanced isolation for customers.
With MIG, each instance's processors have separate and isolated paths through the entire memory system - the on-chip crossbar ports, L2 cache banks, memory controllers, and DRAM address busses are all assigned uniquely to an individual instance.
This ensures that an individual user's workload can run with predictable throughput and latency, with the same L2 cache allocation and DRAM bandwidth, even if other tasks are thrashing their own caches or saturating their DRAM interfaces.
MIG can partition available GPU compute resources (including streaming multiprocessors or SMs, and GPU engines such as copy engines or decoders), to provide a defined quality of service (QoS) with fault isolation for different clients such as VMs, containers or processes.
MIG enables multiple GPU Instances to run in parallel on a single, physical NVIDIA Ampere GPU.
With MIG, users will be able to see and schedule jobs on their new virtual GPU Instances as if they were physical GPUs.
MIG works with Linux operating systems, supports containers using Docker Engine, with support for Kubernetes and virtual machines using hypervisors such as Red Hat Virtualization and VMware vSphere.
MIG supports the following deployment configurations: Bare-metal, including containers GPU pass-through virtualization to Linux guests on top of supported hypervisors vGPU on top of supported hypervisors MIG allows multiple vGPUs (and thereby VMs) to run in parallel on a single GPU, while preserving the isolation guarantees that vGPU provides.
For more information on GPU partitioning using vGPU and MIG, refer to the technical brief .
MIG Overview The purpose of this document is to introduce the concepts behind MIG, deployment considerations and provide examples of MIG management to demonstrate how users can run CUDA applications on MIG supported GPUs.
Supported GPU Products Product Architecture Microarchitecture Compute Capability Memory Size Max Number of Instances H100-SXM5 Hopper GH100 9.0 80GB 7 H100-PCIE Hopper GH100 9.0 80GB 7 H100-SXM5 Hopper GH100 9.0 94GB 7 H100-PCIE Hopper GH100 9.0 94GB 7 H100 on GH200 Hopper GH100 9.0 96GB 7 A100-SXM4 NVIDIA Ampere GA100 8.0 40GB 7 A100-SXM4 NVIDIA Ampere GA100 8.0 80GB 7 A100-PCIE NVIDIA Ampere GA100 8.0 40GB 7 A100-PCIE NVIDIA Ampere GA100 8.0 80GB 7 A30 NVIDIA Ampere GA100 8.0 24GB 4 Additionally, MIG is supported on systems that include the supported products above such as DGX, DGX Station and HGX.
Supported Configurations Supported deployment configurations with MIG include Bare-metal, including containers and Kubernetes GPU pass-through virtualization to Linux guests on top of supported hypervisors vGPU on top of supported hypervisors Virtualization MIG can be used with two types of virtualization: Under Linux guests on supported hypervisors, when MIG-supported GPUs are in GPU pass-through, the same workflows , tools and profiles available on bare-metal can be used.
MIG allows multiple vGPUs (and thereby VMs) to run in parallel on a single MIG-supported GPU, while preserving the isolation guarantees that vGPU provides.
To configure a GPU for use with vGPU VMs, refer to the chapter in the vGPU Software User Guide.
Concepts Terminology This section introduces some terminology used to describe the concepts behind MIG.
Streaming Multiprocessor A streaming multiprocessor (SM) executes compute instructions on the GPU.
It encapsulates all the resources necessary to execute operations on the GPU, including a distinct address space, memory allocations, etc.
A GPU context has the following properties: Fault isolation Individually scheduled Distinct address space GPU Engine A GPU engine is what executes work on the GPU.
The most commonly used engine is the Compute/Graphics engine that executes the compute instructions.
Other engines include the copy engine (CE) that is responsible for performing DMAs, NVDEC for video decoding, NVENC for encoding, etc.
GPU Memory Slice A GPU memory slice is the smallest fraction of the GPU's memory, including the corresponding memory controllers and cache.
A GPU memory slice is roughly one eighth of the total GPU memory resources, including both capacity and bandwidth.
A GPU SM slice is roughly one seventh of the total number of SMs available in the GPU when configured in MIG mode.
GPU Slice A GPU slice is the smallest fraction of the GPU that combines a single GPU memory slice and a single GPU SM slice.
GPU Instance A GPU Instance (GI) is a combination of GPU slices and GPU engines (DMAs, NVDECs, etc.).
Anything within a GPU instance always shares all the GPU memory slices and other GPU engines, but it's SM slices can be further subdivided into compute instances (CI).
Each GPU slice includes dedicated GPU memory resources which limit both the available capacity and bandwidth, and provide memory QoS.
Each GPU memory slice gets 1/8 of the total GPU memory resources and each GPU SM slice gets 1/7 of the total number of SMs.
A Compute Instance (CI) contains a subset of the parent GPU instance's SM slices and other GPU engines (DMAs, NVDECs, etc.).
Partitioning Using the concepts introduced above, this section provides an overview of how the user can create various partitions on the GPU.
For illustration purposes, the document will use the A100-40GB as an example, but the process is similar for other GPUs that support MIG.
GPU Instance Partitioning of the GPU happens using memory slices, so the A100-40GB GPU can be thought of having 8x5GB memory slices and 7 SM slices as shown in the diagram below.
Available Slices on A100 As explained above, then to create a GPU Instance (GI) requires combining some number of memory slices with some number of compute slices.
In the diagram below, a 5GB memory slice is combined with 1 compute slice to create a 1g.5gb GI profile: Figure 3.
Combining Memory and Compute Slices Similarly, 4x5GB memory slices can be combined with 4x1 compute slices to create the 4g.5gb GI profile: Figure 4.
Combining Memory and Compute Slices Compute Instance The compute slices of a GPU Instance can be further subdivided into multiple Compute Instances (CI), with the CIs sharing the engines and memory of the parent GI, but each CI has dedicated SM resources.
Using the same 4g.20gb example above, a CI may be created to consume only the first compute slice as shown below: Figure 5.
Combining Memory and Compute Slices In this case, 4 different CIs can be created by choosing any of the compute slices.
Two compute slices can also be combined together to create a 2c.4g.20gb profile: Figure 6.
Combining Memory and Compute Slices In this example, 3 compute slices can also be combined to create a 3c.4g.20gb profile or all 4 can be combined to create a 4c.4g.20gb profile.
When all 4 compute slices are combined, the profile is simply referred to as the 4g.20gb profile.
The NVIDIA driver APIs provide a number of “GPU Instance Profiles” and users can create GIs by specifying one of these profiles.
On a given GPU, multiple GIs can be created from a mix and match of these profiles, so long as enough slices are available to satisfy the request.
For A100-SXM4-80GB, the profile names will change according to the memory proportion - for example, 1g.10gb , 2g.20gb , 3g.40gb , 4g.40gb , 7g.80gb respectively.
For a list of all supported combinations of profiles on MIG-enabled GPUs, refer to the section on supported profiles .
GPU Instance Profiles on A100 Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g.5gb 1/8 1/7 0 NVDECs /0 JPEG /0 OFA 1/8 1 7 MIG 1g.5gb+me 1/8 1/7 1 NVDEC /1 JPEG /1 OFA 1/8 1 1 (A single 1g profile can include media extensions) MIG 1g.10gb 1/8 1/7 1 NVDECs /0 JPEG /0 OFA 1/8 1 4 MIG 2g.10gb 2/8 2/7 1 NVDECs /0 JPEG /0 OFA 2/8 2 3 MIG 3g.20gb 4/8 3/7 2 NVDECs /0 JPEG /0 OFA 4/8 3 2 MIG 4g.20gb 4/8 4/7 2 NVDECs /0 JPEG /0 OFA 4/8 4 1 MIG 7g.40gb Full 7/7 5 NVDECs /1 JPEG /1 OFA Full 7 1 The diagram below shows a pictorial representation of how to build all valid combinations of GPU instances.
MIG Profiles on A100 In this diagram, a valid combination can be built by starting with an instance profile on the left and combining it with other instance profiles as you move to the right, such that no two profiles overlap vertically.
For a list of all supported combinations and placements of profiles on A100 and A30, refer to the section on supported profiles .
Note that prior to NVIDIA driver release R510, the combination of a (4 memory, 4 compute) and a (4 memory, 3 compute) profile was not supported.
Profile Placements on A100 Note that the diagram represents the physical layout of where the GPU Instances will exist once they are instantiated on the GPU.
As GPU Instances are created and destroyed at different locations, fragmentation can occur, and the physical position of one GPU Instance will play a role in which other GPU Instances can be instantiated next to it.
CUDA Concurrency Mechanisms MIG has been designed to be largely transparent to CUDA applications - so that the CUDA programming model remains unchanged to minimize programming effort.
CUDA already exposes multiple technologies for running work in parallel on the GPU and it is worthwhile showcasing how these technologies compare to MIG.
Note that streams and MPS are part of the CUDA programming model and thus work when used with GPU Instances.
CUDA Streams are a CUDA Programming model feature where, in a CUDA application, different work can be submitted to independent queues and be processed independently by the GPU.
CUDA streams can only be used within a single process and don't offer much isolation - the address space is shared, the SMs are shared, the GPU memory bandwidth, caches and capacity are shared.
It's commonly used by MPI jobs that cooperate, but it has also been used for sharing the GPU resources among unrelated applications, while accepting the challenges that such a solution brings.
MPS currently does not offer error isolation between clients and while streaming multiprocessors used by each MPS client can be optionally limited to a percentage of all SMs, the scheduling hardware is still shared.
Lastly, MIG is the new form of concurrency offered by NVIDIA GPUs while addressing some of the limitations with the other CUDA technologies for running parallel work.
CUDA Concurrency Mechanisms Streams MPS MIG Partition Type Single Process Logical Physical Max Partitions Unlimited 48 7 SM Performance Isolation No Yes (by percentage, not partitioning) Yes Memory Protection No Yes Yes Memory Bandwidth QoS No No Yes Error Isolation No No Yes Cross-Partition Interop Always IPC Limited IPC Reconfigure Dynamic Process Launch When Idle Deployment Considerations MIG functionality is provided as part of the NVIDIA GPU driver.
System Considerations The following system considerations are relevant for when the GPU is in MIG mode.
The /proc mechanism for system-level interfaces is deprecated as of 450.51.06 and it is recommended to use the /dev based system-level interface for controlling access mechanisms of MIG devices through cgroups.
Supported configurations include Bare-metal, including containers GPU pass-through virtualization to Linux guests on top of supported hypervisors vGPU on top of supported hypervisors MIG allows multiple vGPUs (and thereby VMs) to run in parallel on a single A100, while preserving the isolation guarantees that vGPU provides.
On NVIDIA Ampere GPUs, similar to ECC mode, MIG mode setting is persistent across reboots until the user toggles the setting explicitly All daemons holding handles on driver modules need to be stopped before MIG enablement.
This is true for systems such as DGX which may be running system health monitoring services such as nvsm or GPU health monitoring or telemetry services such as DCGM .
Other MIG management, such as creating and destroying instances, requires superuser by default, but can be delegated to non-privileged users by adjusting permissions to MIG capabilities in /proc/ .
Application Considerations Users should note the following considerations when the A100 is in MIG mode: No graphics APIs are supported (e.g.
OpenGL, Vulkan etc.) No GPU to GPU P2P (either PCIe or NVLink) is supported CUDA applications treat a Compute Instance and its parent GPU Instance as a single CUDA device.
See this section on device enumeration by CUDA CUDA IPC across GPU instances is not supported.
using cuda-memcheck or compute-sanitizer) is supported CUDA MPS is supported on top of MIG.
The only limitation is that the maximum number of clients (48) is lowered proportionally to the Compute Instance size GPUDirect RDMA is supported when used from GPU Instances MIG Device Names By default, a MIG device consists of a single “GPU Instance” and a single “Compute Instance”.
The table below highlights a naming convention to refer to a MIG device by its GPU Instance's compute slice count and its total memory in GB (rather than just its memory slice count).
When only a single CI is created (that consumes the entire compute capacity of the GI), then the CI sizing is implied in the device name.
MIG Device Name Note: The description below shows the profile names on the A100-SXM4-40GB product.
Device names when using a single CI Memory 20gb 10gb 5gb GPU Instance 3g 2g 1g Compute Instance 3c 2c 1c MIG Device 3g.20gb 2g.10gb 1g.5gb GPC GPC GPC GPC GPC GPC Each GI can be further sub-divided into multiple CIs as required by users depending on their workloads.
The example shown is for subdividing a 3g.20gb device into a set of sub-devices with different Compute Instance slice counts.
Device names when using multiple CIs Memory 20gb 20gb GPU Instance 3g 3g Compute Instance 1c 1c 1c 2c 1c MIG Device 1c.3g.20gb 1c.3g.20gb 1c.3g.20gb 2c.3g.20gb 1c.3g.20gb GPC GPC GPC GPC GPC GPC Device Enumeration GPU Instances (GIs) and Compute Instances (CIs) are enumerated in the new /proc filesystem layout for MIG $ ls -l /proc/driver/nvidia-caps/ -r--r--r-- 1 root root 0 Nov 21 21:22 mig-minors -r--r--r-- 1 root root 0 Nov 21 21:22 nvlink-minors -r--r--r-- 1 root root 0 Nov 21 21:22 sys-minors The corresponding device nodes (in mig-minors ) are created under /dev/nvidia-caps .
CUDA Device Enumeration MIG supports running CUDA applications by specifying the CUDA device on which the application should be run.
With CUDA 11/R450 and CUDA 12/R525, only enumeration of a single MIG instance is supported.
In other words, regardless of how many MIG devices are created (or made available to a container), a single CUDA process can only enumerate a single MIG device.
CUDA is limited to use a single CI and will pick the first one available if several of them are visible.
To summarize, there are two constraints: CUDA can only enumerate a single compute instance CUDA will not enumerate non-MIG GPU if any compute instance is enumerated on any other GPU Note that these constraints may be relaxed in future NVIDIA driver releases for MIG.
Depending on the driver versions being used, two formats are supported: With drivers >= R470 ( 470.42.01 +), each MIG device is assigned a GPU UUID starting with MIG- .
Note: With the R470 NVIDIA datacenter drivers ( 470.42.01 +), the example below shows how MIG devices are assigned GPU UUIDs in an 8-GPU system with each GPU configured differently.
A30 MIG Profiles The following diagram shows the profiles supported on the NVIDIA A30: Figure 10.
GPU Instance Profiles on A30 Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g.6gb 1/4 1/4 0 NVDECs /0 JPEG /0 OFA 1/4 1 4 MIG 1g.6gb+me 1/4 1/4 1 NVDEC /1 JPEG /1 OFA 1/4 1 1 (A single 1g profile can include media extensions) MIG 2g.12gb 2/4 2/4 2 NVDECs /0 JPEG /0 OFA 2/4 2 2 MIG 2g.12gb+me 2/4 2/4 2 NVDECs /1 JPEG /1 OFA 2/4 2 1 (A single 2g profile can include media extensions) MIG 4g.24gb Full 4/4 4 NVDECs /1 JPEG /1 OFA Full 4 1 Note: The 1g.6gb+me profile is only available starting with R470 drivers.
A100 MIG Profiles The following diagram shows the profiles supported on the NVIDIA A100: Figure 11.
Profiles on A100 The table below shows the supported profiles on the A100-SXM4-40GB product.
For A100-SXM4-80GB, the profile names will change according to the memory proportion - for example, 1g.10gb , 1g.10gb+me , 1g.20gb , 2g.20gb , 3g.40gb , 4g.40gb , 7g.80gb respectively.
GPU Instance Profiles on A100 Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g.5gb 1/8 1/7 0 NVDECs /0 JPEG /0 OFA 1/8 1 7 MIG 1g.5gb+me 1/8 1/7 1 NVDEC /1 JPEG /1 OFA 1/8 1 1 (A single 1g profile can include media extensions) MIG 1g.10gb 1/8 1/7 1 NVDECs /0 JPEG /0 OFA 1/8 1 4 MIG 2g.10gb 2/8 2/7 1 NVDECs /0 JPEG /0 OFA 2/8 2 3 MIG 3g.20gb 4/8 3/7 2 NVDECs /0 JPEG /0 OFA 4/8 3 2 MIG 4g.20gb 4/8 4/7 2 NVDECs /0 JPEG /0 OFA 4/8 4 1 MIG 7g.40gb Full 7/7 5 NVDECs /1 JPEG /1 OFA Full 7 1 Note: The 1g.5gb+me profile is only available starting with R470 drivers.
The 1g.10gb profile is only available starting with R525 drivers. 8.3. H100 MIG Profiles The following diagram shows the profiles supported on the NVIDIA H100: Figure 12.
Profiles on H100 The table below shows the supported profiles on the H100 80GB product (PCIe and SXM5).
GPU Instance Profiles on H100 Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g.10gb 1/8 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 7 MIG 1g.10gb+me 1/8 1/7 1 NVDEC /1 JPEG /1 OFA 1/8 1 1 (A single 1g profile can include media extensions) MIG 1g.20gb 1/4 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 4 MIG 2g.20gb 2/8 2/7 2 NVDECs /2 JPEG /0 OFA 2/8 2 3 MIG 3g.40gb 4/8 3/7 3 NVDECs /3 JPEG /0 OFA 4/8 3 2 MIG 4g.40gb 4/8 4/7 4 NVDECs /4 JPEG /0 OFA 4/8 4 1 MIG 7g.80gb Full 7/7 7 NVDECs /7 JPEG /1 OFA Full 8 1 The table below shows the supported profiles on the H100 94GB product (PCIe and SXM5).
Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g.11gb 1/8 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 7 MIG 1g.11gb+me 1/8 1/7 1 NVDEC /1 JPEG /1 OFA 1/8 1 1 (A single 1g profile can include media extensions) MIG 1g.22gb 1/4 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 4 MIG 2g.22gb 2/8 2/7 2 NVDECs /2 JPEG /0 OFA 2/8 2 3 MIG 3g.44gb 4/8 3/7 3 NVDECs /3 JPEG /0 OFA 4/8 3 2 MIG 4g.44gb 4/8 4/7 4 NVDECs /4 JPEG /0 OFA 4/8 4 1 MIG 7g.88gb Full 7/7 7 NVDECs /7 JPEG /1 OFA Full 8 1 The table below shows the supported profiles on the H100 96GB product (H100 on GH200).
Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g.12gb 1/8 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 7 MIG 1g.12gb+me 1/8 1/7 1 NVDEC /1 JPEG /1 OFA 1/8 1 1 (A single 1g profile can include media extensions) MIG 1g.24gb 1/4 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 4 MIG 2g.24gb 2/8 2/7 2 NVDECs /2 JPEG /0 OFA 2/8 2 3 MIG 3g.48gb 4/8 3/7 3 NVDECs /3 JPEG /0 OFA 4/8 3 2 MIG 4g.48gb 4/8 4/7 4 NVDECs /4 JPEG /0 OFA 4/8 4 1 MIG 7g.96gb Full 7/7 7 NVDECs /7 JPEG /1 OFA Full 8 1 8.4.
H200 MIG Profiles The following diagram shows the profiles supported on the NVIDIA H200: Figure 13.
GPU Instance Profiles on H200 Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g.18gb 1/8 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 7 MIG 1g.18gb+me 1/8 1/7 1 NVDEC /1 JPEG /1 OFA 1/8 1 1 (A single 1g profile can include media extensions) MIG 1g.35gb 1/4 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 4 MIG 2g.35gb 2/8 2/7 2 NVDECs /2 JPEG /0 OFA 2/8 2 3 MIG 3g.71gb 4/8 3/7 3 NVDECs /3 JPEG /0 OFA 4/8 3 2 MIG 4g.71gb 4/8 4/7 4 NVDECs /4 JPEG /0 OFA 4/8 4 1 MIG 7g.141gb Full 7/7 7 NVDECs /7 JPEG /1 OFA Full 8 1 Getting Started with MIG Prerequisites The following prerequisites and minimum software versions are recommended when using supported GPUs in MIG mode.
MIG is supported only on GPUs and systems listed here It is recommended to install the latest NVIDIA datacenter driver.
The minimum versions are provided below: If using H100, then CUDA 12 and NVIDIA driver R525 ( >= 525.53 ) or later If using A100/A30, then CUDA 11 and NVIDIA driver R450 ( >= 450.80.02 ) or later Linux operating system distributions supported by CUDA If running containers or using Kubernetes, then: NVIDIA Container Toolkit ( nvidia-docker2 ): v2.5.0 or later NVIDIA K8s Device Plugin: v0.7.0 or later NVIDIA gpu-feature-discovery: v0.2.0 or later MIG can be managed programmatically using NVIDIA Management Library (NVML) APIs or its command-line-interface, nvidia-smi .
Note that for brevity, some of the nvidia-smi output in the following examples may be cropped to showcase the relevant sections of interest.
For more information on the MIG commands, see the nvidia-smi man page or nvidia-smi mig --help .
For information on the MIG management APIs, see the NVML header ( nvml.h ) included in the CUDA Toolkit packages ( cuda-nvml-dev-* ; installed under /usr/local/cuda/include/nvml.h ) For automated tooling support with configuring MIG, refer to the NVIDIA MIG Part ition Ed itor (or mig-parted ) tools .
For example, running nvidia-smi shows that MIG mode is disabled: $ nvidia-smi -i 0 +-+ | NVIDIA-SMI 450.80.02 Driver Version: 450.80.02 CUDA Version: 11.0 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr.
| |=+=+=| | 0 A100-SXM4-40GB Off | 00000000:36:00.0 Off | 0 | | N/A 29C P0 62W / 400W | 0MiB / 40537MiB | 6% Default | | | | Disabled | +-+-+-+ MIG mode can be enabled on a per-GPU basis with the following command: nvidia-smi -i -mig 1 .
When MIG is enabled on the GPU, depending on the GPU product, the driver will attempt to reset the GPU so that MIG mode can take effect.
$ nvidia-smi -i 0 --query-gpu=pci.bus_id,mig.mode.current --format=csv pci.bus_id, mig.mode.current 00000000:36:00.0, Enabled GPU Reset on Hopper+ GPUs Starting with the Hopper generation of GPUs, enabling MIG mode no longer requires a GPU reset to take effect (and thus the driver does not attempt to reset the GPU in the background).
Note that MIG mode ( Disabled or Enabled states) is only persistent as long as the driver is resident in the system (i.e.
MIG mode is no longer persistent across system reboots (there is no longer a status bit stored in the GPU InfoROM).
GPU Reset on Ampere GPUs On NVIDIA Ampere GPUs, when MIG mode is enabled, the driver will attempt to reset the GPU so that MIG mode can take effect.
Note that MIG mode ( Disabled or Enabled states) is persistent across system reboots (there is a status bit stored in the GPU InfoROM).
Note: If you are using MIG inside a VM with NVIDIA Ampere GPUs (A100 or A30) in passthrough, then you may need to reboot the VM to allow the GPU to be in MIG mode as in some cases, GPU reset is not allowed via the hypervisor for security reasons.
This can be seen in the following example: $ sudo nvidia-smi -i 0 -mig 1 Warning: MIG mode is in pending enable state for GPU 00000000:00:03.0:Not Supported Reboot the system or try nvidia-smi --gpu-reset to make MIG mode effective on GPU 00000000:00:03.0 All done.
For example, on DGX systems, you may encounter the following message: $ sudo nvidia-smi -i 0 -mig 1 Warning: MIG mode is in pending enable state for GPU 00000000:07:00.0:In use by another client 00000000:07:00.0 is currently being used by one or more other processes (e.g.
Please first kill all processes using the device and retry the command or reboot the system to make MIG mode effective.
In this specific DGX example, you would have to stop the nvsm and dcgm services, enable MIG mode on the desired GPU and then restore the monitoring services: $ sudo systemctl stop nvsm $ sudo systemctl stop dcgm $ sudo nvidia-smi -i 0 -mig 1 Enabled MIG Mode for GPU 00000000:07:00.0 All done.
As described in the Device Nodes section, granting read access to mig/config capabilities allows non-root users to manage instances once the GPU has been configured into MIG mode.
$ ls -l /proc/driver/nvidia/capabilities/* /proc/driver/nvidia/capabilities/mig: total 0 -r- 1 root root 0 May 24 16:10 config -r--r--r-- 1 root root 0 May 24 16:10 monitor List GPU Instance Profiles The NVIDIA driver provides a number of profiles that users can opt-in for when configuring the MIG feature in A100.
The profiles are the sizes and capabilities of the GPU instances that can be created by the user.
The driver also provides information about the placements, which indicate the type and number of instances that can be created.
$ nvidia-smi mig -lgip +-+ | GPU instance profiles: | | GPU Name ID Instances Memory P2P SM DEC ENC | | Free/Total GiB CE JPEG OFA | |=| | 0 MIG 1g.5gb 19 7/7 4.75 No 14 0 0 | | 1 0 0 | +-+ | 0 MIG 1g.5gb+me 20 1/1 4.75 No 14 1 0 | | 1 1 1 | +-+ | 0 MIG 1g.10gb 15 4/4 9.62 No 14 1 0 | | 1 0 0 | +-+ | 0 MIG 2g.10gb 14 3/3 9.62 No 28 1 0 | | 2 0 0 | +-+ | 0 MIG 3g.20gb 9 2/2 19.50 No 42 2 0 | | 3 0 0 | +-+ | 0 MIG 4g.20gb 5 1/1 19.50 No 56 2 0 | | 4 0 0 | +-+ | 0 MIG 7g.40gb 0 1/1 39.25 No 98 5 0 | | 7 1 1 | +-+ List the possible placements available using the following command.
The placement index shown indicates how the profiles are mapped on the GPU as shown in the supported profiles tables .
$ nvidia-smi mig -lgipp GPU 0 Profile ID 19 Placements: {0,1,2,3,4,5,6}:1 GPU 0 Profile ID 20 Placements: {0,1,2,3,4,5,6}:1 GPU 0 Profile ID 15 Placements: {0,2,4,6}:2 GPU 0 Profile ID 14 Placements: {0,2,4}:2 GPU 0 Profile ID 9 Placements: {0,4}:4 GPU 0 Profile ID 5 Placement : {0}:4 GPU 0 Profile ID 0 Placement : {0}:8 The command shows that the user can create two instances of type 3g.20gb (profile ID 9) or seven instances of 1g.5gb (profile ID 19).
Creating GPU Instances Before starting to use MIG, the user needs to create GPU instances using the -cgi option.
One of three options can be used to specify the instance profiles to be created: Profile ID (e.g.
MIG 3g.20gb ) Once the GPU instances are created, one needs to create the corresponding Compute Instances (CI).
Note: Without creating GPU instances (and corresponding compute instances), CUDA workloads cannot be run on the GPU.
Thus, the user or system administrator needs to recreate the desired MIG configurations if the GPU or system is reset.
For automated tooling support for this purpose, refer to the NVIDIA MIG Part ition Ed itor (or mig-parted ) tool , including creating a systemd service that could recreate the MIG geometry at system startup.
The following example shows how the user can create GPU instances (and corresponding compute instances).
In this example, the user can create two GPU instances (of type 3g.20gb ), with each GPU instance having half of the available compute and memory capacity.
If a mixed geometry of the profiles is specified by the user, then the NVIDIA driver chooses the placement of the various profiles.
After the instances are created, the placement of the profiles can be observed: $ sudo nvidia-smi mig -cgi 19,14,5 Successfully created GPU instance ID 13 on GPU 0 using profile MIG 1g.5gb (ID 19) Successfully created GPU instance ID 5 on GPU 0 using profile MIG 2g.10gb (ID 14) Successfully created GPU instance ID 1 on GPU 0 using profile MIG 4g.20gb (ID 5) $ sudo nvidia-smi mig -lgi +-+ | GPU instances: | | GPU Name Profile Instance Placement | | ID ID Start:Size | |=| | 0 MIG 1g.5gb 19 13 6:1 | +-+ | 0 MIG 2g.10gb 14 5 4:2 | +-+ | 0 MIG 4g.20gb 5 1 0:4 | +-+ Example 2: Creation of a 3-2-1-1 geometry.
Note: Due to a known issue with the APIs, the profile ID 9 or 3g.20gb must be specified first in order.
$ sudo nvidia-smi mig -cgi 19,19,14,9 Successfully created GPU instance ID 13 on GPU 0 using profile MIG 1g.5gb (ID 19) Successfully created GPU instance ID 11 on GPU 0 using profile MIG 1g.5gb (ID 19) Successfully created GPU instance ID 3 on GPU 0 using profile MIG 2g.10gb (ID 14) Unable to create a GPU instance on GPU 0 using profile 9: Insufficient Resources Failed to create GPU instances: Insufficient Resources Specify the correct order for the 3g.20gb profile.
In this example, the BlackScholes CUDA sample is run simultaneously on the two GIs created on the A100.
From the previous example, the utilization is displayed as N/A when running CUDA programs: $ nvidia-smi +-+ | MIG devices: | +-+-+-+-+ | GPU GI CI MIG | Memory-Usage | Vol| Shared | | ID ID Dev | BAR1-Usage | SM Unc| CE ENC DEC OFA JPG| | | | ECC| | |=+=+=+=| | 0 1 0 0 | 268MiB / 20096MiB | 42 0 | 3 0 2 0 0 | | | 4MiB / 32767MiB | | | +-+-+-+-+ | 0 2 0 1 | 268MiB / 20096MiB | 42 0 | 3 0 2 0 0 | | | 4MiB / 32767MiB | | | +-+-+-+-+ +-+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=| | 0 1 0 6217 C ...inux/release/BlackScholes 253MiB | | 0 2 0 6223 C ...inux/release/BlackScholes 253MiB | +-+ For monitoring MIG devices on MIG capable GPUs such as the A100, including attribution of GPU metrics (including utilization and other profiling metrics), it is recommended to use NVIDIA DCGM v2.0.13 or later.
See the Profiling Metrics section in the DCGM User Guide for more details on getting started.
Compute Instances As explained earlier in this document, a further level of concurrency can be achieved by using Compute Instances (CIs).
The following example shows how 3 CUDA processes (BlackScholes CUDA sample) can be run on the same GI.
First, list the available CI profiles available using our prior configuration of creating 2 GIs on the A100.
$ sudo nvidia-smi mig -lcip -gi 1 +-+ | Compute instance profiles: | | GPU GPU Name Profile Instances Exclusive Shared | | Instance ID Free/Total SM DEC ENC OFA | | ID CE JPEG | |=| | 0 1 MIG 1c.3g.20gb 0 0/3 14 2 0 0 | | 3 0 | +-+ | 0 1 MIG 2c.3g.20gb 1 0/1 28 2 0 0 | | 3 0 | +-+ | 0 1 MIG 3g.20gb 2* 0/1 42 2 0 0 | | 3 0 | +-+ Create 3 CIs, each of type 1c compute capacity (profile ID 0) on the first GI.
$ sudo nvidia-smi mig -cci 0,0,0 -gi 1 Successfully created compute instance on GPU 0 GPU instance ID 1 using profile MIG 1c.3g.20gb (ID 0) Successfully created compute instance on GPU 0 GPU instance ID 1 using profile MIG 1c.3g.20gb (ID 0) Successfully created compute instance on GPU 0 GPU instance ID 1 using profile MIG 1c.3g.20gb (ID 0) Using nvidia-smi, the following CIs are now created on GI 1.
The following example shows how the CIs and GIs created in the previous examples can be destroyed.
Note: If the intention is to destroy all the CIs and GIs, then this can be accomplished with the following commands: $ sudo nvidia-smi mig -dci && sudo nvidia-smi mig -dgi Successfully destroyed compute instance ID 0 from GPU 0 GPU instance ID 1 Successfully destroyed compute instance ID 1 from GPU 0 GPU instance ID 1 Successfully destroyed compute instance ID 2 from GPU 0 GPU instance ID 1 Successfully destroyed GPU instance ID 1 from GPU 0 Successfully destroyed GPU instance ID 2 from GPU 0 In this example, we delete the specific CIs created under GI 1.
Note: On Ampere GPUs (A100 or A30), NVML (and nvidia-smi ) does not support attribution of utilization metrics to MIG devices.
From the previous example, the utilization is displayed as N/A when running CUDA programs: $ nvidia-smi +-+ | MIG devices: | +-+-+-+-+ | GPU GI CI MIG | Memory-Usage | Vol| Shared | | ID ID Dev | BAR1-Usage | SM Unc| CE ENC DEC OFA JPG| | | | ECC| | |=+=+=+=| | 0 1 0 0 | 268MiB / 20096MiB | 42 0 | 3 0 2 0 0 | | | 4MiB / 32767MiB | | | +-+-+-+-+ | 0 2 0 1 | 268MiB / 20096MiB | 42 0 | 3 0 2 0 0 | | | 4MiB / 32767MiB | | | +-+-+-+-+ +-+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=| | 0 1 0 6217 C ...inux/release/BlackScholes 253MiB | | 0 2 0 6223 C ...inux/release/BlackScholes 253MiB | +-+ MIG with CUDA MPS As described in the section on CUDA concurrency mechanisms, CUDA Multi-Process Service (MPS) enables co-operative multi-process CUDA applications to be processed concurrently on the GPU.
MPS and MIG can work together, potentially achieving even higher levels of utilization for certain workloads.
Refer to the MPS documentation to understand the architecture and provisioning sequence for MPS.
Workflow In summary, the workflow for running with MPS is as follows: Configure the desired MIG geometry on the GPU.
Setup the CUDA_MPS_PIPE_DIRECTORY variable to point to unique directories so that the multiple MPS servers and clients can communicate with each other using named pipes and Unix domain sockets.
Note: The MPS documentation recommends setting up EXCLUSIVE_PROCESS mode to ensure that a single MPS server is using the GPU.
However, this mode is not supported when the GPU is in MIG mode as we use multiple MPS servers (one per MIG GPU instance).
Configure GPU Instances Follow the steps outlined in the previous sections to configure the desired MIG geometry on the GPU.
For this example, we configure the GPU into a 3g.20gb , 3g.2gb geometry: $ nvidia-smi +-+ | NVIDIA-SMI 460.73.01 Driver Version: 460.73.01 CUDA Version: 11.2 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr.
$ curl https: get.docker.com | sh \ && sudo systemctl start docker \ && sudo systemctl enable docker Install NVIDIA Container Toolkit Now install the NVIDIA Container Toolkit (previously known as nvidia-docker2 ).
MIG support is available starting with v2.3 of nvidia-docker2 (or v1.1.1 of the nvidia-container-toolkit package).
To get access to the /dev nvidia capabilities , it is recommended to use at least v2.5.0 of nvidia-docker2 .
/etc/os-release;echo $ID$VERSION_ID) \ && curl -fsSL https: nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \ && curl -s -L https: nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \ sed 's#deb https: #deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https: #g' | \ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list Install the NVIDIA Container Toolkit packages (and their dependencies): $ sudo apt-get install -y nvidia-docker2 \ && sudo systemctl restart docker Running Containers To run containers on specific MIG devices - whether these are GIs or specific underlying CIs, then the NVIDIA_VISIBLE_DEVICES variable (or the --gpus option with Docker 19.03+) can be used.
NVIDIA_VISIBLE_DEVICES supports the following formats to specify MIG devices: MIG-  when using R450 and R460 drivers or MIG- starting with R470 drivers.
GPUDeviceIndex>: If using Docker 19.03, the --gpus option can be used to specify MIG devices by using the following format: ‘“device=MIG-device”’ , where MIG-device can follow either of the format specified above for NVIDIA_VISIBLE_DEVICES .
The following example shows running nvidia-smi from within a CUDA container using both formats.
As can be seen in the example, only one MIG device as chosen is visible to the container when using either format.
$ sudo docker run --runtime=nvidia \ -e NVIDIA_VISIBLE_DEVICES=MIG-c7384736-a75d-5afc-978f-d2f1294409fd \ nvidia/cuda nvidia-smi +-+ | MIG devices: | +-+-+-+-+ | GPU GI CI MIG | Memory-Usage | Vol| Shared | | ID ID Dev | | SM Unc| CE ENC DEC OFA JPG| | | | ECC| | |=+=+=+=| | 0 1 0 0 | 11MiB / 20224MiB | 42 0 | 3 0 2 0 0 | +-+-+-+-+ +-+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=| | No running processes found | +-+ # For Docker versions = 19.03 $ sudo docker run --gpus '"device=0:0"' \ nvidia/cuda nvidia-smi -L GPU 0: A100-SXM4-40GB (UUID: GPU-e86cb44c-6756-fd30-cd4a-1e6da3caf9b0) MIG 3g.20gb Device 0: (UUID: MIG-c7384736-a75d-5afc-978f-d2f1294409fd) A more complex example is to run a TensorFlow container to do a training run using GPUs on the MNIST dataset.
This is shown below: $ sudo docker run --gpus '"device=0:1"' \ nvcr.io/nvidia/pytorch:20.11-py3 \ /bin/bash -c 'cd /opt/pytorch/examples/upstream/mnist && python main.py' = == PyTorch == = NVIDIA Release 20.11 (build 17345815) PyTorch Version 1.8.0a0+17f8c32 Container image Copyright (c) 2020, NVIDIA CORPORATION.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert) Copyright (c) 2012-2014 Deepmind Technologies (Koray Kavukcuoglu) Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu) Copyright (c) 2011-2013 NYU (Clement Farabet) Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston) Copyright (c) 2006 Idiap Research Institute (Samy Bengio) Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz) Copyright (c) 2015 Google Inc.
Copyright (c) 2015 Yangqing Jia Copyright (c) 2013-2016 The Caffe contributors All rights reserved.
NVIDIA modifications are covered by the license terms that apply to the underlying project or file.
9920512it [00:01, 7880654.53it/s] 32768it [00:00, 129950.31it/s] 1654784it [00:00, 2353765.88it/s] 8192it [00:00, 41020.33it/s] /opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors.
This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor.
You may want to copy the array to protect its data or make it writeable before converting it to a tensor.
Done! Train Epoch: 1 [0/60000 (0%)] Loss: 2.320747 Train Epoch: 1 [640/60000 (1%)] Loss: 1.278727 MIG with Kubernetes MIG support in Kubernetes is available starting with v0.7.0 of the NVIDIA Device Plugin for Kubernetes .
MIG with Slurm Slurm is a workload manager that is widely used at high performance computing centers such as government labs, universities.
Device Nodes and Capabilities Currently, the NVIDIA kernel driver exposes its interfaces through a few system-wide device nodes.
/dev ├── nvidiactl ├── nvidia-modeset ├── nvidia-uvm ├── nvidia-uvm-tools ├── nvidia-nvswitchctl ├── nvidia0 └── nvidia1 Starting with CUDA 11/R450, a new abstraction known as nvidia-capabilities has been introduced.
The idea being that access to a specific capability is required to perform certain actions through the driver.
The one exception being if you are the root-user (or any user with CAP_SYS_ADMIN privileges).
For example, the mig-config capability allows one to create and destroy MIG instances on any MIG-capable GPU (e.g.
Likewise, the fabric-mgmt capability allows one to run the Fabric Manager as a non-root but privileged daemon.
Without this capability, all attempts to launch the Fabric Manager as a non-root user will fail.
The following sections walk through the system level interface for managing these new nvidia-capabilities , including the steps necessary to grant and revoke access to them.
System Level Interface There are two different system-level interfaces available to work with nvidia-capabilities .
The /proc based interface relies on user-permissions and mount namespaces to limit access to a particular capability, while the /dev based interface relies on cgroups .
Technically, the /dev based interface also relies on user-permissions as a second-level access control mechanism (on the actual device node files themselves), but the primary access control mechanism is cgroups.
The current CUDA 11/R450 GA (Linux driver 450.51.06) supports both mechanisms, but going forward the /dev based interface is the preferred method and the /proc based interface is deprecated.
For now, users can choose the desired interface by using the nv_cap_enable_devfs parameter on the nvidia.ko kernel module: When nv_cap_enable_devfs=0 the /proc based interface is enabled.
A setting of nv_cap_enable_devfs=0 is the default for the R450 driver (as of Linux 450.51.06).
An example of loading the nvidia kernel module with this parameter set can be seen below: $ modprobe nvidia nv_cap_enable_devfs=1 /dev based nvidia-capabilities The system level interface for interacting with /dev based capabilities is actually through a combination of /proc and /dev .
First, a new major device is now associated with nvidia-caps and can be read from the standard /proc/devices file.
$ cat /proc/devices | grep nvidia-caps 508 nvidia-caps Second, the exact same set of files exist under /proc/driver/nvidia/capabilities .
These files no longer control access to the capability directly and instead, the contents of these files point at a device node under /dev , through which cgroups can be used to control access to the capability.
This can be seen in the example below: $ cat /proc/driver/nvidia/capabilities/mig/config DeviceFileMinor: 1 DeviceFileMode: 256 DeviceFileModify: 1 The combination of the device major for nvidia-caps and the value of DeviceFileMinor in this file indicate that the mig-config capability (which allows a user to create and destroy MIG devices) is controlled by the device node with a major:minor of 238:1 .
As such, one will need to use cgroups to grant a process read access to this device in order to configure MIG devices.
The purpose of the DeviceFileMode and DeviceFileModify fields in this file are explained later on in this section.
The standard location for these device nodes is under /dev/nvidia-caps as seen in the example below: $ ls -l /dev/nvidia-caps total 0 cr- 1 root root 508, 1 Nov 21 17:16 nvidia-cap1 cr--r--r-- 1 root root 508, 2 Nov 21 17:16 nvidia-cap2 ...
Unfortunately, these device nodes cannot be automatically created/deleted by the NVIDIA driver at the same time it creates/deletes files underneath /proc/driver/nvidia/capabilities (due to GPL compliance issues).
Instead, a user-level program called nvidia-modprobe is provided, that can be invoked from user-space in order to do this.
For example: $ nvidia-modprobe \ -f /proc/driver/nvidia/capabilities/mig/config \ -f /proc/driver/nvidia/capabilities/mig/monitor $ ls -l /dev/nvidia-caps total 0 cr- 1 root root 508, 1 Nov 21 17:16 nvidia-cap1 cr--r--r-- 1 root root 508, 2 Nov 21 17:16 nvidia-cap2 nvidia-modprobe looks at the DeviceFileMode in each capability file and creates the device node with the permissions indicated (e.g.
Programs such as nvidia-smi will automatically invoke nvidia-modprobe (when available) to create these device nodes on your behalf.
In other scenarios it is not necessarily required to use nvidia-modprobe to create these device nodes, but it does make the process simpler.
If you actually want to prevent nvidia-modprobe from ever creating a particular device node on your behalf, you can do the following: # Give a user write permissions to the capability file under /proc $ chmod +uw /proc/driver/nvidia/capabilities/mig/config # Update the file with a “DeviceFileModify” setting of 0 $ echo "DeviceFileModify: 0" > /proc/driver/nvidia/capabilities/mig/config You will then be responsible for managing creation of the device node referenced by /proc/driver/nvidia/capabilities/mig/config going forward.
If you want to change that in the future, simply reset it to a value of "DeviceFileModify: 1" with the same command sequence.
This is important in the context of containers because we may want to give a container access to a certain capability even if it doesn't exist in the /proc hierarchy yet.
For example, granting a container the mig-config capability implies that we should also grant it capabilities to access all possible gis and cis that could be created for any GPU on the system.
Otherwise the container will have no way of working with those gis and cis once they have actually been created.
One final thing to note about /dev based capabilities is that the minor numbers for all possible capabilities are predetermined and can be queried under various files of the form: /proc/driver/nvidia-caps/*-minors For example, all capabilities related to MIG can be looked up as: $ cat /proc/driver/nvidia-caps/mig-minors config 1 monitor 2 gpu0/gi0/access 3 gpu0/gi0/ci0/access 4 gpu0/gi0/ci1/access 5 gpu0/gi0/ci2/access 6 ...
gpu31/gi14/ci6/access 4321 gpu31/gi14/ci7/access 4322 The format of the content follows: GPU/gi/ci Note that the GPU device minor number can be obtained by using either of these mechanisms: The NVML API nvmlDeviceGetMinorNumber() so it returns the device minor number Or use the PCI BDF available under /proc/driver/nvidia/gpus/domain:bus:device:function/information .
For example, if the MIG geometry was created as below: +-+ | MIG devices: | +-+-+-+-+ | GPU GI CI MIG | Memory-Usage | Vol| Shared | | ID ID Dev | BAR1-Usage | SM Unc| CE ENC DEC OFA JPG| | | | ECC| | |=+=+=+=| | 0 1 0 0 | 19MiB / 40192MiB | 14 0 | 3 0 3 0 3 | | | 0MiB / 65535MiB | | | +-+ +-+-+ | 0 1 1 1 | | 14 0 | 3 0 3 0 3 | | | | | | +-+ +-+-+ | 0 1 2 2 | | 14 0 | 3 0 3 0 3 | | | | | | +-+-+-+-+ Then the corresponding device nodes: /dev/nvidia-cap12 , /dev/nvidia-cap13 and /dev/nvidia-cap14 and /dev/nvidia-cap15 would be created.
/proc based nvidia-capabilities ( **Deprecated** ) The system level interface for interacting with /proc based nvidia-capabilities is rooted at /proc/driver/nvidia/capabilities .
Files underneath this hierarchy are used to represent each capability, with read access to these files controlling whether a user has a given capability or not.
Thus a MIG device is identified by the following format: MIG-  As an example, having read access to the following paths would allow one to run workloads on the MIG device represented by : /proc/driver/nvidia/capabilities/gpu0/mig/gi0/access /proc/driver/nvidia/capabilities/gpu0/mig/gi0/ci0/access Note, that there is no access file representing a capability to run workloads on gpu0 (only on gi0 and ci0 that sit underneath gpu0).
This is because the traditional mechanism of using cgroups to control access to top level GPU devices (and any required meta devices) is still required.
As shown earlier in the document, the cgroups mechanism applies to: /dev/nvidia0 /dev/nvidiactl /dev/nvidiactl-uvm ...
In the context of containers, a new mount namespace should be overlaid on top of the path for /proc/driver/nvidia/capabilities , and only those capabilities a user wishes to grant to a container should be bind-mounted in.
Since the host’s user/group information is retained across the bind-mount, it must be ensured that the correct user permissions are set for these capabilities on the host before injecting them into a container.
Changelog 11/17/2022 (author: PR): Includes the following changes: Updates for Hopper, CUDA 12.0/R525 Reorginzation of several chapters Added more information on /dev based capabilities 7/19/2022 (author: PR): Includes the following changes: Added a chapter on virtualization.
8/26/2021 (author: PR): Includes the following changes: Improve explanation of GPU Partitioning.
6/30/2021 (author: PR): Includes the following changes: Add info on unique UUIDs for MIG devices.
4/22/2021 (author: PR): Includes the following changes: Added information for Slurm and CUDA MPS.
4/14/2021 (author: PR): Includes the following changes: Add additional supported products.
2/17/2021 (author: PR): Includes the following changes: Add note about persistence of MIG devices.
8/7/2020 (author: PR): Added information on device nodes and nvidia-capabilities with CUDA 11.0 GA.
Notices Notice This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.
Trademarks NVIDIA and the NVIDIA logo are trademarks and/or registered trademarks of NVIDIA Corporation in the Unites States and other countries.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact var switchTo5x=true; stLight.options({publisher: "998dc202-a267-4d8e-bce9-14debadb8d92", doNotHash: false, doNotCopy: false, hashAddressBar: false}); if (typeof _satellite !== "undefined"){ _satellite.pageBottom();}
1.
EFLOW User’s Guide v12.5 | PDF | Archive EFLOW User’s Guide Describes how CUDA and NVIDIA GPU accelerated cloud native applications can be deployed on EFLOW enabled Windows devices.
Introduction  Azure IoT Edge For Linux on Windows, otherwise referred to as EFLOW, is a Microsoft Technology for the deployment of Linux AI containers on Windows Edge devices.
This document details how NVIDIA® CUDA® and NVIDIA GPU accelerated cloud native applications can be deployed on such EFLOW-enabled Windows devices.
EFLOW has the following components: The Windows host OS with virtualization enabled A Linux virtual machine IoT Edge Runtime IoT Edge Modules, or otherwise any docker-compatible containerized application (runs on moby/containerd) GPU-accelerated IoT Edge Modules support for GeForce RTX GPUs is based on the GPU Paravirtualization that was foundational to CUDA on Windows Subsystem on Linux.
CUDA on WSL 2 boosted the productivity of CUDA developers by enabling them to build, develop, and containerize GPU accelerated NVIDIA AI/ML Linux applications on Windows desktop computers before deployment on Linux instances on the cloud.
A containerized NVIDIA GPU accelerated Linux application that is either hosted on Azure IoT Hub or NGC registry can be seamlessly deployed at the edge such as a retail service center or hospitals.
These edge deployments are typically IT managed devices entrenched with Windows devices for manageability but the advent of AI/ML use cases in this space seek the convergence for Linux and Windows applications not only to coexist but also seamlessly communicate on the same device.
Because CUDA support on EFLOW is predominantly based on WSL 2, refer to the Software Support, Limitations and Known Issues sections in the CUDA on WSL 2 document to stay abreast of the scope of NVIDIA software support available on EFLOW as well.
The following sections details installation of EFLOW, prerequisites for out-of-the-box CUDA support, followed by sample instructions for running an existing GPU accelerated container on EFLOW. 1.2. Setup and Installation  Follow the Microsoft EFLOW documentation page for various installation options suiting your needs: For up-to-date installation instructions, visit http: aka.ms/AzEFLOW-install .
For quick setup, we have included the steps for installation through Powershell in the following sections. 1.2.1. Driver Installation  On the target Windows device, first install an NVIDIA GeForce or NVIDIA RTX GPU Windows driver that is compatible with the NVIDIA GPU on your device.
EFLOW VM supports deploying containerized CUDA applications and hence only the driver must be installed on the host system.
If you are preparing a CUDA docker container, ensure that the necessary toolchains are installed.
Because EFLOW is based on WSL, the restrictions of the software stack for a hybrid Linux on Windows environment apply, and not all of the NVIDIA software stack is supported.
Refer to the user’s guide of the SDK that you are interested in to determine support. 1.2.2. Installation of EFLOW  In an elevated powershell prompt perform the following: Enable HyperV.
Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All Path : Online : True RestartNeeded : False Set execution policy and verify.
Set-ExecutionPolicy -ExecutionPolicy AllSigned -Force Get-ExecutionPolicy AllSigned Download and install EFLOW.
$msiPath = $([io.Path]::Combine($env:TEMP, 'AzureIoTEdge.msi')) $ProgressPreference = 'SilentlyContinue' Invoke-WebRequest "https: aka.ms/AzEFLOWMSI_1_4_LTS_X64" -OutFile $msiPath Start-Process -Wait msiexec -ArgumentList "/i","$([io.Path]::Combine($env:TEMP, 'AzureIoTEdge.msi'))","/qn" Determine host OS configuration.
>Get-EflowHostConfiguration | format-list FreePhysicalMemoryInMB : 35502 NumberOfLogicalProcessors : {64, 64} DiskInfo : @{Drive=C:; FreeSizeInGB=798} GpuInfo : @{Count=1; SupportedPassthroughTypes=System.Object[]; Name=NVIDIA RTX A2000} Deploy EFLOW.
By default, EFLOW only reserves 1024MB of system memory for use for the workloads and that is insufficient to support GPU accelerated configurations.
For GPU acceleration, you will have to reserve system memory explicitly at EFLOW deployment; otherwise there will not be sufficient system memory for your containerized applications to run.
In order to prevent out of memory errors, reserve memory explicitly as required; see example below.
(Refer to command line argument options available for deploying EFLOW in the official documentation for more details). 1.2.3. Prerequisites for CUDA Support  x86 64-bit support only.
Windows 10/11 (Pro, Enterprise, IoT Enterprise) - Windows 10 users must use the November 2021 update build 19044.1620 or higher.
Deploy-Eflow only allocates 1024 MB memory by default, set it to a larger value to prevent OOM issue, check MS documents for more details at https: learn.microsoft.com/en-us/azure/iot-edge/reference-iot-edge-for-linux-on-windows-functions#deploy-eflow .
Refer to https: learn.microsoft.com/en-us/azure/iot-edge/gpu-acceleration?view=iotedge-1.4 . 1.3. Connecting to the EFLOW VM  Get-EflowVmAddr [10/13/2022 11:41:16] Querying IP and MAC addresses from virtual machine (IPP1-1490-EFLOW) - Virtual machine MAC: 00:15:5d:b2:40:c7 - Virtual machine IP : 172.24.14.242 retrieved directly from virtual machine 00:15:5d:b2:40:c7 172.24.14.242 Connect-EflowVm 1.4.
Running GPU-accelerated Containers  Let us run an N-body simulation containerized CUDA sample from NGC, but this time inside EFLOW.
iotedge-user@IPP1-1490-EFLOW [ ~ ]$ sudo docker run --gpus all --env NVIDIA_DISABLE_REQUIRE=1 nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark Unable to find image 'nvcr.io/nvidia/k8s/cuda-sample:nbody' locally nbody: Pulling from nvidia/k8s/cuda-sample 22c5ef60a68e: Pull complete 1939e4248814: Pull complete 548afb82c856: Pull complete a424d45fd86f: Pull complete 207b64ab7ce6: Pull complete f65423f1b49b: Pull complete 2b60900a3ea5: Pull complete e9bff09d04df: Pull complete edc14edf1b04: Pull complete 1f37f461c076: Pull complete 9026fb14bf88: Pull complete Digest: sha256:59261e419d6d48a772aad5bb213f9f1588fcdb042b115ceb7166c89a51f03363 Status: Downloaded newer image for nvcr.io/nvidia/k8s/cuda-sample:nbody Run "nbody -benchmark [-numbodies=]" to measure performance.
-fullscreen (run n-body simulation in fullscreen mode) -fp64 (use double precision floating point values for simulation) -hostmem (stores simulation data in host memory) -benchmark (run benchmark to measure performance) -numbodies= (number of bodies (>= 1) to run in simulation) -device= (where d=0,1,2.
for the CUDA device to use) -numdevices= (where i=(number of CUDA devices > 0) to use for simulation) -compare (compares simulation results running once on the default GPU and once on the CPU) -cpu (run n-body simulation on the CPU) -tipsy= (load a tipsy model file for simulation) NOTE: The CUDA Samples are not meant for performance measurements.
> Windowed mode > Simulation data stored in video memory > Single precision floating point simulation > 1 Devices used for simulation GPU Device 0: "Ampere" with compute capability 8.6 > Compute 8.6 CUDA device: [NVIDIA RTX A2000] 26624 bodies, total time for 10 iterations: 31.984 ms = 221.625 billion interactions per second = 4432.503 single-precision GFLOP/s at 20 flops per interaction iotedge-user@IPP1-1490-EFLOW [ ~ ]$ 1.6.
Troubleshooting  nvidia-container-cli: requirement error: unsatisfied condition: cuda>=11.7”, need add “–env NVIDIA_DISABLE_REQUIRE=1” The CUDA version cannot be determined correctly from the driver on the host when launching the container.
Out of memory In case of out of memory errors, increase the system memory reserved by EFLOW.
Refer to https: learn.microsoft.com/en-us/azure/iot-edge/reference-iot-edge-for-linux-on-windows-functions#deploy-eflow . 2. Notices  2.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 2.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 2.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
NVIDIA GPU Accelerated Computing on WSL 2 v12.5 | PDF | Archive CUDA on WSL User Guide The guide for using NVIDIA CUDA on Windows Subsystem for Linux.
NVIDIA GPU Accelerated Computing on WSL 2  WSL or Windows Subsystem for Linux is a Windows feature that enables users to run native Linux applications, containers and command-line tools directly on Windows 11 and later OS builds.
CUDA support in this user guide is specifically for WSL 2, which is the second generation of WSL that offers the following benefits Linux applications can run as is in WSL 2.
WSL 2 is characteristically a VM with a Linux WSL Kernel in it that provides full compatibility with mainstream Linux kernel allowing support for native Linux applications including popular Linux distros.
WSL 2 is tightly integrated with the Microsoft Windows operating system, which allows it to run Linux applications alongside and even interop with other Windows desktop and modern store apps.
Typically, developers working across both Linux and Windows environments have a very disruptive workflow.
install Linux and Windows in separate partitions on the same or different hard disks on the system and boot to the OS of choice.
Also this has historically restricted the development of seamless, well integrated tools and software systems across two dominant ecosystems.
WSL enables users to have a seamless transition across the two environments without the need for a resource intensive traditional virtual machine and to improve productivity and develop using tools and integrate their workflow.
More importantly WSL 2 enables applications that were hitherto only available on Linux to be available on Windows.
WSL 2 support for GPU allows for these applications to benefit from GPU accelerated computing and expands the domain of applications that can be developed on WSL 2.
With NVIDIA CUDA support for WSL 2, developers can leverage NVIDIA GPU accelerated computing technology for data science, machine learning and inference on Windows through WSL.
GPU acceleration also serves to bring down the performance overhead of running an application inside a WSL like environment close to near-native by being able to pipeline more parallel work on the GPU with less CPU intervention.
NVIDIA driver support for WSL 2 includes not only CUDA but also DirectX and Direct ML support.
For some helpful examples, see https: docs.microsoft.com/en-us/windows/win32/direct3d12/gpu-tensorflow-wsl .
WSL 2 is a key enabler in making GPU acceleration to be seamlessly shared between Windows and Linux applications on the same system a reality.
This offers flexibility and versatility while also serving to open up GPU accelerated computing by making it more accessible.
Illustration of the possibilities with NVIDIA CUDA software stack on WSL 2  This document describes a workflow for getting started with running CUDA applications or containers in a WSL 2 environment. 1.1. NVIDIA Compute Software Support on WSL 2  This table captures the readiness and suggested software versions for NVIDIA software stack for WSL 2.
Package Suggested Versions Installation NVIDIA Windows Driver x86 Use the latest Windows x86 production driver.
Windows x86 drivers can be directly downloaded from https: www.nvidia.com/Download/index.aspx for WSL 2 support on Pascal or later GPUs.
NVIDIA Container Toolkit - Minimum versions - v2.6.0 with libnvidia-container - 1.5.1+ CLI and Docker Desktop Supported.
CUDA Toolkit and CUDA Developer Tools Preview Support Compute Sanitizer - Pascal and later Nsight Systems CLI, and CUPTI (Trace) - Volta and later Developer tools - Debuggers - Pascal and later (Using driver r535+) Developer tools - Profilers - Volta and later (Using Windows 10 OS build 19044+ with driver r545+ or using Windows 11 with driver r525+ ) Latest Linux CUDA toolkit package - WSL-Ubuntu from 12.x releases can be downloaded from https: developer.nvidia.com/cuda-downloads .
https: docs.rapids.ai/notices/rgn0024/ NCCL 2.12 or later 1.4+ Refer to the NCCL Installation guide for Linux x86 . 2. Getting Started with CUDA on WSL 2  To get started with running CUDA on WSL, complete these steps in order: 2.1.
Step 1: Install NVIDIA Driver for GPU Support  Install NVIDIA GeForce Game Ready or NVIDIA RTX Quadro Windows 11 display driver on your system with a compatible GeForce or NVIDIA RTX/Quadro card from https: www.nvidia.com/Download/index.aspx .
Refer to the system requirements in the Appendix.) Note This is the only driver you need to install.
Do not install any Linux display driver in WSL. 2.2. Step 2: Install WSL 2  Launch your preferred Windows Terminal / Command Prompt / Powershell and install WSL: wsl.exe --install Ensure you have the latest WSL kernel: wsl.exe --update 2.3.
Step 3: Set Up a Linux Development Environment  From a Windows terminal, enter WSL: C:\> wsl.exe The default distro is Ubuntu.
To update the distro to your favorite distro from the command line and to review other WSL commands, refer to the following resources: https: docs.microsoft.com/en-us/windows/wsl/install https: docs.microsoft.com/en-us/windows/wsl/basic-commands From this point you should be able to run any existing Linux application which requires CUDA.
Read the next section for further information. 3. CUDA Support for WSL 2  The latest NVIDIA Windows GPU Driver will fully support WSL 2.
With CUDA support in the driver, existing applications (compiled elsewhere on a Linux system for the same target GPU) can run unmodified within the WSL environment.
CUDA Toolkit support for WSL is still in preview stage as developer tools such as profilers are not available yet.
However, CUDA application development is fully supported in the WSL2 environment, as a result, users should be able to compile new CUDA Linux applications with the latest CUDA Toolkit for x86 Linux.
Once a Windows NVIDIA GPU driver is installed on the system, CUDA becomes available within WSL 2.
The CUDA driver installed on Windows host will be stubbed inside the WSL 2 as libcuda.so , therefore users must not install any NVIDIA GPU Linux driver within WSL 2 .
One has to be very careful here as the default CUDA Toolkit comes packaged with a driver, and it is easy to overwrite the WSL 2 NVIDIA driver with the default installation.
We recommend developers to use a separate CUDA Toolkit for WSL 2 (Ubuntu) available from the CUDA Toolkit Downloads page to avoid this overwriting.
This WSL-Ubuntu CUDA toolkit installer will not overwrite the NVIDIA driver that was already mapped into the WSL 2 environment.
First, remove the old GPG key: sudo apt-key del 7fa2af80 Option 1: Installation of Linux x86 CUDA Toolkit using WSL-Ubuntu Package - Recommended The CUDA WSL-Ubuntu local installer does not contain the NVIDIA Linux GPU driver, so by following the steps on the CUDA download page for WSL-Ubuntu , you will be able to get just the CUDA toolkit installed on WSL.
Option 2: Installation of Linux x86 CUDA Toolkit using Meta Package If you installed the toolkit using the WSL-Ubuntu package, please skip this section.
Meta packages do not contain the driver, so by following the steps on the download page for Ubuntu , you will be able to get just the CUDA toolkit installed on WSL.
The installation instructions for the CUDA Toolkit can be found in the CUDA Toolkit download page for each installer.
But DO NOT choose the “ cuda ”, “ cuda-12-x ”, or “ cuda-drivers ” meta-packages under WSL 2 as these packages will result in an attempt to install the Linux NVIDIA driver under WSL 2.
You can also install other components of the toolkit by choosing the right meta-package . 4. WSL 2 Support Constraints  WSL 2 GPU acceleration will be available on Pascal and later GPU architecture on both GeForce and Quadro product SKUs in WDDM mode.
If you are continuing to use Windows 10, see Windows Insider Preview and Windows 10 Support . 4.1. Known Limitations for Linux CUDA Applications  The following table lists the known limitations on WSL 2 that may affect CUDA applications that use some of these features that are fully supported on Linux.
Unified Memory - Full Managed Memory Support is not available on Windows native and therefore WSL 2 will not support it for the foreseeable future.
UVM full features will not be available and therefore applications relying on UVM full features may not work.
If your application is using Managed Memory, your application could see reduced performance and high system memory usage.
CUDA queries will say whether it is supported or not and applications are expected to check this.
Pinned system memory (example: System memory that an application makes resident for GPU accesses) availability for applications is limited.
For example, some deep learning training workloads, depending on the framework, model and dataset size used, can exceed this limit and may not work.
Root user on bare metal (not containers) will not find nvidia-smi at the expected location.
On multi-GPU systems it is not possible to filter for specific GPU devices by using specific index numbers to enumerate GPUs. 4.2. Features Not Yet Supported  The following table lists the set of features that are currently not supported.
Windows Insider Preview and Windows 10 Support  If you are on Windows 11 please skip this section.
Windows 11 is generally available to the public and therefore does not require special registration.
All the instructions at the beginning of this user guide were mainly focused toward Windows 11 users.
If you are looking to use WSL 2 on Windows 10 or to be on the bleeding edge of WSL 2 development, you may want to register for the Windows Insider Program and choose the appropriate flighting channel (previously fast rings) and get the latest build for your needs.
You can check your build version number by running winver via the Run command. 5.2. Troubleshooting  5.2.1.
Container Runtime Initialization Errors  In some cases, when running a Docker container, you may encounter nvidia-container-cli : initialization error : $ sudo docker run --gpus all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused "process_linux.go:449: container init caused \"process_linux.go:432: running prestart hook 0 caused \\\"error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\ \\\"\"": unknown.
ERRO[0000] error waiting for container: context canceled This usually indicates that the right Windows OS build or Microsoft Windows Insider Preview Builds (Windows 10 only), WSL 2, NVIDIA drivers and NVIDIA Container Toolkit may not be installed correctly.
Review the known issues and changelog sections to ensure the right versions of the driver and container toolkit are installed.
Ensure you have followed through the steps listed under Setup under Running CUDA containers; especially ensure that the docker daemon is still running.
$ sudo service docker stop $ sudo service docker start Or start the daemon directly and see if that resolves the issue: $ sudo dockerd If you are still running into this issue, use the dxdiag tools from the Run dialog and provide the diagnostic logs to NVIDIA by posting in the Developer Forums or by filing a report .
You can also use the CUDA on WSL 2 Developer Forums to get in touch with NVIDIA product and engineering teams for help. 5.2.2. Checking WSL Kernel Version  Ensure you have the latest kernel by running the following command in PowerShell: $ wsl cat /proc/version Linux version 5.10.16.3-microsoft-standard-WSL2 (x86_64-msft-linux-gcc (GCC) 9.3.0, GNU ld (GNU Binutils) 2.34.0.20200220) #1 SMP Fri Apr 2 22:23:49 UTC 2021 If you don’t have the latest WSL kernel, you will see the following blocking warning upon trying to launch a Linux distribution within the WSL 2 container: 5.3.
Traditional Virtual Machines vs WSL 2  Whether to efficiently use hardware resources or to improve productivity, virtualization is a more widely used solution in both consumer and enterprise space.
There are different types of virtualizations, and it is beyond the scope of this document to delve into the specifics.
But traditional virtualization solutions require installation and setup of a virtualization management software to manage the guest virtual machines.
Although WSL 2 is itself a Virtual Machine, unlike traditional VMs it is easy to setup as it is provided by the host operating system provider and is quite lightweight.
Applications running within WSL see less overhead compared to traditional VMs especially if they require access to the hardware or perform privileged operations compared to when run directly on the system.
While VMs allow applications to be run unmodified, due to constraints from setup and performance overhead, they are not the best option in many situations. 5.4. Containers vs WSL 2  While a VM provides a secure self-contained, execution environment with a complete user space for the application, containers enable application composability without the overhead of VMs.
Containers compose all the dependencies of the applications such as libraries, files etc., to be bundled together for development and easy and predictable deployment.
Containers run on the operating system that is installed on the system directly and therefore do not provide full isolation from other containers like a VM does, but keeps overhead negligible as a result.
To learn more about differences between VMs and containers, refer to https: docs.microsoft.com/en-us/virtualization/windowscontainers/about/containers-vs-vm . 6. Notices  6.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 6.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 6.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2020-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Introduction v12.5 | PDF | Archive CUDA Demo Suite The reference guide for the CUDA Demo Suite.
These applications demonstrate the capabilities and details of NVIDIA GPUs. 2. Demos  Below are the demos within the demo suite.
2.1. deviceQuery  This application enumerates the properties of the CUDA devices present in the system and displays them in a human readable format.
2.2. vectorAdd  This application is a very basic demo that implements element by element vector addition.
2.3. bandwidthTest  This application provides the memcopy bandwidth of the GPU and memcpy bandwidth across PCI‑e.
This application is capable of measuring device to device copy bandwidth, host to device copy bandwidth for pageable and page-locked memory, and device to host copy bandwidth for pageable and page-locked memory.
busGrind  Provides detailed statistics about peer-to-peer memory bandwidth amongst GPUs present in the system as well as pinned, unpinned memory bandwidth.
Arguments: Options Explanation -h print usage -p [0,1] enable or disable pinned memory tests (default on) -u [0,1] enable or disable unpinned memory tests (default off) -e [0,1] enable or disable p2p enabled memory tests (default off) -d [0,1] enable or disable p2p disabled memory tests (default off) -a enable all tests -n disable all tests Order of parameters matters.
Examples: ./BusGrind -n -p 1 -e 1 Run all pinned and P2P tests ./BusGrind -n -u 1 Runs only unpinned tests ./BusGrind -a Runs all tests (pinned, unpinned, p2p enabled, p2p disabled) 2.5.
nbody  This demo does an efficient all-pairs simulation of a gravitational n-body simulation in CUDA.
Adding “-numbodies=num_of_bodies” to the command line will allow users to set # of bodies for simulation.
Adding “-numdevices=N” to the command line option will cause the sample to use N devices (if available) for simulation.
In this mode, the position and velocity data for all bodies are read from system memory using “zero copy” rather than from device memory.
For a small number of devices (4 or fewer) and a large enough number of bodies, bandwidth is not a bottleneck so we can achieve strong scaling across these devices.
Arguments: Options Explanation -fullscreen run n-body simulation in fullscreen mode -fp64 use double precision floating point values for simulation -hostmem stores simulation data in host memory -benchmark run benchmark to measure performance -numbodies=N number of bodies (>= 1) to run in simulation -device=d where d=0,1,2….
for the CUDA device to use -numdevices=i where i=(number of CUDA devices > 0) to use for simulation -compare compares simulation results running once on the default GPU and once on the CPU -cpu run n-body simulation on the CPU -tipsy=file.bin load a tipsy model file for simulation 2.6.
oceanFFT  This is a graphical demo which simulates an ocean height field using the CUFFT library, and renders the result using OpenGL.
The following keys can be used to control the output: Keys Function w Toggle wireframe 2.7.
randomFog  This is a graphical demo which does pseudo- and quasi- random numbers visualization produced by CURAND.
On creation, randomFog generates 200,000 random coordinates in spherical coordinate space (radius, angle rho, angle theta) with curand’s XORWOW algorithm.
The following keys can be used to control the output: Keys Function s Generate new set of random nos and display as spherical coordinates (Sphere) e Generate new set of random nos and display on a spherical surface (shEll) b Generate new set of random nos and display as cartesian coordinates (cuBe/Box) p Generate new set of random nos and display on a cartesian plane (Plane) i, l, j Rotate the negative Z-axis up, right, down and left respectively a Toggle auto-rotation t Toggle 10x zoom z Toggle axes display x Select XORWOW generator (default) c Select Sobol’ generator v Select scrambled Sobol’ generator r Reset XORWOW (i.e.
reset to initial seed) and regenerate ] Increment the number of Sobol’ dimensions and regenerate [ Reset the number of Sobol’ dimensions to 1 and regenerate Increment the number of displayed points by 8,000 (max.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2016-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Introduction  The PTX Compiler APIs are a set of APIs which can be used to compile a PTX program into GPU assembly code.
The APIs accept PTX programs in character string form and create handles to the compiler that can be used to obtain the GPU assembly code.
The GPU assembly code string generated by the APIs can be loaded by cuModuleLoadData and cuModuleLoadDataEx , and linked with other modules by cuLinkAddData or nvJitLinkAddData API from nvjitlink of the CUDA Driver API.
The main use cases for these PTX Compiler APIs are: With CUDA driver APIs, compilation and loading are tied together.
This allows applications to perform early compilation and caching of the GPU assembly code.
PTX Compiler APIs allow users to use runtime compilation for the latest PTX version that is supported as part of CUDA Toolkit release.
This support may not be available in the PTX JIT compiler present in the CUDA Driver if the application is running with an older driver installed in the system.
With PTX Compiler APIs, clients can implement a custom caching mechanism with the compiled GPU assembly.
The clients get fine grain control and can specify the compiler options during compilation. 2. Getting Started  2.1.
System Requirements  PTX Compiler library requires the following system configuration: POSIX threads support for non-Windows platform.
CUDA Toolkit and Driver. 2.2. Installation  PTX Compiler library is part of the CUDA Toolkit release and the components are organized as follows in the CUDA toolkit installation directory: On Windows: include vPTXCompiler.h lib\x64 vptxcompiler_static.lib doc\pdf\PTX_Compiler_API_User_Guide.pdf On Linux: include/nvPTXCompiler.h lib64/libnvptxcompiler_static.a doc/pdf/PTX_Compiler_API_User_Guide.pdf 3.
Thread Safety  All PTX Compiler API functions are thread safe and may be invoked by multiple threads concurrently. 4. User Interface  This chapter presents the PTX Compiler APIs.
PTX-Compiler Handle  Typedefs nvPTXCompilerHandle nvPTXCompilerHandle represents a handle to the PTX Compiler. 4.1.1. Typedefs  typedef struct nvPTXCompiler * nvPTXCompilerHandle  nvPTXCompilerHandle represents a handle to the PTX Compiler.
To compile a PTX program string, an instance of nvPTXCompiler must be created and the handle to it must be obtained using the API nvPTXCompilerCreate() .
Then the compilation can be done using the API nvPTXCompilerCompile() . 4.2. Error codes  Enumerations nvPTXCompileResult The nvPTXCompiler APIs return the nvPTXCompileResult codes to indicate the call result.
4.2.1. Enumerations  enum nvPTXCompileResult  The nvPTXCompiler APIs return the nvPTXCompileResult codes to indicate the call result.
Values: enumerator NVPTXCOMPILE_SUCCESS  enumerator NVPTXCOMPILE_ERROR_INVALID_COMPILER_HANDLE  enumerator NVPTXCOMPILE_ERROR_INVALID_INPUT  enumerator NVPTXCOMPILE_ERROR_COMPILATION_FAILURE  enumerator NVPTXCOMPILE_ERROR_INTERNAL  enumerator NVPTXCOMPILE_ERROR_OUT_OF_MEMORY  enumerator NVPTXCOMPILE_ERROR_COMPILER_INVOCATION_INCOMPLETE  enumerator NVPTXCOMPILE_ERROR_UNSUPPORTED_PTX_VERSION  enumerator NVPTXCOMPILE_ERROR_UNSUPPORTED_DEVSIDE_SYNC  4.3.
API Versioning  The PTX compiler APIs are versioned so that any new features or API changes can be done by bumping up the API version.
Functions nvPTXCompileResult nvPTXCompilerGetVersion (unsigned int *major, unsigned int *minor) Queries the current major and minor version of PTX Compiler APIs being used. 4.3.1. Functions  nvPTXCompileResult nvPTXCompilerGetVersion ( unsigned int * major , unsigned int * minor )  Queries the current major and minor version of PTX Compiler APIs being used.
Parameters major – [out] Major version of the PTX Compiler APIs minor – [out] Minor version of the PTX Compiler APIs Returns NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_INTERNAL 4.4.
Compilation APIs  Functions nvPTXCompileResult nvPTXCompilerCompile (nvPTXCompilerHandle compiler, int numCompileOptions, const char *const *compileOptions) Compile a PTX program with the given compiler options.
nvPTXCompileResult nvPTXCompilerCreate (nvPTXCompilerHandle *compiler, size_t ptxCodeLen, const char *ptxCode) Obtains the handle to an instance of the PTX compiler initialized with the given PTX program ptxCode .
nvPTXCompileResult nvPTXCompilerDestroy (nvPTXCompilerHandle *compiler) Destroys and cleans the already created PTX compiler.
nvPTXCompileResult nvPTXCompilerGetCompiledProgram (nvPTXCompilerHandle compiler, void *binaryImage) Obtains the image of the compiled program.
nvPTXCompileResult nvPTXCompilerGetCompiledProgramSize (nvPTXCompilerHandle compiler, size_t *binaryImageSize) Obtains the size of the image of the compiled program.
nvPTXCompileResult nvPTXCompilerGetErrorLog (nvPTXCompilerHandle compiler, char *errorLog) Query the error message that was seen previously for the handle.
nvPTXCompileResult nvPTXCompilerGetErrorLogSize (nvPTXCompilerHandle compiler, size_t *errorLogSize) Query the size of the error message that was seen previously for the handle.
nvPTXCompileResult nvPTXCompilerGetInfoLog (nvPTXCompilerHandle compiler, char *infoLog) Query the information message that was seen previously for the handle.
nvPTXCompileResult nvPTXCompilerGetInfoLogSize (nvPTXCompilerHandle compiler, size_t *infoLogSize) Query the size of the information message that was seen previously for the handle. 4.4.1. Functions  nvPTXCompileResult nvPTXCompilerCompile ( nvPTXCompilerHandle compiler , int numCompileOptions , const char * const * compileOptions )  Compile a PTX program with the given compiler options.
Parameters compiler – [inout] A handle to PTX compiler initialized with the PTX program which is to be compiled.
The compiled program can be accessed using the handle numCompileOptions – [in] Length of the array compileOptions compileOptions – [in] Compiler options with which compilation should be done.
Returns NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_OUT_OF_MEMORY NVPTXCOMPILE_ERROR_INTERNAL NVPTXCOMPILE_ERROR_INVALID_PROGRAM_HANDLE NVPTXCOMPILE_ERROR_COMPILATION_FAILURE NVPTXCOMPILE_ERROR_UNSUPPORTED_PTX_VERSION NVPTXCOMPILE_ERROR_UNSUPPORTED_DEVSIDE_SYNC nvPTXCompileResult nvPTXCompilerCreate ( nvPTXCompilerHandle * compiler , size_t ptxCodeLen , const char * ptxCode )  Obtains the handle to an instance of the PTX compiler initialized with the given PTX program ptxCode .
Parameters compiler – [out] Returns a handle to PTX compiler initialized with the PTX program ptxCode ptxCodeLen – [in] Size of the PTX program ptxCode passed as string ptxCode – [in] The PTX program which is to be compiled passed as string.
Returns NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_OUT_OF_MEMORY NVPTXCOMPILE_ERROR_INTERNAL nvPTXCompileResult nvPTXCompilerDestroy ( nvPTXCompilerHandle * compiler )  Destroys and cleans the already created PTX compiler.
Parameters compiler – [in] A handle to the PTX compiler which is to be destroyed Returns NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_OUT_OF_MEMORY NVPTXCOMPILE_ERROR_INTERNAL NVPTXCOMPILE_ERROR_INVALID_PROGRAM_HANDLE nvPTXCompileResult nvPTXCompilerGetCompiledProgram ( nvPTXCompilerHandle compiler , void * binaryImage )  Obtains the image of the compiled program.
Parameters compiler – [in] A handle to PTX compiler on which nvPTXCompilerCompile() has been performed.
Client should allocate memory for binaryImage Returns NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_INTERNAL NVPTXCOMPILE_ERROR_INVALID_PROGRAM_HANDLE NVPTXCOMPILE_ERROR_COMPILER_INVOCATION_INCOMPLETE nvPTXCompileResult nvPTXCompilerGetCompiledProgramSize ( nvPTXCompilerHandle compiler , size_t * binaryImageSize )  Obtains the size of the image of the compiled program.
binaryImageSize – [out] The size of the image of the compiled program Returns NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_INTERNAL NVPTXCOMPILE_ERROR_INVALID_PROGRAM_HANDLE NVPTXCOMPILE_ERROR_COMPILER_INVOCATION_INCOMPLETE nvPTXCompileResult nvPTXCompilerGetErrorLog ( nvPTXCompilerHandle compiler , char * errorLog )  Query the error message that was seen previously for the handle.
errorLog – [out] The error log which was produced in previous call to nvPTXCompilerCompiler().
Clients should allocate memory for errorLog Returns NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_INTERNAL NVPTXCOMPILE_ERROR_INVALID_PROGRAM_HANDLE nvPTXCompileResult nvPTXCompilerGetErrorLogSize ( nvPTXCompilerHandle compiler , size_t * errorLogSize )  Query the size of the error message that was seen previously for the handle.
errorLogSize – [out] The size of the error log in bytes which was produced in previous call to nvPTXCompilerCompiler().
Returns NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_INTERNAL NVPTXCOMPILE_ERROR_INVALID_PROGRAM_HANDLE nvPTXCompileResult nvPTXCompilerGetInfoLog ( nvPTXCompilerHandle compiler , char * infoLog )  Query the information message that was seen previously for the handle.
infoLog – [out] The information log which was produced in previous call to nvPTXCompilerCompiler().
Clients should allocate memory for infoLog Returns NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_INTERNAL NVPTXCOMPILE_ERROR_INVALID_PROGRAM_HANDLE nvPTXCompileResult nvPTXCompilerGetInfoLogSize ( nvPTXCompilerHandle compiler , size_t * infoLogSize )  Query the size of the information message that was seen previously for the handle.
infoLogSize – [out] The size of the information log in bytes which was produced in previous call to nvPTXCompilerCompiler().
Returns NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_INTERNAL NVPTXCOMPILE_ERROR_INVALID_PROGRAM_HANDLE 5.
Compilation Options  This chapter describes options supported by nvPTXCompilerCompile() API.
Option names with two preceding dashes ( -- ) are long option names and option names with one preceding dash ( - ) are short option names.
When a compile option takes an argument, an assignment operator ( = ) is used to separate the compile option argument from the compile option name, e.g., "--gpu-name=sm_70" .
Alternatively, the compile option name and the argument can be specified in separate strings without an assignment operator, .e.g, "--gpu-name" "sm_70" .
--allow-expensive-optimizations ( -allow-expensive-optimizations ) Enable (disable) to allow compiler to perform expensive optimizations using maximum available resources (memory and compile-time).
--device-function-maxrregcount N ( -func-maxrregcount ) When compiling with -c option, specify the maximum number of registers that device functions can use.
This option is ignored for whole-program compilation and does not affect registers used by entry functions.
If neither --device-function-maxrregcount nor --maxrregcount is specified, then no maximum is assumed.
Note Under certain situations, static device functions can safely inherit a higher register count from the caller entry function.
Value less than the minimum registers required by ABI will be bumped up by the compiler to ABI minimum limit.
--disable-optimizer-constants ( -disable-optimizer-consts ) Disable use of optimizer constant bank.
--dont-merge-basicblocks ( -no-bb-merge ) Prevents basic block merging, at a slight perfomance cost.
Normally ptx compiler attempts to merge consecutive basic blocks as part of its optimization process.
--extensible-whole-program ( -ewp ) Generate extensible whole program device code, which allows some calls to not be resolved until linking with libcudadevrt.
--fmad ( -fmad ) Enables (disables) the contraction of floating-point multiplies and adds/subtracts into floating-point multiply-add operations (FMAD, FFMA, or DFMA) Default value: true --force-load-cache ( -flcm ) Force specified cache modifier on global/generic load.
This option also takes virtual compute architectures, in which case code generation is suppressed.
Allowed values for this option: compute_50 , compute_52 , compute_53 , compute_60 , compute_61 , compute_62 , compute_70 , compute_72 , compute_73 , compute_75 , compute_80 , compute_86 , compute_87 , compute_89 , compute_90 , compute_90a , sm_50 , sm_52 , sm_53 , sm_60 , sm_61 , sm_62 , sm_70 , sm_72 , sm_73 , sm_75 , sm_80 , sm_86 , sm_87 , sm_89 , sm_90 , sm_90a Default value: sm_52 .
--maxrregcount N ( -maxrregcount ) Specify the maximum amount of registers that GPU functions can use.
Until a function-specific limit, a higher value will generally increase the performance of individual GPU threads that execute this function.
However, because thread registers are allocated from a global register pool on each GPU, a higher value of this option will also reduce the maximum thread block size, thereby reducing the amount of thread parallelism.
User program may not be able to make use of all registers as some registers are reserved by compiler.
--preserve-relocs ( -preserve-relocs ) This option will make ptx compiler to generate relocatable references for variables and preserve relocations generated for them in linked executable.
--return-at-end ( -ret-end ) Prevents optimizing return instruction at end of program Normally ptx compiler optimizes return at the end of program.
--suppress-async-bulk-multicast-advisory-warning ( -suppress-async-bulk-multicast-advisory-warning ) Suppress the warning on use of .multicast::cluster modifier on cp.async.bulk{.tensor} instruction with sm_90.
--suppress-stack-size-warning ( -suppress-stack-size-warning ) Suppress the warning that otherwise is printed when stack size cannot be determined.
--warn-on-double-precision-use ( -warn-double-usage ) Warning if double(s) are used in an instruction.
--maxntid ( -maxntid ) Specify the maximum number of threads that a thread block can have.
--minnctapersm ( -minnctapersm ) Specify the minimum number of CTAs to be mapped to an SM.
This option is also ignored for entry functions that have .minnctapersm directive specified.
--override-directive-values ( -override-directive-values ) Override the PTX directives values by the corresponding option values.
--make-errors-visible-at-exit ( -make-errors-visible-at-exit ) Generate required instructions at exit point to make memory faults and errors visible at exit. 6. Basic Usage  This section of the document uses a simple example, Vector Addition , shown in Figure 1 to explain how to use PTX Compiler APIs to compile this PTX program.
Equivalent CUDA source for the simple vector addition extern "C" __global__ void simpleVectorAdd(float *x, float *y, float *out) { size_t tid = blockIdx.x * blockDim.x + threadIdx.x; out[tid] = x[tid] + y[tid]; } With this PTX program as a string, we can create the compiler and obtain a handle to it as shown in Figure 3 .
Compiler creation and initialization of a program nvPTXCompilerHandle compiler; nvPTXCompilerCreate(&compiler, (size_t)strlen(ptxCode), ptxCode); Compilation can now be done by specifying the compile options as shown in Figure 4 .
Compilation of the PTX program const char* compile_options[] = { "--gpu-name=sm_70", "--verbose" }; nvPTXCompilerCompile(compiler, 2, compile_options); The compiled GPU assembly code can now be obtained.
And to allocate memory, we need to query the size of the image of the compiled GPU assembly code which is done as shown in Figure 5 .
Query size of the compiled assembly image nvPTXCompilerGetCompiledProgramSize(compiler, &elfSize); The image of the compiled GPU assembly code can now be queried as shown in Figure 6 .
Query the compiled assembly image elf = (char*) malloc(elfSize); nvPTXCompilerGetCompiledProgram(compiler, (void*)elf); When the compiler is not needed anymore, it can be destroyed as shown in Figure 7 .
for (i = 0; i -I $CUDA_PATH/include -L $CUDA_PATH/lib/x64/ -lcuda nvptxcompiler_static.lib Linux: gcc simpleVectorAddition.c -o simpleVectorAddition \ -I $CUDA_PATH/include \ -L $CUDA_PATH/lib64 \ libnvptxcompiler_static.a -lcuda -lm -lpthread \ -Wl,-rpath,$CUDA_PATH/lib64 7.2.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 7.2.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 7.2.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2020-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Introduction v12.5 | PDF | Archive cuSOLVER API Reference The API reference guide for cuSOLVER, a GPU accelerated library for decompositions and linear system solutions for both dense and sparse matrices.
Introduction  The cuSolver library is a high-level package based on the cuBLAS and cuSPARSE libraries.
It consists of two modules corresponding to two sets of API: The cuSolver API on a single GPU The cuSolverMG API on a single node multiGPU Each of these can be used independently or in concert with other toolkit libraries.
To simplify the notation, cuSolver denotes single GPU API and cuSolverMg denotes multiGPU API.
The intent of cuSolver is to provide useful LAPACK-like features, such as common matrix factorization and triangular solve routines for dense matrices, a sparse least-squares solver and an eigenvalue solver.
In addition cuSolver provides a new refactorization library useful for solving sequences of matrices with a shared sparsity pattern.
The first part of cuSolver is called cuSolverDN, and deals with dense matrix factorization and solve routines such as LU, QR, SVD and LDLT, as well as useful utilities such as matrix and vector permutations.
Next, cuSolverSP provides a new set of sparse routines based on a sparse QR factorization.
Not all matrices have a good sparsity pattern for parallelism in factorization, so the cuSolverSP library also provides a CPU path to handle those sequential-like matrices.
For those matrices with abundant parallelism, the GPU path will deliver higher performance.
The final part is cuSolverRF, a sparse re-factorization package that can provide very good performance when solving a sequence of matrices where only the coefficients are changed but the sparsity pattern remains the same.
It is the responsibility of the developer to allocate memory and to copy data between GPU memory and CPU memory using standard CUDA runtime API routines, such as cudaMalloc() , cudaFree() , cudaMemcpy() , and cudaMemcpyAsync() .
By now, cuSolverMg supports 1-D column block cyclic layout and provides symmetric eigenvalue solver.
Note The cuSolver library requires hardware with a CUDA Compute Capability (CC) of 5.0 or higher.
Please see the CUDA C++ Programming Guide for a list of the Compute Capabilities corresponding to all NVIDIA GPUs. 1.1. cuSolverDN: Dense LAPACK  The cuSolverDN library was designed to solve dense linear systems of the form \(Ax = b\) where the coefficient matrix \(A\in R^{nxn}\) , right-hand-side vector \(b\in R^{n}\) and solution vector \(x\in R^{n}\) The cuSolverDN library provides QR factorization and LU with partial pivoting to handle a general matrix A , which may be non-symmetric.
The cuSolverDN library also provides a helpful bidiagonalization routine and singular value decomposition (SVD).
The cuSolverDN library targets computationally-intensive and popular routines in LAPACK, and provides an API compatible with LAPACK.
The user can accelerate these time-consuming routines with cuSolverDN and keep others in LAPACK without a major change to existing code. 1.2. cuSolverSP: Sparse LAPACK  The cuSolverSP library was mainly designed to a solve sparse linear system \(Ax = b\) and the least-squares problem \(x = {argmin}{||}A*z - b{||}\) where sparse matrix \(A\in R^{mxn}\) , right-hand-side vector \(b\in R^{m}\) and solution vector \(x\in R^{n}\) .
If matrix A is symmetric/Hermitian, the user has to provide a full matrix, ie fill missing lower or upper part.
If matrix A is symmetric positive definite and the user only needs to solve \(Ax = b\) , Cholesky factorization can work and the user only needs to provide the lower triangular part of A .
On top of the linear and least-squares solvers, the cuSolverSP library provides a simple eigenvalue solver based on shift-inverse power method, and a function to count the number of eigenvalues contained in a box in the complex plane. 1.3. cuSolverRF: Refactorization  The cuSolverRF library was designed to accelerate solution of sets of linear systems by fast re-factorization when given new coefficients in the same sparsity pattern \(A_{i}x_{i} = f_{i}\) where a sequence of coefficient matrices \(A_{i}\in R^{nxn}\) , right-hand-sides \(f_{i}\in R^{n}\) and solutions \(x_{i}\in R^{n}\) are given for i=1,...,k .
The cuSolverRF library is applicable when the sparsity pattern of the coefficient matrices \(A_{i}\) as well as the reordering to minimize fill-in and the pivoting used during the LU factorization remain the same across these linear systems.
In that case, the first linear system ( i=1 ) requires a full LU factorization, while the subsequent linear systems ( i=2,...,k ) require only the LU re-factorization.
Notice that because the sparsity pattern of the coefficient matrices, the reordering and pivoting remain the same, the sparsity pattern of the resulting triangular factors \(L_{i}\) and \(U_{i}\) also remains the same.
Therefore, the real difference between the full LU factorization and LU re-factorization is that the required memory is known ahead of time. 1.4. Naming Conventions  The cuSolverDN library provides two different APIs; legacy and generic .
The functions in the legacy API are available for data types float , double , cuComplex , and cuDoubleComplex .
The naming convention for the legacy API is as follows: cusolverDn where can be S , D , C , Z , or X , corresponding to the data types float , double , cuComplex , cuDoubleComplex , and the generic type, respectively.
can be Cholesky factorization ( potrf ), LU with partial pivoting ( getrf ), QR factorization ( geqrf ) and Bunch-Kaufman factorization ( sytrf ).
The functions in the generic API provide a single entry point for each routine and support for 64-bit integers to define matrix and vector dimensions.
The naming convention for the generic API is data-agnostic and is as follows: cusolverDn where can be Cholesky factorization ( potrf ), LU with partial pivoting ( getrf ) and QR factorization ( geqrf ).
The cuSolverSP library functions are available for data types float , double , cuComplex , and cuDoubleComplex .
The naming convention is as follows: cusolverSp[Host] [][] where cuSolverSp is the GPU path and cusolverSpHost is the corresponding CPU path.
can be S , D , C , Z , or X , corresponding to the data types float , double , cuComplex , cuDoubleComplex , and the generic type, respectively.
The can be ls , lsq , eig , eigs , corresponding to linear solver, least-square solver, eigenvalue solver and number of eigenvalues in a box, respectively.
For example, qr (sparse QR factorization) is used in linear solver and least-square solver.
All of the functions have the return type cusolverStatus_t and are explained in more detail in the chapters that follow.
cuSolverSP API  Routine Data format Operation Output format Based on csrlsvlu csr linear solver (ls) vector (v) LU (lu) with partial pivoting csrlsvqr csr linear solver (ls) vector (v) QR factorization (qr) csrlsvchol csr linear solver (ls) vector (v) Cholesky factorization (chol) csrlsqvqr csr least-square solver (lsq) vector (v) QR factorization (qr) csreigvsi csr eigenvalue solver (eig) vector (v) shift-inverse csreigs csr number of eigenvalues in a box (eigs) csrsymrcm csr Symmetric Reverse Cuthill-McKee (symrcm) The cuSolverRF library routines are available for data type double .
Most of the routines follow the naming convention: cusolverRf __[ [Host] ](…) where the trailing optional Host qualifier indicates the data is accessed on the host versus on the device, which is the default.
The can be Setup , Analyze , Refactor , Solve , ResetValues , AccessBundledFactors and ExtractSplitFactors .
Finally, the return type of the cuSolverRF library routines is cusolverStatus_t . 1.5. Asynchronous Execution  The cuSolver library functions prefer to keep asynchronous execution as much as possible.
Developers can always use the cudaDeviceSynchronize() function to ensure that the execution of a particular cuSolver library routine has completed.
A developer can also use the cudaMemcpy() routine to copy data from the device to the host and vice versa, using the cudaMemcpyDeviceToHost and cudaMemcpyHostToDevice parameters, respectively.
In this case there is no need to add a call to cudaDeviceSynchronize() because the call to cudaMemcpy() with the above parameters is blocking and completes only when the results are ready on the host. 1.6. Library Property  The libraryPropertyType data type is an enumeration of library property types.
CUDA version X.Y.Z would yield MAJOR_VERSION=X , MINOR_VERSION=Y , PATCH_LEVEL=Z ) typedef enum libraryPropertyType_t { MAJOR_VERSION , MINOR_VERSION , PATCH_LEVEL } libraryPropertyType ; The following code can show the version of cusolver library.
int major = -1 , minor = -1 , patch = -1 ; cusolverGetProperty ( MAJOR_VERSION , & major ); cusolverGetProperty ( MINOR_VERSION , & minor ); cusolverGetProperty ( PATCH_LEVEL , & patch ); printf ( "CUSOLVER Version (Major,Minor,PatchLevel): %d.%d.%d   " , major , minor , patch ); 1.7.
High Precision Package  The cusolver library uses high precision for iterative refinement when necessary. 2. Using the CUSOLVER API  2.1.
It is not a reference for the cuSolver API data types and functions; that is provided in subsequent chapters. 2.1.1. Thread Safety  The library is thread-safe, and its functions can be called from multiple host threads.
2.1.2. Scalar Parameters  In the cuSolver API, the scalar parameters can be passed by reference on the host.
2.1.3. Parallelism with Streams  If the application performs several small independent computations, or if it makes data transfers in parallel with the computation, then CUDA streams can be used to overlap these tasks.
To achieve the overlap of computation between the tasks, the developer should: Create CUDA streams using the function cudaStreamCreate() , and Set the stream to be used by each individual cuSolver library routine by calling, for example, cusolverDnSetStream() , just prior to calling the actual cuSolverDN routine.
The computations performed in separate streams would then be overlapped automatically on the GPU, when possible.
This approach is especially useful when the computation performed by a single task is relatively small, and is not enough to fill the GPU with work, or when there is a data transfer that can be performed in parallel with the computation. 2.1.4. How to Link cusolver Library  cusolver library provides dynamic library libcusolver.so and static library libcusolver_static.a .
If the user links the application with libcusolver.so , libcublas.so , libcublasLt.so and libcusparse.so are also required.
If the user links the application with libcusolver_static.a , the following libraries are also needed, libcudart_static.a , libculibos.a , libcusolver_lapack_static.a , libcusolver_metis_static.a , libcublas_static.a and libcusparse_static.a . 2.1.5. Link Third-party LAPACK Library  Starting with CUDA 10.1 update 2, NVIDIA LAPACK library libcusolver_lapack_static.a is a subset of LAPACK and only contains GPU accelerated stedc and bdsqr .
The user has to link libcusolver_static.a with libcusolver_lapack_static.a in order to build the application successfully.
Prior to CUDA 10.1 update 2, the user can replace libcusolver_lapack_static.a with a third-party LAPACK library, for example, MKL.
In CUDA 10.1 update 2, the third-party LAPACK library no longer affects the behavior of cusolver library, neither functionality nor performance.
Furthermore the user cannot use libcusolver_lapack_static.a as a standalone LAPACK library because it is only a subset of LAPACK.
If you use libcusolver_static.a , then you must link with libcusolver_lapack_static.a explicitly, otherwise the linker will report missing symbols.
There are no symbol conflicts between libcusolver_lapack_static.a and other third-party LAPACK libraries, which allows linking the same application to libcusolver_lapack_static.a and another third-party LAPACK library.
The libcusolver.so will not pick up any routines from the third-party LAPACK library even if you link the application with it. 2.1.6. Convention of info  Each LAPACK routine returns an info which indicates the position of invalid parameter.
To be consistent with base-1 in LAPACK, cusolver does not report invalid handle into info .
Instead, cusolver returns CUSOLVER_STATUS_NOT_INITIALIZED for invalid handle . 2.1.7. Usage of _bufferSize  There is no cudaMalloc inside cuSolver library, the user must allocate the device workspace explicitly.
The routine xyz_bufferSize is to query the size of workspace of the routine xyz , for example xyz = potrf .
To make the API simple, xyz_bufferSize follows almost the same signature of xyz even it only depends on some parameters, for example, device pointer is not used to decide the size of workspace.
In most cases, xyz_bufferSize is called in the beginning before actual device data (pointing by a device pointer) is prepared or before the device pointer is allocated.
See: cusolverDnLoggerSetCallback() , cusolverDnLoggerSetFile() , cusolverDnLoggerOpenFile() , cusolverDnLoggerSetLevel() , cusolverDnLoggerSetMask() , cusolverDnLoggerForceDisable() . 2.1.9. Deterministic Results  Throughout this documentation, a function is declared as deterministic if it computes the exact same bitwise results for every execution with the same input parameters, hard- and software environment.
Conversely, a non-deterministic function might compute bitwise different results due to a varying order of floating point operations, e.g., a sum s of four values a , b , c , d can be computed in different orders: s = (a + b) + (c + d) s = (a + (b + c)) + d s = a + (b + (c + d)) … Due to the non-associativity of floating point arithmetic, all results might be bitwise different.
For improved performance of some functions, it is possible to allow non-deterministic results with cusolverDnSetDeterministicMode() . 2.2. cuSolver Types Reference  2.2.1.
cuSolverDN Types  The float , double , cuComplex , and cuDoubleComplex data types are supported.
The first two are standard C data types, while the last two are exported from cuComplex.h .
In addition, cuSolverDN uses some familiar types from cuBLAS. 2.2.1.1. cusolverDnHandle_t  This is a pointer type to an opaque cuSolverDN context, which the user must initialize by calling cusolverDnCreate() prior to calling any other library function.
An un-initialized Handle object will lead to unexpected behavior, including crashes of cuSolverDN.
The handle created and returned by cusolverDnCreate() must be passed to every cuSolverDN function. 2.2.1.2. cublasFillMode_t  The type indicates which part (lower or upper) of the dense matrix was filled and consequently should be used by the function.
Notice that BLAS implementations often use Fortran characters ‘L’ or ‘l’ (lower) and ‘U’ or ‘u’ (upper) to describe which part of the matrix is filled. 2.2.1.3. cublasOperation_t  The cublasOperation_t type indicates which operation needs to be performed with the dense matrix.
Notice that BLAS implementations often use Fortran characters ‘N’ or ‘n’ (non-transpose), ‘T’ or ‘t’ (transpose) and ‘C’ or ‘c’ (conjugate transpose) to describe which operations needs to be performed with the dense matrix. 2.2.1.4. cusolverEigType_t  The cusolverEigType_t type indicates which type of eigenvalue the solver is.
Value Meaning CUSOLVER_EIG_TYPE_1 A*x = lambda*B*x CUSOLVER_EIG_TYPE_2 A*B*x = lambda*x CUSOLVER_EIG_TYPE_3 B*A*x = lambda*x Notice that LAPACK implementations often use Fortran integer 1 (A*x = lambda*B*x), 2 (A*B*x = lambda*x), 3 (B*A*x = lambda*x) to indicate which type of eigenvalue the solver is. 2.2.1.5. cusolverEigMode_t  The cusolverEigMode_t type indicates whether or not eigenvectors are computed.
Notice that LAPACK implementations often use Fortran character 'N' (only eigenvalues are computed), 'V' (both eigenvalues and eigenvectors are computed) to indicate whether or not eigenvectors are computed. 2.2.1.6. cusolverIRSRefinement_t  The cusolverIRSRefinement_t type indicates which solver type would be used for the specific cusolver function.
More details about the refinement process can be found in Azzam Haidar, Stanimire Tomov, Jack Dongarra, and Nicholas J.
Higham. 2018. Harnessing GPU tensor cores for fast FP16 arithmetic to speed up mixed-precision iterative refinement solvers.
In Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis (SC ‘18).
Value Meaning CUSOLVER_IRS_REFINE_NOT_SET Solver is not set; this value is what is set when creating the params structure.
CUSOLVER_IRS_REFINE_NONE No refinement solver, the IRS solver performs a factorization followed by a solve without any refinement.
For example if the IRS solver was cusolverDnIRSXgesv() , this is equivalent to a Xgesv routine without refinement and where the factorization is carried out in the lowest precision.
If for example the main precision was CUSOLVER_R_64F and the lowest was CUSOLVER_R_64F as well, then this is equivalent to a call to cusolverDnDgesv() .
CUSOLVER_IRS_REFINE_GMRES GMRES (Generalized Minimal Residual) based iterative refinement solver.
In recent study, the GMRES method has drawn the scientific community attention for its ability to be used as refinement solver that outperforms the classical iterative refinement method.
CUSOLVER_IRS_REFINE_CLASSICAL_GMRES Classical iterative refinement solver that uses the GMRES (Generalized Minimal Residual) internally to solve the correction equation at each iteration.
We call the classical refinement iteration the outer iteration while the GMRES is called inner iteration.
Note that if the tolerance of the inner GMRES is set very low, lets say to machine precision, then the outer classical refinement iteration will performs only one iteration and thus this option will behave like CUSOLVER_IRS_REFINE_GMRES .
CUSOLVER_IRS_REFINE_GMRES_GMRES Similar to CUSOLVER_IRS_REFINE_CLASSICAL_GMRES which consists of classical refinement process that uses GMRES to solve the inner correction system; here it is a GMRES (Generalized Minimal Residual) based iterative refinement solver that uses another GMRES internally to solve the preconditioned system. 2.2.1.7. cusolverDnIRSParams_t  This is a pointer type to an opaque cusolverDnIRSParams_t structure, which holds parameters for the iterative refinement linear solvers such as cusolverDnXgesv() .
Use corresponding helper functions described below to either Create/Destroy this structure or Set/Get solver parameters. 2.2.1.8. cusolverDnIRSInfos_t  This is a pointer type to an opaque cusolverDnIRSInfos_t structure, which holds information about the performed call to an iterative refinement linear solver (e.g., cusolverDnXgesv() ).
Use corresponding helper functions described below to either Create/Destroy this structure or retrieve solve information. 2.2.1.9. cusolverDnFunction_t  The cusolverDnFunction_t type indicates which routine needs to be configured by cusolverDnSetAdvOptions() .
Value Meaning CUSOLVERDN_GETRF Corresponds to Getrf . 2.2.1.10. cusolverAlgMode_t  The cusolverAlgMode_t type indicates which algorithm is selected by cusolverDnSetAdvOptions() .
The set of algorithms supported for each routine is described in detail along with the routine’s documentation.
The user can also provide NULL to use the default algorithm. 2.2.1.11. cusolverStatus_t  This is the same as cusolverStatus_t in the sparse LAPACK section.
2.2.1.12. cusolverDnLoggerCallback_t  cusolverDnLoggerCallback_t is a callback function pointer type.
Parameters Parameter Memory In/out Description logLevel output See cuSOLVERDn Logging functionName output The name of the API that logged this message.
Use the below function to set the callback function: cusolverDnLoggerSetCallback() . 2.2.1.13. cusolverDeterministicMode_t  The cusolverDeterministicMode_t type indicates whether multiple cuSolver function executions with the same input have the same bitwise equal result (deterministic) or might have bitwise different results (non-deterministic).
In comparison to cublasAtomicsMode_t , which only includes the usage of atomic functions, cusolverDeterministicMode_t includes all non-deterministic programming patterns.
The deterministic mode can be set and queried using cusolverDnSetDeterministicMode() and cusolverDnGetDeterministicMode() routines, respectively.
CUSOLVER_ALLOW_NON_DETERMINISTIC_RESULTS Allow non-deterministic results. 2.2.1.14. cusolverStorevMode_t  Specifies how the vectors which define the elementary reflectors are stored.
CUBLAS_STOREV_ROWWISE Rowwise. 2.2.1.15. cusolverDirectMode_t  Specifies the order in which the elementary reflectors are multiplied to form the block reflector.
CUBLAS_DIRECT_BACKWARD Backward. 2.2.2. cuSolverSP Types  The float , double , cuComplex , and cuDoubleComplex data types are supported.
2.2.2.1. cusolverSpHandle_t  This is a pointer type to an opaque cuSolverSP context, which the user must initialize by calling cusolverSpCreate() prior to calling any other library function.
An un-initialized Handle object will lead to unexpected behavior, including crashes of cuSolverSP.
The handle created and returned by cusolverSpCreate() must be passed to every cuSolverSP function. 2.2.2.2. cusparseMatDescr_t  We have chosen to keep the same structure as exists in cuSPARSE to describe the shape and properties of a matrix.
typedef struct { cusparseMatrixType_t MatrixType ; cusparseFillMode_t FillMode ; cusparseDiagType_t DiagType ; cusparseIndexBase_t IndexBase ; } cusparseMatDescr_t ; Please read documentation of the cuSPARSE Library to understand each field of cusparseMatDescr_t . 2.2.2.3. cusolverStatus_t  This is a status type returned by the library functions and it can have the following values.
This is usually caused by the lack of a prior call, an error in the CUDA Runtime API called by the cuSolver routine, or an error in the hardware setup.
To correct: call cusolverDnCreate() prior to the function call; and check that the hardware, an appropriate version of the driver, and the cuSolver library are correctly installed.
To correct: prior to the function call, deallocate previously allocated memory as much as possible.
CUSOLVER_STATUS_INVALID_VALUE An unsupported value or parameter was passed to the function (a negative vector size, for example).
CUSOLVER_STATUS_ARCH_MISMATCH The function requires a feature absent from the device architecture; usually caused by the lack of support for atomic operations or double precision.
To correct: compile and run the application on a device with compute capability 5.0 or above.
This is often caused by a launch failure of the kernel on the GPU, which can be caused by multiple reasons.
To correct: check that the hardware, an appropriate version of the driver, and the cuSolver library are correctly installed.
Also, check that the memory passed as a parameter to the routine is not being deallocated prior to the routine’s completion.
CUSOLVER_STATUS_MATRIX_TYPE_NOT_SUPPORTED The matrix type is not supported by this function.
batched version is not supported or M geqrf() , cusolverDnsyevd() , cusolverDnsyevdx() , cusolverDngesvd() (if m > n ), cusolverDngesvdj() , cusolverDnXgeqrf() , cusolverDnXsyevd() , cusolverDnXsyevdx() , cusolverDnXgesvd() (if m > n ), cusolverDnXgesvdr() and cusolverDnXgesvdp() .
Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
CUSOLVER_STATUS_INTERNAL_ERROR An internal error occurred. 2.4.1.12. cusolverDnGetDeterministicMode()  cusolverStatus_t cusolverDnGetDeterministicMode ( cusolverDnHandle_t handle , cusolverDeterministicMode_t * mode ) This function queries the deterministic mode which is set for handle .
CUSOLVER_STATUS_INVALID_VALUE mode is a NULL pointer. 2.4.1.13. cusolverDnCreateSyevjInfo()  cusolverStatus_t cusolverDnCreateSyevjInfo ( syevjInfo_t * info ); This function creates and initializes the structure of syevj , syevjBatched and sygvj to default values.
CUSOLVER_STATUS_ALLOC_FAILED The resources could not be allocated. 2.4.1.14. cusolverDnDestroySyevjInfo()  cusolverStatus_t cusolverDnDestroySyevjInfo ( syevjInfo_t info ); This function destroys and releases any memory required by the structure.
Status Returned CUSOLVER_STATUS_SUCCESS The resources were released successfully. 2.4.1.15. cusolverDnXsyevjSetTolerance()  cusolverStatus_t cusolverDnXsyevjSetTolerance ( syevjInfo_t info , double tolerance ) This function configures tolerance of syevj .
Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully. 2.4.1.16. cusolverDnXsyevjSetMaxSweeps()  cusolverStatus_t cusolverDnXsyevjSetMaxSweeps ( syevjInfo_t info , int max_sweeps ) This function configures maximum number of sweeps in syevj .
max_sweeps host input Maximum number of sweeps. 2.4.1.17. cusolverDnXsyevjSetSortEig()  cusolverStatus_t cusolverDnXsyevjSetSortEig ( syevjInfo_t info , int sort_eig ) If sort_eig is zero, the eigenvalues are not sorted.
sort_eig host input If sort_eig is zero, the eigenvalues are not sorted. 2.4.1.18. cusolverDnXsyevjGetResidual()  cusolverStatus_t cusolverDnXsyevjGetResidual ( cusolverDnHandle_t handle , syevjInfo_t info , double * residual ) This function reports residual of syevj or sygvj .
If the user calls this function after syevjBatched , the error CUSOLVER_STATUS_NOT_SUPPORTED is returned.
CUSOLVER_STATUS_NOT_SUPPORTED Does not support batched version. 2.4.1.19. cusolverDnXsyevjGetSweeps()  cusolverStatus_t cusolverDnXsyevjGetSweeps ( cusolverDnHandle_t handle , syevjInfo_t info , int * executed_sweeps ) This function reports number of executed sweeps of syevj or sygvj .
executed_sweeps host output Number of executed sweeps. 2.4.1.20. cusolverDnCreateGesvdjInfo()  cusolverStatus_t cusolverDnCreateGesvdjInfo ( gesvdjInfo_t * info ); This function creates and initializes the structure of gesvdj and gesvdjBatched to default values.
Parameter Memory In/out Meaning info host output The pointer to the structure of gesvdj . 2.4.1.21. cusolverDnDestroyGesvdjInfo()  cusolverStatus_t cusolverDnDestroyGesvdjInfo ( gesvdjInfo_t info ); This function destroys and releases any memory required by the structure.
Parameter Memory In/out Meaning info host input The structure of gesvdj . 2.4.1.22. cusolverDnXgesvdjSetTolerance()  cusolverStatus_t cusolverDnXgesvdjSetTolerance ( gesvdjInfo_t info , double tolerance ) This function configures tolerance of gesvdj .
tolerance host input Accuracy of numerical singular values. 2.4.1.23. cusolverDnXgesvdjSetMaxSweeps()  cusolverStatus_t cusolverDnXgesvdjSetMaxSweeps ( gesvdjInfo_t info , int max_sweeps ) This function configures the maximum number of sweeps in gesvdj .
2.4.1.24. cusolverDnXgesvdjSetSortEig()  cusolverStatus_t cusolverDnXgesvdjSetSortEig ( gesvdjInfo_t info , int sort_svd ) If sort_svd is zero, the singular values are not sorted.
sort_svd host input If sort_svd is zero, the singular values are not sorted. 2.4.1.25. cusolverDnXgesvdjGetResidual()  cusolverStatus_t cusolverDnXgesvdjGetResidual ( cusolverDnHandle_t handle , gesvdjInfo_t info , double * residual ) This function reports residual of gesvdj .
If the user calls this function after gesvdjBatched , the error CUSOLVER_STATUS_NOT_SUPPORTED is returned.
cusolverDnXgesvdjGetSweeps()  cusolverStatus_t cusolverDnXgesvdjGetSweeps ( cusolverDnHandle_t handle , gesvdjInfo_t info , int * executed_sweeps ) This function reports number of executed sweeps of gesvdj .
cusolverDnIRSParamsCreate()  cusolverStatus_t cusolverDnIRSParamsCreate ( cusolverDnIRSParams_t * params ); This function creates and initializes the structure of parameters for an IRS solver such as the cusolverDnIRSXgesv() or the cusolverDnIRSXgels() functions to default values.
The params structure created by this function can be used by one or more call to the same or to a different IRS solver.
Note that in CUDA 10.2, the behavior was different and a new params structure was needed to be created per each call to an IRS solver.
Also note that the user can also change configurations of the params and then call a new IRS instance, but be careful that the previous call was done because any change to the configuration before the previous call was done could affect it.
Parameter Memory In/out Meaning params host output Pointer to the cusolverDnIRSParams_t Params structure Status Returned CUSOLVER_STATUS_SUCCESS The structure was created and initialized successfully. 2.4.1.28. cusolverDnIRSParamsDestroy()  cusolverStatus_t cusolverDnIRSParamsDestroy ( cusolverDnIRSParams_t params ); This function destroys and releases any memory required by the Params structure.
Parameter Memory In/out Meaning params host input The cusolverDnIRSParams_t Params structure.
CUSOLVER_STATUS_IRS_INFOS_NOT_DESTROYED Not all the Infos structure associated with this Params structure have been destroyed yet. 2.4.1.29. cusolverDnIRSParamsSetSolverPrecisions()  cusolverStatus_t cusolverDnIRSParamsSetSolverPrecisions ( cusolverDnIRSParams_t params , cusolverPrecType_t solver_main_precision , cusolverPrecType_t solver_lowest_precision ); This function sets both the main and the lowest precision for the Iterative Refinement Solver (IRS).
By lowest precision, we mean the solver is allowed to use as lowest computational precision during the LU factorization process.
Note that the user has to set both the main and lowest precision before the first call to the IRS solver because they are NOT set by default with the params structure creation, as it depends on the Input Output data type and user request.
It is a wrapper to both cusolverDnIRSParamsSetSolverMainPrecision() and cusolverDnIRSParamsSetSolverLowestPrecision() .
The ratio of the performance of the lowest precision over the main precision (e.g., Inputs/Outputs datatype) define the upper bound of the speedup that could be obtained.
More precisely, it depends on many factors, but for large matrices sizes, it is the ratio of the matrix-matrix rank-k product (e.g., GEMM where K is 256 and M=N=size of the matrix) that define the possible speedup.
For instance, if the inout precision is real double precision CUSOLVER_R_64F and the lowest precision is CUSOLVER_R_32F, then we can expect a speedup of at most 2X for large problem sizes.
A reasonable strategy should take the number of right-hand sides, the size of the matrix as well as the convergence rate into account.
Parameter Memory In/out Meaning params host in/out The cusolverDnIRSParams_t Params structure.
solver_main_precision host input Allowed Inputs/Outputs datatype (for example CUSOLVER_R_FP64 for a real double precision data).
solver_lowest_precision host input Allowed lowest compute type (for example CUSOLVER_R_16F for half precision computation).
Supported Inputs/Outputs data type and lower precision for the IRS solver  Inputs/Outputs Data Type (e.g., main precision) Supported values for the lowest precision CUSOLVER_C_64F CUSOLVER_C_64F, CUSOLVER_C_32F, CUSOLVER_C_16F, CUSOLVER_C_16BF, CUSOLVER_C_TF32 CUSOLVER_C_32F CUSOLVER_C_32F, CUSOLVER_C_16F, CUSOLVER_C_16BF, CUSOLVER_C_TF32 CUSOLVER_R_64F CUSOLVER_R_64F, CUSOLVER_R_32F, CUSOLVER_R_16F, CUSOLVER_R_16BF, CUSOLVER_R_TF32 CUSOLVER_R_32F CUSOLVER_R_32F, CUSOLVER_R_16F, CUSOLVER_R_16BF, CUSOLVER_R_TF32 2.4.1.30.
cusolverDnIRSParamsSetSolverMainPrecision()  cusolverStatus_t cusolverDnIRSParamsSetSolverMainPrecision ( cusolverDnIRSParams_t params , cusolverPrecType_t solver_main_precision ); This function sets the main precision for the Iterative Refinement Solver (IRS).
Note that the user has to set both the main and lowest precision before a first call to the IRS solver because they are NOT set by default with the params structure creation, as it depends on the Input Output data type and user request.
user can set it by either calling this function or by calling cusolverDnIRSParamsSetSolverPrecisions() which set both the main and the lowest precision together.
All possible combinations of main/lowest precision are described in the table in the cusolverDnIRSParamsSetSolverPrecisions() section above.
See the table in the cusolverDnIRSParamsSetSolverPrecisions() section above for the supported precisions. 2.4.1.31. cusolverDnIRSParamsSetSolverLowestPrecision()  cusolverStatus_t cusolverDnIRSParamsSetSolverLowestPrecision ( cusolverDnIRSParams_t params , cusolverPrecType_t lowest_precision_type ); This function sets the lowest precision that will be used by Iterative Refinement Solver.
The ratio of the performance of the lowest precision over the main precision (e.g., Inputs/Outputs datatype) define somehow the upper bound of the speedup that could be obtained.
lowest_precision_type host input Allowed lowest compute type (for example CUSOLVER_R_16F for half precision computation). 2.4.1.32. cusolverDnIRSParamsSetRefinementSolver()  cusolverStatus_t cusolverDnIRSParamsSetRefinementSolver ( cusolverDnIRSParams_t params , cusolverIRSRefinement_t solver ); This function sets the refinement solver to be used in the Iterative Refinement Solver functions such as the cusolverDnIRSXgesv() or the cusolverDnIRSXgels() functions.
Note that the user has to set the refinement algorithm before a first call to the IRS solver because it is NOT set by default with the creating of params.
Details about values that can be set to and theirs meaning are described in the table below.
Parameter Memory In/out Meaning params host in/out The cusolverDnIRSParams_t Params structure solver host input Type of the refinement solver to be used by the IRS solver such as cusolverDnIRSXgesv() or cusolverDnIRSXgels() .
CUSOLVER_IRS_REFINE_NOT_SET Solver is not set, this value is what is set when creating the params structure.
CUSOLVER_IRS_REFINE_NONE No refinement solver; the IRS solver performs a factorization followed by a solve without any refinement.
For example, if the IRS solver was cusolverDnIRSXgesv() , this is equivalent to a Xgesv routine without refinement and where the factorization is carried out in the lowest precision.
Note that if the tolerance of the inner GMRES is set very low, let say to machine precision, then the outer classical refinement iteration will performs only one iteration and thus this option will behaves like CUSOLVER_IRS_REFINE_GMRES.
CUSOLVER_IRS_REFINE_GMRES_GMRES Similar to CUSOLVER_IRS_REFINE_CLASSICAL_GMRES which consists of classical refinement process that uses GMRES to solve the inner correction system, here it is a GMRES (Generalized Minimal Residual) based iterative refinement solver that uses another GMRES internally to solve the preconditioned system. 2.4.1.33. cusolverDnIRSParamsSetTol()  cusolverStatus_t cusolverDnIRSParamsSetTol ( cusolverDnIRSParams_t params , double val ); This function sets the tolerance for the refinement solver.
By default it is such that all the RHS satisfy: RNRM LAMCH(‘Epsilon’) BWDMAX, the value BWDMAX is fixed to 1.0 The user can use this function to change the tolerance to a lower or higher value.
Our goal is to give the user more control such a way he can investigate and control every detail of the IRS solver.
Note that the tolerance value is always in real double precision whatever the Inputs/Outputs datatype is.
val host input Double precision real value to which the refinement tolerance will be set. 2.4.1.34. cusolverDnIRSParamsSetTolInner()  cusolverStatus_t cusolverDnIRSParamsSetTolInner ( cusolverDnIRSParams_t params , double val ); This function sets the tolerance for the inner refinement solver when the refinement solver consists of two-levels solver (e.g., CUSOLVER_IRS_REFINE_CLASSICAL_GMRES or CUSOLVER_IRS_REFINE_GMRES_GMRES cases).
It is not referenced in case of one level refinement solver such as CUSOLVER_IRS_REFINE_CLASSICAL or CUSOLVER_IRS_REFINE_GMRES.
For example, if the Refinement Solver was set to CUSOLVER_IRS_REFINE_CLASSICAL_GMRES, setting this tolerance mean that the inner GMRES solver will converge to that tolerance at each outer iteration of the classical refinement solver.
Note the, the tolerance value is always in real double precision whatever the Inputs/Outputs datatype is.
val host input Double precision real value to which the tolerance of the inner refinement solver will be set. 2.4.1.35. cusolverDnIRSParamsSetMaxIters()  cusolverStatus_t cusolverDnIRSParamsSetMaxIters ( cusolverDnIRSParams_t params , int max_iters ); This function sets the total number of allowed refinement iterations after which the solver will stop.
Total means any iteration which means the sum of the outer and the inner iterations (inner is meaningful when two-levels refinement solver is set).
max_iters host input Maximum total number of iterations allowed for the refinement solver. 2.4.1.36. cusolverDnIRSParamsSetMaxItersInner()  cusolverStatus_t cusolverDnIRSParamsSetMaxItersInner ( cusolverDnIRSParams_t params , cusolver_int_t maxiters_inner ); This function sets the maximal number of iterations allowed for the inner refinement solver.
The inner refinement solver will stop after reaching either the inner tolerance or the MaxItersInner value.
Note that this value could not be larger than the MaxIters since MaxIters is the total number of allowed iterations.
Note that if the user calls cusolverDnIRSParamsSetMaxIters after calling this function, SetMaxIters has priority and will overwrite MaxItersInner to the minimum value of (MaxIters, MaxItersInner) .
Parameter Memory In/out Meaning params host in/out The cusolverDnIRSParams_t Params structure maxiters_inner host input Maximum number of allowed inner iterations for the inner refinement solver.
Meaningful when the refinement solver is a two-levels solver such as CUSOLVER_IRS_REFINE_CLASSICAL_GMRES or CUSOLVER_IRS_REFINE_GMRES_GMRES.
CUSOLVER_STATUS_IRS_PARAMS_INVALID If the value was larger than MaxIters . 2.4.1.37. cusolverDnIRSParamsEnableFallback()  cusolverStatus_t cusolverDnIRSParamsEnableFallback ( cusolverDnIRSParams_t params ); This function enable the fallback to the main precision in case the Iterative Refinement Solver (IRS) failed to converge.
In other term, if the IRS solver failed to converge, the solver will return a no convergence code (e.g., niter potrf()  These helper functions calculate the necessary size of work buffers.
cusolverStatus_t cusolverDnSpotrf_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , float * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnDpotrf_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , double * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnCpotrf_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuComplex * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnZpotrf_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , int * Lwork ); The S and D data types are real valued single and double precision, respectively.
cusolverStatus_t cusolverDnSpotrf ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , float * A , int lda , float * Workspace , int Lwork , int * devInfo ); cusolverStatus_t cusolverDnDpotrf ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , double * A , int lda , double * Workspace , int Lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnCpotrf ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuComplex * A , int lda , cuComplex * Workspace , int Lwork , int * devInfo ); cusolverStatus_t cusolverDnZpotrf ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , cuDoubleComplex * Workspace , int Lwork , int * devInfo ); This function computes the Cholesky factorization of a Hermitian positive-definite matrix.
If input parameter uplo is CUBLAS_FILL_MODE_LOWER , only the lower triangular part of A is processed, and replaced by the lower triangular Cholesky factor L .
\(A = L*L^{H}\) If input parameter uplo is CUBLAS_FILL_MODE_UPPER , only upper triangular part of A is processed, and replaced by upper triangular Cholesky factor U .
\(A = U^{H}*U\) The user has to provide working space which is pointed by input parameter Workspace .
The input parameter Lwork is size of the working space, and it is returned by potrf_bufferSize() .
some leading minor of A is not positive definite, or equivalently some diagonal elements of L or U is not a real number.
The output parameter devInfo would indicate smallest leading minor of A which is not positive definite.
If output parameter devInfo = -i (less than zero), the i-th parameter is wrong (not counting handle).
API of potrf Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
uplo host input Indicates if matrix A lower or upper part is stored; the other part is not referenced.
If input parameter uplo is CUBLAS_FILL_MODE_LOWER , A is lower triangular Cholesky factor L corresponding to \(A = L*L^{H}\) .
If input parameter uplo is CUBLAS_FILL_MODE_UPPER , A is upper triangular Cholesky factor U corresponding to \(A = U^{H}*U\) .
API of potrs Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
uplo host input Indicates if matrix A lower or upper part is stored, the other part is not referenced.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( npotri()  These helper functions calculate the necessary size of work buffers.
cusolverStatus_t cusolverDnSpotri_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , float * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnDpotri_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , double * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnCpotri_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuComplex * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnZpotri_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , int * Lwork ); The S and D data types are real valued single and double precision, respectively.
cusolverStatus_t cusolverDnSpotri ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , float * A , int lda , float * Workspace , int Lwork , int * devInfo ); cusolverStatus_t cusolverDnDpotri ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , double * A , int lda , double * Workspace , int Lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnCpotri ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuComplex * A , int lda , cuComplex * Workspace , int Lwork , int * devInfo ); cusolverStatus_t cusolverDnZpotri ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , cuDoubleComplex * Workspace , int Lwork , int * devInfo ); This function computes the inverse of a positive-definite matrix A using the Cholesky factorization \(A = L*L^{H} = U^{H}*U\) computed by potrf() .
A is a n×n matrix containing the triangular factor L or U computed by the Cholesky factorization.
Only lower or upper part is meaningful and the input parameter uplo indicates which part of the matrix is used.
If the input parameter uplo is CUBLAS_FILL_MODE_LOWER , only lower triangular part of A is processed, and replaced the by lower triangular part of the inverse of A .
If the input parameter uplo is CUBLAS_FILL_MODE_UPPER , only upper triangular part of A is processed, and replaced by the upper triangular part of the inverse of A .
The user has to provide the working space which is pointed to by input parameter Workspace .
The input parameter Lwork is the size of the working space, returned by potri_bufferSize() .
some leading minor of L or U , is null, the output parameter devInfo would indicate the smallest leading minor of L or U which is not positive definite.
If the output parameter devInfo = -i (less than zero), the i-th parameter is wrong (not counting the handle).
API of potri Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( ngetrf()  These helper functions calculate the size of work buffers needed.
cusolverStatus_t cusolverDnSgetrf_bufferSize ( cusolverDnHandle_t handle , int m , int n , float * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnDgetrf_bufferSize ( cusolverDnHandle_t handle , int m , int n , double * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnCgetrf_bufferSize ( cusolverDnHandle_t handle , int m , int n , cuComplex * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnZgetrf_bufferSize ( cusolverDnHandle_t handle , int m , int n , cuDoubleComplex * A , int lda , int * Lwork ); The S and D data types are real single and double precision, respectively.
cusolverStatus_t cusolverDnSgetrf ( cusolverDnHandle_t handle , int m , int n , float * A , int lda , float * Workspace , int * devIpiv , int * devInfo ); cusolverStatus_t cusolverDnDgetrf ( cusolverDnHandle_t handle , int m , int n , double * A , int lda , double * Workspace , int * devIpiv , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnCgetrf ( cusolverDnHandle_t handle , int m , int n , cuComplex * A , int lda , cuComplex * Workspace , int * devIpiv , int * devInfo ); cusolverStatus_t cusolverDnZgetrf ( cusolverDnHandle_t handle , int m , int n , cuDoubleComplex * A , int lda , cuDoubleComplex * Workspace , int * devIpiv , int * devInfo ); This function computes the LU factorization of a m×n matrix \(P*A = L*U\) where A is a m×n matrix, P is a permutation matrix, L is a lower triangular matrix with unit diagonal, and U is an upper triangular matrix.
The input parameter Lwork is size of the working space, and it is returned by getrf_bufferSize() .
No matter LU factorization failed or not, the output parameter devIpiv contains pivoting sequence, row i is interchanged with row devIpiv(i) .
The user can choose the legacy implementation with minimal workspace by Getrf and cusolverDnSetAdvOptions(params, CUSOLVERDN_GETRF, CUSOLVER_ALG_1) .
API of getrf Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,n array of dimension lda * n with lda is not less than max(1,m) .
workspaceInBytes host input Size in bytes of pBuffer , returned by cusolverDnGetrf_bufferSize .
The generic API has two different types, dataTypeA is data type of the matrix A , computeType is compute type of the operation.
valid combination of data type and compute type DataTypeA ComputeType Meaning CUDA_R_32F CUDA_R_32F SGETRF CUDA_R_64F CUDA_R_64F DGETRF CUDA_C_32F CUDA_C_32F CGETRF CUDA_C_64F CUDA_C_64F ZGETRF Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,ngetrs()  Please visit cuSOLVER Library Samples - getrf for a code example.
The input parameter trans is defined by \(\text{op}(A) = \left\{ \begin{matrix} A & {\text{if~}\textsf{trans\ ==\ CUBLAS\_OP\_N}} \\ A^{T} & {\text{if~}\textsf{trans\ ==\ CUBLAS\_OP\_T}} \\ A^{H} & {\text{if~}\textsf{trans\ ==\ CUBLAS\_OP\_C}} \\ \end{matrix}  ight.\) The input parameter devIpiv is an output of getrf .
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( n array of dimension ldb * nrhs with ldb is not less than max(1,n) .
The generic API has two different types, dataTypeA is data type of the matrix A and dataTypeB is data type of the matrix B .
Valid combination of data type and compute type DataTypeA dataTypeB Meaning CUDA_R_32F CUDA_R_32F SGETRS CUDA_R_64F CUDA_R_64F DGETRS CUDA_C_32F CUDA_C_32F CGETRS CUDA_C_64F CUDA_C_64F ZGETRS Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( ngesv()  These functions are modelled after functions DSGESV and ZCGESV from LAPACK.
They compute the solution of a system of linear equations with one or multiple right hand sides using mixed precision iterative refinement techniques based on the LU factorization Xgesv .
These functions are similar in term of functionalities to the full precision LU solver ( Xgesv , where X denotes Z,C,D,S) but it uses lower precision internally in order to provide faster time to solution, from here comes the name mixed precision.
Mixed precision iterative refinement techniques means that the solver compute an LU factorization in lower precision and then iteratively refine the solution to achieve the accuracy of the Inputs/Outputs datatype precision.
The corresponds to the Inputs/Outputs datatype precision while represent the internal lower precision at which the factorization will be carried on.
Functions API are designed to be as close as possible to LAPACK API to be considered as a quick and easy drop-in replacement.
gesv() functions are designated by two floating point precisions The corresponds to the main precision (e.g., Inputs/Outputs datatype precision) and the represent the internal lower precision at which the factorization will be carried on.
cusolvergesv() first attempts to factorize the matrix in lower precision and use this factorization within an iterative refinement procedure to obtain a solution with same normwise backward error as the main precision .
If the approach fails to converge, then the method fallback to the main precision factorization and solve (Xgesv) such a way that there is always a good solution at the output of these functions.
If is equal to , then it is not a mixed precision process but rather a full one precision factorization, solve and refinement within the same main precision.
The iterative refinement process is stopped if ITER > ITERMAX or for all the RHS we have: RNRM LAMCH(‘Epsilon’) The value ITERMAX and BWDMAX are fixed to 50 and 1.0 respectively.
A CUSOLVER_STATUS_SUCCESS indicates that the function finished with success otherwise, it indicates if one of the API arguments is incorrect, or if the function did not finish with success.
The amount of bytes required can be queried by calling the respective function gesv_bufferSize() .
Note that in addition to the two mixed precision functions available in LAPACK (e.g., dsgesv and zcgesv ), we provide a large set of mixed precision functions that include half, bfloat and tensorfloat as a lower precision as well as same precision functions (e.g., main and lowest precision are equal is equal to ).
Tensor Float (TF32), introduced with NVIDIA Ampere Architecture GPUs, is the most robust tensor core accelerated compute mode for the iterative refinement solver.
It is able to solve the widest range of problems in HPC arising from different applications and provides up to 4X and 5X speedup for real and complex systems, respectively.
On Volta and Turing architecture GPUs, half precision tensor core acceleration is recommended.
In cases where the iterative refinement solver fails to converge to the desired accuracy (main precision, INOUT data precision), it is recommended to use main precision as internal lowest precision (i.e., cusolverDn[DD,ZZ]gesv for the FP64 case).
lddb host input Leading dimension of two-dimensional array used to store matrix of right hand sides B .
lddx host input Leading dimension of two-dimensional array used to store matrix of solution vectors X .
lwork_bytes host output Pointer to a variable where required size of temporary workspace in bytes will be stored.
If not - will contains the factorization of the matrix A in the main precision ( A = P * L * U , where P - permutation matrix defined by vector ipiv, L and U - lower and upper triangular matrices).
dipiv device output Vector that defines permutation for the factorization - row i was interchanged with row ipiv[i] dB device input Set of right hand sides B of size n-by-nrhs .
dWorkspace device input Pointer to an allocated workspace in device memory of size lwork_bytes .
niters host output If iter is 0 : iter is a number of iterations solver performed to reach convergence criteria dinfo device output Status of the IRS solver on the return.
The factorization has been completed, but the factor U is exactly singular, so the solution could not be computed.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed, for example: ngesv() functions, but wrapped in a more generic and expert interface that gives user more control to parametrize the function as well as it provides more information on output.
cusolverDnIRSXgesv() allows additional control of the solver parameters such as setting: the main precision (Inputs/Outputs precision) of the solver the lowest precision to be used internally by the solver the refinement solver type the maximum allowed number of iterations in the refinement phase the tolerance of the refinement solver the fallback to main precision and more through the configuration parameters structure gesv_irs_params and its helper functions.
For more details about what configuration can be set and its meaning please refer to all the functions in the cuSolverDN Helper Function Section that start with cusolverDnIRSParamsxxxx() .
Moreover, cusolverDnIRSXgesv() provides additional information on the output such as the convergence history (e.g., the residual norms) at each iteration and the number of iterations needed to converge.
For more details about what information can be retrieved and its meaning please refer to all the functions in the cuSolverDN Helper Function Section that start with cusolverDnIRSInfosxxxx() The function returns value describes the results of the solving process.
A CUSOLVER_STATUS_SUCCESS indicates that the function finished with success otherwise, it indicates if one of the API arguments is incorrect, or if the configurations of params/infos structure is incorrect or if the function did not finish with success.
More details about the error can be found by checking the niters and the dinfo API parameters.
User should provide the required workspace allocated on device for the cusolverDnIRSXgesv() function.
The amount of bytes required for the function can be queried by calling the respective function cusolverDnIRSXgesv_bufferSize() .
Note that, if the user would like a particular configuration to be set via the params structure, it should be set before the call to cusolverDnIRSXgesv_bufferSize() to get the size of the required workspace.
In cases where the iterative refinement solver fails to converge to the desired accuracy (main precision, INOUT data precision), it is recommended to use main precision as internal lowest precision.
The following table provides all possible combinations values for the lowest precision corresponding to the Inputs/Outputs data type.
Note that if the lowest precision matches the Inputs/Outputs datatype, then the main precision factorization will be used.
Supported Inputs/Outputs data type and lower precision for the IRS solver  Inputs/Outputs Data Type (e.g., main precision) Supported values for the lowest precision CUSOLVER_C_64F CUSOLVER_C_64F, CUSOLVER_C_32F, CUSOLVER_C_16F, CUSOLVER_C_16BF, CUSOLVER_C_TF32 CUSOLVER_C_32F CUSOLVER_C_32F, CUSOLVER_C_16F, CUSOLVER_C_16BF, CUSOLVER_C_TF32 CUSOLVER_R_64F CUSOLVER_R_64F, CUSOLVER_R_32F, CUSOLVER_R_16F, CUSOLVER_R_16BF, CUSOLVER_R_TF32 CUSOLVER_R_32F CUSOLVER_R_32F, CUSOLVER_R_16F, CUSOLVER_R_16BF, CUSOLVER_R_TF32 The cusolverDnIRSXgesv_bufferSize() function returns the required workspace buffer size in bytes for the corresponding cusolverDnXgesv() call with the given gesv_irs_params configuration.
cusolverStatus_t cusolverDnIRSXgesv_bufferSize ( cusolverDnHandle_t handle , cusolverDnIRSParams_t gesv_irs_params , cusolver_int_t n , cusolver_int_t nrhs , size_t * lwork_bytes ); Table 5.
Parameters of cusolverDnIRSXgesv_bufferSize() functions  Parameter Memory In/out Meaning handle host input Handle to the cusolverDn library context.
params host input Xgesv configuration parameters n host input Number of rows and columns of the square matrix A .
Note that nrhs is limited to 1 if the selected IRS refinement solver is CUSOLVER_IRS_REFINE_GMRES, CUSOLVER_IRS_REFINE_GMRES_GMRES, CUSOLVER_IRS_REFINE_CLASSICAL_GMRES.
lwork_bytes host out Pointer to a variable, where the required size in bytes, of the workspace will be stored after a call to cusolverDnIRSXgesv_bufferSize .
cusolverStatus_t cusolverDnIRSXgesv ( cusolverDnHandle_t handle , cusolverDnIRSParams_t gesv_irs_params , cusolverDnIRSInfos_t gesv_irs_infos , int n , int nrhs , void * dA , int ldda , void * dB , int lddb , void * dX , int lddx , void * dWorkspace , size_t lwork_bytes , int * dinfo ); Table 6.
Parameters of cusolverDnIRSXgesv() functions  Parameter Memory In/out Meaning handle host input Handle to the cusolverDn library context.
gesv_irs_params host input Configuration parameters structure, can serve one or more calls to any IRS solver gesv_irs_infos host in/out Info structure, where information about a particular solve will be stored.
Thus different calls requires different gesv_irs_infos structure otherwise, it will be overwritten.
Note that, nrhs is limited to 1 if the selected IRS refinement solver is CUSOLVER_IRS_REFINE_GMRES, CUSOLVER_IRS_REFINE_GMRES_GMRES, CUSOLVER_IRS_REFINE_CLASSICAL_GMRES.
On return - will contain the factorization of the matrix A in the main precision ( A = P * L * U , where P - permutation matrix defined by vector ipiv, L and U - lower and upper triangular matrices) if the iterative refinement solver was set to CUSOLVER_IRS_REFINE_NONE and the lowest precision is equal to the main precision (Inputs/Outputs datatype), or if the iterative refinement solver did not converge and the fallback to main precision was enabled (fallback enabled is the default setting); unchanged otherwise.
dWorkspace device input Pointer to an allocated workspace in device memory of size lwork_bytes.
Should be at least what was returned by cusolverDnIRSXgesv_bufferSize() function niters host output If iter is 0 : iter is a number of iterations solver performed to reach convergence criteria dinfo device output Status of the IRS solver on the return.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed, for example: n1, and refinement solver was set to CUSOLVER_IRS_REFINE_GMRES.
CUSOLVER_STATUS_IRS_INFOS_NOT_INITIALIZED The information structure gesv_irs_infos was not created.
CUSOLVER_STATUS_ALLOC_FAILED CPU memory allocation failed, most likely during the allocation of the residual array that store the residual norms. 2.4.2.12. cusolverDngeqrf()  These helper functions calculate the size of work buffers needed.
cusolverStatus_t cusolverDnSgeqrf_bufferSize ( cusolverDnHandle_t handle , int m , int n , float * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnDgeqrf_bufferSize ( cusolverDnHandle_t handle , int m , int n , double * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnCgeqrf_bufferSize ( cusolverDnHandle_t handle , int m , int n , cuComplex * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnZgeqrf_bufferSize ( cusolverDnHandle_t handle , int m , int n , cuDoubleComplex * A , int lda , int * Lwork ); The S and D data types are real valued single and double precision, respectively.
cusolverStatus_t cusolverDnSgeqrf ( cusolverDnHandle_t handle , int m , int n , float * A , int lda , float * TAU , float * Workspace , int Lwork , int * devInfo ); cusolverStatus_t cusolverDnDgeqrf ( cusolverDnHandle_t handle , int m , int n , double * A , int lda , double * TAU , double * Workspace , int Lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnCgeqrf ( cusolverDnHandle_t handle , int m , int n , cuComplex * A , int lda , cuComplex * TAU , cuComplex * Workspace , int Lwork , int * devInfo ); cusolverStatus_t cusolverDnZgeqrf ( cusolverDnHandle_t handle , int m , int n , cuDoubleComplex * A , int lda , cuDoubleComplex * TAU , cuDoubleComplex * Workspace , int Lwork , int * devInfo ); This function computes the QR factorization of a m×n matrix \(A = Q*R\) where A is an m×n matrix, Q is an m×n matrix, and R is a n×n upper triangular matrix.
The input parameter Lwork is size of the working space, and it is returned by geqrf_bufferSize() .
The matrix Q is not formed explicitly, instead, a sequence of householder vectors are stored in lower triangular part of A .
The leading nonzero element of householder vector is assumed to be 1 such that output parameter TAU contains the scaling factor τ .
If v is original householder vector, q is the new householder vector corresponding to τ , satisfying the following relation \(I - 2*v*v^{H} = I - \tau*q*q^{H}\) If output parameter devInfo = -i (less than zero), the i-th parameter is wrong (not counting handle).
API of geqrf Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,ngels()  These functions compute the solution of a system of linear equations with one or multiple right hand sides using mixed precision iterative refinement techniques based on the QR factorization Xgels.
These functions are similar in term of functionalities to the full precision LAPACK QR (least squares) solver (Xgels, where X denotes Z,C,D,S) but it uses lower precision internally in order to provide faster time to solution, from here comes the name mixed precision.
Mixed precision iterative refinement techniques means that the solver compute an QR factorization in lower precision and then iteratively refine the solution to achieve the accuracy of the Inputs/Outputs datatype precision.
\(A \times X = B\) Where A is m-by-n matrix and X is n-by-nrhs and B is m-by-nrhs matrices.
gels() functions are designated by two floating point precisions The corresponds to the main precision (e.g., Inputs/Outputs datatype precision) and the represent the internal lower precision at which the factorization will be carried on.
cusolvergels() first attempts to factorize the matrix in lower precision and use this factorization within an iterative refinement procedure to obtain a solution with same normwise backward error as the main precision .
If the approach fails to converge, then the method fallback to the main precision factorization and solve (Xgels) such a way that there is always a good solution at the output of these functions.
The iterative refinement process is stopped if: ITER > ITERMAX or for all the RHS we have: RNRM LAMCH('Epsilon') The values ITERMAX and BWDMAX are fixed to 50 and 1.0 respectively.
The amount of bytes required can be queried by calling the respective function gels_bufferSize() .
We provide a large set of mixed precision functions that include half, bfloat and tensorfloat as a lower precision as well as same precision functions (e.g., main and lowest precision are equal is equal to ).
The following table specifies which precisions will be used for which interface function: Tensor Float (TF32), introduced with NVIDIA Ampere Architecture GPUs, is the most robust tensor core accelerated compute mode for the iterative refinement solver.
In cases where the iterative refinement solver fails to converge to the desired accuracy (main precision, INOUT data precision), it is recommended to use main precision as internal lowest precision (i.e., cusolverDn[DD,ZZ]gels for the FP64 case).
Parameters of cusolverDngels_bufferSize() functions  Parameter Memory In/out Meaning handle host input Handle to the cusolverDN library context.
Parameters of cusolverDngels() functions  Parameter Memory In/out Meaning handle host input Handle to the cusolverDN library context.
Should be at least what was returned by cusolverDngels_bufferSize() function niters host output If iter is 0 : iter is a number of iterations solver performed to reach convergence criteria dinfo device output Status of the IRS solver on the return.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed, for example: ngels() functions, but wrapped in a more generic and expert interface that gives user more control to parametrize the function as well as it provides more information on output.
cusolverDnIRSXgels() allows additional control of the solver parameters such as setting: the main precision (Inputs/Outputs precision) of the solver, the lowest precision to be used internally by the solver, the refinement solver type the maximum allowed number of iterations in the refinement phase the tolerance of the refinement solver the fallback to main precision and others through the configuration parameters structure gels_irs_params and its helper functions.
Moreover, cusolverDnIRSXgels() provides additional information on the output such as the convergence history (e.g., the residual norms) at each iteration and the number of iterations needed to converge.
For more details about what information can be retrieved and its meaning please refer to all the functions in the cuSolverDN Helper Function Section that start with cusolverDnIRSInfosxxxx() .
Users should provide the required workspace allocated on device for the cusolverDnIRSXgels() function.
The amount of bytes required for the function can be queried by calling the respective function cusolverDnIRSXgels_bufferSize() .
Note that, if the user would like a particular configuration to be set via the params structure, it should be set before the call to cusolverDnIRSXgels_bufferSize() to get the size of the required workspace.
Note that if the lowest precision matches the Inputs/Outputs datatype, then main precision factorization will be used Tensor Float (TF32), introduced with NVIDIA Ampere Architecture GPUs, is the most robust tensor core accelerated compute mode for the iterative refinement solver.
Supported Inputs/Outputs data type and lower precision for the IRS solver :class: table-no-stripes  Inputs/Outputs Data Type (e.g., main precision) Supported values for the lowest precision CUSOLVER_C_64F CUSOLVER_C_64F, CUSOLVER_C_32F, CUSOLVER_C_16F, CUSOLVER_C_16BF, CUSOLVER_C_TF32 CUSOLVER_C_32F CUSOLVER_C_32F, CUSOLVER_C_16F, CUSOLVER_C_16BF, CUSOLVER_C_TF32 CUSOLVER_R_64F CUSOLVER_R_64F, CUSOLVER_R_32F, CUSOLVER_R_16F, CUSOLVER_R_16BF, CUSOLVER_R_TF32 CUSOLVER_R_32F CUSOLVER_R_32F, CUSOLVER_R_16F, CUSOLVER_R_16BF, CUSOLVER_R_TF32 The cusolverDnIRSXgels_bufferSize() function return the required workspace buffer size in bytes for the corresponding cusolverDnXgels() call with given gels_irs_params configuration.
cusolverStatus_t cusolverDnIRSXgels_bufferSize ( cusolverDnHandle_t handle , cusolverDnIRSParams_t gels_irs_params , cusolver_int_t m , cusolver_int_t n , cusolver_int_t nrhs , size_t * lwork_bytes ); Parameters of cusolverDnIRSXgels_bufferSize() functions Parameter Memory In/out Meaning handle host input Handle to the cusolverDn library context.
params host input Xgels configuration parameters m host input Number of rows of the matrix A .
CUSOLVER_STATUS_IRS_INFOS_NOT_INITIALIZED The information structure gels_irs_infos was not created. 2.4.2.16. cusolverDnormqr()  These helper functions calculate the size of work buffers needed.
cusolverStatus_t cusolverDnSormqr ( cusolverDnHandle_t handle , cublasSideMode_t side , cublasOperation_t trans , int m , int n , int k , const float * A , int lda , const float * tau , float * C , int ldc , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDormqr ( cusolverDnHandle_t handle , cublasSideMode_t side , cublasOperation_t trans , int m , int n , int k , const double * A , int lda , const double * tau , double * C , int ldc , double * work , int lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively.
Q = H(1) H(2) … H(k) Q is of order m if side = CUBLAS_SIDE_LEFT and of order n if side = CUBLAS_SIDE_RIGHT .
The input parameter lwork is size of the working space, and it is returned by geqrf_bufferSize() or ormqr_bufferSize() .
The user can combine geqrf , ormqr and trsm to complete a linear solver or a least-square solver.
API of ormqr Parameter Memory In/out Meaning handle host input Handle to the cuSolverDn library context.
trans host input Operation op(Q) that is non- or (conj.) m host input Number of rows of matrix C .
if side is CUBLAS_SIDE_LEFT , lda >= max(1,m); if side is CUBLAS_SIDE_RIGHT , lda >= max(1,n).
The vector tau is from geqrf , so tau(i) is the scalar of i-th elementary reflection vector.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,norgqr()  These helper functions calculate the size of work buffers needed.
cusolverStatus_t cusolverDnSorgqr_bufferSize ( cusolverDnHandle_t handle , int m , int n , int k , const float * A , int lda , const float * tau , int * lwork ); cusolverStatus_t cusolverDnDorgqr_bufferSize ( cusolverDnHandle_t handle , int m , int n , int k , const double * A , int lda , const double * tau , int * lwork ); cusolverStatus_t cusolverDnCungqr_bufferSize ( cusolverDnHandle_t handle , int m , int n , int k , const cuComplex * A , int lda , const cuComplex * tau , int * lwork ); cusolverStatus_t cusolverDnZungqr_bufferSize ( cusolverDnHandle_t handle , int m , int n , int k , const cuDoubleComplex * A , int lda , const cuDoubleComplex * tau , int * lwork ); The S and D data types are real valued single and double precision, respectively.
cusolverStatus_t cusolverDnSorgqr ( cusolverDnHandle_t handle , int m , int n , int k , float * A , int lda , const float * tau , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDorgqr ( cusolverDnHandle_t handle , int m , int n , int k , double * A , int lda , const double * tau , double * work , int lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnCungqr ( cusolverDnHandle_t handle , int m , int n , int k , cuComplex * A , int lda , const cuComplex * tau , cuComplex * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnZungqr ( cusolverDnHandle_t handle , int m , int n , int k , cuDoubleComplex * A , int lda , const cuDoubleComplex * tau , cuDoubleComplex * work , int lwork , int * devInfo ); This function overwrites m×n matrix A by \(Q = {H(1)}*{H(2)}*{...}*{H(k)}\) where Q is a unitary matrix formed by a sequence of elementary reflection vectors stored in A .
The input parameter lwork is size of the working space, and it is returned by orgqr_bufferSize() .
API of orgqr Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
m >= n >= 0; k host input Number of elementary reflections whose product defines the matrix Q .
n >= k >= 0; A device in/out array of dimension lda * n with lda is not less than max(1,m) .
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,n,km , k>n or ldasytrf()  These helper functions calculate the size of the needed buffers.
cusolverStatus_t cusolverDnSsytrf_bufferSize ( cusolverDnHandle_t handle , int n , float * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnDsytrf_bufferSize ( cusolverDnHandle_t handle , int n , double * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnCsytrf_bufferSize ( cusolverDnHandle_t handle , int n , cuComplex * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnZsytrf_bufferSize ( cusolverDnHandle_t handle , int n , cuDoubleComplex * A , int lda , int * Lwork ); The S and D data types are real valued single and double precision, respectively.
cusolverStatus_t cusolverDnSsytrf ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , float * A , int lda , int * ipiv , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDsytrf ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , double * A , int lda , int * ipiv , double * work , int lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnCsytrf ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuComplex * A , int lda , int * ipiv , cuComplex * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnZsytrf ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , int * ipiv , cuDoubleComplex * work , int lwork , int * devInfo ); This function computes the Bunch-Kaufman factorization of a n×n symmetric indefinite matrix A is a n×n symmetric matrix, only lower or upper part is meaningful.
If input parameter uplo is CUBLAS_FILL_MODE_LOWER , only lower triangular part of A is processed, and replaced by lower triangular factor L and block diagonal matrix D .
\(P*A*P^{T} = L*D*L^{T}\) If input parameter uplo is CUBLAS_FILL_MODE_UPPER , only upper triangular part of A is processed, and replaced by upper triangular factor U and block diagonal matrix D .
\(P*A*P^{T} = U*D*U^{T}\) The user has to provide working space which is pointed by input parameter work .
The input parameter lwork is size of the working space, and it is returned by sytrf_bufferSize() .
If devIpiv(i) = k > 0 , D(i,i) is 1x1 block, and i-th row/column of A is interchanged with k-th row/column of A .
If uplo is CUBLAS_FILL_MODE_UPPER and devIpiv(i-1) = devIpiv(i) = -m array of dimension lda * n with lda is not less than max(1,n) .
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( npotrfBatched()  The S and D data types are real valued single and double precision, respectively.
cusolverStatus_t cusolverDnSpotrfBatched ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , float * Aarray [], int lda , int * infoArray , int batchSize ); cusolverStatus_t cusolverDnDpotrfBatched ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , double * Aarray [], int lda , int * infoArray , int batchSize ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnCpotrfBatched ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuComplex * Aarray [], int lda , int * infoArray , int batchSize ); cusolverStatus_t cusolverDnZpotrfBatched ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuDoubleComplex * Aarray [], int lda , int * infoArray , int batchSize ); This function computes the Cholesky factorization of a sequence of Hermitian positive-definite matrices.
Each Aarray[i] for i=0,1,..., batchSize-1 is a n×n Hermitian matrix, only lower or upper part is meaningful.
If input parameter uplo is CUBLAS_FILL_MODE_LOWER , only lower triangular part of A is processed, and replaced by lower triangular Cholesky factor L .
The output parameter infoArray would indicate smallest leading minor of A which is not positive definite.
If potrfBatched returns CUSOLVER_STATUS_INVALID_VALUE , infoArray[0] = -i (less than zero), meaning that the i-th parameter is wrong (not counting handle).
If potrfBatched returns CUSOLVER_STATUS_SUCCESS but infoArray[i] = k is positive, then i-th matrix is not positive definite and the Cholesky factorization failed at row k .
For example, if uplo is CUBLAS_FILL_MODE_UPPER , upper triangle of A contains Cholesky factor U and lower triangle of A is destroyed after potrfBatched .
API of potrfBatched  Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
uplo host input Indicates if lower or upper part is stored; the other part is used as a workspace.
Aarray device in/out Array of pointers to array of dimension lda * n with lda is not less than max(1,n) .
lda host input Leading dimension of two-dimensional array used to store each matrix Aarray[i] .
if potrfBatched returns CUSOLVER_STATUS_INVALID_VALUE , infoArray[0] = -i (less than zero) means the i-th parameter is wrong (not counting handle).
if potrfBatched returns CUSOLVER_STATUS_SUCCESS , infoArray[i] = 0 means the Cholesky factorization of i-th matrix is successful, and infoArray[i] = k means the leading submatrix of order k of i-th matrix is not positive definite.
For example, if uplo is CUBLAS_FILL_MODE_UPPER , upper triangle of A contains Cholesky factor U and lower triangle of A is destroyed after potrsBatched .
API of potrsBatched Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
ldb host input Leading dimension of two-dimensional array used to store each matrix Barray[i] .
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( ngebrd()  These helper functions calculate the size of work buffers needed.
cusolverStatus_t cusolverDnSgebrd_bufferSize ( cusolverDnHandle_t handle , int m , int n , int * Lwork ); cusolverStatus_t cusolverDnDgebrd_bufferSize ( cusolverDnHandle_t handle , int m , int n , int * Lwork ); cusolverStatus_t cusolverDnCgebrd_bufferSize ( cusolverDnHandle_t handle , int m , int n , int * Lwork ); cusolverStatus_t cusolverDnZgebrd_bufferSize ( cusolverDnHandle_t handle , int m , int n , int * Lwork ); The S and D data types are real valued single and double precision, respectively.
cusolverStatus_t cusolverDnSgebrd ( cusolverDnHandle_t handle , int m , int n , float * A , int lda , float * D , float * E , float * TAUQ , float * TAUP , float * Work , int Lwork , int * devInfo ); cusolverStatus_t cusolverDnDgebrd ( cusolverDnHandle_t handle , int m , int n , double * A , int lda , double * D , double * E , double * TAUQ , double * TAUP , double * Work , int Lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively.
API of gebrd Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
The off-diagonal elements of the bidiagonal matrix B : if m>=n , E(i) = A(i,i+1) for i = 1,2,...,n-1 ; if m array of dimension min(m,n) .
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,norgbr()  These helper functions calculate the size of work buffers needed.
cusolverStatus_t cusolverDnSorgbr ( cusolverDnHandle_t handle , cublasSideMode_t side , int m , int n , int k , float * A , int lda , const float * tau , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDorgbr ( cusolverDnHandle_t handle , cublasSideMode_t side , int m , int n , int k , double * A , int lda , const double * tau , double * work , int lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnCungbr ( cusolverDnHandle_t handle , cublasSideMode_t side , int m , int n , int k , cuComplex * A , int lda , const cuComplex * tau , cuComplex * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnZungbr ( cusolverDnHandle_t handle , cublasSideMode_t side , int m , int n , int k , cuDoubleComplex * A , int lda , const cuDoubleComplex * tau , cuDoubleComplex * work , int lwork , int * devInfo ); This function generates one of the unitary matrices Q or P**H determined by gebrd when reducing a matrix A to bidiagonal form: \(Q^{H}*A*P = B\) Q and P**H are defined as products of elementary reflectors H(i) or G(i) respectively.
The input parameter lwork is size of the working space, and it is returned by orgbr_bufferSize() .
API of orgbr Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
k host input If side = CUBLAS_SIDE_LEFT , the number of columns in the original m-by-k matrix reduced by gebrd .
if side = CUBLAS_SIDE_RIGHT , the number of rows in the original k-by-n matrix reduced by gebrd .
A device in/out array of dimension lda * n On entry, the vectors which define the elementary reflectors, as returned by gebrd .
lda >= max(1,m); tau device input array of dimension min(m,k) if side is CUBLAS_SIDE_LEFT ; of dimension min(n,k) if side is CUBLAS_SIDE_RIGHT ; tau(i) must contain the scalar factor of the elementary reflector H(i) or G(i), which determines Q or P**T, as returned by gebrd in its array argument TAUQ or TAUP .
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,nsytrd()  These helper functions calculate the size of work buffers needed.
cusolverStatus_t cusolverDnSsytrd ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , float * A , int lda , float * d , float * e , float * tau , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDsytrd ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , double * A , int lda , double * d , double * e , double * tau , double * work , int lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnChetrd ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuComplex * A , int lda , float * d , float * e , cuComplex * tau , cuComplex * work , int lwork , int * devInfo ); cusolverStatus_t CUDENSEAPI cusolverDnZhetrd ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , double * d , double * e , cuDoubleComplex * tau , cuDoubleComplex * work , int lwork , int * devInfo ); This function reduces a general symmetric (Hermitian) n×n matrix A to real symmetric tridiagonal form T by an orthogonal transformation: \(Q^{H}*A*Q = T\) As an output, A contains T and householder reflection vectors.
If uplo = CUBLAS_FILL_MODE_UPPER , the diagonal and first superdiagonal of A are overwritten by the corresponding elements of the tridiagonal matrix T , and the elements above the first superdiagonal, with the array tau , represent the orthogonal matrix Q as a product of elementary reflectors; If uplo = CUBLAS_FILL_MODE_LOWER , the diagonal and first subdiagonal of A are overwritten by the corresponding elements of the tridiagonal matrix T , and the elements below the first subdiagonal, with the array tau , represent the orthogonal matrix Q as a product of elementary reflectors.
The input parameter lwork is size of the working space, and it is returned by sytrd_bufferSize() .
API of sytrd Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
If uplo = CUBLAS_FILL_MODE_UPPER , the leading n-by-n upper triangular part of A contains the upper triangular part of the matrix A , and the strictly lower triangular part of A is not referenced.
If uplo = CUBLAS_FILL_MODE_LOWER , the leading n-by-n lower triangular part of A contains the lower triangular part of the matrix A , and the strictly upper triangular part of A is not referenced.
The off-diagonal elements of the tridiagonal matrix T : if uplo = CUBLAS_FILL_MODE_UPPER , E(i) = A(i,i+1) .
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( normtr()  These helper functions calculate the size of work buffers needed.
cusolverStatus_t cusolverDnSormtr ( cusolverDnHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , int m , int n , float * A , int lda , float * tau , float * C , int ldc , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDormtr ( cusolverDnHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , int m , int n , double * A , int lda , double * tau , double * C , int ldc , double * work , int lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively.
The operation on Q is defined by \(\text{op}(Q) = \left\{ \begin{matrix} Q & {\text{if~}\textsf{transa\ ==\ CUBLAS\_OP\_N}} \\ Q^{T} & {\text{if~}\textsf{transa\ ==\ CUBLAS\_OP\_T}} \\ Q^{H} & {\text{if~}\textsf{transa\ ==\ CUBLAS\_OP\_C}} \\ \end{matrix}  ight.\) The user has to provide working space which is pointed by input parameter work .
The input parameter lwork is size of the working space, and it is returned by ormtr_bufferSize() .
API of ormtr Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
side host input side = CUBLAS_SIDE_LEFT , apply Q or Q**T from the Left; side = CUBLAS_SIDE_RIGHT , apply Q or Q**T from the Right.
uplo host input uplo = CUBLAS_FILL_MODE_LOWER : Lower triangle of A contains elementary reflectors from sytrd .
uplo = CUBLAS_FILL_MODE_UPPER : Upper triangle of A contains elementary reflectors from sytrd .
A device in/out array of dimension lda * m if side = CUBLAS_SIDE_LEFT ; lda * n if side = CUBLAS_SIDE_RIGHT .
tau device output array of dimension (m-1) if side is CUBLAS_SIDE_LEFT ; of dimension (n-1) if side is CUBLAS_SIDE_RIGHT ; The vector tau is from sytrd , so tau(i) is the scalar of i-th elementary reflection vector.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,norgtr()  These helper functions calculate the size of work buffers needed.
cusolverStatus_t cusolverDnSorgtr_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , const float * A , int lda , const float * tau , int * lwork ); cusolverStatus_t cusolverDnDorgtr_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , const double * A , int lda , const double * tau , int * lwork ); cusolverStatus_t cusolverDnCungtr_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * A , int lda , const cuComplex * tau , int * lwork ); cusolverStatus_t cusolverDnZungtr_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * A , int lda , const cuDoubleComplex * tau , int * lwork ); The S and D data types are real valued single and double precision, respectively.
cusolverStatus_t cusolverDnSorgtr ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , float * A , int lda , const float * tau , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDorgtr ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , double * A , int lda , const double * tau , double * work , int lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnCungtr ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuComplex * A , int lda , const cuComplex * tau , cuComplex * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnZungtr ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , const cuDoubleComplex * tau , cuDoubleComplex * work , int lwork , int * devInfo ); This function generates a unitary matrix Q which is defined as the product of n-1 elementary reflectors of order n, as returned by sytrd : The user has to provide working space which is pointed by input parameter work .
The input parameter lwork is size of the working space, and it is returned by orgtr_bufferSize() .
API of orgtr Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
A device in/out array of dimension lda * n On entry, matrix A from sytrd contains the elementary reflectors.
tau device input array of dimension (n-1) tau(i) is the scalar of i-th elementary reflection vector.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( ngesvd()  The helper functions below can calculate the sizes needed for pre-allocated buffer.
cusolverStatus_t cusolverDnSgesvd_bufferSize ( cusolverDnHandle_t handle , int m , int n , int * lwork ); cusolverStatus_t cusolverDnDgesvd_bufferSize ( cusolverDnHandle_t handle , int m , int n , int * lwork ); cusolverStatus_t cusolverDnCgesvd_bufferSize ( cusolverDnHandle_t handle , int m , int n , int * lwork ); cusolverStatus_t cusolverDnZgesvd_bufferSize ( cusolverDnHandle_t handle , int m , int n , int * lwork ); The S and D data types are real valued single and double precision, respectively.
cusolverStatus_t cusolverDnSgesvd ( cusolverDnHandle_t handle , signed char jobu , signed char jobvt , int m , int n , float * A , int lda , float * S , float * U , int ldu , float * VT , int ldvt , float * work , int lwork , float * rwork , int * devInfo ); cusolverStatus_t cusolverDnDgesvd ( cusolverDnHandle_t handle , signed char jobu , signed char jobvt , int m , int n , double * A , int lda , double * S , double * U , int ldu , double * VT , int ldvt , double * work , int lwork , double * rwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnCgesvd ( cusolverDnHandle_t handle , signed char jobu , signed char jobvt , int m , int n , cuComplex * A , int lda , float * S , cuComplex * U , int ldu , cuComplex * VT , int ldvt , cuComplex * work , int lwork , float * rwork , int * devInfo ); cusolverStatus_t cusolverDnZgesvd ( cusolverDnHandle_t handle , signed char jobu , signed char jobvt , int m , int n , cuDoubleComplex * A , int lda , double * S , cuDoubleComplex * U , int ldu , cuDoubleComplex * VT , int ldvt , cuDoubleComplex * work , int lwork , double * rwork , int * devInfo ); This function computes the singular value decomposition (SVD) of a m×n matrix A and corresponding the left and/or right singular vectors.
The SVD is written \(A = U*\Sigma*V^{H}\) where Σ is an m×n matrix which is zero except for its min(m,n) diagonal elements, U is an m×m unitary matrix, and V is an n×n unitary matrix.
The diagonal elements of Σ are the singular values of A ; they are real and non-negative, and are returned in descending order.
The input parameter lwork is size of the working space, and it is returned by gesvd_bufferSize() .
if bdsqr did not converge, devInfo specifies how many superdiagonals of an intermediate bidiagonal form did not converge to zero.
If devInfo >0 and rwork is not NULL, rwork contains the unconverged superdiagonal elements of an upper bidiagonal matrix.
This is slightly different from LAPACK which puts unconverged superdiagonal elements in work if type is real ; in rwork if type is complex .
API of gesvd Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
jobu host input Specifies options for computing all or part of the matrix U : = ‘A’: all m columns of U are returned in array U: = ‘S’: the first min(m,n) columns of U (the left singular vectors) are returned in the array U; = ‘O’: the first min(m,n) columns of U (the left singular vectors) are overwritten on the array A; = ‘N’: no columns of U (no left singular vectors) are computed.
jobvt host input Specifies options for computing all or part of the matrix V**T: = ‘A’: all N rows of V**T are returned in the array VT; = ‘S’: the first min(m,n) rows of V**T (the right singular vectors) are returned in the array VT; = ‘O’: the first min(m,n) rows of V**T (the right singular vectors) are overwritten on the array A; = ‘N’: no rows of V**T (no right singular vectors) are computed.
It contains the unconverged superdiagonal elements of an upper bidiagonal matrix if devInfo > 0 .
If devInfo > 0 , devInfo indicates how many superdiagonals of an intermediate bidiagonal form did not converge to zero.
List of input arguments for cusolverDnGesvd_bufferSize and cusolverDnGesvd : API of cusolverDnGesvd Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
workspaceInBytes host input Size in bytes of pBuffer , returned by cusolverDnGesvd_bufferSize .
if info > 0 , info indicates how many superdiagonals of an intermediate bidiagonal form did not converge to zero.
The generic API has three different types, dataTypeA is data type of the matrix A , dataTypeS is data type of the vector S and dataTypeU is data type of the matrix U , dataTypeVT is data type of the matrix VT , computeType is compute type of the operation.
Valid combination of data type and compute type DataTypeA DataTypeS DataTypeU DataTypeVT ComputeType Meaning CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F SGESVD CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F DGESVD CUDA_C_32F CUDA_R_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CGESVD CUDA_C_64F CUDA_R_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F ZGESVD Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,ngesvdj()  The helper functions below can calculate the sizes needed for pre-allocated buffer.
cusolverStatus_t cusolverDnSgesvdj ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , int econ , int m , int n , float * A , int lda , float * S , float * U , int ldu , float * V , int ldv , float * work , int lwork , int * info , gesvdjInfo_t params ); cusolverStatus_t cusolverDnDgesvdj ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , int econ , int m , int n , double * A , int lda , double * S , double * U , int ldu , double * V , int ldv , double * work , int lwork , int * info , gesvdjInfo_t params ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnCgesvdj ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , int econ , int m , int n , cuComplex * A , int lda , float * S , cuComplex * U , int ldu , cuComplex * V , int ldv , cuComplex * work , int lwork , int * info , gesvdjInfo_t params ); cusolverStatus_t cusolverDnZgesvdj ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , int econ , int m , int n , cuDoubleComplex * A , int lda , double * S , cuDoubleComplex * U , int ldu , cuDoubleComplex * V , int ldv , cuDoubleComplex * work , int lwork , int * info , gesvdjInfo_t params ); This function computes the singular value decomposition (SVD) of a m×n matrix A and corresponding the left and/or right singular vectors.
The SVD is written: \(A = U*\Sigma*V^{H}\) where Σ is an m×n matrix which is zero except for its min(m,n) diagonal elements, U is an m×m unitary matrix, and V is an n×n unitary matrix.
The parallelism of Jacobi method gives GPU better performance on small and medium size matrices.
gesvdj iteratively generates a sequence of unitary matrices to transform matrix A to the following form \(U^{H}*A*V = S + E\) where S is diagonal and diagonal of E is zero.
In practice, Jacobi method stops if \({||E||}_{F}\leq\operatorname{eps}*{||A||}_{F}\) where eps is given tolerance.
The default value is machine accuracy but The user can use function cusolverDnXgesvdjSetTolerance to set a priori tolerance.
The second parameter is maximum number of sweeps which controls number of iterations of Jacobi method.
The default value is 100 but the user can use function cusolverDnXgesvdjSetMaxSweeps to set a proper bound.
Jacobi method has quadratic convergence, so the accuracy is not proportional to number of sweeps.
The input parameter lwork is the size of the working space, and it is returned by gesvdj_bufferSize() .
If output parameter info = -i (less than zero), the i-th parameter is wrong (not counting handle).
API of gesvdj Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
jobz host input Specifies options to either compute singular value only or singular vectors as well: jobz = CUSOLVER_EIG_MODE_NOVECTOR : Compute singular values only; jobz = CUSOLVER_EIG_MODE_VECTOR : Compute singular values and singular vectors.
params host input Structure filled with parameters of Jacobi algorithm and results of gesvdj .
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,ngesvdjBatched()  The helper functions below can calculate the sizes needed for pre-allocated buffer.
cusolverStatus_t cusolverDnSgesvdjBatched ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , int m , int n , float * A , int lda , float * S , float * U , int ldu , float * V , int ldv , float * work , int lwork , int * info , gesvdjInfo_t params , int batchSize ); cusolverStatus_t cusolverDnDgesvdjBatched ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , int m , int n , double * A , int lda , double * S , double * U , int ldu , double * V , int ldv , double * work , int lwork , int * info , gesvdjInfo_t params , int batchSize ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnCgesvdjBatched ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , int m , int n , cuComplex * A , int lda , float * S , cuComplex * U , int ldu , cuComplex * V , int ldv , cuComplex * work , int lwork , int * info , gesvdjInfo_t params , int batchSize ); cusolverStatus_t cusolverDnZgesvdjBatched ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , int m , int n , cuDoubleComplex * A , int lda , double * S , cuDoubleComplex * U , int ldu , cuDoubleComplex * V , int ldv , cuDoubleComplex * work , int lwork , int * info , gesvdjInfo_t params , int batchSize ); This function computes singular values and singular vectors of a sequence of general m×n matrices \(A_{j} = U_{j}*\Sigma_{j}*V_{j}^{H}\) where \(\Sigma_{j}\) is a real m×n diagonal matrix which is zero except for its min(m,n) diagonal elements.
\(U_{j}\) (left singular vectors) is a m×m unitary matrix and \(V_{j}\) (right singular vectors) is a n×n unitary matrix.
The diagonal elements of \(\Sigma_{j}\) are the singular values of \(A_{j}\) in either descending order or non-sorting order.
It requires that all matrices are of the same size m,n no greater than 32 and are packed in contiguous way, \(A = \begin{pmatrix} {A0} & {A1} & \cdots \\ \end{pmatrix}\) Each matrix is column-major with leading dimension lda , so the formula for random access is \(A_{k}\operatorname{(i,j)} = {A\lbrack\ i\ +\ lda*j\ +\ lda*n*k brack}\) .
The parameter S also contains singular values of each matrix in contiguous way, \(S = \begin{pmatrix} {S0} & {S1} & \cdots \\ \end{pmatrix}\) The formula for random access of S is \(S_{k}\operatorname{(j)} = {S\lbrack\ j\ +\ min(m,n)*k brack}\) .
Except for tolerance and maximum sweeps, gesvdjBatched can either sort the singular values in descending order (default) or chose as-is (without sorting) by the function cusolverDnXgesvdjSetSortEig .
If the user packs several tiny matrices into diagonal blocks of one matrix, non-sorting option can separate singular values of those tiny matrices.
gesvdjBatched cannot report residual and executed sweeps by function cusolverDnXgesvdjGetResidual and cusolverDnXgesvdjGetSweeps .
The input parameter lwork is the size of the working space, and it is returned by gesvdjBatched_bufferSize() .
If the function returns CUSOLVER_STATUS_INVALID_VALUE , the first element info[0] = -i (less than zero) indicates i-th parameter is wrong (not counting handle).
Otherwise, if info[i] = min(m,n)+1 , gesvdjBatched does not converge on i-th matrix under given tolerance and maximum sweeps.
API of gesvdjBatched Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
A device in/out array of dimension lda * n * batchSize with lda is not less than max(1,n) .
If CUSOLVER_STATUS_INVALID_VALUE is returned, info[0] = -i (less than zero) indicates i-th parameter is wrong (not counting handle).
if info[i] = min(m,n)+1 , gesvdjBatched dose not converge on i-th matrix under given tolerance and maximum sweeps.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,ngesvdaStridedBatched()  The helper functions below can calculate the sizes needed for pre-allocated buffer.
gesvda computes eigenvalues of A**T*A , or A**H*A (if A is complex), to approximate singular values and singular vectors.
It generates matrices U and V and transforms the matrix A to the following form \(U^{H}*A*V = S + E\) where S is diagonal and E depends on rounding errors.
To certain conditions, U , V and S approximate singular values and singular vectors up to machine zero of single precision.
In other words, the accuracy of singular values and left singular vectors depend on the distance between singular value and zero.
The input parameter rank decides the number of singular values and singular vectors are computed in parameter S , U and V .
Otherwise, h_RnrmF reports \({||}U*S*V^{H}{||} - {||S||}\) in Frobenius norm sense, that is, how far U is from unitary.
It requires that all matrices are of the same size m,n and are packed in a contiguous way, \(A = \begin{pmatrix} {A0} & {A1} & \cdots \\ \end{pmatrix}\) Each matrix is column-major with leading dimension lda , so the formula for random access is \(A_{k}\operatorname{(i,j)} = {A\lbrack\ i\ +\ lda*j\ +\ strideA*k brack}\) .
Similarly, the formula for random access of S is \(S_{k}\operatorname{(j)} = {S\lbrack\ j\ +\ StrideS*k brack}\) , the formula for random access of U is \(U_{k}\operatorname{(i,j)} = {U\lbrack\ i\ +\ ldu*j\ +\ strideU*k brack}\) and the formula for random access of V is \(V_{k}\operatorname{(i,j)} = {V\lbrack\ i\ +\ ldv*j\ +\ strideV*k brack}\) .
The input parameter lwork is the size of the working space, and it is returned by gesvdaStridedBatched_bufferSize() .
Otherwise, if info[i] = min(m,n)+1 , gesvdaStridedBatched does not converge on i-th matrix under given tolerance.
Remark 2: if the user is confident on the accuracy of singular values and singular vectors, for example, certain conditions hold (required singular value is far from zero), then the performance can be improved by passing a null pointer to h_RnrmF , i.e.
API of gesvda Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
A device input array of dimension strideA * batchSize with lda is not less than max(1,m) .
strideA host input Value of type long long int that gives the address offset between A[i] and A[i+1] .
Sj is of dimension rank * 1 strideS host input Value of type long long int that gives the address offset between S[i] and S[i+1] .
strideU host input Value of type long long int that gives the address offset between U[i] and U[i+1] .
strideV host input Value of type long long int that gives the address offset between V[i] and V[i+1] .
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,nsyevd()  The helper functions below can calculate the sizes needed for pre-allocated buffer.
cusolverStatus_t cusolverDnSsyevd ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , float * A , int lda , float * W , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDsyevd ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , double * A , int lda , double * W , double * work , int lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnCheevd ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuComplex * A , int lda , float * W , cuComplex * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnZheevd ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , double * W , cuDoubleComplex * work , int lwork , int * devInfo ); This function computes eigenvalues and eigenvectors of a symmetric (Hermitian) n×n matrix A .
The standard symmetric eigenvalue problem is \(A*V = V*\Lambda\) where Λ is a real n×n diagonal matrix.
The input parameter lwork is size of the working space, and it is returned by syevd_bufferSize() .
If devInfo = i (greater than zero), i off-diagonal elements of an intermediate tridiagonal form did not converge to zero.
If jobz = CUSOLVER_EIG_MODE_VECTOR, A contains the orthonormal eigenvectors of the matrix A .
API of syevd Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
jobz host input Specifies options to either compute eigenvalue only or compute eigen-pair: jobz = CUSOLVER_EIG_MODE_NOVECTOR : Compute eigenvalues only; jobz = CUSOLVER_EIG_MODE_VECTOR : Compute eigenvalues and eigenvectors.
If uplo = CUBLAS_FILL_MODE_UPPER , the leading n-by-n upper triangular part of A contains the upper triangular part of the matrix A .
If uplo = CUBLAS_FILL_MODE_LOWER , the leading n-by-n lower triangular part of A contains the lower triangular part of the matrix A .
On exit, if jobz = CUSOLVER_EIG_MODE_VECTOR , and devInfo = 0, A contains the orthonormal eigenvectors of the matrix A .
The eigenvalue values of A , in ascending order ie, sorted so that W(i) array of size lwork .
If devInfo = i (> 0) , devInfo indicates i off-diagonal elements of an intermediate tridiagonal form did not converge to zero; Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( n 0) , info indicates i off-diagonal elements of an intermediate tridiagonal form did not converge to zero; The generic API has three different types, dataTypeA is data type of the matrix A , dataTypeW is data type of the matrix W and computeType is compute type of the operation.
Valid combination of data type and compute type DataTypeA DataTypeW ComputeType Meaning CUDA_R_32F CUDA_R_32F CUDA_R_32F SSYEVD CUDA_R_64F CUDA_R_64F CUDA_R_64F DSYEVD CUDA_C_32F CUDA_R_32F CUDA_C_32F CHEEVD CUDA_C_64F CUDA_R_64F CUDA_C_64F ZHEEVD Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( nsyevdx()  The helper functions below can calculate the sizes needed for pre-allocated buffer.
cusolverStatus_t cusolverDnSsyevdx ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cusolverEigRange_t range , cublasFillMode_t uplo , int n , float * A , int lda , float vl , float vu , int il , int iu , int * h_meig , float * W , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDsyevdx ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cusolverEigRange_t range , cublasFillMode_t uplo , int n , double * A , int lda , double vl , double vu , int il , int iu , int * h_meig , double * W , double * work , int lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnCheevdx ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cusolverEigRange_t range , cublasFillMode_t uplo , int n , cuComplex * A , int lda , float vl , float vu , int il , int iu , int * h_meig , float * W , cuComplex * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnZheevdx ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cusolverEigRange_t range , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , double vl , double vu , int il , int iu , int * h_meig , double * W , cuDoubleComplex * work , int lwork , int * devInfo ); This function computes all or selection of the eigenvalues and optionally eigenvectors of a symmetric (Hermitian) n×n matrix A .
The standard symmetric eigenvalue problem is: \(A*V = V*\Lambda\) where Λ is a real n×h_meig diagonal matrix.
h_meig is the number of eigenvalues/eigenvectors computed by the routine, h_meig is equal to n when the whole spectrum (e.g., range = CUSOLVER_EIG_RANGE_ALL ) is requested.
The input parameter lwork is size of the working space, and it is returned by syevdx_bufferSize() .
API of syevdx Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
range host input Specifies options to which selection of eigenvalues and optionally eigenvectors that need to be computed: range = CUSOLVER_EIG_RANGE_ALL : all eigenvalues/eigenvectors will be found, will becomes the classical syevd/heevd routine; range = CUSOLVER_EIG_RANGE_V : all eigenvalues/eigenvectors in the half-open interval (vl,vu] will be found; range = CUSOLVER_EIG_RANGE_I : the il-th through iu-th eigenvalues/eigenvectors will be found; uplo host input Specifies which part of A is stored.
If range = CUSOLVER_EIG_RANGE_V , the lower and upper bounds of the interval to be searched for eigenvalues.
Note that, if eigenvalues are very close to each other, it is well known that two different eigenvalues routines might find slightly different number of eigenvalues inside the same interval.
This is due to the fact that different eigenvalue algorithms, or even same algorithm but different run might find eigenvalues within some rounding error close to the machine precision.
Thus, if the user wants to be sure not to miss any eigenvalue within the interval bound, we suggest that the user subtract/add epsilon (machine precision) to the interval bound such as (vl=vl-eps, vu=vu+eps] .
If range = CUSOLVER_EIG_RANGE_I , the indices (in ascending order) of the smallest and largest eigenvalues to be returned.
If devInfo = i (> 0) , devInfo indicates i off-diagonal elements of an intermediate tridiagonal form did not converge to zero.
Thus, if the user want to be sure not to miss any eigenvalue within the interval bound, we suggest that, the user subtract/add epsilon (machine precision) to the interval bound such as (vl=vl-eps, vu=vu+eps].
0 0) , info indicates i off-diagonal elements of an intermediate tridiagonal form did not converge to zero; The generic API has three different types, dataTypeA is data type of the matrix A , dataTypeW is data type of the matrix W and computeType is compute type of the operation.
Valid combination of data type and compute type DataTypeA DataTypeW ComputeType Meaning CUDA_R_32F CUDA_R_32F CUDA_R_32F SSYEVDX CUDA_R_64F CUDA_R_64F CUDA_R_64F DSYEVDX CUDA_C_32F CUDA_R_32F CUDA_C_32F CHEEVDX CUDA_C_64F CUDA_R_64F CUDA_C_64F ZHEEVDX Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( nsygvd()  The helper functions below can calculate the sizes needed for pre-allocated buffer.
cusolverStatus_t cusolverDnSsygvd ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , float * A , int lda , float * B , int ldb , float * W , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDsygvd ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , double * A , int lda , double * B , int ldb , double * W , double * work , int lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnChegvd ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuComplex * A , int lda , cuComplex * B , int ldb , float * W , cuComplex * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnZhegvd ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , cuDoubleComplex * B , int ldb , double * W , cuDoubleComplex * work , int lwork , int * devInfo ); This function computes eigenvalues and eigenvectors of a symmetric (Hermitian) n×n matrix-pair ( A , B ).
The generalized symmetric-definite eigenvalue problem is \({eig(A,B)} = \left\{ \begin{matrix} {A*V = B*V*\Lambda} & {\text{if~}\textsf{itype\ =\ CUSOLVER\_EIG\_TYPE\_1}} \\ {A*B*V = V*\Lambda} & {\text{if~}\textsf{itype\ =\ CUSOLVER\_EIG\_TYPE\_2}} \\ {B*A*V = V*\Lambda} & {\text{if~}\textsf{itype\ =\ CUSOLVER\_EIG\_TYPE\_3}} \\ \end{matrix}  ight.\) where the matrix B is positive definite.
The eigenvectors are normalized as follows: \(\left\{ \begin{matrix} {V^{H}*B*V = I} & {\text{if~}\textsf{itype\ =\ CUSOLVER\_EIG\_TYPE\_1,\ CUSOLVER\_EIG\_TYPE\_2}} \\ {V^{H}*{inv(B)}*V = I} & {\text{if~}\textsf{itype\ =\ CUSOLVER\_EIG\_TYPE\_3}} \\ \end{matrix}  ight.\) The user has to provide working space which is pointed by input parameter work .
The input parameter lwork is size of the working space, and it is returned by sygvd_bufferSize() .
If devInfo = i (i > 0 and i 0), then the leading minor of order i of B is not positive definite.
The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.
if jobz = CUSOLVER_EIG_MODE_VECTOR, A contains the orthogonal eigenvectors of the matrix A .
API of sygvd Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
itype host input Specifies the problem type to be solved: itype =CUSOLVER_EIG_TYPE_1: A*x = (lambda)*B*x.
jobz host input Specifies options to either compute eigenvalue only or compute eigen-pair: jobz = CUSOLVER_EIG_MODE_NOVECTOR : Compute eigenvalues only; jobz = CUSOLVER_EIG_MODE_VECTOR : Compute eigenvalues and eigenvectors.
If uplo = CUBLAS_FILL_MODE_UPPER , the leading n-by-n upper triangular part of B contains the upper triangular part of the matrix B .
If uplo = CUBLAS_FILL_MODE_LOWER , the leading n-by-n lower triangular part of B contains the lower triangular part of the matrix B .
On exit, if devInfo is less than n , B is overwritten by triangular factor U or L from the Cholesky factorization of B .
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( nsygvdx()  The helper functions below can calculate the sizes needed for pre-allocated buffer.
cusolverStatus_t cusolverDnSsygvdx ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cusolverEigRange_t range , cublasFillMode_t uplo , int n , float * A , int lda , float * B , int ldb , float vl , float vu , int il , int iu , int * h_meig , float * W , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDsygvdx ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cusolverEigRange_t range , cublasFillMode_t uplo , int n , double * A , int lda , double * B , int ldb , double vl , double vu , int il , int iu , int * h_meig , double * W , double * work , int lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnChegvdx ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cusolverEigRange_t range , cublasFillMode_t uplo , int n , cuComplex * A , int lda , cuComplex * B , int ldb , float vl , float vu , int il , int iu , int * h_meig , float * W , cuComplex * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnZhegvdx ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cusolverEigRange_t range , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , cuDoubleComplex * B , int ldb , double vl , double vu , int il , int iu , int * h_meig , double * W , cuDoubleComplex * work , int lwork , int * devInfo ); This function computes all or selection of the eigenvalues and optionally eigenvectors of a symmetric (Hermitian) n×n matrix-pair ( A , B ).
The input parameter lwork is size of the working space, and it is returned by sygvdx_bufferSize() .
If jobz = CUSOLVER_EIG_MODE_VECTOR, A contains the orthogonal eigenvectors of the matrix A .
API of sygvdx Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
itype host input Specifies the problem type to be solved: itype =CUSOLVER_EIG_TYPE_1: A*x = (lambda)*B*x itype =CUSOLVER_EIG_TYPE_2: A*B*x = (lambda)*x itype =CUSOLVER_EIG_TYPE_3: B*A*x = (lambda)*x jobz host input Specifies options to either compute eigenvalue only or compute eigen-pair: jobz = CUSOLVER_EIG_MODE_NOVECTOR : Compute eigenvalues only; jobz = CUSOLVER_EIG_MODE_VECTOR : Compute eigenvalues and eigenvectors.
range host input Specifies options to which selection of eigenvalues and optionally eigenvectors that need to be computed: range = CUSOLVER_EIG_RANGE_ALL : all eigenvalues/eigenvectors will be found, will becomes the classical syevd/heevd routine; range = CUSOLVER_EIG_RANGE_V : all eigenvalues/eigenvectors in the half-open interval (vl,vu] will be found; range = CUSOLVER_EIG_RANGE_I : the il-th through iu-th eigenvalues/eigenvectors will be found; uplo host input Specifies which part of A and B are stored.
Thus, if the user want to be sure not to miss any eigenvalue within the interval bound, we suggest that, the user subtract/add epsilon (machine precision) to the interval bound such as ( vl = vl - eps , vu = vu + eps ].
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( nsyevj()  The helper functions below can calculate the sizes needed for pre-allocated buffer.
cusolverStatus_t cusolverDnSsyevj ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , float * A , int lda , float * W , float * work , int lwork , int * info , syevjInfo_t params ); cusolverStatus_t cusolverDnDsyevj ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , double * A , int lda , double * W , double * work , int lwork , int * info , syevjInfo_t params ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnCheevj ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuComplex * A , int lda , float * W , cuComplex * work , int lwork , int * info , syevjInfo_t params ); cusolverStatus_t cusolverDnZheevj ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , double * W , cuDoubleComplex * work , int lwork , int * info , syevjInfo_t params ); This function computes eigenvalues and eigenvectors of a symmetric (Hermitian) n×n matrix A .
The standard symmetric eigenvalue problem is \(A*Q = Q*\Lambda\) where Λ is a real n×n diagonal matrix.
How does it work? syevj iteratively generates a sequence of unitary matrices to transform matrix A to the following form \(V^{H}*A*V = W + E\) where W is diagonal and E is symmetric without diagonal.
In practice, Jacobi method stops if \({||E||}_{F}\leq\operatorname{eps}*{||A||}_{F}\) where eps is the given tolerance.
The default value is machine accuracy but The user can use function cusolverDnXsyevjSetTolerance to set a priori tolerance.
The default value is 100 but the user can use function cusolverDnXsyevjSetMaxSweeps to set a proper bound.
The Jacobi method has quadratic convergence, so the accuracy is not proportional to number of sweeps.
After syevj , the user can query residual by function cusolverDnXsyevjGetResidual and number of executed sweeps by function cusolverDnXsyevjGetSweeps .
However the user needs to be aware that residual is the Frobenius norm of E , not accuracy of individual eigenvalue, i.e.
\({residual}={||E||}_{F} = {{||}\Lambda - W{||}}_{F}\) The same as syevd , the user has to provide working space pointed by input parameter work .
The input parameter lwork is the size of the working space, and it is returned by syevj_bufferSize() .
API of syevj Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
On exit, if jobz = CUSOLVER_EIG_MODE_VECTOR , and info = 0, A contains the orthonormal eigenvectors of the matrix A .
params host in/out Structure filled with parameters of Jacobi algorithm and results of syevj .
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( nsygvj()  The helper functions below can calculate the sizes needed for pre-allocated buffer.
cusolverStatus_t cusolverDnSsygvj ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , float * A , int lda , float * B , int ldb , float * W , float * work , int lwork , int * info , syevjInfo_t params ); cusolverStatus_t cusolverDnDsygvj ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , double * A , int lda , double * B , int ldb , double * W , double * work , int lwork , int * info , syevjInfo_t params ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnChegvj ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuComplex * A , int lda , cuComplex * B , int ldb , float * W , cuComplex * work , int lwork , int * info , syevjInfo_t params ); cusolverStatus_t cusolverDnZhegvj ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , cuDoubleComplex * B , int ldb , double * W , cuDoubleComplex * work , int lwork , int * info , syevjInfo_t params ); This function computes eigenvalues and eigenvectors of a symmetric (Hermitian) n×n matrix-pair ( A , B ).
The generalized symmetric-definite eigenvalue problem is \({eig(A,B)} = \left\{ \begin{matrix} {A*V = B*V*\Lambda} & {\text{if }\textsf{itype = CUSOLVER_EIG_TYPE_1}} \\ {A*B*V = V*\Lambda} & {\text{if }\textsf{itype = CUSOLVER_EIG_TYPE_2}} \\ {B*A*V = V*\Lambda} & {\text{if }\textsf{itype = CUSOLVER_EIG_TYPE_3}} \\ \end{matrix}  ight.\) where the matrix B is positive definite.
The eigenvectors are normalized as follows: \(\left\{ \begin{matrix} {V^{H}*B*V = I} & {\text{if }\textsf{itype = CUSOLVER_EIG_TYPE_1, CUSOLVER_EIG_TYPE_2}} \\ {V^{H}*{inv(B)}*V = I} & {\text{if }\textsf{itype = CUSOLVER_EIG_TYPE_3}} \\ \end{matrix}  ight.\) This function has the same functionality as sygvd except that syevd in sygvd is replaced by syevj in sygvj .
Therefore, sygvj inherits properties of syevj , the user can use cusolverDnXsyevjSetTolerance and cusolverDnXsyevjSetMaxSweeps to configure tolerance and maximum sweeps.
sygvj first computes Cholesky factorization of matrix B , \(B = L*L^{H}\) transform the problem to standard eigenvalue problem, then calls syevj .
For example, the standard eigenvalue problem of type I is \(M*Q = Q*\Lambda\) where matrix M is symmetric \(M = L^{-1}*A*L^{-H}\) The residual is the result of syevj on matrix M , not A .
The input parameter lwork is the size of the working space, and it is returned by sygvj_bufferSize() .
On exit, if info is less than n , B is overwritten by triangular factor U or L from the Cholesky factorization of B .
If info = i (> 0) , info indicates either B is not positive definite or syevj (called by sygvj ) does not converge.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( nsyevjBatched()  The helper functions below can calculate the sizes needed for pre-allocated buffer.
cusolverStatus_t cusolverDnSsyevjBatched ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , float * A , int lda , float * W , float * work , int lwork , int * info , syevjInfo_t params , int batchSize ); cusolverStatus_t cusolverDnDsyevjBatched ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , double * A , int lda , double * W , double * work , int lwork , int * info , syevjInfo_t params , int batchSize ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverDnCheevjBatched ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuComplex * A , int lda , float * W , cuComplex * work , int lwork , int * info , syevjInfo_t params , int batchSize ); cusolverStatus_t cusolverDnZheevjBatched ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , double * W , cuDoubleComplex * work , int lwork , int * info , syevjInfo_t params , int batchSize ); This function computes eigenvalues and eigenvectors of a sequence of symmetric (Hermitian) n×n matrices \(A_{j}*Q_{j} = Q_{j}*\Lambda_{j}\) where \(\Lambda_{j}\) is a real n×n diagonal matrix.
The diagonal elements of \(\Lambda_{j}\) are the eigenvalues of \(A_{j}\) in either ascending order or non-sorting order.
It requires that all matrices are of the same size n and are packed in contiguous way, \(A = \begin{pmatrix} {A0} & {A1} & \cdots \\ \end{pmatrix}\) Each matrix is column-major with leading dimension lda , so the formula for random access is \(A_{k}\operatorname{(i,j)} = {A\lbrack\ i\ +\ lda*j\ +\ lda*n*k brack}\) .
The parameter W also contains eigenvalues of each matrix in contiguous way, \(W = \begin{pmatrix} {W0} & {W1} & \cdots \\ \end{pmatrix}\) The formula for random access of W is \(W_{k}\operatorname{(j)} = {W\lbrack\ j\ +\ n*k brack}\) .
Except for tolerance and maximum sweeps, syevjBatched can either sort the eigenvalues in ascending order (default) or chose as-is (without sorting) by the function cusolverDnXsyevjSetSortEig .
If the user packs several tiny matrices into diagonal blocks of one matrix, non-sorting option can separate spectrum of those tiny matrices.
syevjBatched cannot report residual and executed sweeps by function cusolverDnXsyevjGetResidual and cusolverDnXsyevjGetSweeps .
The input parameter lwork is the size of the working space, and it is returned by syevjBatched_bufferSize() .
Otherwise, if info[i] = n+1 , syevjBatched does not converge on i-th matrix under given tolerance and maximum sweeps.
If jobz = CUSOLVER_EIG_MODE_VECTOR, \(A_{j}\) contains the orthonormal eigenvectors \(V_{j}\) .
API of syevjBatched Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
If uplo = CUBLAS_FILL_MODE_UPPER , the leading n-by-n upper triangular part of Aj contains the upper triangular part of the matrix Aj .
If uplo = CUBLAS_FILL_MODE_LOWER , the leading n-by-n lower triangular part of Aj contains the lower triangular part of the matrix Aj .
On exit, if jobz = CUSOLVER_EIG_MODE_VECTOR , and info[j] = 0, Aj contains the orthonormal eigenvectors of the matrix Aj .
If info[i] = n+1 , syevjBatched does not converge on i-th matrix under given tolerance and maximum sweeps.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( n array of dimension lda * n with lda is not less than max(1,m) .
workspaceInBytesOnDevice host input Size in bytes of bufferOnDevice , returned by cusolverDnXgetrf_bufferSize .
workspaceInBytesOnHost host input Size in bytes of bufferOnHost , returned by cusolverDnXgetrf_bufferSize .
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,n array of dimension ldb * nrhs with ldb is not less than max(1,n) .
The generic API has two different types: dataTypeA is data type of the matrix A and dataTypeB is data type of the matrix B .
cusolverDnXgetrs only supports the following four combinations: Valid combination of data type and compute type DataTypeA dataTypeB Meaning CUDA_R_32F CUDA_R_32F SGETRS CUDA_R_64F CUDA_R_64F DGETRS CUDA_C_32F CUDA_C_32F CGETRS CUDA_C_64F CUDA_C_64F ZGETRS Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( n= k scenario is supported.
API of larft Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
direct host input Specifies the order in which the elementary reflectors are multiplied to form the block reflector.
storev host input Specifies how the vectors which define the elementary reflectors are stored.
k host input The order of the triangular factor T (= the number of elementary reflectors).
If direct == CUBLAS_DIRECT_FORWARD , T is upper triangular; if direct == CUBLAS_DIRECT_BACKWARD , T is lower triangular.
workspaceInBytesOnDevice host input Size in bytes of bufferOnDevice , returned by cusolverDnXlarft_bufferSize .
workspaceInBytesOnHost host input Size in bytes of bufferOnHost , returned by cusolverDnXlarft_bufferSize .
The generic API has four different types: dataTypeV is data type of the array V dataTypeTau is data type of the array tau dataTypeT is data type of the array T computeType is compute type of the operation cusolverDnXlarft only supports the following four combinations.
Valid combinations of data types and compute types DataTypeV DataTypeTau DataTypeT ComputeType Meaning CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F SLARFT CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F DLARFT CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CLARFT CUDA_C_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F ZLARFT Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( n == 0 , k > n , or storev == CUBLAS_STOREV_ROWWISE ).
CUSOLVER_STATUS_INTERNAL_ERROR An internal operation failed. 2.4.5. Dense Eigenvalue Solver Reference (64-bit API)  This section describes eigenvalue solver API of cuSolverDN, including bidiagonalization and SVD.
2.4.5.1. cusolverDnXgesvd()  The helper functions below can calculate the sizes needed for pre-allocated buffer.
The user has to provide device and host working spaces which are pointed by input parameters bufferOnDevice and bufferOnHost .
The input parameters workspaceInBytesOnDevice (and workspaceInBytesOnHost ) is size in bytes of the device (and host) working space, and it is returned by cusolverDnXgesvd_bufferSize() .
if bdsqr did not converge, info specifies how many superdiagonals of an intermediate bidiagonal form did not converge to zero.
Table of algorithms supported by cusolverDnXgesvd CUSOLVER_ALG_0 or NULL Default algorithm.
List of input arguments for cusolverDnXgesvd_bufferSize and cusolverDnXgesvd : API of cusolverDnXgesvd Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
workspaceInBytesOnDevice host input Size in bytes of bufferOnDevice , returned by cusolverDnXgesvd_bufferSize .
workspaceInBytesOnHost host input Size in bytes of bufferOnHost , returned by cusolverDnXgesvd_bufferSize .
If info > 0 , info indicates how many superdiagonals of an intermediate bidiagonal form did not converge to zero.
Remark 2: the routine returns V , not \(V^{H}\) List of input arguments for cusolverDnXgesvdp_bufferSize and cusolverDnXgesvdp : API of cusolverDnXgesvdp Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
jobz host input Specifies options to either compute singular values only or compute singular vectors as well: jobz = CUSOLVER_EIG_MODE_NOVECTOR : Compute singular values only.
workspaceInBytesOnDevice host input Size in bytes of bufferOnDevice , returned by cusolverDnXgesvdp_bufferSize .
workspaceInBytesOnHost host input Size in bytes of bufferOnHost , returned by cusolverDnXgesvdp_bufferSize .
The generic API has three different types, dataTypeA is data type of the matrix A , dataTypeS is data type of the vector S and dataTypeU is data type of the matrix U , dataTypeV is data type of the matrix V , computeType is compute type of the operation.
cusolverDnXgesvdp only supports the following four combinations: Valid combination of data type and compute type DataTypeA DataTypeS DataTypeU DataTypeV ComputeType Meaning CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F SGESVDP CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F DGESVDP CUDA_C_32F CUDA_R_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CGESVDP CUDA_C_64F CUDA_R_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F ZGESVDP Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully.
Remark 2: the routine returns V , not \(V^{H}\) List of input arguments for cusolverDnXgesvdr_bufferSize and cusolverDnXgesvdr : API of cusolverDnXgesvdr Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context.
jobu host input Specifies options for computing all or part of the matrix U : = ‘S’: the first k columns of U (the left singular vectors) are returned in the array U; = ‘N’: no columns of U (no left singular vectors) are computed.
jobv host input Specifies options for computing all or part of the matrix V: = ‘S’: the first k rows of V (the right singular vectors) are returned in the array V; = ‘N’: no rows of V (no right singular vectors) are computed.
workspaceInBytesOnDevice host input Size in bytes of bufferOnDevice , returned by cusolverDnXgesvdr_bufferSize .
workspaceInBytesOnHost host input Size in bytes of bufferOnHost , returned by cusolverDnXgesvdr_bufferSize .
The generic API has five different types, dataTypeA is data type of the matrix A , dataTypeS is data type of the vector S and dataTypeU is data type of the matrix U , dataTypeV is data type of the matrix V , computeType is compute type of the operation.
Valid combination of data type and compute type DataTypeA DataTypeS DataTypeU DataTypeV ComputeType Meaning CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F SGESVDR CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F DGESVDR CUDA_C_32F CUDA_R_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CGESVDR CUDA_C_64F CUDA_R_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F ZGESVDR Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,n 0) , info indicates i off-diagonal elements of an intermediate tridiagonal form did not converge to zero.
The generic API has three different types, dataTypeA is data type of the matrix A , dataTypeW is data type of the matrix W and computeType is compute type of the operation.
0 0) , info indicates i off-diagonal elements of an intermediate tridiagonal form did not converge to zero.
cusolverDnXsyevdx only supports the following four combinations: Valid combination of data type and compute type DataTypeA DataTypeW ComputeType Meaning CUDA_R_32F CUDA_R_32F CUDA_R_32F SSYEVDX CUDA_R_64F CUDA_R_64F CUDA_R_64F DSYEVDX CUDA_C_32F CUDA_R_32F CUDA_C_32F CHEEVDX CUDA_C_64F CUDA_R_64F CUDA_C_64F ZHEEVDX Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully.
If matrix A is symmetric/Hermitian and only lower/upper part is used or meaningful, the user has to extend the matrix into its missing upper/lower part, otherwise the result would be wrong.
The linear system is solved by sparse LU with partial pivoting: \(P*A = L*U\) cusolver library provides three reordering schemes, symrcm symamd , and csrmetisnd to reduce zero fill-in which dramatically affects the performance of LU factorization.
The input parameter reorder can enable symrcm ( symamd or csrmetisnd ) if reorder is 1 (2, or 3), otherwise, no reordering is performed.
If reorder is nonzero, csrlsvlu does \(P*A*Q^{T} = L*U\) where \(Q = {symrcm}(A + A^{T})\) .
If A is singular under given tolerance ( max(tol,0) ), then some diagonal elements of U is zero, i.e.
\({|U(j,j)|} array of nnzA \(( =\) csrRowPtrA(n) \(-\) csrRowPtrA(0) \()\) nonzero elements of matrix A .
csrRowPtrA device host Integer array of n \(+ 1\) elements that contains the start of every row and the end of the last row plus one.
csrColIndA device host Integer array of nnzA \(( =\) csrRowPtrA(n) \(-\) csrRowPtrA(0) \()\) column indices of the nonzero elements of matrix A .
Output Parameter cusolverSp MemSpace *Host MemSpace Description x device host Solution vector of size n , x = inv(A)*b.
Otherwise, first index j such that U(j,j)≈0 Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully.
The linear system is solved by sparse QR factorization, \(A\ =\ Q*R\) If A is singular under given tolerance ( max(tol,0) ), then some diagonal elements of R is zero, i.e.
\({|R(j,j)|} array of nnz \(( =\) csrRowPtrA(m) \(-\) csrRowPtrA(0) \()\) nonzero elements of matrix A .
csrRowPtrA device host Integer array of m \(+ 1\) elements that contains the start of every row and the end of the last row plus one.
csrColIndA device host Integer array of nnz \(( =\) csrRowPtrA(m) \(-\) csrRowPtrA(0) \()\) column indices of the nonzero elements of matrix A .
Output Parameter cusolverSp MemSpace *Host MemSpace Description x device host Solution vector of size m , x = inv(A)*b.
Otherwise, first index j such that R(j,j)≈0 Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully.
The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL and upper triangular part of A is ignored (if parameter reorder is zero).
In other words, suppose input matrix A is decomposed as \(A = L + D + U\) , where L is lower triangular, D is diagonal and U is upper triangular.
The function would ignore U and regard A as a symmetric matrix with the formula \(A = L + D + L^{H}\) .
If parameter reorder is nonzero, the user has to extend A to a full matrix, otherwise the solution would be wrong.
The linear system is solved by sparse Cholesky factorization, \(A = G*G^{H}\) where G is the Cholesky factor, a lower triangular matrix.
The output parameter singularity has two meanings: If A is not positive definite, there exists some integer k such that A(0:k, 0:k) is not positive definite.
there exists some integer k such that \(G\begin{pmatrix} {k,k} \\ \end{pmatrix} array of nnz \(( =\) csrRowPtrA(m) \(-\) csrRowPtrA(0) \()\) nonzero elements of matrix A .
Output Parameter cusolverSp MemSpace *Host MemSpace Description x device host Solution vector of size m , x = inv(A)*b.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,nnzcsrlsqvqr()  The S and D data types are real valued single and double precision, respectively.
cusolverStatus_t cusolverSpScsrlsqvqr [ Host ]( cusolverSpHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , const float * csrValA , const int * csrRowPtrA , const int * csrColIndA , const float * b , float tol , int * rankA , float * x , int * p , float * min_norm ); cusolverStatus_t cusolverSpDcsrlsqvqr [ Host ]( cusolverSpHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , const double * csrValA , const int * csrRowPtrA , const int * csrColIndA , const double * b , double tol , int * rankA , double * x , int * p , double * min_norm ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverSpCcsrlsqvqr [ Host ]( cusolverSpHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , const cuComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , const cuComplex * b , float tol , int * rankA , cuComplex * x , int * p , float * min_norm ); cusolverStatus_t cusolverSpZcsrlsqvqr [ Host ]( cusolverSpHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , const cuDoubleComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , const cuDoubleComplex * b , double tol , int * rankA , cuDoubleComplex * x , int * p , double * min_norm ); This function solves the following least-square problem: \(x = {argmin}{||}A*z - b{||}\) A is an m×n sparse matrix that is defined in CSR storage format by the three arrays csrValA , csrRowPtrA , and csrColIndA .
b is the right-hand-side vector of size m , and x is the least-square solution vector of size n .
If A is square, symmetric/Hermitian and only lower/upper part is used or meaningful, the user has to extend the matrix into its missing upper/lower part, otherwise the result is wrong.
This function only works if m is greater or equal to n , in other words, A is a tall matrix.
The least-square problem is solved by sparse QR factorization with column pivoting, \(A*P^{T} = Q*R\) If A is of full rank (i.e.
Suppose rank of A is k , less than n , the permutation matrix P reorders columns of A in the following sense: \(A*P^{T} = \begin{pmatrix} A_{1} & A_{2} \\ \end{pmatrix} = \begin{pmatrix} Q_{1} & Q_{2} \\ \end{pmatrix}\begin{pmatrix} R_{11} & R_{12} & R_{22} \\ \end{pmatrix}\) where \(R_{11}\) and A have the same rank, but \(R_{22}\) is almost zero, i.e.
The absolute value of every entry in \(R_{22}\) is less than or equal to tolerance=max(tol,0) .
Suppose \(y = P*x\) and \(c = Q^{H}*b\) , the least square problem can be reformed by \(\left.
\min||A*x - b|| = \min||R*y - c  ight.||\) or in matrix form \(\begin{pmatrix} R_{11} & R_{12} & R_{22} \\ \end{pmatrix}\begin{pmatrix} y_{1} \\ y_{2} \\ \end{pmatrix} = \begin{pmatrix} c_{1} \\ c_{2} \\ \end{pmatrix}\) The output parameter min_norm is \(\left.
\min||y  ight.|| \\ {{subject\ to}R_{11}*y_{1} + R_{12}*y_{2} = c_{1}} \\ \end{matrix}\) Or equivalently another least-square problem min|| R 1 1 \ R 1 2 I * y 2 - R 1 1 \ c 1 O || The output parameter x is \(P^{T}*y\) , the solution of least-square problem.
Input Parameter cusolverSp MemSpace *Host MemSpace Description handle host host Handle to the cuSolver library context.
Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE .
csrValA device host array of nnz \(( =\) csrRowPtrA(m) \(-\) csrRowPtrA(0) \()\) nonzero elements of matrix A .
Output Parameter cusolverSp MemSpace *Host MemSpace Description rankA host host Numerical rank of A .
p device host A vector of size n , which represents the permutation matrix P satisfying A*P^T=Q*R.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,n,nnzcsreigvsi()  The S and D data types are real valued single and double precision, respectively.
cusolverStatus_t cusolverSpScsreigvsi [ Host ]( cusolverSpHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const float * csrValA , const int * csrRowPtrA , const int * csrColIndA , float mu0 , const float * x0 , int maxite , float tol , float * mu , float * x ); cusolverStatus_t cusolverSpDcsreigvsi [ Host ]( cusolverSpHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const double * csrValA , const int * csrRowPtrA , const int * csrColIndA , double mu0 , const double * x0 , int maxite , double tol , double * mu , double * x ); The C and Z data types are complex valued single and double precision, respectively.
cusolverStatus_t cusolverSpCcsreigvsi [ Host ]( cusolverSpHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const cuComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , cuComplex mu0 , const cuComplex * x0 , int maxite , float tol , cuComplex * mu , cuComplex * x ); cusolverStatus_t cusolverSpZcsreigvsi ( cusolverSpHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const cuDoubleComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , cuDoubleComplex mu0 , const cuDoubleComplex * x0 , int maxite , double tol , cuDoubleComplex * mu , cuDoubleComplex * x ); This function solves the simple eigenvalue problem \(A*x = \lambda*x\) by shift-inverse method.
A is an m×m sparse matrix that is defined in CSR storage format by the three arrays csrValA , csrRowPtrA , and csrColIndA .
The output parameter x is the approximated eigenvector of size m , The following shift-inverse method corrects eigenpair step-by-step until convergence.
The shift-inverse method will converge to the eigenvalue mu nearest mu0 if mu is a singleton.
It is useful when shift-inverse method does not converge because the tolerance is too small or the desired eigenvalue is not a singleton.
Shift-Inverse Method Given a initial guess of eigenvalue μ0 and initial vector x0 x (0) = x0 of unit length for j = 0 : maxite solve ( A - μ0 * I ) * x (k+1) = x (k) normalize x (k+1) to unit length compute approx.
eigenvalue μ = x H * A * x where x = x (k+1) if || A * x (k+1) - μ * x (k+1) || array of nnz \(( =\) csrRowPtrA(m) \(-\) csrRowPtrA(0) \()\) nonzero elements of matrix A .
Output Parameter cusolverSp MemSpace *Host MemSpace Description mu device host Approximated eigenvalue nearest mu0 under tolerance.
This number may not be accurate due to several reasons: The contour C is close to some eigenvalues or even passes through some eigenvalues.
Even though csreigs may not be accurate, it still can give the user some idea how many eigenvalues in a region where the resolution of disk theorem is bad.
For example, standard 3-point stencil of finite difference of Laplacian operator is a tridiagonal matrix, and disk theorem would show “all eigenvalues are in the interval [0, 4*N^2]” where N is number of grids.
Input Parameter cusolverSp MemSpace *Host MemSpace Description handle host host Handle to the cuSolverSP library context.
Output Parameter cusolverSp MemSpace *Host MemSpace Description num_eigs host host Number of algebraic eigenvalues in a box.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,nnz array of nnzA \(( =\) csrRowPtrA(m) \(-\) csrRowPtrA(0) \()\) nonzero elements of matrix A .
csrRowPtrA host Integer array of n+1 elements that contains the start of every row and the end of the last row plus one.
csrColIndA host Integer array of nnzA column indices of the nonzero elements of matrix A .
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( n,nnzA array of nnzA*batchSize nonzero elements of matrices A0, A1, ... . All matrices are aggregated one after another.
csrRowPtrA device Integer array of m+1 elements that contains the start of every row and the end of the last row plus one.
csrColIndA device Integer array of nnzA column indices of the nonzero elements of each matrix Aj .
b device array of m*batchSize of right-hand-side vectors b0, b1, ... . All vectors are aggregated one after another.
pBuffer device Buffer allocated by the user, the size is returned by cusolverSpXcsrqrBufferInfoBatched() .
Output Parameter cusolverSp MemSpace Description x device array of m*batchSize of solution vectors x0, x1, ... . internalDataInBytes host Number of bytes of the internal data.
boost host output The value which is substituted for zero pivot (if the later is flagged). 2.6.11. cusolverRfGetNumericBoostReport()  cusolverStatus_t cusolverRfGetNumericBoostReport ( cusolverRfHandle_t handle , cusolverRfNumericBoostReport_t * report ); This routine gets the report whether numeric boosting was used in the cusolverRfRefactor() and cusolverRfSolve() routines.
report host output The enumerated boosting report type. 2.6.12. cusolverRfGetResetValuesFastMode()  cusolverStatus_t cusolverRfGetResetValuesFastMode ( cusolverRfHandle_t handle , cusolverRfResetValuesFastMode_t * fastMode ); This routine gets the mode used in the cusolverRfResetValues routine.
fastMode host output The enumerated mode type. 2.6.13. cusolverRfGet_Algs()  cusolverStatus_t cusolverRfGet_Algs ( cusolverRfHandle_t handle , cusolverRfFactorization_t * fact_alg , cusolverRfTriangularSolve_t * solve_alg ); This routine gets the algorithm used for the refactorization in cusolverRfRefactor() and the triangular solve in cusolverRfSolve() .
alg host output The enumerated algorithm type. 2.6.14. cusolverRfRefactor()  cusolverStatus_t cusolverRfRefactor ( cusolverRfHandle_t handle ); This routine performs the LU re-factorization: \(A = L*U\) exploring the available parallelism on the GPU.
It is assumed that a prior call to the cusolverRfAnalyze() was done in order to find the available parallelism.
This routine may be called multiple times, once for each of the linear systems: \(A_{i}x_{i} = f_{i}\) There are some constraints to the combination of algorithms used for refactorization and solving routines, cusolverRfRefactor() and cusolverRfSolve() .
The table below summarizes the supported combinations of algorithms: Compatible algorithms for solving and refactorization routines.
Factorization Solving CUSOLVERRF_FACTORIZATION_ALG0 TRIANGULAR_SOLVE_ALG1 CUSOLVERRF_FACTORIZATION_ALG1 TRIANGULAR_SOLVE_ALG2, TRIANGULAR_SOLVE_ALG3 CUSOLVERRF_FACTORIZATION_ALG2 TRIANGULAR_SOLVE_ALG2, TRIANGULAR_SOLVE_ALG3 Parameter MemSpace In/out Meaning handle host in/out The handle to the cuSolverRF library.
CUSOLVER_STATUS_ZERO_PIVOT A zero pivot was encountered during the computation. 2.6.15. cusolverRfResetValues()  cusolverStatus_t cusolverRfResetValues ( /* Input (in the device memory) */ int n , int nnzA , int * csrRowPtrA , int * csrColIndA , double * csrValA , int * P , int * Q , /* Output */ cusolverRfHandle_t handle ); This routine updates internal data structures with the values of the new coefficient matrix.
It is assumed that the arrays csrRowPtrA , csrColIndA , P and Q have not changed since the last call to the cusolverRfSetup[Host|Device] routine.
This assumption reflects the fact that the sparsity pattern of coefficient matrices as well as reordering to minimize fill-in and pivoting remain the same in the set of linear systems: \(A_{i}x_{i} = f_{i}\) This routine may be called multiple times, once for each of the linear systems: \(A_{i}x_{i} = f_{i}\) Parameter MemSpace In/out Meaning n host input The number of rows (and columns) of matrix A .
csrRowPtrA device input The array of offsets corresponding to the start of each row in the arrays csrColIndA and csrValA .
This array has also an extra entry at the end that stores the number of non-zero elements in the matrix.
csrColIndA device input The array of column indices corresponding to the non-zero elements in the matrix.
csrValA device input The array of values corresponding to the non-zero elements in the matrix.
CUSOLVER_STATUS_INVALID_VALUE An unsupported value or parameter was passed. 2.6.16. cusolverRfSetMatrixFormat()  cusolverStatus_t cusolverRfSetMatrixFormat ( cusolverRfHandle_t handle , cusolverRfMatrixFormat_t format , cusolverRfUnitDiagonal_t diag ); This routine sets the matrix format used in the cusolverRfSetupDevice() , cusolverRfSetupHost() , cusolverRfResetValues() , cusolverRfExtractBundledFactorsHost() and cusolverRfExtractSplitFactorsHost() routines.
It may be called once prior to cusolverRfSetupDevice() and cusolverRfSetupHost() routines.
CUSOLVER_STATUS_INVALID_VALUE An enumerated mode parameter is wrong. 2.6.17. cusolverRfSetNumericProperties()  cusolverStatus_t cusolverRfSetNumericProperties ( cusolverRfHandle_t handle , double zero , double boost ); This routine sets the numeric values used for checking for ‘’zero’’ pivot and for boosting it in the cusolverRfRefactor() and cusolverRfSolve() routines.
It may be called multiple times prior to cusolverRfRefactor() and cusolverRfSolve() routines.
boost host input The value which is substituted for zero pivot (if the later is flagged). 2.6.18. cusolverRfSetResetValuesFastMode()  cusolverStatus_t cusolverRfSetResetValuesFastMode ( cusolverRfHandle_t handle , cusolverRfResetValuesFastMode_t fastMode ); This routine sets the mode used in the cusolverRfResetValues routine.
The fast mode requires extra memory and is recommended only if very fast calls to cusolverRfResetValues() are needed.
fastMode host input The enumerated mode type. 2.6.19. cusolverRfSetAlgs()  cusolverStatus_t cusolverRfSetAlgs ( cusolverRfHandle_t handle , cusolverRfFactorization_t fact_alg , cusolverRfTriangularSolve_t alg ); This routine sets the algorithm used for the refactorization in cusolverRfRefactor() and the triangular solve in cusolverRfSolve() .
alg host input The enumerated algorithm type. 2.6.20. cusolverRfSolve()  cusolverStatus_t cusolverRfSolve ( /* Input (in the device memory) */ cusolverRfHandle_t handle , int * P , int * Q , int nrhs , double * Temp , int ldt , /* Input/Output (in the device memory) */ double * XF , /* Input */ int ldxf ); This routine performs the forward and backward solve with the lower \(L\in R^{nxn}\) and upper \(U\in R^{nxn}\) triangular factors resulting from the LU re-factorization: \(A = L*U\) which is assumed to have been computed by a prior call to the cusolverRfRefactor() routine.
The routine can solve linear systems with multiple right-hand-sides (RHS): \(AX = {(LU)}X = L{(UX)} = LY = F~{where}~UX = Y\) even though currently only a single RHS is supported.
This routine may be called multiple times, once for each of the linear systems: \(A_{i}x_{i} = f_{i}\) Parameter MemSpace In/out Meaning handle host output The handle to the cuSolverRF library.
XF device in/out The dense matrix that contains the right-hand-sides F and solutions X (of size ldxf*nrhs ).
ldxf host input The leading dimension of dense matrix XF ( ldxf >= n ). 2.6.21. cusolverRfBatchSetupHost()  cusolverStatus_t cusolverRfBatchSetupHost ( /* Input (in the host memory) */ int batchSize , int n , int nnzA , int * h_csrRowPtrA , int * h_csrColIndA , double * h_csrValA_array [], int nnzL , int * h_csrRowPtrL , int * h_csrColIndL , double * h_csrValL , int nnzU , int * h_csrRowPtrU , int * h_csrColIndU , double * h_csrValU , int * h_P , int * h_Q , /* Output */ cusolverRfHandle_t handle ); This routine assembles the internal data structures of the cuSolverRF library for batched operation.
It is called after the call to the cusolverRfCreate() routine, and before any other batched routines.
The batched operation assumes that the user has the following linear systems: \(A_{j}x_{j} = b_{j}{,\ j\ =\ 1,2,...,\ batchSize}\) where each matrix in the set: \(\{ A_{j}\}\) has the same sparsity pattern, and quite similar such that factorization can be done by the same permutation P and Q .
This routine accepts as input (on the host) the original matrix A (sparsity pattern and batched values), the lower (L) and upper (U) triangular factors, as well as the left (P) and the right (Q) permutations resulting from the full LU factorization of the first (i=1) linear system: \(A_{i}x_{i} = f_{i}\) The permutations P and Q represent the final composition of all the left and right reorderings applied to the original matrix A , respectively.
However, these permutations are often associated with partial pivoting and reordering to minimize fill-in, respectively.
Remark 2: to get best performance, batchSize should be multiple of 32 and greater or equal to 32.
The algorithm is memory-bound, once bandwidth limit is reached, there is no room to improve performance by large batchSize .
In practice, batchSize of 32 - 128 is often enough to obtain good performance, but in some cases larger batchSize might be beneficial.
The following routine needs to be called only once for a single linear system: \(A_{i}x_{i} = f_{i}\) Parameter MemSpace In/out Meaning batchSize host input The number of matrices in the batched mode.
h_csrRowPtrA host input The array of offsets corresponding to the start of each row in the arrays h_csrColIndA and h_csrValA .
h_csrColIndA host input The array of column indices corresponding to the non-zero elements in the matrix.
h_csrValA_array host input Array of pointers of size batchSize , each pointer points to the array of values corresponding to the non-zero elements in the matrix.
h_csrRowPtrL host input The array of offsets corresponding to the start of each row in the arrays h_csrColIndL and h_csrValL .
This array has also an extra entry at the end that stores the number of non-zero elements in the matrix L .
h_csrColIndL host input The array of column indices corresponding to the non-zero elements in the matrix L .
h_csrValL host input The array of values corresponding to the non-zero elements in the matrix L .
h_csrRowPtrU host input The array of offsets corresponding to the start of each row in the arrays h_csrColIndU and h_csrValU .
This array has also an extra entry at the end that stores the number of non-zero elements in the matrix U .
h_csrColIndU host input The array of column indices corresponding to the non-zero elements in the matrix U .
h_csrValU host input The array of values corresponding to the non-zero elements in the matrix U .
CUSOLVER_STATUS_ALLOC_FAILED An allocation of memory failed. 2.6.22. cusolverRfBatchAnalyze()  cusolverStatus_t cusolverRfBatchAnalyze ( cusolverRfHandle_t handle ); This routine performs the appropriate analysis of parallelism available in the batched LU re-factorization.
It is assumed that a prior call to the cusolverRfBatchSetup[Host]() was done in order to create internal data structures needed for the analysis.
The following routine needs to be called only once for a single linear system: \(A_{j}x_{j} = b_{j}{,\ j\ =\ 1,2,...,\ batchSize}\) Parameter Memory In/out Meaning handle host in/out The handle to the cuSolverRF library. 2.6.23. cusolverRfBatchResetValues()  cusolverStatus_t cusolverRfBatchResetValues ( /* Input (in the device memory) */ int batchSize , int n , int nnzA , int * csrRowPtrA , int * csrColIndA , double * csrValA_array [], int * P , int * Q , /* Output */ cusolverRfHandle_t handle ); This routine updates internal data structures with the values of the new coefficient matrix.
It is assumed that the arrays csrRowPtrA , csrColIndA , P and Q have not changed since the last call to the cusolverRfbatch_setup_host routine.
This assumption reflects the fact that the sparsity pattern of coefficient matrices as well as reordering to minimize fill-in and pivoting remain the same in the set of linear systems: \(A_{j}x_{j} = b_{j}{,\ j\ =\ 1,2,...,\ batchSize}\) The input parameter csrValA_array is an array of pointers on device memory.
Parameter MemSpace In/out Meaning batchSize host input The number of matrices in batched mode.
csrValA_array device input Array of pointers of size batchSize , each pointer points to the array of values corresponding to the non-zero elements in the matrix. 2.6.24. cusolverRfBatchRefactor()  cusolverStatus_t cusolverRfBatchRefactor ( cusolverRfHandle_t handle ); This routine performs the LU re-factorization: \(M_{j} = P*A_{j}*Q^{T} = L_{j}*U_{j}\) exploring the available parallelism on the GPU.
It is assumed that a prior call to the cusolverRfBatchAnalyze() was done in order to find the available parallelism.
The user has to call cusolverRfBatchZeroPivot() to know which matrix failed the LU refactorization.
Parameter Memory In/out Meaning handle host in/out The handle to the cuSolverRF library. 2.6.25. cusolverRfBatchSolve()  cusolverStatus_t cusolverRfBatchSolve ( /* Input (in the device memory) */ cusolverRfHandle_t handle , int * P , int * Q , int nrhs , double * Temp , int ldt , /* Input/Output (in the device memory) */ double * XF_array [], /* Input */ int ldxf ); To solve \(A_{j}*x_{j} = b_{j}\) , first we reform the equation by \(M_{j}*Q*x_{j} = P*b_{j}\) where \(M_{j} = P*A_{j}*Q^{T}\) .
Further cusolverRfBatch_Solve() takes over the remaining steps, including: \(z_{j} = P*b_{j}\) \(M_{j}*y_{j} = z_{j}\) \(x_{j} = Q^{T}*y_{j}\) The input parameter XF_array is an array of pointers on device memory.
If some matrix \(A_{j}\) failed the refactorization and \(U_{j}\) has some zero diagonal, backward solve would compute NAN.
The user has to call cusolverRfBatch_Zero_Pivot to check if refactorization is successful or not.
Parameter MemSpace In/out Meaning handle host output The handle to the cuSolverRF library.
XF_array device in/out Array of pointers of size batchSize , each pointer points to the dense matrix that contains the right-hand-sides F and solutions X (of size ldxf*nrhs ). 2.6.26. cusolverRfBatchZeroPivot()  cusolverStatus_t cusolverRfBatchZeroPivot ( /* Input */ cusolverRfHandle_t handle /* Output (in the host memory) */ int * position ); Although \(A_{j}\) is close to each other, it does not mean \(M_{j} = P*A_{j}*Q^{T} = L_{j}*U_{j}\) exists for every j.
The user can query which matrix failed LU refactorization by checking corresponding value in position array.
If position(j) is k >= 0 , matrix \(A_{j}\) is not LU factorizable and its matrix \(U_{j}{(j,j)}\) is zero.
The return value of cusolverRfBatch_Zero_Pivot is CUSOLVER_STATUS_ZERO_PIVOT if there exists one \(A_{j}\) which failed LU refactorization.
The user can redo LU factorization to get new permutation P and Q if error code CUSOLVER_STATUS_ZERO_PIVOT is returned.
The value of position(j) reports singularity of matrix Aj , -1 if no structural/numerical zero, k >= 0 if Aj(k,k) is either structural zero or numerical zero. 3. Using the CUSOLVERMG API  3.1.
It is not a reference for the cuSolverMG API data types and functions; that is provided in subsequent chapters. 3.1.1. Thread Safety  The library is thread-safe only if there is one cuSolverMG context per thread.
3.1.2. Determinism  Currently all cuSolverMG API routines from a given toolkit version generate the same bit-wise results when the following conditions are respected : all GPUs participating to the computation have the same compute-capabilities and the same number of SMs.
The order of GPUs are not important because all have the same compute-capabilities. 3.1.3. Tile Strategy  The tiling strategy of cuSolverMG is compatible with ScaLAPACK.
There are seven columns of tiles, labeled as 0,1,2,3,4,5,6, distributed into three GPUs in a cyclic way, i.e.
For example, GPU 0 has column tile 0, 3, 6 (yellow tiles) and GPU 1 takes column tiles next to GPU 0 (blue tiles).
Not all GPUs have the same number of tiles; in this example, GPU 0 has three tiles, others have only two tiles.
PACKED format aggregates three column tiles in a contiguous memory block while UNPACKED format distributes these three column tiles into different memory blocks.
The only difference between them is that PACKED format can have a big GEMM call instead of three GEMM calls in UNPACKED format.
So theoretically speaking, PACKED format can deliver better performance than UNPACKED format.
In order to achieve maximal performance, the user just needs to choose the proper tile size T_A to partition the matrix, not too small, for example 256 or above is enough.
There is another parameter, called LLD_A , to control the leading dimension of the local matrix in each GPU.
Example of cuSolverMG tiling for 3 GPUs  The processing grid in cuSolverMG is a list of GPU IDs, similar to the process ID in ScaLAPACK .
The former describes three logical devices that are selected to run cuSolverMG routines, and all have the same physical ID, 0.
The current design only accepts 32 logical devices, that is, the length of deviceId is less or equal to 32.
If the user chooses deviceId=1,1,1 , all columns tile are located in GPU 1, this will limit the size of the problem because of memory capacity of one GPU.
Besides, multiGPU routine adds extra overhead on data communication through the off-chip bus, which has a big performance impact if NVLINK is not supported or used.
It would be faster to run on a single GPU instead of running multiGPU version with devices of the same GPU ID. 3.1.4. Global Matrix Versus Local Matrix  Operating a submatrix of the matrix A is simple in dense linear algebra, just shift the pointer to the starting point of the submatrix relative to A.
However it is not simple to operate on a submatrix of a distributed matrix because different starting point of the submatrix changes the distribution of the layout of that submatrix.
Given a distributed matrix A , the user can compute eigenvalues of the submatrix sub(A) by either calling syevd(A, IA, JA) or gathering sub(A) to another distributed matrix B and calling syevd(B, IB=1, JB=1) .
Usage of _bufferSize  There is no cudaMalloc inside cuSolverMG library, so the user must allocate the device workspace explicitly.
The routine xyz_bufferSize is to query the size of workspace of the routine xyz , for example xyz = syevd .
To make the API simple, xyz_bufferSize follows almost the same signature of xyz even it only depends on some parameters, for example, the device pointer is not used to decide the size of workspace.
In such cases, the user can pass a null pointer to xyz_bufferSize without breaking the functionality.
The size is number of elements, not number of bytes. 3.1.6. Synchronization  All routines are in synchronous (blocking call) manner.
For example, if the user has multiple streams to set up the matrix, stream synchronization or device synchronization is necessary to guarantee the distributed matrix is ready. 3.1.7. Context Switch  The user does not need to restore the device by cudaSetDevice() after each cuSolverMG call.
All routines set the device back to what the caller has. 3.1.8. NVLINK  The peer-to-peer communication via NVLINK can dramatically reduce the overhead of data exchange among GPUs.
cuSolverMG does not enable NVLINK implicitly, instead, it gives this option back to the user, not to interfere with other libraries.
The example code H.1 shows how to enable peer-to-peer communication. 3.2. cuSolverMG Types Reference  3.2.1.
cuSolverMG Types  The float , double , cuComplex , and cuDoubleComplex data types are supported.
In addition, cuSolverMG uses some familiar types from cuBLAS. 3.2.2. cusolverMgHandle_t  This is a pointer type to an opaque cuSolverMG context, which the user must initialize by calling cusolverMgCreate() prior to calling any other library function.
An un-initialized handle object will lead to unexpected behavior, including crashes of cuSolverMG.
The handle created and returned by cusolverMgCreate() must be passed to every cuSolverMG function. 3.2.3. cusolverMgGridMapping_t  The type indicates layout of grids.
CUDALIBMG_GRID_MAPPING_COL_MAJOR Column-major ordering. 3.2.4. cudaLibMgGrid_t  Opaque structure of the distributed grid.
cusolverMgCreate()  cusolverStatus_t cusolverMgCreate ( cusolverMgHandle_t * handle ) This function initializes the cuSolverMG library and creates a handle on the cuSolverMG context.
Status Returned CUSOLVER_STATUS_SUCCESS The initialization succeeded. 3.3.2. cusolverMgDestroy()  cusolverStatus_t cusolverMgDestroy ( cusolverMgHandle_t handle ) This function releases CPU-side resources used by the cuSolverMG library.
Status Returned CUSOLVER_STATUS_SUCCESS The shutdown succeeded. 3.3.3. cusolverMgDeviceSelect()  cusolverStatus_t cusolverMgDeviceSelect ( cusolverMgHandle_t handle , int nbDevices , int deviceId [] ) This function registers a subset of devices (GPUs) to cuSolverMG handle.
If the user sets deviceId=0,0,0 , then cuSolverMG treats them as three independent GPUs, one stream each, so concurrent kernel launches still hold.
CUSOLVER_STATUS_INVALID_VALUE nbDevices must be greater than zero, and less or equal to 32.
CUSOLVER_STATUS_INTERNAL_ERROR Internal error occurred when setting internal streams and events. 3.3.4. cusolverMgCreateDeviceGrid()  cusolverStatus_t cusolverMgCreateDeviceGrid ( cusolverMgGrid_t * grid , int32_t numRowDevices , int32_t numColDevices , const int32_t deviceId [], cusolverMgGridMapping_t mapping ) This function sets up a grid of devices.
WARNING: cusolverMgCreateDeviceGrid() must be consistent with cusolverMgDeviceSelect() , i.e.
numRowDevices is not 1. 3.3.5. cusolverMgDestroyGrid()  cusolverStatus_t cusolverMgDestroyGrid ( cusolverMgGrid_t grid ) This function releases resources of a grid.
Parameter MemSpace In/out Meaning grid host input/output The pointer to the opaque structure. 3.3.6. cusolverMgCreateMatDescr()  cusolverStatus_t cusolverMgCreateMatrixDesc ( cusolverMgMatrixDesc_t * desc , int64_t numRows , int64_t numCols , int64_t rowBlockSize , int64_t colBlockSize , cudaDataType_t dataType , const cusolverMgGrid_t grid ) This function sets up the matrix descriptor desc .
CUSOLVER_STATUS_INVALID_VALUE numRows , numCols , or rowBlockSize or colBlockSize is less than 0.
numRows is not equal to rowBlockSize . 3.3.7. cusolverMgDestroyMatrixDesc()  cusolverStatus_t cusolverMgDestroyMatrixDesc ( cusolverMgMatrixDesc_t desc ) This function releases the matrix descriptor desc .
Parameter Memory In/out Meaning desc host input/output The matrix descriptor. 3.4. Dense Linear Solver Reference  This section describes the linear solver API of cuSolverMG.
3.4.1. cusolverMgPotrf()  The following helper function can calculate the sizes needed for pre-allocated buffer for cusolverMgPotrf : cusolverStatus_t cusolverMgPotrf_bufferSize ( cusolverMgHandle_t handle , cublasFillMode_t uplo , int N , void * array_d_A [], int IA , int JA , cudaLibMgMatrixDesc_t descrA , cudaDataType computeType , int64_t * lwork ) The following routine: cusolverStatus_t cusolverMgPotrf ( cusolverMgHandle_t handle , cublasFillMode_t uplo , int N , void * array_d_A [], int IA , int JA , cudaLibMgMatrixDesc_t descrA , cudaDataType computeType , void * array_d_work [], int64_t lwork , int * info ) computes the Cholesky factorization of a Hermitian positive-definite matrix using the generic API interface.
If input parameter uplo is CUBLAS_FILL_MODE_LOWER , only lower triangular part of A is processed, and replaced by lower triangular Cholesky factor L : \(A = L*L^{H}\) If input parameter uplo is CUBLAS_FILL_MODE_UPPER , only upper triangular part of A is processed, and replaced by upper triangular Cholesky factor U : \(A = U^{H}*U\) The user has to provide device working space in array_d_work .
The size of array_d_work[j] is lwork which is the number of elements per device, returned by cusolverMgPotrf_bufferSize() .
The output parameter info would indicate smallest leading minor of A which is not positive definite.
The generic API has two different types, dataTypeA is data type of the matrix A , and computeType is compute type of the operation and data type of the workspace ( array_d_work ) descrA contains dataTypeA , so there is no explicit parameter of dataTypeA .
valid combination of data type and compute type DataTypeA ComputeType Meaning CUDA_R_32F CUDA_R_32F SPOTRF CUDA_R_64F CUDA_R_64F DPOTRF CUDA_C_32F CUDA_C_32F CPOTRF CUDA_C_64F CUDA_C_64F ZPOTRF API of potrf Parameter Memory In/out Meaning handle host input Handle to the cuSolverMg library context.
JA host input The column index in the global array A indicating the first column of sub(A) .
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( M,N array containing sub(A) of dimension M * N .
JB host input The column index in the global array B indicating the first column of sub(B) .
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( M,N array containing sub(A) of dimension N * N .
On exit, sub(A) contains the upper or lower triangular part of the inverse of A depending on the value of uplo argument.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( N array containing sub(A) of dimension N * N .
If uplo = CUBLAS_FILL_MODE_UPPER , the leading N-by-N upper triangular part of sub(A) contains the upper triangular part of the matrix sub(A) .
If uplo = CUBLAS_FILL_MODE_LOWER , the leading N-by-N lower triangular part of sub(A) contains the lower triangular part of the matrix sub(A) .
On exit, if jobz = CUSOLVER_EIG_MODE_VECTOR , and info = 0, sub(A) contains the orthonormal eigenvectors of the matrix sub(A) .
The eigenvalue values of sub(A) , in ascending order ie, sorted so that W(i) array of size lwork .
If info = i (> 0) , info indicates i off-diagonal elements of an intermediate tridiagonal form did not converge to zero.
CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( N<0 , or lda<max(1,N) , or jobz is not CUSOLVER_EIG_MODE_NOVECTOR or CUSOLVER_EIG_MODE_VECTOR , or uplo is not CUBLAS_FILL_MODE_LOWER , or IA and JA are not 1, or N is bigger than dimension of global A , or the combination of dataType and computeType is not valid. 4. Acknowledgements  NVIDIA would like to thank the following individuals and institutions for their contributions: CPU LAPACK routines from netlib, CLAPACK-3.2.1 ( http: www.netlib.org/clapack/ ) The following is license of CLAPACK-3.2.1.
Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer listed in this license in the documentation and/or other materials provided with the distribution.
Neither the name of the copyright holders nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
METIS-5.1.0 ( http: glaros.dtc.umn.edu/gkhome/metis/metis/overview ) The following is license of METIS (Apache 2.0 license).
Copyright 1995-2013, Regents of the University of Minnesota Licensed under the Apache License, Version 2.0 (the “License”); you may not use this file except in compliance with the License.
You may obtain a copy of the License at http: www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and limitations under the License.
QD (A C++/fortran-90 double-double and quad-double package) ( http: crd-legacy.lbl.gov/~dhbailey/mpdist/ ) The following is license of QD (modified BSD license).
Copyright (c) 2003-2009, The Regents of the University of California, through Lawrence Berkeley National Laboratory (subject to receipt of any required approvals from U.S.
Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the copyright notice, this list of conditions and the following disclaimer.
Redistributions in binary form must reproduce the copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
Neither the name of the University of California, Lawrence Berkeley National Laboratory, U.S.
of Energy nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
You are under no obligation whatsoever to provide any bug fixes, patches, or upgrades to the features, functionality or performance of the source code (“Enhancements”) to anyone; however, if you choose to make your Enhancements available either publicly, or directly to Lawrence Berkeley National Laboratory, without imposing a separate written license agreement for such Enhancements, then you hereby grant the following license: a non-exclusive, royalty-free perpetual license to install, use, modify, prepare derivative works, incorporate into other computer software, distribute, and sublicense such enhancements or derivative works thereof, in binary and source code form. 5. Bibliography  [1] Timothy A.
McKee, reducing the bandwidth of sparse symmetric matrices, ACM ‘69 Proceedings of the 1969 24th national conference, Pages 157-172.
Liu, An Implementation of a Pseudoperipheral Node Finder, ACM Transactions on Mathematical Software (TOMS) Volume 5 Issue 3, Sept.
Comput., 9 (1988), pp. 862-874. [5] Alan George and Esmond Ng, An Implementation of Gaussian Elimination with Partial Pivoting for Sparse Systems, SIAM J.
[6] Alan George and Esmond Ng, Symbolic Factorization for Sparse Gaussian Elimination with Partial Pivoting, SIAM J.
Peyton, Computing Row and Column Counts for Sparse QR and LU Factorization, BIT 2001, Vol.
Liu, A Fast Implementation of the Minimum Degree Algorithm Using Quotient Graphs, ACM Transactions on Mathematical Software, Vol 6, No.
Liu, Computer Solution of Large Sparse Positive Definite Systems, Englewood Cliffs, New Jersey: Prentice-Hall, 1981.
Duff, ALGORITHM 575 Permutations for a Zero-Free Diagonal, ACM Transactions on Mathematical Software, Vol 7, No 3, September 1981, Page 387-390 [12] Iain S.
Duff and Jacko Koster, On algorithms for permuting large entries to the diagonal of a sparse matrix, SIAM Journal on Matrix Analysis and Applications, 2001, Vol.
973-996 [13] “A Fast and Highly Quality Multilevel Scheme for Partitioning Irregular Graphs”.
1, pp. 359-392, 1999. [14] YUJI NAKATSUKASA, ZHAOJUN BAI, AND FRANC¸OIS GYGI, OPTIMIZING HALLEY’S ITERATION FOR COMPUTING THE MATRIX POLAR DECOMPOSITION, SIAM J.
“Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions.” SIAM review 53.2 (2011): 217-288. 6. Notices  6.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 6.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 6.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2014-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
It accepts CUDA C++ source code in character string form and creates handles that can be used to obtain the PTX.
The PTX string generated by NVRTC can be loaded by cuModuleLoadData and cuModuleLoadDataEx , and linked with other modules by using the nvJitLink library or using cuLinkAddData of the CUDA Driver API.
This facility can often provide optimizations and performance not possible in a purely offline static compilation.
In the absence of NVRTC (or any runtime compilation support in CUDA), users needed to spawn a separate process to execute nvcc at runtime if they wished to implement runtime compilation in their applications or libraries, and, unfortunately, this approach has the following drawbacks: The compilation overhead tends to be higher than necessary.
End users are required to install nvcc and related tools which make it complicated to distribute applications that use runtime compilation.
NVRTC addresses these issues by providing a library interface that eliminates overhead associated with spawning separate processes, disk I/O,and so on, while keeping application deployment simple. 2. Getting Started  2.1.
System Requirements  NVRTC requires the following system configuration: Operating System: Linux x86_64, Linux ppc64le, Linux aarch64 or Windows x86_64.
CUDA Toolkit and Driver. 2.2. Installation  NVRTC is part of the CUDA Toolkit release and the components are organized as follows in the CUDA toolkit installation directory: On Windows: include vrtc.h bin vrtc64_Major Release VersionMinor Release Version_0.dll bin vrtc-builtins64_Major Release VersionMinor Release Version.dll lib\x64 vrtc.lib lib\x64 vrtc_static.lib lib\x64 vrtc-builtins_static.lib doc\pdf\NVRTC_User_Guide.pdf On Linux: include/nvrtc.h lib64/libnvrtc.so lib64/libnvrtc.so.Major Release Version.Minor Release Version lib64/libnvrtc.so.Major Release Version.Minor Release Version.
lib64/libnvrtc-builtins.so lib64/libnvrtc-builtins.so.Major Release Version.Minor Release Version lib64/libnvrtc-builtins.so.Major Release Version.Minor Release Version.
Error Handling General Information Query Compilation Supported Compile Options Host Helper 3.1.
Error Handling  NVRTC defines the following enumeration type and function for API call error handling.
Functions const char * nvrtcGetErrorString (nvrtcResult result) nvrtcGetErrorString is a helper function that returns a string describing the given nvrtcResult code, e.g., NVRTC_SUCCESS to "NVRTC_SUCCESS" . 3.1.1. Enumerations  enum nvrtcResult  The enumerated type nvrtcResult defines API call result codes.
Values: enumerator NVRTC_SUCCESS  enumerator NVRTC_ERROR_OUT_OF_MEMORY  enumerator NVRTC_ERROR_PROGRAM_CREATION_FAILURE  enumerator NVRTC_ERROR_INVALID_INPUT  enumerator NVRTC_ERROR_INVALID_PROGRAM  enumerator NVRTC_ERROR_INVALID_OPTION  enumerator NVRTC_ERROR_COMPILATION  enumerator NVRTC_ERROR_BUILTIN_OPERATION_FAILURE  enumerator NVRTC_ERROR_NO_NAME_EXPRESSIONS_AFTER_COMPILATION  enumerator NVRTC_ERROR_NO_LOWERED_NAMES_BEFORE_COMPILATION  enumerator NVRTC_ERROR_NAME_EXPRESSION_NOT_VALID  enumerator NVRTC_ERROR_INTERNAL_ERROR  enumerator NVRTC_ERROR_TIME_FILE_WRITE_FAILED  3.1.2.
Functions  const char * nvrtcGetErrorString ( nvrtcResult result )  nvrtcGetErrorString is a helper function that returns a string describing the given nvrtcResult code, e.g., NVRTC_SUCCESS to "NVRTC_SUCCESS" .
Returns Message string for the given nvrtcResult code. 3.2. General Information Query  NVRTC defines the following function for general information query.
Functions nvrtcResult nvrtcGetNumSupportedArchs (int *numArchs) nvrtcGetNumSupportedArchs sets the output parameter numArchs with the number of architectures supported by NVRTC.
nvrtcResult nvrtcGetSupportedArchs (int *supportedArchs) nvrtcGetSupportedArchs populates the array passed via the output parameter supportedArchs with the architectures supported by NVRTC.
nvrtcResult nvrtcVersion (int *major, int *minor) nvrtcVersion sets the output parameters major and minor with the CUDA Runtime Compilation version number. 3.2.1. Functions  nvrtcResult nvrtcGetNumSupportedArchs ( int * numArchs )  nvrtcGetNumSupportedArchs sets the output parameter numArchs with the number of architectures supported by NVRTC.
This can then be used to pass an array to nvrtcGetSupportedArchs to get the supported architectures.
Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT nvrtcResult nvrtcGetSupportedArchs ( int * supportedArchs )  nvrtcGetSupportedArchs populates the array passed via the output parameter supportedArchs with the architectures supported by NVRTC.
see nvrtcGetNumSupportedArchs Parameters supportedArchs – [out] sorted array of supported architectures.
Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT nvrtcResult nvrtcVersion ( int * major , int * minor )  nvrtcVersion sets the output parameters major and minor with the CUDA Runtime Compilation version number.
Functions nvrtcResult nvrtcAddNameExpression (nvrtcProgram prog, const char *const name_expression) nvrtcAddNameExpression notes the given name expression denoting the address of a global function or device /__constant__ variable.
nvrtcResult nvrtcCompileProgram (nvrtcProgram prog, int numOptions, const char *const *options) nvrtcCompileProgram compiles the given program.
nvrtcResult nvrtcCreateProgram (nvrtcProgram *prog, const char *src, const char *name, int numHeaders, const char *const *headers, const char *const *includeNames) nvrtcCreateProgram creates an instance of nvrtcProgram with the given input parameters, and sets the output parameter prog with it.
nvrtcResult nvrtcDestroyProgram (nvrtcProgram *prog) nvrtcDestroyProgram destroys the given program.
nvrtcResult nvrtcGetCUBIN (nvrtcProgram prog, char *cubin) nvrtcGetCUBIN stores the cubin generated by the previous compilation of prog in the memory pointed by cubin .
nvrtcResult nvrtcGetCUBINSize (nvrtcProgram prog, size_t *cubinSizeRet) nvrtcGetCUBINSize sets the value of cubinSizeRet with the size of the cubin generated by the previous compilation of prog .
nvrtcResult nvrtcGetLTOIR (nvrtcProgram prog, char *LTOIR) nvrtcGetLTOIR stores the LTO IR generated by the previous compilation of prog in the memory pointed by LTOIR .
nvrtcResult nvrtcGetLTOIRSize (nvrtcProgram prog, size_t *LTOIRSizeRet) nvrtcGetLTOIRSize sets the value of LTOIRSizeRet with the size of the LTO IR generated by the previous compilation of prog .
nvrtcResult nvrtcGetLoweredName (nvrtcProgram prog, const char *const name_expression, const char **lowered_name) nvrtcGetLoweredName extracts the lowered (mangled) name for a global function or device /__constant__ variable, and updates *lowered_name to point to it.
nvrtcResult nvrtcGetNVVM (nvrtcProgram prog, char *nvvm) DEPRECATION NOTICE: This function will be removed in a future release.
nvrtcResult nvrtcGetNVVMSize (nvrtcProgram prog, size_t *nvvmSizeRet) DEPRECATION NOTICE: This function will be removed in a future release.
nvrtcResult nvrtcGetOptiXIR (nvrtcProgram prog, char *optixir) nvrtcGetOptiXIR stores the OptiX IR generated by the previous compilation of prog in the memory pointed by optixir .
nvrtcResult nvrtcGetOptiXIRSize (nvrtcProgram prog, size_t *optixirSizeRet) nvrtcGetOptiXIRSize sets the value of optixirSizeRet with the size of the OptiX IR generated by the previous compilation of prog .
nvrtcResult nvrtcGetPTX (nvrtcProgram prog, char *ptx) nvrtcGetPTX stores the PTX generated by the previous compilation of prog in the memory pointed by ptx .
nvrtcResult nvrtcGetPTXSize (nvrtcProgram prog, size_t *ptxSizeRet) nvrtcGetPTXSize sets the value of ptxSizeRet with the size of the PTX generated by the previous compilation of prog (including the trailing NULL ).
nvrtcResult nvrtcGetProgramLog (nvrtcProgram prog, char *log) nvrtcGetProgramLog stores the log generated by the previous compilation of prog in the memory pointed by log .
nvrtcResult nvrtcGetProgramLogSize (nvrtcProgram prog, size_t *logSizeRet) nvrtcGetProgramLogSize sets logSizeRet with the size of the log generated by the previous compilation of prog (including the trailing NULL ).
Typedefs nvrtcProgram nvrtcProgram is the unit of compilation, and an opaque handle for a program. 3.3.1. Functions  nvrtcResult nvrtcAddNameExpression ( nvrtcProgram prog , const char * const name_expression )  nvrtcAddNameExpression notes the given name expression denoting the address of a global function or device /__constant__ variable.
The identical name expression string must be provided on a subsequent call to nvrtcGetLoweredName to extract the lowered name.
name_expression – [in] constant expression denoting the address of a global function or device /__constant__ variable.
Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_PROGRAM NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_NO_NAME_EXPRESSIONS_AFTER_COMPILATION nvrtcResult nvrtcCompileProgram ( nvrtcProgram prog , int numOptions , const char * const * options )  nvrtcCompileProgram compiles the given program.
Returns NVRTC_SUCCESS NVRTC_ERROR_OUT_OF_MEMORY NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM NVRTC_ERROR_INVALID_OPTION NVRTC_ERROR_COMPILATION NVRTC_ERROR_BUILTIN_OPERATION_FAILURE NVRTC_ERROR_TIME_FILE_WRITE_FAILED nvrtcResult nvrtcCreateProgram ( nvrtcProgram * prog , const char * src , const char * name , int numHeaders , const char * const * headers , const char * const * includeNames )  nvrtcCreateProgram creates an instance of nvrtcProgram with the given input parameters, and sets the output parameter prog with it.
includeNames – [in] Name of each header by which they can be included in the CUDA program source.
Returns NVRTC_SUCCESS NVRTC_ERROR_OUT_OF_MEMORY NVRTC_ERROR_PROGRAM_CREATION_FAILURE NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcDestroyProgram ( nvrtcProgram * prog )  nvrtcDestroyProgram destroys the given program.
Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetCUBIN ( nvrtcProgram prog , char * cubin )  nvrtcGetCUBIN stores the cubin generated by the previous compilation of prog in the memory pointed by cubin .
No cubin is available if the value specified to -arch is a virtual architecture instead of an actual architecture.
Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetCUBINSize ( nvrtcProgram prog , size_t * cubinSizeRet )  nvrtcGetCUBINSize sets the value of cubinSizeRet with the size of the cubin generated by the previous compilation of prog .
The value of cubinSizeRet is set to 0 if the value specified to -arch is a virtual architecture instead of an actual architecture.
Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetLTOIR ( nvrtcProgram prog , char * LTOIR )  nvrtcGetLTOIR stores the LTO IR generated by the previous compilation of prog in the memory pointed by LTOIR .
Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetLTOIRSize ( nvrtcProgram prog , size_t * LTOIRSizeRet )  nvrtcGetLTOIRSize sets the value of LTOIRSizeRet with the size of the LTO IR generated by the previous compilation of prog .
Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetLoweredName ( nvrtcProgram prog , const char * const name_expression , const char * * lowered_name )  nvrtcGetLoweredName extracts the lowered (mangled) name for a global function or device /__constant__ variable, and updates *lowered_name to point to it.
The memory containing the name is released when the NVRTC program is destroyed by nvrtcDestroyProgram.
The identical name expression must have been previously provided to nvrtcAddNameExpression.
lowered_name – [out] initialized by the function to point to a C string containing the lowered (mangled) name corresponding to the provided name expression.
Returns NVRTC_SUCCESS NVRTC_ERROR_NO_LOWERED_NAMES_BEFORE_COMPILATION NVRTC_ERROR_INVALID_PROGRAM NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_NAME_EXPRESSION_NOT_VALID nvrtcResult nvrtcGetNVVM ( nvrtcProgram prog , char * nvvm )  DEPRECATION NOTICE: This function will be removed in a future release.
nvrtcResult nvrtcGetNVVMSize ( nvrtcProgram prog , size_t * nvvmSizeRet )  DEPRECATION NOTICE: This function will be removed in a future release.
nvrtcResult nvrtcGetOptiXIR ( nvrtcProgram prog , char * optixir )  nvrtcGetOptiXIR stores the OptiX IR generated by the previous compilation of prog in the memory pointed by optixir .
No OptiX IR is available if the program was compiled with options incompatible with OptiX IR generation.
Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetOptiXIRSize ( nvrtcProgram prog , size_t * optixirSizeRet )  nvrtcGetOptiXIRSize sets the value of optixirSizeRet with the size of the OptiX IR generated by the previous compilation of prog .
The value of nvrtcGetOptiXIRSize is set to 0 if the program was compiled with options incompatible with OptiX IR generation.
Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetPTX ( nvrtcProgram prog , char * ptx )  nvrtcGetPTX stores the PTX generated by the previous compilation of prog in the memory pointed by ptx .
Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetPTXSize ( nvrtcProgram prog , size_t * ptxSizeRet )  nvrtcGetPTXSize sets the value of ptxSizeRet with the size of the PTX generated by the previous compilation of prog (including the trailing NULL ).
Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetProgramLog ( nvrtcProgram prog , char * log )  nvrtcGetProgramLog stores the log generated by the previous compilation of prog in the memory pointed by log .
Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetProgramLogSize ( nvrtcProgram prog , size_t * logSizeRet )  nvrtcGetProgramLogSize sets logSizeRet with the size of the log generated by the previous compilation of prog (including the trailing NULL ).
Note that compilation log may be generated with warnings and informative messages, even when the compilation of prog succeeds.
Typedefs  typedef struct _nvrtcProgram * nvrtcProgram  nvrtcProgram is the unit of compilation, and an opaque handle for a program.
To compile a CUDA program string, an instance of nvrtcProgram must be created first with nvrtcCreateProgram , then compiled with nvrtcCompileProgram . 3.4. Supported Compile Options  NVRTC supports the compile options below.
Option names with two preceding dashs ( -- ) are long option names and option names with one preceding dash ( - ) are short option names.
When a compile option takes an argument, an assignment operator ( = ) is used to separate the compile option argument from the compile option name, e.g., "--gpu-architecture=compute_60" .
Alternatively, the compile option name and the argument can be specified in separate strings without an assignment operator, .e.g, "--gpu-architecture" "compute_60" .
Single-character short option names, such as -D , -U , and -I , do not require an assignment operator, and the compile option name and the argument can be present in the same string with or without spaces between them.
The valid compiler options are: Compilation targets --gpu-architecture= ( -arch ) Specify the name of the class of GPU architectures for which the input must be compiled.
Valid s: compute_50 compute_52 compute_53 compute_60 compute_61 compute_62 compute_70 compute_72 compute_75 compute_80 compute_87 compute_89 compute_90 compute_90a sm_50 sm_52 sm_53 sm_60 sm_61 sm_62 sm_70 sm_72 sm_75 sm_80 sm_87 sm_89 sm_90 sm_90a Default: compute_52 Separate compilation / whole-program compilation --device-c ( -dc ) Generate relocatable code that can be linked with other relocatable device code.
--relocatable-device-code={true|false} ( -rdc ) Enable (disable) the generation of relocatable device code.
Default: false --extensible-whole-program ( -ewp ) Do extensible whole program compilation of device code.
When specified along with ‘-G’, enables limited debug information generation for optimized device code (currently, only line number information).
--ptxas-options ( -Xptxas ) --ptxas-options= Specify options directly to ptxas, the PTX optimizing assembler.
--maxrregcount= ( -maxrregcount ) Specify the maximum amount of registers that GPU functions can use.
Until a function-specific limit, a higher value will generally increase the performance of individual GPU threads that execute this function.
However, because thread registers are allocated from a global register pool on each GPU, a higher value of this option will also reduce the maximum thread block size, thereby reducing the amount of thread parallelism.
Value less than the minimum registers required by ABI will be bumped up by the compiler to ABI minimum limit.
--ftz={true|false} ( -ftz ) When performing single-precision floating-point operations, flush denormal values to zero or preserve denormal values.
Default: false --prec-sqrt={true|false} ( -prec-sqrt ) For single-precision floating-point square root, use IEEE round-to-nearest mode or use a faster approximation.
Default: true --prec-div={true|false} ( -prec-div ) For single-precision floating-point division and reciprocals, use IEEE round-to-nearest mode or use a faster approximation.
Default: true --fmad={true|false} ( -fmad ) Enables (disables) the contraction of floating-point multiplies and adds/subtracts into floating-point multiply-add operations (FMAD, FFMA, or DFMA).
--extra-device-vectorization ( -extra-device-vectorization ) Enables more aggressive device code vectorization in the NVVM optimizer.
--modify-stack-limit={true|false} ( -modify-stack-limit ) On Linux, during compilation, use setrlimit() to increase stack size to maximum allowed.
Default: true --dlink-time-opt ( -dlto ) Generate intermediate code for later link-time optimization.
Note: when this option is used the nvrtcGetLTOIR API should be used, as PTX or Cubin will not be generated.
Note: when this option is used the nvrtcGetOptiX API should be used, as PTX or Cubin will not be generated.
--jump-table-density= [0-101] ( -jtd ) Specify the case density percentage in switch statements, and use it as a minimal threshold to determine whether jump table(brx.idx instruction) will be used to implement a switch statement.
= The contents of are tokenized and preprocessed as if they appeared during translation phase three in a #define directive.
--include-path= ( -I ) Add the directory to the list of directories to be searched for headers.
--no-source-include ( -no-source-include ) The preprocessor by default adds the directory of each input sources to the include path.
Language Dialect --std={c++03|c++11|c++14|c++17|c++20} ( -std={c++11|c++14|c++17|c++20} ) Set language dialect to C++03, C++11, C++14, C++17 or C++20 Default: c++17 --builtin-move-forward={true|false} ( -builtin-move-forward ) Provide builtin definitions of std::move and std::forward , when C++11 or later language dialect is selected.
Default: true --builtin-initializer-list={true|false} ( -builtin-initializer-list ) Provide builtin definitions of std::initializer_list class and member functions when C++11 or later language dialect is selected.
--restrict ( -restrict ) Programmer assertion that all kernel pointer parameters are restrict pointers.
--device-as-default-execution-space ( -default-device ) Treat entities with no execution space annotation as __device__ entities.
--optimization-info= ( -opt-info ) Provide optimization reports for the specified kind of optimization.
(Default) --no-display-error-number ( -no-err-no ) Disables the display of a diagnostic number for warning messages.
--brief-diagnostics={true|false} ( -brief-diag ) This option disables or enables showing source line and column info in a diagnostic.
Default: false --time= ( -time ) Generate a comma separated value table with the time taken by each compilation phase, and append it at the end of the file given as the option argument.
If the file does not exist, the column headings are generated in the first row of the table.
Split compilation attempts to reduce compile time by enabling the compiler to run certain optimization passes concurrently.
This option accepts a numerical value that specifies the maximum number of threads the compiler can use.
One can also allow the compiler to use the maximum threads available on the system by setting —split-compile=0.
--fdevice-syntax-only ( -fdevice-syntax-only ) Ends device compilation after front-end syntax checking.
--minimal ( -minimal ) Omit certain language features to reduce compile time for small programs.
In particular, the following are omitted: Texture and surface functions and associated types, e.g., cudaTextureObject_t .
CUDA Runtime Functions that are provided by the cudadevrt device code library, typically named with prefix “cuda”, e.g., cudaMalloc .
Types and macros associated with CUDA Runtime and Driver APIs, provided by cuda/tools/cudart/driver_types.h, typically named with prefix “cuda”, e.g., cudaError_t . 3.5. Host Helper  NVRTC defines the following functions for easier interaction with host code.
Functions nvrtcResult nvrtcGetTypeName (const std::type_info &tinfo, std::string *result) nvrtcGetTypeName stores the source level name of a type in the given std::string location.
nvrtcResult nvrtcGetTypeName (std::string *result) nvrtcGetTypeName stores the source level name of the template type argument T in the given std::string location. 3.5.1. Functions  inline nvrtcResult nvrtcGetTypeName ( const std :: type_info & tinfo , std :: string * result )  nvrtcGetTypeName stores the source level name of a type in the given std::string location.
This function is only provided when the macro NVRTC_GET_TYPE_NAME is defined with a non-zero value.
It uses abi::__cxa_demangle or UnDecorateSymbolName function calls to extract the type name, when using gcc/clang or cl.exe compilers, respectively.
If the name extraction fails, it will return NVRTC_INTERNAL_ERROR, otherwise *result is initialized with the extracted name.
Windows-specific notes: nvrtcGetTypeName() is not multi-thread safe because it calls UnDecorateSymbolName(), which is not multi-thread safe.
Returns NVRTC_SUCCESS NVRTC_ERROR_INTERNAL_ERROR template nvrtcResult nvrtcGetTypeName ( std :: string * result )  nvrtcGetTypeName stores the source level name of the template type argument T in the given std::string location.
Language  Unlike the offline nvcc compiler, NVRTC is meant for compiling only device CUDA C++ code.
It does not accept host code or host compiler extensions in the input code, unless otherwise noted. 4.1. Execution Space  NVRTC uses __host__ as the default execution space, and it generates an error if it encounters any host code in the input.
That is, if the input contains entities with explicit __host__ annotations or no execution space annotation, NVRTC will emit an error.
NVRTC provides a compile option, --device-as-default-execution-space (refer to Supported Compile Options ), that enables an alternative compilation mode, in which entities with no execution space annotations are treated as __device__ entities . 4.2. Separate Compilation  NVRTC itself does not provide any linker.
Users can, however, use the nvJitLink library or cuLinkAddData in the CUDA Driver API to link the generated relocatable PTX code with other relocatable code.
To generate relocatable PTX code, the compile option --relocatable-device-code=true or --device-c is required. 4.3. Dynamic Parallelism  NVRTC supports dynamic parallelism under the following conditions: Compilation target must be compute 35 or higher.
Either separate compilation ( --relocatable-device-code=true or --device-c ) or extensible whole program compilation ( --extensible-whole-program ) must be enabled.
Generated PTX must be linked against the CUDA device runtime (cudadevrt) library (refer to Separate Compilation ).
Example: Dynamic Parallelism provides a simple example. 4.4. Integer Size  Different operating systems define integer type sizes differently.
Integer sizes in bits for LLP64 and LP64  short int long long long pointers and size_t LLP64 16 32 32 64 64 LP64 16 32 64 64 64 NVRTC implements LP64 on Linux and LLP64 on Windows.
128-bit integer support is not available on Windows. 4.5. Include Syntax  When nvrtcCompileProgram() is called, the current working directory is added to the header search path used for locating files included with the quoted syntax (for example, #include "foo.h" ), before the code is compiled.
4.6. Predefined Macros  __CUDACC_RTC__ : useful for distinguishing between runtime and offline nvcc compilation in user code.
__CUDACC_VER_MAJOR__ : defined with the major version number as returned by nvrtcVersion .
__CUDACC_VER_MINOR__ : defined with the minor version number as returned by nvrtcVersion .
__NVCC_DIAG_PRAGMA_SUPPORT__ : defined with same semantics as with offline nvcc compilation.
__CUDACC_RTC_INT128__ : defined when -device-int128 flag is specified during compilation, and indicates that __int128 type is supported.
va_start va_end va_arg va_copy : defined when language dialect C++11 or later is selected.
__CUDACC_RTC_MINIMAL__ : defined when -minimal flag is specified during compilation (since CUDA 12.4).
Macros defined in nv/target header are implicitly provided, e.g., NV_IF_TARGET . 4.7. Predefined Types  clock_t size_t ptrdiff_t va_list : Note that the definition of this type may be different than the one selected by nvcc when compiling CUDA code.
Predefined types such as dim3 , char4 , etc., that are available in the CUDA Runtime headers when compiling offline with nvcc are also available, unless otherwise noted.
std::initializer_list : implicitly provided in C++11 and later dialects, unless -builtin-initializer-list=false is specified.
std::move, std::forward : implicitly provided in C++11 and later dialects, unless -builtin-move-forward=false is specified. 4.8. Builtin Functions  Builtin functions provided by the CUDA Runtime headers when compiling offline with nvcc are available, unless otherwise noted.
Other dialects can be selected using the -std flag. 5. Basic Usage  This section of the document uses a simple example, Single-Precision α⋅X Plus Y (SAXPY), shown in Figure 1 to explain what is involved in runtime compilation with NVRTC.
CUDA source string for SAXPY const char * saxpy = "   \ extern \" C \" __global__   \ void saxpy(float a, float *x, float *y, float *out, size_t n)   \ {   \ size_t tid = blockIdx.x * blockDim.x + threadIdx.x;   \ if (tid and #include would require 2 as numHeaders , { "", "" } as headers, and { "foo.h", "bar.h" } as includeNames ( and must be replaced by the actual contents of foo.h and bar.h ).
Alternatively, the compile option -I can be used if the header is guaranteed to exist in the file system at runtime.
Once the instance of nvrtcProgram for compilation is created, it can be compiled by nvrtcCompileProgram as shown in Figure 3.
Two compile options are used in this example, --gpu-architecture=compute_80 and --fmad=false , to generate code for the compute_80 architecture and to disable the contraction of floating-point multiplies and adds/subtracts into floating-point multiply-add operations.
Other combinations of compile options can be used as needed and Supported Compile Options lists valid compile options.
Compilation of SAXPY for compute_80 with FMAD enabled const char * opts [] = { "--gpu-architecture=compute_80" , "--fmad=false" }; nvrtcCompileProgram ( prog ,   prog 2 ,   numOptions opts );   options After the compilation completes, users can obtain the program compilation log and the generated PTX as Figure 4 shows.
NVRTC does not generate valid PTX when the compilation fails, and it may generate program compilation log even when the compilation succeeds if needed.
An nvrtcProgram can be compiled by nvrtcCompileProgram multiple times with different compile options, and users can only retrieve the PTX and the log generated by the last compilation.
Obtaining generated PTX and program compilation log   Obtain compilation log from the program.
size_t logSize ; nvrtcGetProgramLogSize ( prog , & logSize ); char * log = new char [ logSize ]; nvrtcGetProgramLog ( prog , log );   Obtain PTX from the program.
size_t ptxSize ; nvrtcGetPTXSize ( prog , & ptxSize ); char * ptx = new char [ ptxSize ]; nvrtcGetPTX ( prog , ptx ); When the instance of nvrtcProgram is no longer needed, it can be destroyed by nvrtcDestroyProgram as shown in Figure 5.
Destruction of nvrtcProgram nvrtcDestroyProgram ( & prog ); The generated PTX can be further manipulated by the CUDA Driver API for execution or linking.
Accessing Lowered Names  NVRTC will mangle __global__ function names and names of __device__ and __constant__ variables as specified by the IA64 ABI.
If the generated PTX is being loaded using the CUDA Driver API, the kernel function or __device__ / __constant__ variable must be looked up by name, but this is hard to do when the name has been mangled.
To address this problem, NVRTC provides API functions that map source level __global__ function or __device__ / __constant__ variable names to the mangled names present in the generated PTX.
The two API functions nvrtcAddNameExpression and nvrtcGetLoweredName work together to provide this functionality.
First, a ‘name expression’ string denoting the address for the __global__ function or __device__ / __constant__ variable is provided to nvrtcAddNameExpression .
During compilation, NVRTC will parse the name expression string as a C++ constant expression at the end of the user program.
The constant expression must provide the address of the __global__ function or __device__ / __constant__ variable.
Finally, the function nvrtcGetLoweredName is called with the original name expression and it returns a pointer to the lowered name.
NVRTC guarantees that any __global__ function or __device__/__constant__ variable referenced in a call to nvrtcAddNameExpression will be present in the generated PTX (if the definition is available in the input source code). 6.1. Example  Example: Using Lowered Name`_ lists a complete runnable example.
Some relevant snippets: The GPU source code (‘gpu_program’) contains definitions of various __global__ functions/function templates and __device__ / __constant__ variables: const char * gpu_program = "   \ __device__ int V1;   set from host code   \ static __global__ void f1(int *result) { *result = V1 + 10; }   \ namespace N1 {   \ namespace N2 {   \ __constant__ int V2;   set from host code   \ __global__ void f2(int *result) { *result = V2 + 20; }   \ }   \ }   \ template   \ __global__ void f3(int *result) { *result = sizeof(T); }   \ The host source code invokes nvrtcAddNameExpression with various name expressions referring to the address of __global__ functions and __device__ / __constant__ variables: kernel_name_vec .
for ( size_t i = 0 ; i   \ __global__ void f3(int *result) { *result = sizeof(T); }   \   " ; The host code function getKernelNameForType creates the name expression for a __global__ function template instantiation based on the host template type T.
The name of the type T is extracted using nvrtcGetTypeName : template std :: string getKernelNameForType ( void ) {   Look up the source level name string for the type "T" using   nvrtcGetTypeName() and use it to create the kernel name std :: string type_name ; NVRTC_SAFE_CALL ( nvrtcGetTypeName ( & type_name )); return std :: string ( "f3" ; } The name expressions are presented to NVRTC using the nvrtcAddNameExpression function: name_vec .
Windows: In CUDA toolkits prior to cuda 11.3, the DLL name was of the form “nvrtc64_XY_0.dll”, where X = MAJOR, Y = MINOR.
The NVRTC shared library in this CUDA toolkit will have the same soname (Linux) or DLL name (Windows) as an NVRTC shared library in a previous minor version of the same CUDA toolkit.
Similarly, the NVRTC shared library in CUDA 11.3 and later 11.x releases will have the same soname (Linux) or DLL name (Windows) as the NVRTC shared library in CUDA 11.2.
As a consequence of the versioning scheme described above, an NVRTC client that links against a particular NVRTC shared library will continue to work with a future NVRTC shared library with a matching soname (Linux) or DLL name (Windows).
This allows the NVRTC client to take advantage of bug fixes and enhancements available in the more recent NVRTC shared library 1 .
However, the more recent NVRTC shared library may generate PTX with a version that is not accepted by the CUDA Driver API functions of an older CUDA driver, as explained in the Best Practices Guide .
Some approaches to resolving this issue: Install a more recent CUDA driver that is compatible with the CUDA toolkit containing the NVRTC library being used.
Alternately, an NVRTC client can either link against the static NVRTC library or redistribute a specific version of the NVRTC shared library and use dlopen (Linux) or LoadLibrary (Windows) functions to use that library at run time.
Either approach allows the NVRTC client to maintain control over the version of NVRTC being used during deployment, to ensure predictable functionality and performance. 8.2. NVRTC-builtins Library  The NVRTC-builtins library contains helper code that is part of the NVRTC package.
Each NVRTC library is only compatible with the NVRTC-builtins library from the same CUDA toolkit. 9. Miscellaneous Notes  9.1.
Thread Safety  Multiple threads can invoke NVRTC API functions concurrently, as long as there is no race condition.
In this context, a race condition is defined to occur if multiple threads concurrently invoke NVRTC API functions with the same nvrtcProgram argument, where at least one thread is invoking either nvrtcCompileProgram or nvrtcAddNameExpression 2 .
Since CUDA 12.3, NVRTC allows concurrent invocations of nvrtcCompileProgram to potentially concurrently also invoke the embedded NVVM optimizer/codegen phase.
Setting the environment variable NVRTC_DISABLE_CONCURRENT_NVVM disables this behavior, i.e., invocations of the embedded NVVM optimizer/codegen phase will be serialized. 9.2. Stack Size  On Linux, NVRTC will increase the stack size to the maximum allowed using the setrlimit() function during compilation.
This reduces the chance that the compiler will run out of stack when processing complex input sources.
Because setrlimit() changes the stack size for the entire process, it will also affect other application threads that may be executing concurrently.
The command line flag -modify-stack-limit=false will prevent NVRTC from modifying the stack limit. 9.3. NVRTC Static Library  The NVRTC static library references functions defined in the NVRTC-builtins static library and the PTX compiler static library.
Code (saxpy.cpp)  #include #include #include #define NUM_THREADS 128 #define NUM_BLOCKS 32 #define NVRTC_SAFE_CALL(x) \ do { \ nvrtcResult result = x; \ if (result != NVRTC_SUCCESS) { \ std::cerr ( i ); hY [ i ] = static_cast ( i * 2 ); } CUdeviceptr dX , dY , dOut ; CUDA_SAFE_CALL ( cuMemAlloc ( & dX , bufferSize )); CUDA_SAFE_CALL ( cuMemAlloc ( & dY , bufferSize )); CUDA_SAFE_CALL ( cuMemAlloc ( & dOut , bufferSize )); CUDA_SAFE_CALL ( cuMemcpyHtoD ( dX , hX , bufferSize )); CUDA_SAFE_CALL ( cuMemcpyHtoD ( dY , hY , bufferSize ));   Execute SAXPY.
void * args [] = { & a , & dX , & dY , & dOut , & n }; CUDA_SAFE_CALL ( cuLaunchKernel ( kernel , NUM_BLOCKS , 1 , 1 ,   grid dim NUM_THREADS , 1 , 1 ,   block dim 0 , NULL ,   shared mem and stream args , 0 ));   arguments CUDA_SAFE_CALL ( cuCtxSynchronize ());   Retrieve and print output.
CUDA_SAFE_CALL ( cuMemcpyDtoH ( hOut , dOut , bufferSize )); for ( size_t i = 0 ; i #include #include #include #include #define NVRTC_SAFE_CALL(x) \ do { \ nvrtcResult result = x; \ if (result != NVRTC_SUCCESS) { \ std::cerr  \ __global__ void f3(int *result) { *result = sizeof(T); }  \  "; int main () {   Create an instance of nvrtcProgram nvrtcProgram prog ; NVRTC_SAFE_CALL ( nvrtcCreateProgram ( & prog ,   prog gpu_program ,   buffer "prog.cu" ,   name 0 ,   numHeaders NULL ,   headers NULL ));   includeNames   add all name expressions for kernels std :: vector kernel_name_vec ; std :: vector variable_name_vec ; std :: vector variable_initial_value ; std :: vector expected_result ;   note the name expressions are parsed as constant expressions kernel_name_vec .
for ( size_t i = 0 ; i #include #include #include #include #define NVRTC_SAFE_CALL(x) \ do { \ nvrtcResult result = x; \ if (result != NVRTC_SUCCESS) { \ std::cerr   \ __global__ void f3(int *result) { *result = sizeof(T); }   \   " ;   note: this structure is also defined in GPU code string.
namespace N1 { struct S1_t { int i ; double d ; }; }; template std :: string getKernelNameForType ( void ) {   Look up the source level name string for the type "T" using   nvrtcGetTypeName() and use it to create the kernel name std :: string type_name ; NVRTC_SAFE_CALL ( nvrtcGetTypeName ( & type_name )); return std :: string ( "f3" ; } int main () {   Create an instance of nvrtcProgram nvrtcProgram prog ; NVRTC_SAFE_CALL ( nvrtcCreateProgram ( & prog ,   prog gpu_program ,   buffer "gpu_program.cu" ,   name 0 ,   numHeaders NULL ,   headers NULL ));   includeNames   add all name expressions for kernels std :: vector name_vec ; std :: vector expected_result ;   note the name expressions are parsed as constant expressions name_vec .
CUDA_SAFE_CALL ( cuMemcpyDtoH ( hOut , dOut , bufferSize )); for ( size_t i = 0 ; i < n ; ++ i ) { std :: cout << a << " * " << hX [ i ] << " + " << hY [ i ] << " = " << hOut [ i ] << ' ' ; }   Release resources.
CUDA_SAFE_CALL ( cuMemFree ( dX )); CUDA_SAFE_CALL ( cuMemFree ( dY )); CUDA_SAFE_CALL ( cuMemFree ( dOut )); CUDA_SAFE_CALL ( cuModuleUnload ( module )); CUDA_SAFE_CALL ( cuCtxDestroy ( context )); free ( cubin ); delete [] hX ; delete [] hY ; delete [] hOut ; delete [] LTOIR ; return 0 ; } 14.3.
Device LTO Build Instructions  Assuming the environment variable CUDA_PATH points to the CUDA Toolkit installation directory, build this example as: Compile offline.cu to fatbinary containing LTO IR (change lto_52 to a different lto_XX architecture as appropriate).
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 14.4.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 14.4.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
1 Changes to compiler optimizer heuristics in the newer NVRTC shared library may also potentially cause performance perturbations for generated code.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2014-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Introduction  The Fatbin Creator APIs are a set of APIs which can be used at runtime to combine multiple CUDA objects into one CUDA fat binary (fatbin).
The functionality in this library is similar to the fatbinary offline tool in the CUDA toolkit, with the following advantages: Support for runtime fatbin creation.
Supports direct input from memory, rather than requiring inputs be written to files. 2. Getting Started  2.1.
System Requirements  The Fatbin Creator library requires no special system configuration.
It does not require a GPU. 2.2. Installation  The Fatbin Creator library is part of the CUDA Toolkit release and the components are organized as follows in the CUDA toolkit installation directory: On Windows: include vFatbin.h lib\x64 vFatbin.dll lib\x64 vFatbin_static.lib doc\pdf vFatbin_User_Guide.pdf On Linux: include/nvFatbin.h lib64/libnvfatbin.so lib64/libnvfatbin_static.a doc/pdf/nvFatbin_User_Guide.pdf 3.
Error codes  Enumerations nvFatbinResult The enumerated type nvFatbinResult defines API call result codes.
Functions const char * nvFatbinGetErrorString (nvFatbinResult result) nvFatbinGetErrorString returns an error description string for each error code. 3.1.1. Enumerations  enum nvFatbinResult  The enumerated type nvFatbinResult defines API call result codes.
Values: enumerator NVFATBIN_SUCCESS  enumerator NVFATBIN_ERROR_INTERNAL  enumerator NVFATBIN_ERROR_ELF_ARCH_MISMATCH  enumerator NVFATBIN_ERROR_ELF_SIZE_MISMATCH  enumerator NVFATBIN_ERROR_MISSING_PTX_VERSION  enumerator NVFATBIN_ERROR_NULL_POINTER  enumerator NVFATBIN_ERROR_COMPRESSION_FAILED  enumerator NVFATBIN_ERROR_COMPRESSED_SIZE_EXCEEDED  enumerator NVFATBIN_ERROR_UNRECOGNIZED_OPTION  enumerator NVFATBIN_ERROR_INVALID_ARCH  enumerator NVFATBIN_ERROR_INVALID_NVVM  enumerator NVFATBIN_ERROR_EMPTY_INPUT  enumerator NVFATBIN_ERROR_MISSING_PTX_ARCH  enumerator NVFATBIN_ERROR_PTX_ARCH_MISMATCH  enumerator NVFATBIN_ERROR_MISSING_FATBIN  enumerator NVFATBIN_ERROR_INVALID_INDEX  enumerator NVFATBIN_ERROR_IDENTIFIER_REUSE  3.1.2.
Functions  const char * nvFatbinGetErrorString ( nvFatbinResult result )  nvFatbinGetErrorString returns an error description string for each error code.
Parameters result – [in] error code Returns nullptr, if result is NVFATBIN_SUCCESS a string, if result is not NVFATBIN_SUCCESS 3.2.
Fatbinary Creation  Functions nvFatbinResult nvFatbinAddCubin (nvFatbinHandle handle, const void *code, size_t size, const char *arch, const char *identifier) nvFatbinAddCubin adds a CUDA binary to the fatbinary.
nvFatbinResult nvFatbinAddIndex (nvFatbinHandle handle, const void *code, size_t size, const char *identifier) nvFatbinAddIndex adds an index file to the fatbinary.
nvFatbinResult nvFatbinAddLTOIR (nvFatbinHandle handle, const void *code, size_t size, const char *arch, const char *identifier, const char *optionsCmdLine) nvFatbinAddLTOIR adds LTOIR to the fatbinary.
nvFatbinResult nvFatbinAddPTX (nvFatbinHandle handle, const char *code, size_t size, const char *arch, const char *identifier, const char *optionsCmdLine) nvFatbinAddPTX adds PTX to the fatbinary.
nvFatbinResult nvFatbinAddReloc (nvFatbinHandle handle, const void *code, size_t size) nvFatbinAddReloc adds relocatable PTX entries from a host object to the fatbinary.
nvFatbinResult nvFatbinCreate (nvFatbinHandle *handle_indirect, const char **options, size_t optionsCount) nvFatbinCreate creates a new handle nvFatbinResult nvFatbinDestroy (nvFatbinHandle *handle_indirect) nvFatbinDestroy destroys the handle.
nvFatbinResult nvFatbinGet (nvFatbinHandle handle, void *buffer) nvFatbinGet returns the completed fatbinary.
nvFatbinResult nvFatbinSize (nvFatbinHandle handle, size_t *size) nvFatbinSize returns the fatbinary's size.
nvFatbinResult nvFatbinVersion (unsigned int *major, unsigned int *minor) nvFatbinVersion returns the current version of nvFatbin Typedefs nvFatbinHandle nvFatbinHandle is the unit of fatbin creation, and an opaque handle for a program. 3.2.1. Functions  nvFatbinResult nvFatbinAddCubin ( nvFatbinHandle handle , const void * code , size_t size , const char * arch , const char * identifier )  nvFatbinAddCubin adds a CUDA binary to the fatbinary.
identifier – [in] Name of the cubin, useful when extracting the fatbin with tools like cuobjdump.
Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_INVALID_ARCH NVFATBIN_ERROR_ELF_ARCH_MISMATCH NVFATBIN_ERROR_ELF_SIZE_MISMATCH NVFATBIN_ERROR_COMPRESSION_FAILED, NVFATBIN_ERROR_UNRECOGNIZED_OPTION NVFATBIN_ERROR_COMPRESSED_SIZE_EXCEEDED NVFATBIN_ERROR_EMPTY_INPUT NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinAddIndex ( nvFatbinHandle handle , const void * code , size_t size , const char * identifier )  nvFatbinAddIndex adds an index file to the fatbinary.
identifier – [in] Name of the index, useful when extracting the fatbin with tools like cuobjdump.
Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_INVALID_INDEX NVFATBIN_ERROR_COMPRESSION_FAILED, NVFATBIN_ERROR_UNRECOGNIZED_OPTION NVFATBIN_ERROR_COMPRESSED_SIZE_EXCEEDED NVFATBIN_ERROR_EMPTY_INPUT NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinAddLTOIR ( nvFatbinHandle handle , const void * code , size_t size , const char * arch , const char * identifier , const char * optionsCmdLine )  nvFatbinAddLTOIR adds LTOIR to the fatbinary.
identifier – [in] Name of the LTOIR, useful when extracting the fatbin with tools like cuobjdump.
Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INVALID_ARCH NVFATBIN_ERROR_COMPRESSION_FAILED, NVFATBIN_ERROR_UNRECOGNIZED_OPTION NVFATBIN_ERROR_COMPRESSED_SIZE_EXCEEDED NVFATBIN_ERROR_EMPTY_INPUT NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinAddPTX ( nvFatbinHandle handle , const char * code , size_t size , const char * arch , const char * identifier , const char * optionsCmdLine )  nvFatbinAddPTX adds PTX to the fatbinary.
If the final character is not ‘\0’, one will be added automatically, but in doing so, the code will be copied if it hasn’t already been copied.
identifier – [in] Name of the PTX, useful when extracting the fatbin with tools like cuobjdump.
Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INVALID_ARCH NVFATBIN_ERROR_PTX_ARCH_MISMATCH NVFATBIN_ERROR_COMPRESSION_FAILED, NVFATBIN_ERROR_UNRECOGNIZED_OPTION NVFATBIN_ERROR_COMPRESSED_SIZE_EXCEEDED NVFATBIN_ERROR_EMPTY_INPUT NVFATBIN_ERROR_MISSING_PTX_VERSION NVFATBIN_ERROR_MISSING_PTX_ARCH NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinAddReloc ( nvFatbinHandle handle , const void * code , size_t size )  nvFatbinAddReloc adds relocatable PTX entries from a host object to the fatbinary.
Note that each relocatable ptx source must have a unique identifier (the identifiers are taken from the object’s entries).
Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INVALID_ARCH NVFATBIN_ERROR_PTX_ARCH_MISMATCH NVFATBIN_ERROR_COMPRESSION_FAILED, NVFATBIN_ERROR_UNRECOGNIZED_OPTION NVFATBIN_ERROR_COMPRESSED_SIZE_EXCEEDED NVFATBIN_ERROR_EMPTY_INPUT NVFATBIN_ERROR_MISSING_PTX_VERSION NVFATBIN_ERROR_MISSING_PTX_ARCH NVFATBIN_ERROR_IDENTIFIER_REUSE NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinCreate ( nvFatbinHandle * handle_indirect , const char * * options , size_t optionsCount )  nvFatbinCreate creates a new handle Parameters handle_indirect – [out] Address of nvFatbin handle options – [in] An array of strings, each containing a single option.
Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_UNRECOGNIZED_OPTION NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinDestroy ( nvFatbinHandle * handle_indirect )  nvFatbinDestroy destroys the handle.
Use of any other pointers to the handle after calling this will result in undefined behavior.
Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinGet ( nvFatbinHandle handle , void * buffer )  nvFatbinGet returns the completed fatbinary.
Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinSize ( nvFatbinHandle handle , size_t * size )  nvFatbinSize returns the fatbinary’s size.
size – [out] The fatbinary’s size Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinVersion ( unsigned int * major , unsigned int * minor )  nvFatbinVersion returns the current version of nvFatbin Parameters major – [out] The major version.
Typedefs  typedef struct _nvFatbinHandle * nvFatbinHandle  nvFatbinHandle is the unit of fatbin creation, and an opaque handle for a program.
To create a fatbin, an instance of nvFatbinHandle must be created first with nvFatbinCreate() . 3.3. Supported Options  nvFatbin supports the options below.
Options that take a value have an assignment operator ( = ) followed by the option value, with no spaces, e.g.
Didn’t do anything from the start.) -compress= Enable (true) / disable (false) compression (default: true).
-opencl Specify OpenCL (rather than CUDA). 4. Basic Usage  This section of the document uses a simple example to explain how to use the Fatbin Creator APIs to link a program.
This example assumes we want to create a fatbin with a CUBIN for sm_52, PTX for sm_61, and LTOIR for sm_70.
We can create an instance of the fatbin creator and obtain an api handle to it as shown in Figure 1 .
Fatbin Creator creation and initialization of a program nvFatbinHandle handle ; nvFatbinCreate ( & handle , nullptr , 0 ); Assume that we already have three inputs stored in std::vector ‘s (CUBIN, PTX, and LTOIR), which could be from code created with nvrtc and stored into vectors.
(They do not have to be in vectors, this merely illustrates that both the data itself and its size are needed.) We can add the inputs as shown in Figure 2 .
And to allocate memory, we need to query the size of the fatbin which is done as shown in Figure 3 .
Query size of the created fatbin nvFatbinSize ( linker , & fatbinSize ); The fatbin can now be queried as shown in Figure 4 .
Query the created fatbin void * fatbin = malloc ( fatbinSize ); nvFatbinGet ( handle , fatbin ); When the fatbin creator is not needed anymore, it can be destroyed as shown in Figure 5 .
For example, you can create a fatbin from a cubin created with 11.8 and one with 12.4 if your nvFatbin library is at least version 12.x. 6. Example: Runtime fatbin creation  This section demonstrates runtime fatbin creation.
These two cubins are then passed to nvFatbin* API functions, which put the cubins into a fatbin.
Note that this example requires a compatible GPU with drivers and NVRTC to work, even though the library doesn’t require either. 6.1. Code (online.cpp)  #include #include #include #include #include #define NUM_THREADS 128 #define NUM_BLOCKS 32 #define NVRTC_SAFE_CALL(x) \ do { \ nvrtcResult result = x; \ if (result != NVRTC_SUCCESS) { \ std::cerr ( i ); hY [ i ] = static_cast ( i * 2 ); } CUdeviceptr dX , dY , dOut ; CUDA_SAFE_CALL ( cuMemAlloc ( & dX , bufferSize )); CUDA_SAFE_CALL ( cuMemAlloc ( & dY , bufferSize )); CUDA_SAFE_CALL ( cuMemAlloc ( & dOut , bufferSize )); CUDA_SAFE_CALL ( cuMemcpyHtoD ( dX , hX , bufferSize )); CUDA_SAFE_CALL ( cuMemcpyHtoD ( dY , hY , bufferSize ));   Execute SAXPY.
void * args [] = { & a , & dX , & dY , & dOut , & n }; CUDA_SAFE_CALL ( cuLaunchKernel ( kernel , NUM_BLOCKS , 1 , 1 ,   grid dim NUM_THREADS , 1 , 1 ,   block dim 0 , NULL ,   shared mem and stream args , 0 ));   arguments CUDA_SAFE_CALL ( cuCtxSynchronize ());   Retrieve and print output.
CUDA_SAFE_CALL ( cuMemcpyDtoH ( hOut , dOut , bufferSize )); for ( size_t i = 0 ; i < n ; ++ i ) { std :: cout << a << " * " << hX [ i ] << " + " << hY [ i ] << " = " << hOut [ i ] << ' ' ; }   Release resources.
CUDA_SAFE_CALL ( cuMemFree ( dX )); CUDA_SAFE_CALL ( cuMemFree ( dY )); CUDA_SAFE_CALL ( cuMemFree ( dOut )); CUDA_SAFE_CALL ( cuModuleUnload ( module )); CUDA_SAFE_CALL ( cuCtxDestroy ( context )); delete [] hX ; delete [] hY ; delete [] hOut ;   Release resources.
free ( fatbin ); delete [] (( char * ) known ); delete [] (( char * ) dynamic ); return 0 ; } 6.2.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 6.3.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 6.3.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2023-2024, NVIDIA Corporation & Affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
Introduction  The JIT Link APIs are a set of APIs which can be used at runtime to link together GPU devide code.
The APIs accept inputs in multiple formats, either host objects, host libraries, fatbins, device cubins, PTX, or LTO-IR.
The output is a linked cubin that can be loaded by cuModuleLoadData and cuModuleLoadDataEx of the CUDA Driver API.
Link Time Optimization can also be performed when given LTO-IR or higher level formats that include LTO-IR.
The functionality in this library is similar to the cuLink* APIs in the CUDA Driver, with the following advantages: Support for Link Time Optimization Allow users to use runtime linking with the latest Toolkit version that is supported as part of CUDA Toolkit release.
This support may not be available in the CUDA Driver APIs if the application is running with an older driver installed in the system.
The clients get fine grain control and can specify low-level compiler options during linking. 2. Getting Started  2.1.
System Requirements  The JIT Link library requires the following system configuration: POSIX threads support for non-Windows platform.
CUDA Toolkit and Driver. 2.2. Installation  The JIT Link library is part of the CUDA Toolkit release and the components are organized as follows in the CUDA toolkit installation directory: On Windows: include vJitLink.h lib\x64 vJitLink.dll lib\x64 vJitLink_static.lib doc\pdf vJitLink_User_Guide.pdf On Linux: include/nvJitLink.h lib64/libnvJitLink.so lib64/libnvJitLink_static.a doc/pdf/nvJitLink_User_Guide.pdf 3.
Error codes  Enumerations nvJitLinkResult The enumerated type nvJitLinkResult defines API call result codes. 3.1.1. Enumerations  enum nvJitLinkResult  The enumerated type nvJitLinkResult defines API call result codes.
Values: enumerator NVJITLINK_SUCCESS  enumerator NVJITLINK_ERROR_UNRECOGNIZED_OPTION  enumerator NVJITLINK_ERROR_MISSING_ARCH  enumerator NVJITLINK_ERROR_INVALID_INPUT  enumerator NVJITLINK_ERROR_PTX_COMPILE  enumerator NVJITLINK_ERROR_NVVM_COMPILE  enumerator NVJITLINK_ERROR_INTERNAL  enumerator NVJITLINK_ERROR_THREADPOOL  enumerator NVJITLINK_ERROR_UNRECOGNIZED_INPUT  3.2.
Linking  Enumerations nvJitLinkInputType The enumerated type nvJitLinkInputType defines the kind of inputs that can be passed to nvJitLinkAdd* APIs.
Functions nvJitLinkResult nvJitLinkAddData (nvJitLinkHandle handle, nvJitLinkInputType inputType, const void *data, size_t size, const char *name) nvJitLinkAddData adds data image to the link.
nvJitLinkResult nvJitLinkAddFile (nvJitLinkHandle handle, nvJitLinkInputType inputType, const char *fileName) nvJitLinkAddFile reads data from file and links it in.
nvJitLinkResult nvJitLinkComplete (nvJitLinkHandle handle) nvJitLinkComplete does the actual link.
nvJitLinkResult nvJitLinkCreate (nvJitLinkHandle *handle, uint32_t numOptions, const char **options) nvJitLinkCreate creates an instance of nvJitLinkHandle with the given input options, and sets the output parameter handle .
nvJitLinkResult nvJitLinkDestroy (nvJitLinkHandle *handle) nvJitLinkDestroy frees the memory associated with the given handle and sets it to NULL.
nvJitLinkResult nvJitLinkGetErrorLog (nvJitLinkHandle handle, char *log) nvJitLinkGetErrorLog puts any error messages in the log.
nvJitLinkResult nvJitLinkGetErrorLogSize (nvJitLinkHandle handle, size_t *size) nvJitLinkGetErrorLogSize gets the size of the error log.
nvJitLinkResult nvJitLinkGetInfoLog (nvJitLinkHandle handle, char *log) nvJitLinkGetInfoLog puts any info messages in the log.
nvJitLinkResult nvJitLinkGetInfoLogSize (nvJitLinkHandle handle, size_t *size) nvJitLinkGetInfoLogSize gets the size of the info log.
nvJitLinkResult nvJitLinkGetLinkedCubin (nvJitLinkHandle handle, void *cubin) nvJitLinkGetLinkedCubin gets the linked cubin.
nvJitLinkResult nvJitLinkGetLinkedCubinSize (nvJitLinkHandle handle, size_t *size) nvJitLinkGetLinkedCubinSize gets the size of the linked cubin.
nvJitLinkResult nvJitLinkGetLinkedPtx (nvJitLinkHandle handle, char *ptx) nvJitLinkGetLinkedPtx gets the linked ptx.
nvJitLinkResult nvJitLinkGetLinkedPtxSize (nvJitLinkHandle handle, size_t *size) nvJitLinkGetLinkedPtxSize gets the size of the linked ptx.
nvJitLinkResult nvJitLinkVersion (unsigned int *major, unsigned int *minor) nvJitLinkVersion returns the current version of nvJitLink.
Typedefs nvJitLinkHandle nvJitLinkHandle is the unit of linking, and an opaque handle for a program. 3.2.1. Enumerations  enum nvJitLinkInputType  The enumerated type nvJitLinkInputType defines the kind of inputs that can be passed to nvJitLinkAdd* APIs.
Values: enumerator NVJITLINK_INPUT_NONE  enumerator NVJITLINK_INPUT_CUBIN  enumerator NVJITLINK_INPUT_PTX  enumerator NVJITLINK_INPUT_LTOIR  enumerator NVJITLINK_INPUT_FATBIN  enumerator NVJITLINK_INPUT_OBJECT  enumerator NVJITLINK_INPUT_LIBRARY  enumerator NVJITLINK_INPUT_ANY  3.2.2.
Functions  static inline nvJitLinkResult nvJitLinkAddData ( nvJitLinkHandle handle , nvJitLinkInputType inputType , const void * data , size_t size , const char * name )  nvJitLinkAddData adds data image to the link.
Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkAddFile ( nvJitLinkHandle handle , nvJitLinkInputType inputType , const char * fileName )  nvJitLinkAddFile reads data from file and links it in.
Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkComplete ( nvJitLinkHandle handle )  nvJitLinkComplete does the actual link.
Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkCreate ( nvJitLinkHandle * handle , uint32_t numOptions , const char * * options )  nvJitLinkCreate creates an instance of nvJitLinkHandle with the given input options, and sets the output parameter handle .
Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_UNRECOGNIZED_OPTION NVJITLINK_ERROR_MISSING_ARCH NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkDestroy ( nvJitLinkHandle * handle )  nvJitLinkDestroy frees the memory associated with the given handle and sets it to NULL.
Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetErrorLog ( nvJitLinkHandle handle , char * log )  nvJitLinkGetErrorLog puts any error messages in the log.
Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetErrorLogSize ( nvJitLinkHandle handle , size_t * size )  nvJitLinkGetErrorLogSize gets the size of the error log.
Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetInfoLog ( nvJitLinkHandle handle , char * log )  nvJitLinkGetInfoLog puts any info messages in the log.
Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetInfoLogSize ( nvJitLinkHandle handle , size_t * size )  nvJitLinkGetInfoLogSize gets the size of the info log.
Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetLinkedCubin ( nvJitLinkHandle handle , void * cubin )  nvJitLinkGetLinkedCubin gets the linked cubin.
Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetLinkedCubinSize ( nvJitLinkHandle handle , size_t * size )  nvJitLinkGetLinkedCubinSize gets the size of the linked cubin.
Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetLinkedPtx ( nvJitLinkHandle handle , char * ptx )  nvJitLinkGetLinkedPtx gets the linked ptx.
Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetLinkedPtxSize ( nvJitLinkHandle handle , size_t * size )  nvJitLinkGetLinkedPtxSize gets the size of the linked ptx.
Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL nvJitLinkResult nvJitLinkVersion ( unsigned int * major , unsigned int * minor )  nvJitLinkVersion returns the current version of nvJitLink.
Typedefs  typedef struct nvJitLink * nvJitLinkHandle  nvJitLinkHandle is the unit of linking, and an opaque handle for a program.
To link inputs, an instance of nvJitLinkHandle must be created first with nvJitLinkCreate(). 3.3. Supported Link Options  nvJitLink supports the link options below.
Options that take a value have an assignment operator ( = ) followed by the option value, with no spaces, e.g.
-ptx Emit ptx after linking instead of cubin; only supported with -lto -O Optimization level.
-variables-used= Pass list of variables that are used; any not in the list can be removed.
-optimize-unused-variables Normally device code optimization is limited by not knowing what the host code references.
With this option it can assume that if a variable is not referenced in device code then it can be removed.
-jump-table-density= When doing LTO, specify the case density percentage in switch statements, and use it as a minimal threshold to determine whether jump table(brx.idx instruction) will be used to implement a switch statement.
-no-cache Don’t cache the intermediate steps of nvJitLink. 4. Basic Usage  This section of the document uses a simple example to explain how to use the JIT Link APIs to link a program.
This example assumes we want to link for sm_80, but whatever arch is installed on the system should be used.
Linker creation and initialization of a program nvJitLink_t linker ; const char * link_options [] = { "-arch=sm_80" }; nvJitLinkCreate ( & linker , 1 , link_options ); Assume that we already have two relocatable input files (a.o and b.o), which could be created with the nvcc -dc command.
Inputs to linker nvJitLinkAddFile ( linker , NVJITLINK_INPUT_OBJECT , "a.o" ); nvJitLinkAddFile ( linker , NVJITLINK_INPUT_OBJECT , "b.o" ); Now the actual link can be done as shown in Figure 3 .
Linking of the PTX program nvJitLinkComplete ( linker ); The linked GPU assembly code can now be obtained.
And to allocate memory, we need to query the size of the image of the linked GPU assembly code which is done as shown in Figure 4 .
Query size of the linked assembly image nvJitLinkGetLinkedCubinSize ( linker , & cubinSize ); The image of the linked GPU assembly code can now be queried as shown in Figure 5 .
Query the linked assembly image elf = ( char * ) malloc ( cubinSize ); nvJitLinkGetLinkedCubin ( linker , ( void * ) elf ); When the linker is not needed anymore, it can be destroyed as shown in Figure 6 .
Compatibility  The nvJitLink library is compatible across minor versions in a release, but may not be compatible across major versions.
The library version itself must be >= the maximum version of the inputs, and the shared library version must be >= the version that was linked with.
For example, you can link an object created with 12.0 and one with 12.1 if your nvJitLink library is version 12.x where x >= 1.
If it was linked with 12.1, then you can replace and use the nvJitLink shared library with any version 12.x where x >= 1.
On the flip side, you cannot use 12.0 to link 12.1 objects, nor use 12.0 nvJitLink library to run 12.1 code.
Linking across major versions (like 11.x with 12.x) works for ELF and PTX inputs, but does not work with LTOIR inputs.
If using LTO, then compatibility is only guaranteed within a major release. 6. Example: Device LTO (link time optimization)  This section demonstrates device link time optimization (LTO).
The first unit is generated offline using nvcc , by specifying the architecture as ‘ -arch lto_XX ’ (see offline.cu).
The second unit is generated online using NVRTC, by specifying the flag ‘ -dlto ’ (see online.cpp).
These two units are then passed to libnvJitLink* API functions, which link together the LTO IR, run the optimizer on the linked IR, and generate a cubin (see online.cpp).
The cubin is then loaded on the GPU and executed. 6.1. Code (offline.cu)  __device__ float compute ( float a , float x , float y ) { return a * x + y ; } 6.2.
Code (online.cpp)  #include #include #include #include #include #define NUM_THREADS 128 #define NUM_BLOCKS 32 #define NVRTC_SAFE_CALL(x) \ do { \ nvrtcResult result = x; \ if (result != NVRTC_SUCCESS) { \ std::cerr 0) { \ char *log = (char*)malloc(lsize); \ result = nvJitLinkGetErrorLog(h, log); \ if (result == NVJITLINK_SUCCESS) { \ std::cerr ( i ); hY [ i ] = static_cast ( i * 2 ); } CUdeviceptr dX , dY , dOut ; CUDA_SAFE_CALL ( cuMemAlloc ( & dX , bufferSize )); CUDA_SAFE_CALL ( cuMemAlloc ( & dY , bufferSize )); CUDA_SAFE_CALL ( cuMemAlloc ( & dOut , bufferSize )); CUDA_SAFE_CALL ( cuMemcpyHtoD ( dX , hX , bufferSize )); CUDA_SAFE_CALL ( cuMemcpyHtoD ( dY , hY , bufferSize ));   Execute SAXPY.
void * args [] = { & a , & dX , & dY , & dOut , & n }; CUDA_SAFE_CALL ( cuLaunchKernel ( kernel , NUM_BLOCKS , 1 , 1 ,   grid dim NUM_THREADS , 1 , 1 ,   block dim 0 , NULL ,   shared mem and stream args , 0 ));   arguments CUDA_SAFE_CALL ( cuCtxSynchronize ());   Retrieve and print output.
CUDA_SAFE_CALL ( cuMemcpyDtoH ( hOut , dOut , bufferSize )); for ( size_t i = 0 ; i < n ; ++ i ) { std :: cout << a << " * " << hX [ i ] << " + " << hY [ i ] << " = " << hOut [ i ] << ' ' ; }   Release resources.
CUDA_SAFE_CALL ( cuMemFree ( dX )); CUDA_SAFE_CALL ( cuMemFree ( dY )); CUDA_SAFE_CALL ( cuMemFree ( dOut )); CUDA_SAFE_CALL ( cuModuleUnload ( module )); CUDA_SAFE_CALL ( cuCtxDestroy ( context )); free ( cubin ); delete [] hX ; delete [] hY ; delete [] hOut ; delete [] LTOIR ; return 0 ; } 6.3.
Build Instructions  Assuming the environment variable CUDA_PATH points to CUDA Toolkit installation directory, build this example as: Compile offline.cu to fatbinary containing LTO IR (change lto_52 to a different lto_XX architecture as appropriate).
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 6.4.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 6.4.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates.
Introduction v12.5 | PDF | Archive cuSPARSE The API reference guide for cuSPARSE, the CUDA sparse matrix library.
Introduction  The cuSPARSE library contains a set of GPU-accelerated basic linear algebra subroutines used for handling sparse matrices that perform significantly faster than CPU-only alternatives.
Depending on the specific operation, the library targets matrices with sparsity ratios in the range between 70%-99.9%.
It is implemented on top of the NVIDIA® CUDA™ runtime (which is part of the CUDA Toolkit) and is designed to be called from C and C++.
see also cuSPARSELt: A High-Performance CUDA Library for Sparse Matrix-Matrix Multiplication cuSPARSE Release Notes : cuda-toolkit-release-notes cuSPARSE GitHub Samples : CUDALibrarySamples Nvidia Developer Forum : GPU-Accelerated Libraries Provide Feedback : Math-Libs-Feedback @ nvidia .
Library Organization and Features  The cuSPARSE library is organized in two set of APIs: The Legacy APIs , inspired by the Sparse BLAS standard, provide a limited set of functionalities and will not be improved in future releases , even if standard maintenance is still ensured.
A replacement will be provided for the most important of them during the deprecation process.
They allow computing the most common sparse linear algebra operations, such as sparse matrix-vector (SpMV) and sparse matrix-matrix multiplication (SpMM), in a flexible way.
The new APIs have the following capabilities and features: Set matrix data layouts , number of batches , and storage formats (for example, CSR, COO, and so on).
Provide constant descriptors for vector and matrix inputs to support const-safe interface and guarantee that the APIs do not modify their inputs. 1.2. Static Library Support  Starting with CUDA 6.5, the cuSPARSE library is also delivered in a static form as libcusparse_static.a on Linux.
For example, to compile a small application using cuSPARSE against the dynamic library , the following command can be used: nvcc my_cusparse_app .
cu - lcusparse - o my_cusparse_app Whereas to compile against the static library , the following command has to be used: nvcc my_cusparse_app .
cu - lcusparse_static - o my_cusparse_app It is also possible to use the native Host C++ compiler.
Depending on the Host Operating system, some additional libraries like pthread or dl might be needed on the linking line.
c - lcusparse_static - lcudart_static - lpthread - ldl - I / include - L / lib64 - o my_cusparse_app Note that in the latter case, the library cuda is not needed.
In the case of a system which does not have the CUDA driver installed, this allows the application to gracefully manage this issue and potentially run if a CPU-only path is available. 1.3. Library Dependencies  Starting with CUDA 12.0, cuSPARSE will depend on nvJitLink library for JIT (Just-In-Time) LTO (Link-Time-Optimization) capabilities; refer to the cusparseSpMMOp APIs for more information.
If the user links to the dynamic library , the environment variables for loading the libraries at run-time (such as LD_LIBRARY_PATH on Linux and PATH on Windows) must include the path where libnvjitlink.so is located.
If linking to the static library , the user needs to link with -lnvjitlink and set the environment variables for loading the libraries at compile-time LIBRARY_PATH/PATH accordingly. 2. Using the cuSPARSE API  This chapter describes how to use the cuSPARSE library API.
It is not a reference for the cuSPARSE API data types and functions; that is provided in subsequent chapters. 2.1. APIs Usage Notes  The cuSPARSE library allows developers to access the computational resources of the NVIDIA graphics processing unit (GPU).
The cuSPARSE APIs assume that input and output data (vectors and matrices) reside in GPU (device) memory .
\(\alpha\) and \(\beta\) ) can be passed by reference on the host or the device, instead of only being allowed to be passed by value on the host.
This allows library functions to execute asynchronously using streams even when they are generated by a previous kernel resulting in maximum parallelism.
The handle to the cuSPARSE library context is initialized using the function and is explicitly passed to every subsequent library function call.
This allows the user to have more control over the library setup when using multiple host threads and multiple GPUs.
It is the responsibility of the developer to allocate memory and to copy data between GPU memory and CPU memory using standard CUDA runtime API routines, such as cudaMalloc() , cudaFree() , cudaMemcpy() , and cudaMemcpyAsync() .
The cuSPARSE library functions are executed asynchronously with respect to the host and may return control to the application on the host before the result is ready.
Developers can use the cudaDeviceSynchronize() function to ensure that the execution of a particular cuSPARSE library routine has completed.
A developer can also use the cudaMemcpy() routine to copy data from the device to the host and vice versa, using the cudaMemcpyDeviceToHost and cudaMemcpyHostToDevice parameters, respectively.
In this case there is no need to add a call to cudaDeviceSynchronize() because the call to cudaMemcpy() with the above parameters is blocking and completes only when the results are ready on the host. 2.2. Deprecated APIs  The cuSPARSE library documentation explicitly indicates the set of APIs/enumerators/data structures that are deprecated.
The library policy for deprecated APIs is the following: An API is marked [[DEPRECATED]] on a release X.Y (e.g.
11.2) The documentation indices a replacement if available Otherwise, the functionality will not be maintained in the future The API will be removed in the release X+1.0 (e.g.
12.0) Correctness bugs are still addressed even for deprecated APIs, while performance issues are not always ensured.
In addition to the documentation, deprecated APIs generate a compile-time warning for most platforms when used.
Deprecation warnings can be disabled by defining the macro DISABLE_CUSPARSE_DEPRECATED before including cusparse.h or by passing the flag -DDISABLE_CUSPARSE_DEPRECATED to the compiler. 2.3. Thread Safety  The library is thread safe and its functions can be called from multiple host threads, even with the same handle.
When multiple threads share the same handle, extreme care needs to be taken when the handle configuration is changed because that change will affect potentially subsequent cuSPARSE calls in all threads.
So it is not recommended that multiple thread share the same cuSPARSE handle. 2.4. Result Reproducibility  The design of cuSPARSE prioritizes performance over bit-wise reproducibility.
Operations using transpose or conjugate-transpose cusparseOperation_t have no reproducibility guarantees.
For the remaining operations, performing the same API call twice with the exact same arguments, on the same machine, with the same executable will produce bit-wise identical results.
This bit-wise reproducibility can be disrupted by changes to: hardware, CUDA drivers, cuSPARSE version, memory alignment of the data, or algorithm selection. 2.5. NaN and Inf Propagation  Floating-point numbers have special values for NaN (not-a-number) and Inf (infinity).
NaN and Inf appear in the output only if the algorithms happen to generate or propagate them.
Because the algorithms are subject to change based on toolkit version and runtime considerations, so too are the propagation behaviours of NaN and Inf.
NaN propagation is different in cuSPARSE than in typical dense numerical linear algebra, such as cuBLAS.
The dot product between vectors [0, 1, 0] and [1, 1, NaN] is NaN when using typical dense numerical algorithms, but will be 1.0 with typical sparse numerical algorithms. 2.6. Parallelism with Streams  If the application performs several small independent computations, or if it makes data transfers in parallel with the computation, CUDA streams can be used to overlap these tasks.
To achieve the overlap of computation between the tasks, the developer should create CUDA streams using the function cudaStreamCreate() and set the stream to be used by each individual cuSPARSE library routine by calling cusparseSetStream() just before calling the actual cuSPARSE routine.
Then, computations performed in separate streams would be overlapped automatically on the GPU, when possible.
This approach is especially useful when the computation performed by a single task is relatively small and is not enough to fill the GPU with work, or when there is a data transfer that can be performed in parallel with the computation.
When streams are used, we recommend using the new cuSPARSE API with scalar parameters and results passed by reference in the device memory to achieve maximum computational overlap.
Although a developer can create many streams, in practice it is not possible to have more than 16 concurrent kernels executing at the same time. 2.7. Compatibility and Versioning  The cuSPARSE APIs are intended to be backward compatible at the source level with future releases (unless stated otherwise in the release notes of a specific future release).
In other words, if a program uses cuSPARSE, it should continue to compile and work correctly with newer versions of cuSPARSE without source code changes.
Using different versions of the cusparse.h header file and shared library is not supported.
The library uses the standard version semantic convention for identify different releases.
The version takes the form of four fields joined by periods: MAJOR.MINOR.PATCH.BUILD These version fields are incremented based on the following rules: MAJOR : API breaking changes or new CUDA major version (breaking changes at lower level, e.g.
drivers, compilers, libraries) MINOR : new APIs and functionalities PATCH : Bug fixes or performance improvements (or * new CUDA release) BUILD : Internal build number * Different CUDA toolkit releases ensure distinct library versions even if there are no changes at library level. 2.8. Optimization Notes  Most of the cuSPARSE routines can be optimized by exploiting CUDA Graphs capture and Hardware Memory Compression features.
More in details, a single cuSPARSE call or a sequence of calls can be captured by a CUDA Graph and executed in a second moment.
This minimizes kernels launch overhead and allows the CUDA runtime to optimize the whole workflow.
A full example of CUDA graphs capture applied to a cuSPARSE routine can be found in cuSPARSE Library Samples - CUDA Graph .
Secondly, the data types and functionalities involved in cuSPARSE are suitable for Hardware Memory Compression available in Ampere GPU devices (compute capability 8.0) or above.
The feature allows memory compression for data with enough zero bytes without no loss of information.
A full example of Hardware Memory Compression applied to a cuSPARSE routine can be found in cuSPARSE Library Samples - Memory Compression . 3. cuSPARSE Storage Formats  The cuSPARSE library supports dense and sparse vector, and dense and sparse matrix formats.
3.1. Index Base  The library supports zero- and one-based indexing to ensure the compatibility with C/C++ and Fortran languages respectively.
The index base is selected through the cusparseIndexBase_t type. 3.2. Vector Formats  This section describes dense and sparse vector formats.
3.2.1. Dense Vector Format  Dense vectors are represented with a single data array that is stored linearly in memory, such as the following \(7 \times 1\) dense vector.
The indices array represent the positions of the corresponding nonzero values in the equivalent array in dense format.
For example, the dense vector in section 3.2.1 can be stored as a sparse vector with zero-based or one-based indexing.
Sparse vector representation  Note The cuSPARSE routines assume that the indices are provided in increasing order and that each index appears only once.
In the opposite case, the correctness of the computation is not always ensured. 3.3. Matrix Formats  Dense and several sparse formats for matrices are discussed in this section.
3.3.1. Dense Matrix Format  A dense matrix can be stored in both row-major and column-major memory layout (ordering) and it is represented by the following parameters.
The leading dimension , which must be Greater than or equal to the number of columns in the row-major layout Greater than or equal to the number of rows in the column-major layout The pointers to the values array of length \(rows \times leading\; dimension\) in the row-major layout \(columns \times leading\; dimension\) in the column-major layout The following figure represents a \(5 \times 2\) dense matrix with both memory layouts Dense matrix representations  The indices within the matrix represents the contiguous locations in memory.
The leading dimension is useful to represent a sub-matrix within the original one Sub-matrix representations  3.3.2.
Coordinate (COO)  A sparse matrix stored in COO format is represented by the following parameters.
The pointers to the row indices array of length nnz that contains the row indices of the corresponding elements in the values array .
The pointers to the column indices array of length nnz that contains the column indices of the corresponding elements in the values array .
The pointers to the values array of length nnz that holds all nonzero values of the matrix in row-major ordering.
Note If the column indices within a given row are not unique, the correctness of the computation is not always ensured.
Given an entry in the COO format (zero-base), the corresponding position in the dense matrix is computed as:   row-major rows_indices [ i ] * leading_dimension + column_indices [ i ]   column-major column_indices [ i ] * leading_dimension + rows_indices [ i ] 3.3.3.
Compressed Sparse Row (CSR)  The CSR format is similar to COO, where the row indices are compressed and replaced by an array of offsets .
The pointers to the row offsets array of length number of rows + 1 that represents the starting position of each row in the columns and values arrays .
Given an entry in the CSR format (zero-base), the corresponding position in the dense matrix is computed as:   row-major row * leading_dimension + column_indices [ row_offsets [ row ] + k ]   column-major column_indices [ row_offsets [ row ] + k ] * leading_dimension + row 3.3.4.
Compressed Sparse Column (CSC)  The CSC format is similar to COO, where the column indices are compressed and replaced by an array of offsets .
The pointers to the column offsets array of length number of column + 1 that represents the starting position of each column in the columns and values arrays .
The pointers to the row indices array of length nnz that contains row indices of the corresponding elements in the values array .
The pointers to the values array of length nnz that holds all nonzero values of the matrix in column-major ordering.
Note The CSR format has exactly the same memory layout as its transpose in CSC format (and vice versa).
Note If the row indices within a given column are not unique, the correctness of the computation is not always ensured.
Given an entry in the CSC format (zero-base), the corresponding position in the dense matrix is computed as:   row-major column * leading_dimension + row_indices [ column_offsets [ column ] + k ]   column-major row_indices [ column_offsets [ column ] + k ] * leading_dimension + column 3.3.5.
Sliced Ellpack (SELL)  The Sliced Ellpack format is standardized and well-known as the state of the art.
This format allows to significantly improve the performance of all problems that involve low variability in the number of nonzero elements per row.
A matrix in the Sliced Ellpack format is divided into slices of an exact number of rows ( \(sliceSize\) ), defined by the user.
The maximum row length (i.e., the maximum non-zeros per row) is found for each slice, and every row in the slice is padded to the maximum row length.
A \(m \times n\) sparse matrix \(A\) is equivalent to a sliced sparse matrix \(A_{s}\) with \(nslices = \left \lceil{\frac{m}{sliceSize}} ight  ceil\) slice rows and \(n\) columns.
To improve memory coalescing and memory utilization, each slice is stored in column-major order.
The total number elements ( sellValuesSize ), including non-zero values and padded elements.
The pointer to the slice offsets of length \(nslices + 1\) that holds offsets of the slides corresponding to the columns and values arrays.
The pointer to the column indices array of length sellValuesSize that contains column indices of the corresponding elements in the values array.
The pointer to the values array of length sellValuesSize that holds all non-zero values and padding in column-major layout.
The following example shows a \(5 \times 4\) matrix represented in SELL format. 3.3.6. Block Sparse Row (BSR)  The BSR format is similar to CSR, where the column indices represent two-dimensional blocks instead of a single matrix entry.
A matrix in the Block Sparse Row format is organized into blocks of size \(blockSize\) , defined by the user.
A \(m \times n\) sparse matrix \(A\) is equivalent to a block sparse matrix \(A_{B}\) : \(mb \times nb\) with \(mb = \frac{m}{blockSize}\) block rows and \(nb = \frac{n}{blockSize}\) block columns .
If \(m\) or \(n\) is not multiple of \(blockSize\) , the user needs to pad the matrix with zeros.
However, the internal storage format of blocks can be column-major ( cusparseDirection_t=CUSPARSE_DIRECTION_COLUMN ) or row-major ( cusparseDirection_t=CUSPARSE_DIRECTION_ROW ), independently of the base index.
The pointers to the row block offsets array of length number of row blocks + 1 that represents the starting position of each row block in the columns and values arrays .
The pointers to the column block indices array of length nnzb that contains the location of the corresponding elements in the values array.
The pointers to the values array of length nnzb that holds all nonzero values of the matrix.
The following example shows a \(4 \times 7\) matrix represented in BSR format. 3.3.7. Blocked Ellpack (BLOCKED-ELL)  The Blocked Ellpack format is similar to the standard Ellpack, where the column indices represent two-dimensional blocks instead of a single matrix entry.
A matrix in the Blocked Ellpack format is organized into blocks of size \(blockSize\) , defined by the user.
The number of columns per row \(nEllCols\) is also defined by the user ( \(nEllCols \le n\) ).
A \(m \times n\) sparse matrix \(A\) is equivalent to a Blocked-ELL matrix \(A_{B}\) : \(mb \times nb\) with \(mb = \left \lceil{\frac{m}{blockSize}} ight  ceil\) block rows , and \(nb = \left \lceil{\frac{nEllCols}{blockSize}} ight  ceil\) block columns.
If \(m\) or \(n\) is not multiple of \(blockSize\) , then the remaining elements are zero.
The pointers to the column block indices array of length \(mb \times nb\) that contains the location of the corresponding elements in the values array.
The pointers to the values array of length \(m \times nEllCols\) that holds all nonzero values of the matrix in row-major ordering.
The following example shows a \(9 \times 9\) matrix represented in Blocked-ELL format. 3.3.8. Extended BSR Format (BSRX) [DEPRECATED]  BSRX is the same as the BSR format, but the array bsrRowPtrA is separated into two parts.
The first nonzero block of each row is still specified by the array bsrRowPtrA , which is the same as in BSR, but the position next to the last nonzero block of each row is specified by the array bsrEndPtrA .
bsrValA (pointer) Points to the data array of length \(nnzb \ast blockDim^{2}\) that holds all the elements of the nonzero blocks of A .
bsrRowPtrA (pointer) Points to the integer array of length mb that holds indices into the arrays bsrColIndA and bsrValA ; bsrRowPtrA(i) is the position of the first nonzero block of the i th block row in bsrColIndA and bsrValA .
bsrEndPtrA (pointer) Points to the integer array of length mb that holds indices into the arrays bsrColIndA and bsrValA ; bsrRowPtrA(i) is the position next to the last nonzero block of the i th block row in bsrColIndA and bsrValA .
bsrColIndA (pointer) Points to the integer array of length nnzb that contains the column indices of the corresponding blocks in array bsrValA .
\(A_{b} = \begin{bmatrix} A_{00} & A_{01} & A_{02} \\ A_{10} & A_{11} & A_{12} \\ \end{bmatrix}\) Assume it has this BSR format.
\(\begin{matrix} \text{bsrValA of BSR} & = & \begin{bmatrix} A_{00} & A_{01} & A_{10} & A_{11} & A_{12} \\ \end{bmatrix} \\ \text{bsrRowPtrA of BSR} & = & \begin{bmatrix} {0\phantom{.0}} & {2\phantom{.0}} & 5 \\ \end{bmatrix} \\ \text{bsrColIndA of BSR} & = & \begin{bmatrix} {0\phantom{.0}} & {1\phantom{.0}} & {0\phantom{.0}} & {1\phantom{.0}} & 2 \\ \end{bmatrix} \\ \end{matrix}\) The bsrRowPtrA of the BSRX format is simply the first two elements of the bsrRowPtrA BSR format.
\(\begin{matrix} \text{bsrRowPtrA of BSRX} & = & \begin{bmatrix} {0\phantom{.0}} & 2 \\ \end{bmatrix} \\ \text{bsrEndPtrA of BSRX} & = & \begin{bmatrix} {2\phantom{.0}} & 5 \\ \end{bmatrix} \\ \end{matrix}\) The advantage of the BSRX format is that the developer can specify a submatrix in the original BSR format by modifying bsrRowPtrA and bsrEndPtrA while keeping bsrColIndA and bsrValA unchanged.
For example, to create another block matrix \(\widetilde{A} = \begin{bmatrix} O & O & O \\ O & A_{11} & O \\ \end{bmatrix}\) that is slightly different from \(A\) , the developer can keep bsrColIndA and bsrValA , but reconstruct \(\widetilde{A}\) by properly setting of bsrRowPtrA and bsrEndPtrA .
\(\begin{matrix} {\text{bsrValA of }\widetilde{A}} & = & \begin{bmatrix} A_{00} & A_{01} & A_{10} & A_{11} & A_{12} \\ \end{bmatrix} \\ {\text{bsrColIndA of }\widetilde{A}} & = & \begin{bmatrix} {0\phantom{.0}} & {1\phantom{.0}} & {0\phantom{.0}} & {1\phantom{.0}} & 2 \\ \end{bmatrix} \\ {\text{bsrRowPtrA of }\widetilde{A}} & = & \begin{bmatrix} {0\phantom{.0}} & 3 \\ \end{bmatrix} \\ {\text{bsrEndPtrA of }\widetilde{A}} & = & \begin{bmatrix} {0\phantom{.0}} & 4 \\ \end{bmatrix} \\ \end{matrix}\) 4.
cudaDataType_t  The section describes the types shared by multiple CUDA Libraries and defined in the header file library_types.h .
If a specific GPU model does not provide native support for a given data type, the routine returns CUSPARSE_STATUS_ARCH_MISMATCH error.
Unsupported data types and Compute Capability (CC): __half on GPUs with CC - while level is one of the following levels: 0 - Off - logging is disabled (default) 1 - Error - only errors will be logged 2 - Trace - API calls that launch CUDA kernels will log their parameters and important information 3 - Hints - hints that can potentially improve the application’s performance 4 - Info - provides general information about the library execution, may contain details about heuristic status 5 - API Trace - API calls will log their parameter and important information CUSPARSE_LOG_MASK= - while mask is a combination of the following masks: 0 - Off 1 - Error 2 - Trace 4 - Hints 8 - Info 16 - API Trace CUSPARSE_LOG_FILE= - while file name is a path to a logging file.
Starting from CUDA 12.3, it is also possible to dump sparse matrices (CSR, CSC, COO, SELL, BSR) in binary files during the creation by setting the environment variable CUSPARSE_STORE_INPUT_MATRIX .
com for debugging and reproducibility purposes of a specific correctness/performance issue.
See: cusparseLoggerSetCallback() cusparseLoggerSetFile() cusparseLoggerOpenFile() cusparseLoggerSetLevel() cusparseLoggerSetMask() cusparseLoggerForceDisable() Note The logging mechanism is not available for the legacy APIs. 4.3.1. cusparseLoggerSetCallback()  cusparseStatus_t cusparseLoggerSetCallback ( cusparseLoggerCallback_t callback ) Experimental : The function sets the logging callback function.
In/out Meaning callback IN Pointer to a callback function where cusparseLoggerCallback_t has the following signature: void ( * cusparseLoggerCallback_t )( int logLevel , const char * functionName , const char * message ) Param.
In/out Meaning logLevel IN Selected log level functionName IN The name of the API that logged this message message IN The log message See cusparseStatus_t for the description of the return status 4.3.2.
cusparseLoggerSetFile()  cusparseStatus_t cusparseLoggerSetFile ( FILE * file ) Experimental : The function sets the logging output file.
Note: once registered using this function call, the provided file handle must not be closed unless the function is called again to switch to a different file handle.
File should have write permission See cusparseStatus_t for the description of the return status 4.3.3.
cusparseLoggerOpenFile()  cusparseStatus_t cusparseLoggerOpenFile ( const char * logFile ) Experimental : The function opens a logging output file in the given path.
In/out Meaning logFile IN Path of the logging output file See cusparseStatus_t for the description of the return status 4.3.4.
cusparseLoggerSetLevel()  cusparseStatus_t cusparseLoggerSetLevel ( int level ) Experimental : The function sets the value of the logging level.
In/out Meaning level IN Value of the logging level See cusparseStatus_t for the description of the return status 4.3.5.
cusparseLoggerSetMask()  cusparseStatus_t cusparseLoggerSetMask ( int mask ) Experimental : The function sets the value of the logging mask.
In/out Meaning mask IN Value of the logging mask See cusparseStatus_t for the description of the return status 5.
Naming Conventions  The cuSPARSE legacy functions are available for data types float , double , cuComplex , and cuDoubleComplex .
The sparse Level 2, and Level 3 functions follow this naming convention: cusparse [][] where can be S , D , C , Z , or X , corresponding to the data types float , double , cuComplex , cuDoubleComplex , and the generic type, respectively.
The can be dense , coo , csr , or csc , corresponding to the dense, coordinate, compressed sparse row, and compressed sparse column formats, respectively. 5.2. cuSPARSE Legacy Types Reference  5.2.1.
cusparseAction_t  This type indicates whether the operation is performed only on indices or on data and indices.
CUSPARSE_ACTION_NUMERIC the operation is performed on data and indices. 5.2.2. cusparseMatDescr_t  This structure is used to describe the shape and properties of a matrix.
typedef struct { cusparseMatrixType_t MatrixType ; cusparseFillMode_t FillMode ; cusparseDiagType_t DiagType ; cusparseIndexBase_t IndexBase ; } cusparseMatDescr_t ; 5.2.3.
Notice that for symmetric, Hermitian and triangular matrices only their lower or upper part is assumed to be stored.
The whole idea of matrix type and fill mode is to keep minimum storage for symmetric/Hermitian matrix, and also to take advantage of symmetric property on SpMV (Sparse Matrix Vector multiplication).
To compute y=A*x when A is symmetric and only lower triangular part is stored, two steps are needed.
Given the fact that the transpose operation y=L^T*x is 10x slower than non-transpose version y=L*x , the symmetric property does not show up any performance gain.
It is better for the user to extend the symmetric matrix to a general matrix and apply y=A*x with matrix type CUSPARSE_MATRIX_TYPE_GENERAL .
In general, SpMV, preconditioners (incomplete Cholesky or incomplete LU) and triangular solver are combined together in iterative solvers, for example PCG and GMRES.
If the user always uses general matrix (instead of symmetric matrix), there is no need to support other than general matrix in preconditioners.
Therefore the new routines, [bsr|csr]sv2 (triangular solver), [bsr|csr]ilu02 (incomplete LU) and [bsr|csr]ic02 (incomplete Cholesky), only support matrix type CUSPARSE_MATRIX_TYPE_GENERAL .
CUSPARSE_MATRIX_TYPE_TRIANGULAR the matrix is triangular. 5.2.4. cusparseColorInfo_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in csrcolor() .
5.2.5. cusparseSolvePolicy_t [DEPRECATED]  This type indicates whether level information is generated and used in csrsv2, csric02, csrilu02, bsrsv2, bsric02 and bsrilu02 .
CUSPARSE_SOLVE_POLICY_USE_LEVEL generate and use level information. 5.2.6. bsric02Info_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in bsric02_bufferSize() , bsric02_analysis() , and bsric02() .
5.2.7. bsrilu02Info_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in bsrilu02_bufferSize() , bsrilu02_analysis() , and bsrilu02() .
5.2.8. bsrsm2Info_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in bsrsm2_bufferSize() , bsrsm2_analysis() , and bsrsm2_solve() .
5.2.9. bsrsv2Info_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in bsrsv2_bufferSize() , bsrsv2_analysis() , and bsrsv2_solve() .
5.2.10. csric02Info_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in csric02_bufferSize() , csric02_analysis() , and csric02() .
5.2.11. csrilu02Info_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in csrilu02_bufferSize() , csrilu02_analysis() , and csrilu02() .
5.3. cuSPARSE Helper Function Reference  The cuSPARSE helper functions are described in this section.
5.3.1. cusparseCreateColorInfo() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateColorInfo ( cusparseColorInfo_t * info ) This function creates and initializes the cusparseColorInfo_t structure to default values.
Input info the pointer to the cusparseColorInfo_t structure See cusparseStatus_t for the description of the return status 5.3.2.
cusparseCreateMatDescr()  cusparseStatus_t cusparseCreateMatDescr ( cusparseMatDescr_t * descrA ) This function initializes the matrix descriptor.
It sets the fields MatrixType and IndexBase to the default values CUSPARSE_MATRIX_TYPE_GENERAL and CUSPARSE_INDEX_BASE_ZERO , respectively, while leaving other fields uninitialized.
cusparseDestroyColorInfo() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyColorInfo ( cusparseColorInfo_t info ) This function destroys and releases any memory required by the structure.
Input info the pointer to the structure of csrcolor() See cusparseStatus_t for the description of the return status 5.3.4.
cusparseDestroyMatDescr()  cusparseStatus_t cusparseDestroyMatDescr ( cusparseMatDescr_t descrA ) This function releases the memory allocated for the matrix descriptor.
cusparseGetMatDiagType()  cusparseDiagType_t cusparseGetMatDiagType ( const cusparseMatDescr_t descrA ) This function returns the DiagType field of the matrix descriptor descrA .
Returned One of the enumerated diagType types. 5.3.6. cusparseGetMatFillMode()  cusparseFillMode_t cusparseGetMatFillMode ( const cusparseMatDescr_t descrA ) This function returns the FillMode field of the matrix descriptor descrA .
Returned One of the enumerated fillMode types. 5.3.7. cusparseGetMatIndexBase()  cusparseIndexBase_t cusparseGetMatIndexBase ( const cusparseMatDescr_t descrA ) This function returns the IndexBase field of the matrix descriptor descrA .
Returned One of the enumerated indexBase types. 5.3.8. cusparseGetMatType()  cusparseMatrixType_t cusparseGetMatType ( const cusparseMatDescr_t descrA ) This function returns the MatrixType field of the matrix descriptor descrA .
Returned One of the enumerated matrix types. 5.3.9. cusparseSetMatDiagType()  cusparseStatus_t cusparseSetMatDiagType ( cusparseMatDescr_t descrA , cusparseDiagType_t diagType ) This function sets the DiagType field of the matrix descriptor descrA .
cusparseSetMatFillMode()  cusparseStatus_t cusparseSetMatFillMode ( cusparseMatDescr_t descrA , cusparseFillMode_t fillMode ) This function sets the FillMode field of the matrix descriptor descrA .
cusparseSetMatIndexBase()  cusparseStatus_t cusparseSetMatIndexBase ( cusparseMatDescr_t descrA , cusparseIndexBase_t base ) This function sets the IndexBase field of the matrix descriptor descrA .
cusparseSetMatType()  cusparseStatus_t cusparseSetMatType ( cusparseMatDescr_t descrA , cusparseMatrixType_t type ) This function sets the MatrixType field of the matrix descriptor descrA .
cusparseCreateCsric02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateCsric02Info ( csric02Info_t * info ); This function creates and initializes the solve and analysis structure of incomplete Cholesky to default values.
cusparseDestroyCsric02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyCsric02Info ( csric02Info_t info ); This function destroys and releases any memory required by the structure.
cusparseCreateCsrilu02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateCsrilu02Info ( csrilu02Info_t * info ); This function creates and initializes the solve and analysis structure of incomplete LU to default values.
cusparseDestroyCsrilu02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyCsrilu02Info ( csrilu02Info_t info ); This function destroys and releases any memory required by the structure.
cusparseCreateBsrsv2Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateBsrsv2Info ( bsrsv2Info_t * info ); This function creates and initializes the solve and analysis structure of bsrsv2 to default values.
cusparseDestroyBsrsv2Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyBsrsv2Info ( bsrsv2Info_t info ); This function destroys and releases any memory required by the structure.
cusparseCreateBsrsm2Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateBsrsm2Info ( bsrsm2Info_t * info ); This function creates and initializes the solve and analysis structure of bsrsm2 to default values.
cusparseDestroyBsrsm2Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyBsrsm2Info ( bsrsm2Info_t info ); This function destroys and releases any memory required by the structure.
cusparseCreateBsric02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateBsric02Info ( bsric02Info_t * info ); This function creates and initializes the solve and analysis structure of block incomplete Cholesky to default values.
cusparseDestroyBsric02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyBsric02Info ( bsric02Info_t info ); This function destroys and releases any memory required by the structure.
cusparseCreateBsrilu02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateBsrilu02Info ( bsrilu02Info_t * info ); This function creates and initializes the solve and analysis structure of block incomplete LU to default values.
cusparseDestroyBsrilu02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyBsrilu02Info ( bsrilu02Info_t info ); This function destroys and releases any memory required by the structure.
cusparseCreatePruneInfo() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreatePruneInfo ( pruneInfo_t * info ); This function creates and initializes structure of prune to default values.
cusparseDestroyPruneInfo() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyPruneInfo ( pruneInfo_t info ); This function destroys and releases any memory required by the structure.
The size of vector x should be \((nb \ast blockDim)\) at least, and the size of vector y should be \((mb \ast blockDim)\) at least; otherwise, the kernel may return CUSPARSE_STATUS_EXECUTION_FAILED because of an out-of-bounds array.
For example, suppose the user has a CSR format and wants to try bsrmv() , the following code demonstrates how to use csr2bsr() conversion and bsrmv() multiplication in single precision.
Suppose that A is m x n sparse matrix represented by CSR format,   hx is a host vector of size n, and hy is also a host vector of size m.
dir storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN .
Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE .
bsrVal array of nnz \(( =\) csrRowPtrA(mb) \(-\) csrRowPtrA(0) \()\) nonzero blocks of matrix \(A\) .
bsrRowPtr integer array of mb \(+ 1\) elements that contains the start of every block row and the end of the last block row plus one.
bsrColInd integer array of nnz \(( =\) csrRowPtrA(mb) \(-\) csrRowPtrA(0) \()\) column indices of the nonzero blocks of matrix \(A\) .
If row \(i\) is not specified in bsrMaskPtr , then bsrxmv() does not touch row block \(i\) of \(A\) and \(y\) .
For example, consider the \(2 \times 3\) block matrix \(A\) : \(\begin{matrix} {A = \begin{bmatrix} A_{11} & A_{12} & O \\ A_{21} & A_{22} & A_{23} \\ \end{bmatrix}} \\ \end{matrix}\) and its one-based BSR format (three vector form) is \(\begin{matrix} \text{bsrVal} & = & \begin{bmatrix} A_{11} & A_{12} & A_{21} & A_{22} & A_{23} \\ \end{bmatrix} \\ \text{bsrRowPtr} & = & \begin{bmatrix} {1\phantom{.0}} & {3\phantom{.0}} & 6 \\ \end{bmatrix} \\ \text{bsrColInd} & = & \begin{bmatrix} {1\phantom{.0}} & {2\phantom{.0}} & {1\phantom{.0}} & {2\phantom{.0}} & 3 \\ \end{bmatrix} \\ \end{matrix}\) Suppose we want to do the following bsrmv operation on a matrix \(\overset{¯}{A}\) which is slightly different from \(A\) .
\(\begin{bmatrix} y_{1} \\ y_{2} \\ \end{bmatrix}:=alpha \ast (\widetilde{A} = \begin{bmatrix} O & O & O \\ O & A_{22} & O \\ \end{bmatrix}) \ast \begin{bmatrix} x_{1} \\ x_{2} \\ x_{3} \\ \end{bmatrix} + \begin{bmatrix} y_{1} \\ {beta \ast y_{2}} \\ \end{bmatrix}\) We don’t need to create another BSR format for the new matrix \(\overset{¯}{A}\) , all that we should do is to keep bsrVal and bsrColInd unchanged, but modify bsrRowPtr and add an additional array bsrEndPtr which points to the last nonzero elements per row of \(\overset{¯}{A}\) plus 1.
For example, the following bsrRowPtr and bsrEndPtr can represent matrix \(\overset{¯}{A}\) : \(\begin{matrix} \text{bsrRowPtr} & = & \begin{bmatrix} {1\phantom{.0}} & 4 \\ \end{bmatrix} \\ \text{bsrEndPtr} & = & \begin{bmatrix} {1\phantom{.0}} & 5 \\ \end{bmatrix} \\ \end{matrix}\) Further we can use a mask operator (specified by array bsrMaskPtr ) to update particular block row indices of \(y\) only because \(y_{1}\) is never changed.
The mask operator is equivalent to the following operation: \(\begin{bmatrix} ? \\ y_{2} \\ \end{bmatrix}:=alpha \ast \begin{bmatrix} ? & ? \\ O & A_{22} & O \\ \end{bmatrix} \ast \begin{bmatrix} x_{1} \\ x_{2} \\ x_{3} \\ \end{bmatrix} + beta \ast \begin{bmatrix} ? \\ y_{2} \\ \end{bmatrix}\) If a block row is not present in the bsrMaskPtr , then no calculation is performed on that row, and the corresponding value in y is unmodified.
In this case, first row block is not present in bsrMaskPtr , so bsrRowPtr[0] and bsrEndPtr[0] are not touched also.
\(\begin{matrix} \text{bsrRowPtr} & = & \begin{bmatrix} {?\phantom{.0}} & 4 \\ \end{bmatrix} \\ \text{bsrEndPtr} & = & \begin{bmatrix} {?\phantom{.0}} & 5 \\ \end{bmatrix} \\ \end{matrix}\) bsrxmv() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture A couple of comments on bsrxmv() : Only blockDim > 1 is supported Only CUSPARSE_OPERATION_NON_TRANSPOSE and CUSPARSE_MATRIX_TYPE_GENERAL are supported.
Parameters bsrMaskPtr , bsrRowPtr , bsrEndPtr and bsrColInd are consistent with base index, either one-based or zero-based.
bsrMaskPtr integer array of sizeOfMask elements that contains the indices corresponding to updated block rows.
bsrEndPtr integer array of mb elements that contains the end of the every block row plus one.
A is an (mb*blockDim)x(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA ); x and y are the right-hand-side and the solution vectors; \(\alpha\) is a scalar; and \(\text{op}(A) = \begin{cases} A & \text{if trans == CUSPARSE_OPERATION_NON_TRANSPOSE} \\ A^{T} & \text{if trans == CUSPARSE_OPERATION_TRANSPOSE} \\ A^{H} & \text{if trans == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE} \\ \end{cases}\) Although there are six combinations in terms of parameter trans and the upper (lower) triangular part of A , bsrsv2_bufferSize() returns the maximum size buffer among these combinations.
The buffer size depends on the dimensions mb , blockDim , and the number of nonzero blocks of the matrix nnzb .
If the user changes the matrix, it is necessary to call bsrsv2_bufferSize() again to have the correct buffer size; otherwise a segmentation fault may occur.
The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context.
dirA storage format of blocks, either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN .
The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL , while the supported diagonal types are CUSPARSE_DIAG_TYPE_UNIT and CUSPARSE_DIAG_TYPE_NON_UNIT .
bsrValA array of nnzb \(( =\) bsrRowPtrA(mb) \(-\) bsrRowPtrA(0) \()\) nonzero blocks of matrix A .
bsrRowPtrA integer array of mb \(+ 1\) elements that contains the start of every block row and the end of the last block row plus one.
bsrColIndA integer array of nnzb \(( =\) bsrRowPtrA(mb) \(-\) bsrRowPtrA(0) \()\) column indices of the nonzero blocks of matrix A .
A is an (mb*blockDim)x(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA ); x and y are the right-hand side and the solution vectors; \(\alpha\) is a scalar; and \(\text{op}(A) = \begin{cases} A & \text{if trans == CUSPARSE_OPERATION_NON_TRANSPOSE} \\ A^{T} & \text{if trans == CUSPARSE_OPERATION_TRANSPOSE} \\ A^{H} & \text{if trans == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE} \\ \end{cases}\) The block of BSR format is of size blockDim*blockDim , stored as column-major or row-major as determined by parameter dirA , which is either CUSPARSE_DIRECTION_COLUMN or CUSPARSE_DIRECTION_ROW .
The matrix type must be CUSPARSE_MATRIX_TYPE_GENERAL , and the fill mode and diagonal type are ignored.
It is expected that this function will be executed only once for a given matrix and a particular operation type.
Function bsrsv2_analysis() reports a structural zero and computes level information, which stored in the opaque structure info .
To disable level information, the user needs to specify the policy of the triangular solver as CUSPARSE_SOLVE_POLICY_NO_LEVEL .
Function bsrsv2_analysis() always reports the first structural zero, even when parameter policy is CUSPARSE_SOLVE_POLICY_NO_LEVEL .
No structural zero is reported if CUSPARSE_DIAG_TYPE_UNIT is specified, even if block A(j,j) is missing for some j .
It is the user’s choice whether to call bsrsv2_solve() if bsrsv2_analysis() reports a structural zero.
In this case, the user can still call bsrsv2_solve() , which will return a numerical zero at the same position as a structural zero.
This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context.
policy the supported policies are CUSPARSE_SOLVE_POLICY_NO_LEVEL and CUSPARSE_SOLVE_POLICY_USE_LEVEL .
A is an (mb*blockDim)x(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA ); x and y are the right-hand-side and the solution vectors; \(\alpha\) is a scalar; and \(\text{op}(A) = \begin{cases} A & \text{if trans == CUSPARSE_OPERATION_NON_TRANSPOSE} \\ A^{T} & \text{if trans == CUSPARSE_OPERATION_TRANSPOSE} \\ A^{H} & \text{if trans == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE} \\ \end{cases}\) The block in BSR format is of size blockDim*blockDim , stored as column-major or row-major as determined by parameter dirA , which is either CUSPARSE_DIRECTION_COLUMN or CUSPARSE_DIRECTION_ROW .
This function may be executed multiple times for a given matrix and a particular operation type.
Although bsrsv2_solve() can be done without level information, the user still needs to be aware of consistency.
If bsrsv2_analysis() is called with policy CUSPARSE_SOLVE_POLICY_USE_LEVEL , bsrsv2_solve() can be run with or without levels.
On the other hand, if bsrsv2_analysis() is called with CUSPARSE_SOLVE_POLICY_NO_LEVEL , bsrsv2_solve() can only accept CUSPARSE_SOLVE_POLICY_NO_LEVEL ; otherwise, CUSPARSE_STATUS_INVALID_VALUE is returned.
The level information may not improve the performance, but may spend extra time doing analysis.
In this case, CUSPARSE_SOLVE_POLICY_NO_LEVEL performs better than CUSPARSE_SOLVE_POLICY_USE_LEVEL .
If the user has an iterative solver, the best approach is to do bsrsv2_analysis() with CUSPARSE_SOLVE_POLICY_USE_LEVEL once.
Then do bsrsv2_solve() with CUSPARSE_SOLVE_POLICY_NO_LEVEL in the first run, and with CUSPARSE_SOLVE_POLICY_USE_LEVEL in the second run, and pick the fastest one to perform the remaining iterations.
The numerical zero of bsrsv02_solve() means there exists some block A(j,j) that is not invertible.
No numerical zero is reported if CUSPARSE_DIAG_TYPE_UNIT is specified, even if A(j,j) is not invertible for some j .
The function supports the following properties if pBuffer != NULL The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture For example, suppose L is a lower triangular matrix with unit diagonal, then the following code solves L*y=x by level information.
Suppose that L is m x m sparse matrix represented by BSR format,   The number of block rows/columns is mb, and   the number of nonzero blocks is nnzb.
Assumption:   - dimension of matrix L is m(=mb*blockDim),   - matrix L has nnz(=nnzb*blockDim*blockDim) nonzero elements,   - handle is already created by cusparseCreate(),   - (d_bsrRowPtr, d_bsrColInd, d_bsrVal) is BSR of L on device memory,   - d_x is right hand side vector on device memory.
cusparseMatDescr_t descr = 0 ; bsrsv2Info_t info = 0 ; int pBufferSize ; void * pBuffer = 0 ; int structural_zero ; int numerical_zero ; const double alpha = 1.
cudaMalloc (( void ** ) & pBuffer , pBufferSize );   step 4: perform analysis cusparseDbsrsv2_analysis ( handle , dir , trans , mb , nnzb , descr , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info , policy , pBuffer );   L has unit diagonal, so no structural zero is reported.
status = cusparseXbsrsv2_zeroPivot ( handle , info , & structural_zero ); if ( CUSPARSE_STATUS_ZERO_PIVOT == status ){ printf ( "L(%d,%d) is missing   " , structural_zero , structural_zero ); }   step 5: solve L*y = x cusparseDbsrsv2_solve ( handle , dir , trans , mb , nnzb , & alpha , descr , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info , d_x , d_y , policy , pBuffer );   L has unit diagonal, so no numerical zero is reported.
status = cusparseXbsrsv2_zeroPivot ( handle , info , & numerical_zero ); if ( CUSPARSE_STATUS_ZERO_PIVOT == status ){ printf ( "L(%d,%d) is zero   " , numerical_zero , numerical_zero ); }   step 6: free resources cudaFree ( pBuffer ); cusparseDestroyBsrsv2Info ( info ); cusparseDestroyMatDescr ( descr ); cusparseDestroy ( handle ); Input handle handle to the cuSPARSE library context.
info structure with information collected during the analysis phase (that should have been passed to the solve phase unchanged).
Output y solution vector of size m . 5.4.6. cusparseXbsrsv2_zeroPivot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseXbsrsv2_zeroPivot ( cusparseHandle_t handle , bsrsv2Info_t info , int * position ) If the returned error code is CUSPARSE_STATUS_ZERO_PIVOT , position=j means A(j,j) is either structural zero or numerical zero (singular block).
The routine requires no extra storage The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context.
info info contains a structural zero or numerical zero if the user already called bsrsv2_analysis() or bsrsv2_solve() .
Output position if no structural or numerical zero, position is -1; otherwise if A(j,j) is missing or U(j,j) is zero, position=j .
Notice that by interchanging the rows and columns of the result you are implicitly transposing the matrix.
Call the gemvi() function with the cusparseOperation_t parameter set to CUSPARSE_OPERATION_NON_TRANSPOSE and with the interchanged rows and columns of the matrix stored in CSC format.
This (implicitly) multiplies the vector by the transpose of the matrix in the original CSR format.
The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture The function cusparsegemvi_bufferSize() returns the size of buffer used in cusparsegemvi() .
x sparse vector of nnz elements of size n if \(\text{op}(A)=A\) , and size m if \(\text{op}(A)=A^{T}\) .
y dense vector of m elements if \(\text{op}(A)=A\) , and n elements if \(\text{op}(A)=A^{T}\) .
Output y updated dense vector. 5.5. cuSPARSE Level 3 Function Reference  This chapter describes sparse linear algebra functions that perform operations between sparse and (usually tall) dense matrices.
If op(B)=B , it must be at least \(\max\text{(1,\ k)}\) If op(B) != B , it must be at least max(1, n) .
It must be at least \(\max\text{(1,\ m)}\) if op(A)=A and at least \(\max\text{(1,\ k)}\) otherwise.
A is an (mb*blockDim)x(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA ); B and X are the right-hand-side and the solution matrices; \(\alpha\) is a scalar; and \(\text{op}(A) == \text{CUSPARSE_OPERATION_NON_TRANSPOSE}\) Although there are six combinations in terms of parameter trans and the upper (and lower) triangular part of A , bsrsm2_bufferSize() returns the maximum size of the buffer among these combinations.
The buffer size depends on dimension mb,blockDim and the number of nonzeros of the matrix, nnzb .
If the user changes the matrix, it is necessary to call bsrsm2_bufferSize() again to get the correct buffer size, otherwise a segmentation fault may occur.
pBufferSizeInBytes number of bytes of the buffer used in bsrsm2_analysis() and bsrsm2_solve() .
A is an (mb*blockDim)x(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA ); B and X are the right-hand-side and the solution matrices; \(\alpha\) is a scalar; and \(\text{op}(A) == \text{CUSPARSE_OPERATION_NON_TRANSPOSE}\) and \(\text{op}(X) = \begin{cases} X & \text{if transX == CUSPARSE_OPERATION_NON_TRANSPOSE} \\ X^{T} & \text{if transX == CUSPARSE_OPERATION_TRANSPOSE} \\ X^{H} & \text{if transX == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE (not supported)} \\ \end{cases}\) and op(B) and op(X) are equal.
The block of BSR format is of size blockDim*blockDim , stored in column-major or row-major as determined by parameter dirA , which is either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN .
Function bsrsm2_analysis() reports a structural zero and computes the level information stored in opaque structure info .
Function bsrsm2_analysis() always reports the first structural zero, even if the parameter policy is CUSPARSE_SOLVE_POLICY_NO_LEVEL .
Besides, no structural zero is reported if CUSPARSE_DIAG_TYPE_UNIT is specified, even if block A(j,j) is missing for some j .
The user must call cusparseXbsrsm2_query_zero_pivot() to know where the structural zero is.
If bsrsm2_analysis() reports a structural zero, the solve will return a numerical zero in the same position as the structural zero but this result X is meaningless.
policy The supported policies are CUSPARSE_SOLVE_POLICY_NO_LEVEL and CUSPARSE_SOLVE_POLICY_USE_LEVEL .
The block of BSR format is of size blockDim*blockDim , stored as column-major or row-major as determined by parameter dirA , which is either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN .
Although bsrsm2_solve() can be done without level information, the user still needs to be aware of consistency.
If bsrsm2_analysis() is called with policy CUSPARSE_SOLVE_POLICY_USE_LEVEL , bsrsm2_solve() can be run with or without levels.
On the other hand, if bsrsm2_analysis() is called with CUSPARSE_SOLVE_POLICY_NO_LEVEL , bsrsm2_solve() can only accept CUSPARSE_SOLVE_POLICY_NO_LEVEL ; otherwise, CUSPARSE_STATUS_INVALID_VALUE is returned.
Function bsrsm02_solve() has the same behavior as bsrsv02_solve() , reporting the first numerical zero, including a structural zero.
The user must call cusparseXbsrsm2_query_zero_pivot() to know where the numerical zero is.
The computational pattern of transpose(X) with matrix X in column-major order is equivalent to X with matrix X in row-major order.
In-place is supported and requires that B and X point to the same memory block, and ldb=ldx .
The function supports the following properties if pBuffer != NULL : The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context.
bsrValA array of nnzb \(( =\) bsrRowPtrA(mb) \(-\) bsrRowPtrA(0) \()\) non-zero blocks of matrix A .
Output X solution array with leading dimensions ldx . 5.5.5. cusparseXbsrsm2_zeroPivot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseXbsrsm2_zeroPivot ( cusparseHandle_t handle , bsrsm2Info_t info , int * position ) If the returned error code is CUSPARSE_STATUS_ZERO_PIVOT , position=j means A(j,j) is either a structural zero or a numerical zero (singular block).
info info contains a structural zero or a numerical zero if the user already called bsrsm2_analysis() or bsrsm2_solve() .
Output position if no structural or numerical zero, position is -1; otherwise, if A(j,j) is missing or U(j,j) is zero, position=j . 5.6. cuSPARSE Extra Function Reference  This chapter describes the extra routines used to manipulate sparse matrices.
Since A and B have different sparsity patterns, cuSPARSE adopts a two-step approach to complete sparse matrix C .
In the first step, the user allocates csrRowPtrC of m+1 elements and uses function cusparseXcsrgeam2Nnz() to determine csrRowPtrC and the total number of nonzero elements.
In the second step, the user gathers nnzC (number of nonzero elements of matrix C ) from either (nnzC=*nnzTotalDevHostPtr) or (nnzC=csrRowPtrC(m)-csrRowPtrC(0)) and allocates csrValC, csrColIndC of nnzC elements respectively, then finally calls function cusparse[S|D|C|Z]csrgeam2() to complete matrix C .
In order to do any one of the three, the user should use the routine csr2csc() to convert \(A\) | \(B\) to \(A^{T}\) | \(B^{T}\) .
If either A or B is symmetric or Hermitian, then the user must extend the matrix to a full one and reconfigure the MatrixType field of the descriptor to CUSPARSE_MATRIX_TYPE_GENERAL .
If the sparsity pattern of matrix C is known, the user can skip the call to function cusparseXcsrgeam2Nnz() .
For example, suppose that the user has an iterative algorithm which would update A and B iteratively but keep the sparsity patterns.
The user can call function cusparseXcsrgeam2Nnz() once to set up the sparsity pattern of C , then call function cusparse[S|D|C|Z]geam() only for each iteration.
If the user wants \(C = 0 \times A + 1 \times B^{T}\) , then csr2csc() is better than csrgeam2() .
csrgeam2() is the same as csrgeam() except csrgeam2() needs explicit buffer where csrgeam() allocates the buffer internally.
csrValA array of nnzA \(( =\) csrRowPtrA(m) \(-\) csrRowPtrA(0) \()\) nonzero elements of matrix A .
csrRowPtrA integer array of m \(+ 1\) elements that contains the start of every row and the end of the last row plus one.
csrColIndA integer array of nnzA \(( =\) csrRowPtrA(m) \(-\) csrRowPtrA(0) \()\) column indices of the nonzero elements of matrix A .
csrValB array of nnzB \(( =\) csrRowPtrB(m) \(-\) csrRowPtrB(0) \()\) nonzero elements of matrix B .
csrRowPtrB integer array of m \(+ 1\) elements that contains the start of every row and the end of the last row plus one.
csrColIndB integer array of nnzB \(( =\) csrRowPtrB(m) \(-\) csrRowPtrB(0) \()\) column indices of the nonzero elements of matrix B .
Output csrValC array of nnzC \(( =\) csrRowPtrC(m) \(-\) csrRowPtrC(0) \()\) nonzero elements of matrix C .
csrRowPtrC integer array of m \(+ 1\) elements that contains the start of every row and the end of the last row plus one.
csrColIndC integer array of nnzC \(( =\) csrRowPtrC(m) \(-\) csrRowPtrC(0) \()\) column indices of the nonzero elements of matrix C .
cuSPARSE Preconditioners Reference  This chapter describes the routines that implement different preconditioners. 5.7.1. Incomplete Cholesky Factorization: level 0 [DEPRECATED]  Different algorithms for ic0 are discussed in this section.
If the user changes the matrix, it is necessary to call csric02_bufferSize() again to have the correct buffer size; otherwise, a segmentation fault may occur.
csrValA array of nnz \(( =\) csrRowPtrA(m) \(-\) csrRowPtrA(0) \()\) nonzero elements of matrix A .
csrColIndA integer array of nnz \(( =\) csrRowPtrA(m) \(-\) csrRowPtrA(0) \()\) column indices of the nonzero elements of matrix A .
Function csric02_analysis() reports a structural zero and computes level information stored in the opaque structure info .
The level information can extract more parallelism during incomplete Cholesky factorization.
To disable level information, the user must specify the policy of csric02_analysis() and csric02() as CUSPARSE_SOLVE_POLICY_NO_LEVEL .
Function csric02_analysis() always reports the first structural zero, even if the policy is CUSPARSE_SOLVE_POLICY_NO_LEVEL .
It is the user’s choice whether to call csric02() if csric02_analysis() reports a structural zero.
In this case, the user can still call csric02() , which will return a numerical zero at the same position as the structural zero.
Although csric02() can be done without level information, the user still needs to be aware of consistency.
If csric02_analysis() is called with policy CUSPARSE_SOLVE_POLICY_USE_LEVEL , csric02() can be run with or without levels.
On the other hand, if csric02_analysis() is called with CUSPARSE_SOLVE_POLICY_NO_LEVEL , csric02() can only accept CUSPARSE_SOLVE_POLICY_NO_LEVEL ; otherwise, CUSPARSE_STATUS_INVALID_VALUE is returned.
Function csric02() only takes the lower triangular part of matrix A to perform factorization.
The matrix type must be CUSPARSE_MATRIX_TYPE_GENERAL , the fill mode and diagonal type are ignored, and the strictly upper triangular part is ignored and never touched.
In other words, from the point of view of csric02() A is Hermitian and only the lower triangular part is provided.
Note In practice, a positive definite matrix may not have incomplete cholesky factorization.
To the best of our knowledge, only matrix M can guarantee the existence of incomplete cholesky factorization.
If csric02() failed cholesky factorization and reported a numerical zero, it is possible that incomplete cholesky factorization does not exist.
For example, suppose A is a real m × m matrix, the following code solves the precondition system M*y = x where M is the product of Cholesky factorization L and its transpose.
\(M = LL^{H}\)   Suppose that A is m x m sparse matrix represented by CSR format,   Assumption:   - handle is already created by cusparseCreate(),   - (d_csrRowPtr, d_csrColInd, d_csrVal) is CSR of A on device memory,   - d_x is right hand side vector on device memory,   - d_y is solution vector on device memory.
cusparseMatDescr_t descr_M = 0 ; cusparseMatDescr_t descr_L = 0 ; csric02Info_t info_M = 0 ; csrsv2Info_t info_L = 0 ; csrsv2Info_t info_Lt = 0 ; int pBufferSize_M ; int pBufferSize_L ; int pBufferSize_Lt ; int pBufferSize ; void * pBuffer = 0 ; int structural_zero ; int numerical_zero ; const double alpha = 1.
cudaMalloc (( void ** ) & pBuffer , pBufferSize );   step 4: perform analysis of incomplete Cholesky on M   perform analysis of triangular solve on L   perform analysis of triangular solve on L'   The lower triangular part of M has the same sparsity pattern as L, so   we can do analysis of csric02 and csrsv2 simultaneously.
csrValA_valM array of nnz \(( =\) csrRowPtrA(m) \(-\) csrRowPtrA(0) \()\) nonzero elements of matrix A .
Output csrValA_valM matrix containing the incomplete-Cholesky lower triangular factor. 5.7.1.4. cusparseXcsric02_zeroPivot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseXcsric02_zeroPivot ( cusparseHandle_t handle , csric02Info_t info , int * position ) If the returned error code is CUSPARSE_STATUS_ZERO_PIVOT , position=j means A(j,j) has either a structural zero or a numerical zero; otherwise, position=-1 .
info info contains structural zero or numerical zero if the user already called csric02_analysis() or csric02() .
The buffer size depends on the dimensions of mb , blockDim , and the number of nonzero blocks of the matrix nnzb .
If the user changes the matrix, it is necessary to call bsric02_bufferSize() again to have the correct buffer size; otherwise, a segmentation fault may occur.
The block in BSR format is of size blockDim*blockDim , stored as column-major or row-major as determined by parameter dirA , which is either CUSPARSE_DIRECTION_COLUMN or CUSPARSE_DIRECTION_ROW .
Function bsric02_analysis() reports structural zero and computes level information stored in the opaque structure info .
To disable level information, the user needs to specify the parameter policy of bsric02[_analysis| ] as CUSPARSE_SOLVE_POLICY_NO_LEVEL .
Function bsric02_analysis always reports the first structural zero, even when parameter policy is CUSPARSE_SOLVE_POLICY_NO_LEVEL .
It is the user’s choice whether to call bsric02() if bsric02_analysis() reports a structural zero.
In this case, the user can still call bsric02() , which returns a numerical zero in the same position as the structural zero.
Although bsric02() can be done without level information, the user must be aware of consistency.
If bsric02_analysis() is called with policy CUSPARSE_SOLVE_POLICY_USE_LEVEL , bsric02() can be run with or without levels.
On the other hand, if bsric02_analysis() is called with CUSPARSE_SOLVE_POLICY_NO_LEVEL , bsric02() can only accept CUSPARSE_SOLVE_POLICY_NO_LEVEL ; otherwise, CUSPARSE_STATUS_INVALID_VALUE is returned.
The numerical zero of bsric02() means there exists some block Lj,j) that is not invertible.
The bsric02() function only takes the lower triangular part of matrix A to perform factorization.
In other words, from the point of view of bsric02() , A is Hermitian and only the lower triangular part is provided.
The following code solves precondition system M*y = x , where M is the product of Cholesky factorization L and its transpose.
\(M = LL^{H}\)   Suppose that A is m x m sparse matrix represented by BSR format,   The number of block rows/columns is mb, and   the number of nonzero blocks is nnzb.
Assumption:   - handle is already created by cusparseCreate(),   - (d_bsrRowPtr, d_bsrColInd, d_bsrVal) is BSR of A on device memory,   - d_x is right hand side vector on device memory,   - d_y is solution vector on device memory.
cusparseMatDescr_t descr_M = 0 ; cusparseMatDescr_t descr_L = 0 ; bsric02Info_t info_M = 0 ; bsrsv2Info_t info_L = 0 ; bsrsv2Info_t info_Lt = 0 ; int pBufferSize_M ; int pBufferSize_L ; int pBufferSize_Lt ; int pBufferSize ; void * pBuffer = 0 ; int structural_zero ; int numerical_zero ; const double alpha = 1.
cudaMalloc (( void ** ) & pBuffer , pBufferSize );   step 4: perform analysis of incomplete Cholesky on M   perform analysis of triangular solve on L   perform analysis of triangular solve on L'   The lower triangular part of M has the same sparsity pattern as L, so   we can do analysis of bsric02 and bsrsv2 simultaneously.
Output bsrValA matrix containing the incomplete-Cholesky lower triangular factor. 5.7.1.8. cusparseXbsric02_zeroPivot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseXbsric02_zeroPivot ( cusparseHandle_t handle , bsric02Info_t info , int * position ) If the returned error code is CUSPARSE_STATUS_ZERO_PIVOT , position=j means A(j,j) has either a structural zero or a numerical zero (the block is not positive definite).
info info contains a structural zero or a numerical zero if the user already called bsric02_analysis() or bsric02() .
Output position If no structural or numerical zero, position is -1, otherwise if A(j,j) is missing or L(j,j) is not positive definite, position=j . 5.7.2. Incomplete LU Factorization: level 0 [DEPRECATED]  Different algorithms for ilu0 are discussed in this section.
5.7.2.1. cusparsecsrilu02_numericBoost() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseScsrilu02_numericBoost ( cusparseHandle_t handle , csrilu02Info_t info , int enable_boost , double * tol , float * boost_val ) cusparseStatus_t cusparseDcsrilu02_numericBoost ( cusparseHandle_t handle , csrilu02Info_t info , int enable_boost , double * tol , double * boost_val ) cusparseStatus_t cusparseCcsrilu02_numericBoost ( cusparseHandle_t handle , csrilu02Info_t info , int enable_boost , double * tol , cuComplex * boost_val ) cusparseStatus_t cusparseZcsrilu02_numericBoost ( cusparseHandle_t handle , csrilu02Info_t info , int enable_boost , double * tol , cuDoubleComplex * boost_val ) The user can use a boost value to replace a numerical value in incomplete LU factorization.
The tol is used to determine a numerical zero, and the boost_val is used to replace a numerical zero.
To enable a boost value, the user has to set parameter enable_boost to 1 before calling csrilu02() .
To disable a boost value, the user can call csrilu02_numericBoost() again with parameter enable_boost=0 .
The buffer size depends on the dimension m and nnz , the number of nonzeros of the matrix.
If the user changes the matrix, it is necessary to call csrilu02_bufferSize() again to have the correct buffer size; otherwise, a segmentation fault may occur.
Function csrilu02_analysis() reports a structural zero and computes level information stored in the opaque structure info .
The level information can extract more parallelism during incomplete LU factorization; however csrilu02() can be done without level information.
To disable level information, the user must specify the policy of csrilu02() as CUSPARSE_SOLVE_POLICY_NO_LEVEL .
It is the user’s choice whether to call csrilu02() if csrilu02_analysis() reports a structural zero.
In this case, the user can still call csrilu02() , which will return a numerical zero at the same position as the structural zero.
Although csrilu02() can be done without level information, the user still needs to be aware of consistency.
If csrilu02_analysis() is called with policy CUSPARSE_SOLVE_POLICY_USE_LEVEL , csrilu02() can be run with or without levels.
On the other hand, if csrilu02_analysis() is called with CUSPARSE_SOLVE_POLICY_NO_LEVEL , csrilu02() can only accept CUSPARSE_SOLVE_POLICY_NO_LEVEL ; otherwise, CUSPARSE_STATUS_INVALID_VALUE is returned.
For example, suppose A is a real m × m matrix, the following code solves precondition system M*y = x where M is the product of LU factors L and U .
Suppose that A is m x m sparse matrix represented by CSR format,   Assumption:   - handle is already created by cusparseCreate(),   - (d_csrRowPtr, d_csrColInd, d_csrVal) is CSR of A on device memory,   - d_x is right hand side vector on device memory,   - d_y is solution vector on device memory.
cusparseMatDescr_t descr_M = 0 ; cusparseMatDescr_t descr_L = 0 ; cusparseMatDescr_t descr_U = 0 ; csrilu02Info_t info_M = 0 ; csrsv2Info_t info_L = 0 ; csrsv2Info_t info_U = 0 ; int pBufferSize_M ; int pBufferSize_L ; int pBufferSize_U ; int pBufferSize ; void * pBuffer = 0 ; int structural_zero ; int numerical_zero ; const double alpha = 1.
cudaMalloc (( void ** ) & pBuffer , pBufferSize );   step 4: perform analysis of incomplete Cholesky on M   perform analysis of triangular solve on L   perform analysis of triangular solve on U   The lower(upper) triangular part of M has the same sparsity pattern as L(U),   we can do analysis of csrilu0 and csrsv2 simultaneously.
Output csrValA_valM matrix containing the incomplete-LU lower and upper triangular factors. 5.7.2.5. cusparseXcsrilu02_zeroPivot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseXcsrilu02_zeroPivot ( cusparseHandle_t handle , csrilu02Info_t info , int * position ) If the returned error code is CUSPARSE_STATUS_ZERO_PIVOT , position=j means A(j,j) has either a structural zero or a numerical zero; otherwise, position=-1 .
The routine requires no extra storage The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle Handle to the cuSPARSE library context.
info info contains structural zero or numerical zero if the user already called csrilu02_analysis() or csrilu02() .
Parameter tol is used to determine a numerical zero, and boost_val is used to replace a numerical zero.
The behavior is as follows: if tol >= fabs(A(j,j)) , then reset each diagonal element of block A(j,j) by boost_val .
To enable a boost value, the user sets parameter enable_boost to 1 before calling bsrilu02() .
To disable the boost value, the user can call bsrilu02_numericBoost() with parameter enable_boost=0 .
\(A \approx LU\) A is an (mb*blockDim)*(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA .
If the user changes the matrix, it is necessary to call bsrilu02_bufferSize() again to have the correct buffer size; otherwise, a segmentation fault may occur.
pBufferSizeInBytes number of bytes of the buffer used in bsrilu02_analysis() and bsrilu02() .
\(A \approx LU\) A is an (mb*blockDim)×(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA .
Function bsrilu02_analysis() reports a structural zero and computes level information stored in the opaque structure info .
To disable level information, the user needs to specify the parameter policy of bsrilu02[_analysis| ] as CUSPARSE_SOLVE_POLICY_NO_LEVEL .
Function bsrilu02_analysis() always reports the first structural zero, even with parameter policy is CUSPARSE_SOLVE_POLICY_NO_LEVEL .
It is the user’s choice whether to call bsrilu02() if bsrilu02_analysis() reports a structural zero.
In this case, the user can still call bsrilu02() , which will return a numerical zero at the same position as the structural zero.
The block in BSR format is of size blockDim*blockDim , stored as column-major or row-major determined by parameter dirA , which is either CUSPARSE_DIRECTION_COLUMN or CUSPARSE_DIRECTION_ROW .
Although bsrilu02() can be used without level information, the user must be aware of consistency.
If bsrilu02_analysis() is called with policy CUSPARSE_SOLVE_POLICY_USE_LEVEL , bsrilu02() can be run with or without levels.
On the other hand, if bsrilu02_analysis() is called with CUSPARSE_SOLVE_POLICY_NO_LEVEL , bsrilu02() can only accept CUSPARSE_SOLVE_POLICY_NO_LEVEL ; otherwise, CUSPARSE_STATUS_INVALID_VALUE is returned.
The numerical zero of bsrilu02() means there exists some block U(j,j) that is not invertible.
The following code solves precondition system M*y = x , where M is the product of LU factors L and U .
Suppose that A is m x m sparse matrix represented by BSR format,   The number of block rows/columns is mb, and   the number of nonzero blocks is nnzb.
Assumption:   - handle is already created by cusparseCreate(),   - (d_bsrRowPtr, d_bsrColInd, d_bsrVal) is BSR of A on device memory,   - d_x is right hand side vector on device memory.
cusparseMatDescr_t descr_M = 0 ; cusparseMatDescr_t descr_L = 0 ; cusparseMatDescr_t descr_U = 0 ; bsrilu02Info_t info_M = 0 ; bsrsv2Info_t info_L = 0 ; bsrsv2Info_t info_U = 0 ; int pBufferSize_M ; int pBufferSize_L ; int pBufferSize_U ; int pBufferSize ; void * pBuffer = 0 ; int structural_zero ; int numerical_zero ; const double alpha = 1.
cudaMalloc (( void ** ) & pBuffer , pBufferSize );   step 4: perform analysis of incomplete LU factorization on M   perform analysis of triangular solve on L   perform analysis of triangular solve on U   The lower(upper) triangular part of M has the same sparsity pattern as L(U),   we can do analysis of bsrilu0 and bsrsv2 simultaneously.
dirA storage format of blocks: either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN .
Output bsrValA matrix containing the incomplete-LU lower and upper triangular factors See cusparseStatus_t for the description of the return status. 5.7.2.10. cusparseXbsrilu02_zeroPivot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseXbsrilu02_zeroPivot ( cusparseHandle_t handle , bsrilu02Info_t info , int * position ) If the returned error code is CUSPARSE_STATUS_ZERO_PIVOT , position=j means A(j,j) has either a structural zero or a numerical zero (the block is not invertible).
info info contains structural zero or numerical zero if the user already called bsrilu02_analysis() or bsrilu02() .
Output position if no structural or numerical zero, position is -1; otherwise if A(j,j) is missing or U(j,j) is not invertible, position=j . 5.7.3. Tridiagonal Solve  Different algorithms for tridiagonal solve are discussed in this section.
\(A \ast X = B\) The coefficient matrix A of each of these tri-diagonal linear system is defined with three vectors corresponding to its lower ( dl ), main ( d ), and upper ( du ) matrix diagonals; the right-hand sides are stored in the dense matrix B .
Assuming A is of size m and base-1, dl , d and du are defined by the following formula: dl(i) := A(i, i-1) for i=1,2,...,m The first element of dl is out-of-bound ( dl(1) := A(1,0) ), so dl(1) = 0 .
d(i) = A(i,i) for i=1,2,...,m du(i) = A(i,i+1) for i=1,2,...,m The last element of du is out-of-bound ( du(m) := A(m,m+1) ), so du(m) = 0 .
The routine does perform pivoting, which usually results in more accurate and more stable results than cusparsegtsv_nopivot() or cusparsegtsv2_nopivot() at the expense of some execution time.
The routine does not perform any pivoting and uses a combination of the Cyclic Reduction (CR) and the Parallel Cyclic Reduction (PCR) algorithms to find the solution.
pBuffer buffer allocated by the user, the size is return by gtsv2_nopivot_bufferSizeExt . 5.7.4. Batched Tridiagonal Solve  Different algorithms for batched tridiagonal solve are discussed in this section.
The different matrices are assumed to be of the same size and are stored with a fixed batchStride in memory.
The lower diagonal \(dl^{(i)}\) that corresponds to the i th linear system starts at location dl+batchStride×i in memory.
The main diagonal \(d^{(i)}\) that corresponds to the i th linear system starts at location d+batchStride×i in memory.
The upper diagonal \(du^{(i)}\) that corresponds to the i th linear system starts at location du+batchStride×i in memory.
The right-hand-side \(x^{(i)}\) that corresponds to the i th linear system starts at location x+batchStride×i in memory.
batchStride stride (number of elements) that separates the vectors of every system (must be at least m ).
batchStride stride (number of elements) that separates the vectors of every system (must be at least n ).
pBuffer buffer allocated by the user, the size is return by gtsv2StridedBatch_bufferSizeExt .
The data layout is different from gtsvStridedBatch which aggregates all matrices one after another.
Instead, gtsvInterleavedBatch gathers different matrices of the same element in a continous manner.
From stability perspective, cuThomas is not numerically stable because it does not have pivoting.
From performance perspective, LU with partial pivoting and QR is about 10% to 20% slower than cuThomas.
algo algo = 0: cuThomas (unstable algorithm); algo = 1: LU with pivoting (stable algorithm); algo = 2: QR (stable algorithm) m the size of the linear system.
pBuffer buffer allocated by the user, the size is return by gtsvInterleavedBatch_bufferSizeExt .
Output x dense solution array of dimensions (batchCount, n) . 5.7.5. Batched Pentadiagonal Solve  Different algorithms for batched pentadiagonal solve are discussed in this section.
Assuming A is of size m and base-1, ds , dl , d , du and dw are defined by the following formula: ds(i) := A(i, i-2) for i=1,2,...,m The first two elements of ds is out-of-bound ( ds(1) := A(1,-1) , ds(2) := A(2,0) ), so ds(1) = 0 and ds(2) = 0 .
dl(i) := A(i, i-1) for i=1,2,...,m The first element of dl is out-of-bound ( dl(1) := A(1,0) ), so dl(1) = 0 .
dw(i) = A(i,i+2) for i=1,2,...,m The last two elements of dw is out-of-bound ( dw(m-1) := A(m-1,m+1) , dw(m) := A(m,m+2) ), so dw(m-1) = 0 and dw(m) = 0 .
The function supports the following properties if pBuffer != NULL The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context.
ds dense array containing the lower diagonal (distance 2 to the diagonal) of the penta-diagonal linear system.
dl dense array containing the lower diagonal (distance 1 to the diagonal) of the penta-diagonal linear system.
du dense array containing the upper diagonal (distance 1 to the diagonal) of the penta-diagonal linear system.
dw dense array containing the upper diagonal (distance 2 to the diagonal) of the penta-diagonal linear system.
pBuffer buffer allocated by the user, the size is return by gpsvInterleavedBatch_bufferSizeExt .
Please visit cuSPARSE Library Samples - cusparseSgpsvInterleavedBatch for a code example. 5.8. cuSPARSE Reorderings Reference  This chapter describes the reordering routines used to manipulate sparse matrices.
The coloring is an assignment of colors (integer numbers) to nodes, such that neighboring nodes have distinct colors.
An approximate coloring algorithm is used in this routine, and is stopped when a certain percentage of nodes has been colored.
The rest of the nodes are assigned distinct colors (an increasing sequence of integers numbers, starting from the last integer used previously).
The last two auxiliary routines can be used to extract the resulting number of colors, their assignment and the associated reordering.
The reordering is such that nodes that have been assigned the same color are reordered to be next to each other.
The matrix A passed to this routine, must be stored as a general matrix and have a symmetric sparsity pattern.
csrRowPtrA integer array of m+1 elements that contains the start of every row and the end of the last row plus one.
fractionToColor fraction of nodes to be colored, which should be in the interval [0.0,1.0], for example 0.8 implies that 80 percent of nodes will be colored.
Output ncolors The number of distinct colors used (at most the size of the matrix, but likely much smaller).
coloring The resulting coloring permutation reordering The resulting reordering permutation (untouched if NULL) See cusparseStatus_t for the description of the return status. 5.9. cuSPARSE Format Conversion Reference  This chapter describes the conversion routines between different sparse and dense storage formats.
coosort , csrsort , cscsort , and csru2csr are sorting routines without malloc inside, the following table estimates the buffer size.
routine buffer size maximum problem size if buffer is limited by 2GB coosort > 16*n bytes 125M csrsort or cscsort > 20*n bytes 100M csru2csr 'd' > 28*n bytes ; 'z' > 36*n bytes 71M for ‘d’ and 55M for ‘z’ 5.9.1.
Let m(=mb*blockDim) be the number of rows of A and n(=nb*blockDim) be number of columns of A , then A and C are m*n sparse matrices.
The BSR format of A contains nnzb(=bsrRowPtrA[mb] - bsrRowPtrA[0]) nonzero blocks, whereas the sparse matrix A contains nnz(=nnzb*blockDim*blockDim) elements.
The requirements are as follows: csrRowPtrC of m+1 elements csrValC of nnz elements csrColIndC of nnz elements The general procedure is as follows:   Given BSR format (bsrRowPtrA, bsrcolIndA, bsrValA) and   blocks of BSR format are stored in column-major order.
bsrRowPtrA integer array of mb+1 elements that contains the start of every block row and the end of the last block row plus one of matrix A .
csrRowPtrC integer array of m+1 elements that contains the start of every row and the end of the last row plus one of matrix C .
This sparsity pattern of the result matrix can also be seen as the transpose of the original sparse matrix, but the memory layout of a block does not change.
The user must call gebsr2gebsc_bufferSize() to determine the size of the buffer required by gebsr2gebsc() , allocate the buffer, and pass the buffer pointer to gebsr2gebsc() .
The routine requires no extra storage if pBuffer != NULL The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context.
bsrRowPtr integer array of mb+1 elements that contains the start of every block row and the end of the last block row plus one.
If rowBlockDimA=1 and colBlockDimA=1 , cusparse[S|D|C|Z]gebsr2gebsr() is the same as cusparse[S|D|C|Z]csr2gebsr() .
If rowBlockDimC=1 and colBlockDimC=1 , cusparse[S|D|C|Z]gebsr2gebsr() is the same as cusparse[S|D|C|Z]gebsr2csr() .
A is an m*n sparse matrix where m(=mb*rowBlockDim) is the number of rows of A , and n(=nb*colBlockDim) is the number of columns of A .
The general BSR format of A contains nnzb(=bsrRowPtrA[mb] - bsrRowPtrA[0]) nonzero blocks.
The matrix C is also general BSR format with a different block size, rowBlockDimC*colBlockDimC .
If m is not a multiple of rowBlockDimC , or n is not a multiple of colBlockDimC , zeros are filled in.
First, the user allocates bsrRowPtrC of mc+1 elements and uses function cusparseXgebsr2gebsrNnz() to determine the number of nonzero block columns per block row of matrix C .
Second, the user gathers nnzc (number of non-zero block columns of matrix C ) from either (nnzc=*nnzTotalDevHostPtr) or (nnzc=bsrRowPtrC[mc]-bsrRowPtrC[0]) and allocates bsrValC of nnzc*rowBlockDimC*colBlockDimC elements and bsrColIndC of nnzc integers.
The user must call gebsr2gebsr_bufferSize() to know the size of the buffer required by gebsr2gebsr() , allocate the buffer, and pass the buffer pointer to gebsr2gebsr() .
The general procedure is as follows:   Given general BSR format (bsrRowPtrA, bsrColIndA, bsrValA) and   blocks of BSR format are stored in column-major order.
bsrRowPtrC integer array of mc+1 elements that contains the start of every block row and the end of the last block row plus one of matrix C .
Let m(=mb*rowBlockDim) be number of rows of A and n(=nb*colBlockDim) be number of columns of A , then A and C are m*n sparse matrices.
The general BSR format of A contains nnzb(=bsrRowPtrA[mb] - bsrRowPtrA[0]) non-zero blocks, whereas sparse matrix A contains nnz(=nnzb*rowBlockDim*colBlockDim) elements.
The requirements are as follows: csrRowPtrC of m+1 elements csrValC of nnz elements csrColIndC of nnz elements The general procedure is as follows:   Given general BSR format (bsrRowPtrA, bsrColIndA, bsrValA) and   blocks of BSR format are stored in column-major order.
cusparseDirection_t dir = CUSPARSE_DIRECTION_COLUMN ; int m = mb * rowBlockDim ; int n = nb * colBlockDim ; int nnzb = bsrRowPtrA [ mb ] - bsrRowPtrA [ 0 ];   number of blocks int nnz = nnzb * rowBlockDim * colBlockDim ;   number of elements cudaMalloc (( void ** ) & csrRowPtrC , sizeof ( int ) * ( m + 1 )); cudaMalloc (( void ** ) & csrColIndC , sizeof ( int ) * nnz ); cudaMalloc (( void ** ) & csrValC , sizeof ( float ) * nnz ); cusparseSgebsr2csr ( handle , dir , mb , nb , descrA , bsrValA , bsrRowPtrA , bsrColIndA , rowBlockDim , colBlockDim , descrC , csrValC , csrRowPtrC , csrColIndC ); The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context.
The matrix A is a m*n sparse matrix and matrix C is a (mb*rowBlockDim)*(nb*colBlockDim) sparse matrix, where mb(=(m+rowBlockDim-1)/rowBlockDim) is the number of block rows of C , and nb(=(n+colBlockDim-1)/colBlockDim) is the number of block columns of C .
If m is not multiple of rowBlockDim or n is not multiple of colBlockDim , zeros are filled in.
First, the user allocates bsrRowPtrC of mb+1 elements and uses function cusparseXcsr2gebsrNnz() to determine the number of nonzero block columns per block row.
Second, the user gathers nnzb (number of nonzero block columns of matrix C ) from either (nnzb=*nnzTotalDevHostPtr) or (nnzb=bsrRowPtrC[mb]-bsrRowPtrC[0]) and allocates bsrValC of nnzb*rowBlockDim*colBlockDim elements and bsrColIndC of nnzb integers.
The user must obtain the size of the buffer required by csr2gebsr() by calling csr2gebsr_bufferSize() , allocate the buffer, and pass the buffer pointer to csr2gebsr() .
The general procedure is as follows:   Given CSR format (csrRowPtrA, csrColIndA, csrValA) and   blocks of BSR format are stored in column-major order.
csrRowPtrA integer array of m+1 elements that contains the start of every row and the end of the last row plus one of matrix A .
bsrRowPtrC integer array of mb+1 elements that contains the start of every block row and the end of the last block row plus one of matrix C .
Pointer nnzTotalDevHostPtr can point to a device memory or host memory. 5.9.6. cusparsecoo2csr()  cusparseStatus_t cusparseXcoo2csr ( cusparseHandle_t handle , const int * cooRowInd , int nnz , int m , int * csrRowPtr , cusparseIndexBase_t idxBase ) This function converts the array containing the uncompressed row indices (corresponding to COO format) into an array of compressed row pointers (corresponding to CSR format).
It can also be used to convert the array containing the uncompressed column indices (corresponding to COO format) into an array of column pointers (corresponding to CSC format).
nnz number of non-zeros of the sparse matrix (that is also the length of array cooRowInd ).
Output csrRowPtr integer array of m+1 elements that contains the start of every row and the end of the last row plus one. 5.9.7. cusparsecsr2coo()  cusparseStatus_t cusparseXcsr2coo ( cusparseHandle_t handle , const int * csrRowPtr , int nnz , int m , int * cooRowInd , cusparseIndexBase_t idxBase ) This function converts the array containing the compressed row pointers (corresponding to CSR format) into an array of uncompressed row indices (corresponding to COO format).
It can also be used to convert the array containing the compressed column indices (corresponding to CSC format) into an array of uncompressed column indices (corresponding to COO format).
csrRowPtr integer array of m+1 elements that contains the start of every row and the end of the last row plus one.
nnz number of nonzeros of the sparse matrix (that is also the length of array cooRowInd ).
Notice that this routine can also be used to convert a matrix in CSC format into a matrix in CSR format.
It is executed asynchronously with respect to the host, and it may return control to the application on the host before the result is ready.
The function cusparseCsr2cscEx2_bufferSize() returns the size of the workspace needed by cusparseCsr2cscEx2() .
User needs to allocate a buffer of this size and give that buffer to cusparseCsr2cscEx2() as an argument.
If m == 0 or n == 0 , the pointers are not checked and the routine returns CUSPARSE_STATUS_SUCCESS .
dirA direction that specifies whether to count nonzero elements by CUSPARSE_DIRECTION_ROW or by CUSPARSE_DIRECTION_COLUMN .
Output nnzPerRowColumn array of size m or n containing the number of nonzero elements per row or column, respectively. 5.9.10. cusparseCreateIdentityPermutation() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateIdentityPermutation ( cusparseHandle_t handle , int n , int * p ); This function creates an identity map.
The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input parameter device or host description handle host handle to the cuSPARSE library context.
Output parameter device or host description p device integer array of dimensions n . 5.9.11. cusparseXcoosort()  cusparseStatus_t cusparseXcoosort_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnz , const int * cooRows , const int * cooCols , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseXcoosortByRow ( cusparseHandle_t handle , int m , int n , int nnz , int * cooRows , int * cooCols , int * P , void * pBuffer ) cusparseStatus_t cusparseXcoosortByColumn ( cusparseHandle_t handle , int m , int n , int nnz , int * cooRows , int * cooCols , int * P , void * pBuffer ); This function sorts COO format.
A is an m×n sparse matrix that is defined in COO storage format by the three arrays cooVals , cooRows , and cooCols .
coosort uses stable sort on signed integer, so the value of cooRows or cooCols can be negative.
If the user wants to compute sorted cooVal , P must be set as 0:1:(nnz-1) before coosort() , and after coosort() , new sorted value array satisfies cooVal_sorted = cooVal(P) .
This usually happens if the user only reads a COO array first and needs to decide the dimension m or n later.
The routine requires no extra storage if pBuffer != NULL The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input parameter device or host description handle host handle to the cuSPARSE library context.
pBuffer device buffer allocated by the user; the size is returned by coosort_bufferSizeExt() .
Output parameter device or host description cooRows device integer array of nnz sorted row indices of A .
See cusparseStatus_t for the description of the return status Please visit cuSPARSE Library Samples - cusparseXcoosortByRow for a code example. 5.9.12. cusparseXcsrsort()  cusparseStatus_t cusparseXcsrsort_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnz , const int * csrRowPtr , const int * csrColInd , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseXcsrsort ( cusparseHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , const int * csrRowPtr , int * csrColInd , int * P , void * pBuffer ) This function sorts CSR format.
If the user wants to compute sorted csrVal , P must be set as 0:1:(nnz-1) before csrsort() , and after csrsort() , new sorted value array satisfies csrVal_sorted = csrVal(P) .
csrRowsPtr device integer array of m+1 elements that contains the start of every row and the end of the last row plus one.
pBuffer device buffer allocated by the user; the size is returned by csrsort_bufferSizeExt() .
Output parameter device or host description csrColInd device integer array of nnz sorted column indices of A . 5.9.13. cusparseXcscsort()  cusparseStatus_t cusparseXcscsort_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnz , const int * cscColPtr , const int * cscRowInd , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseXcscsort ( cusparseHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , const int * cscColPtr , int * cscRowInd , int * P , void * pBuffer ) This function sorts CSC format.
If the user wants to compute sorted cscVal , P must be set as 0:1:(nnz-1) before cscsort() , and after cscsort() , new sorted value array satisfies cscVal_sorted = cscVal(P) .
cscColPtr device integer array of n+1 elements that contains the start of every column and the end of the last column plus one.
pBuffer device buffer allocated by the user; the size is returned by cscsort_bufferSizeExt() .
If the user has a matrix A of CSR format which is unsorted, and implements his own code (which can be CPU or GPU kernel) based on this special order (for example, diagonal first, then lower triangle, then upper triangle), and wants to convert it to CSR format when calling CUSPARSE library, and then convert it back when doing something else on his/her kernel.
For example, suppose the user wants to solve a linear system Ax=b by the following iterative scheme \(x^{(k+1)} = x^{(k)} + L^{(-1)}*(b - Ax^{(k)})\) The code heavily uses SpMV and triangular solve.
Assume that the user has an in-house design of SpMV (Sparse Matrix-Vector multiplication) based on special order of A .
do step 1: compute residual vector r = b - A x (k) by in-house SpMV step 2: B := sort(A), and L is lower triangular part of B (only sort A once and keep the permutation vector) step 3: solve z = L (-1) * ( b - A x (k) ) by cusparseXcsrsv step 4: add correction x (k+1) = x (k) + z step 5: A := unsort(B) (use permutation vector to get back the unsorted CSR) until convergence The requirements of step 2 and step 5 are In-place operation.
The conversion between unsorted CSR and sorted CSR may needs several times, but the function only generates the permutation vector P once.
In order to keep the permutation vector invisible, we need an opaque structure called csru2csrInfo .
Then two functions ( cusparseCreateCsru2csrInfo , cusparseDestroyCsru2csrInfo ) are used to initialize and to destroy the opaque structure.
cusparse[S|D|C|Z]csru2csr performs forward transformation from unsorted CSR to sorted CSR.
First call uses csrsort to generate the permutation vector P , and subsequent call uses P to do transformation.
cusparse[S|D|C|Z]csr2csru performs backward transformation from sorted CSR to unsorted CSR.
The routine cusparsecsru2csr() has the following properties: The routine requires no extra storage if pBuffer != NULL The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available The routine cusparsecsr2csru() has the following properties if pBuffer != NULL : The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture The following tables describe parameters of csr2csru_bufferSizeExt and csr2csru .
Input parameter device or host description handle host handle to the cuSPARSE library context.
The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL , Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE .
pBuffer device buffer allocated by the user; the size is returned by csru2csr_bufferSizeExt() .
Output parameter device or host description csrVal device array of nnz sorted nonzero elements of matrix A .
Given a dense matrix A and a non-negative value threshold , the function returns a sparse matrix C , defined by \(\begin{matrix} {{C(i,j)} = {A(i,j)}} & \text{if\ |A(i,j)|\ >\ threshold} \\ \end{matrix}\) The implementation adopts a two-step approach to do the conversion.
First, the user allocates csrRowPtrC of m+1 elements and uses function pruneDense2csrNnz() to determine the number of nonzeros columns per row.
Second, the user gathers nnzC (number of nonzeros of matrix C ) from either (nnzC=*nnzTotalDevHostPtr) or (nnzC=csrRowPtrC[m]-csrRowPtrC[0]) and allocates csrValC of nnzC elements and csrColIndC of nnzC integers.
The user must obtain the size of the buffer required by pruneDense2csr() by calling pruneDense2csr_bufferSizeExt() , allocate the buffer, and pass the buffer pointer to pruneDense2csr() .
The routine cusparsepruneDense2csrNnz() has the following properties: This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available The routine cusparseDpruneDense2csr() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input parameter device or host description handle host handle to the cuSPARSE library context.
pBuffer device buffer allocated by the user; the size is returned by pruneDense2csr_bufferSizeExt() .
Output parameter device or host description nnzTotalDevHostPtr device or host total number of nonzero of matrix C .
csrRowsPtrC device integer array of m+1 elements that contains the start of every row and the end of the last row plus one.
Given a sparse matrix A and a non-negative value threshold , the function returns a sparse matrix C , defined by \(\begin{matrix} {{C(i,j)} = {A(i,j)}} & \text{if\ |A(i,j)|\ >\ threshold} \\ \end{matrix}\) The implementation adopts a two-step approach to do the conversion.
First, the user allocates csrRowPtrC of m+1 elements and uses function pruneCsr2csrNnz() to determine the number of nonzeros columns per row.
The user must obtain the size of the buffer required by pruneCsr2csr() by calling pruneCsr2csr_bufferSizeExt() , allocate the buffer, and pass the buffer pointer to pruneCsr2csr() .
The routine cusparsepruneCsr2csrNnz() has the following properties: This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available The routine cusparsepruneCsr2csr() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input parameter device or host description handle host handle to the cuSPARSE library context.
csrRowsPtrA device integer array of m+1 elements that contains the start of every row and the end of the last row plus one.
Given a dense matrix A and a non-negative value percentage , the function computes sparse matrix C by the following three steps: Step 1: sort absolute value of A in ascending order.
\(\begin{matrix} {key\ :=\ sort(\ |A|\ )} \\ \end{matrix}\) Step 2: choose threshold by the parameter percentage \(\begin{matrix} {pos\ =\ ceil(m*n*(percentage/100))\ -\ 1} \\ {pos\ =\ min(pos,\ m*n-1)} \\ {pos\ =\ max(pos,\ 0)} \\ {threshold\ =\ key\lbrack pos brack} \\ \end{matrix}\) Step 3: call pruneDense2csr() by with the parameter threshold .
First, the user allocates csrRowPtrC of m+1 elements and uses function pruneDense2csrNnzByPercentage() to determine the number of nonzeros columns per row.
The user must obtain the size of the buffer required by pruneDense2csrByPercentage() by calling pruneDense2csrByPercentage_bufferSizeExt() , allocate the buffer, and pass the buffer pointer to pruneDense2csrByPercentage() .
This is different from pruneCsr2csrByPercentage() Examples of prune chapter provides a simple example of pruneDense2csrNnzByPercentage() .
The routine cusparsepruneDense2csrNnzByPercentage() has the following properties: This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available The routine cusparsepruneDense2csrByPercentage() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input parameter device or host description handle host handle to the cuSPARSE library context.
Given a sparse matrix A and a non-negative value percentage , the function computes sparse matrix C by the following three steps: Step 1: sort absolute value of A in ascending order.
\(\begin{matrix} {key\ :=\ sort(\ |csrValA|\ )} \\ \end{matrix}\) Step 2: choose threshold by the parameter percentage \(\begin{matrix} {pos\ =\ ceil(nnzA*(percentage/100))\ -\ 1} \\ {pos\ =\ min(pos,\ nnzA-1)} \\ {pos\ =\ max(pos,\ 0)} \\ {threshold\ =\ key\lbrack pos brack} \\ \end{matrix}\) Step 3: call pruneCsr2csr() by with the parameter threshold .
First, the user allocates csrRowPtrC of m+1 elements and uses function pruneCsr2csrNnzByPercentage() to determine the number of nonzeros columns per row.
The user must obtain the size of the buffer required by pruneCsr2csrByPercentage() by calling pruneCsr2csrByPercentage_bufferSizeExt() , allocate the buffer, and pass the buffer pointer to pruneCsr2csrByPercentage() .
The routine cusparsepruneCsr2csrNnzByPercentage() has the following properties: This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available The routine cusparsepruneCsr2csrByPercentage() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input parameter device or host description handle host handle to the cuSPARSE library context.
Given a sparse matrix A and a non-negative value threshold, the function returns nnzPerRow(the number of nonzeros columns per row) and nnzC(the total number of nonzeros) of a sparse matrix C, defined by \(\begin{matrix} {{C(i,j)} = {A(i,j)}} & \text{if\ |A(i,j)|\ >\ threshold} \\ \end{matrix}\) A key assumption for the cuComplex and cuDoubleComplex case is that this tolerance is given as the real part.
For example tol = 1e-8 + 0*i and we extract cureal, that is the x component of this struct.
csrValA csr noncompressed values array csrRowPtrA the corresponding input noncompressed row pointer.
Output nnzPerRow this array contains the number of elements whose absolute values are greater than tol per row.
nnzC host/device pointer of the total number of elements whose absolute values are greater than tol. 6. cuSPARSE Generic APIs  The cuSPARSE Generic APIs allow computing the most common sparse linear algebra operations, such as sparse matrix-vector (SpMV) and sparse matrix-matrix multiplication (SpMM), in a flexible way.
The new APIs have the following capabilities and features: Set matrix data layouts, number of batches, and storage formats (for example, CSR, COO, and so on).
This also allows mixed data-type computation. 6.1. Generic Types Reference  The cuSPARSE generic type references are described in this section.
Value Meaning CUSPARSE_FORMAT_COO The matrix is stored in Coordinate (COO) format organized in Structure of Arrays (SoA) layout CUSPARSE_FORMAT_CSR The matrix is stored in Compressed Sparse Row (CSR) format CUSPARSE_FORMAT_CSC The matrix is stored in Compressed Sparse Column (CSC) format CUSPARSE_FORMAT_BLOCKED_ELL The matrix is stored in Blocked-Ellpack (Blocked-ELL) format CUSPARSE_FORMAT_SLICED_ELL The matrix is stored in Sliced-Ellpack (Sliced-ELL) format CUSPARSE_FORMAT_BSR The matrix is stored in Block Sparse Row (BSR) format 6.1.2.
Value Meaning CUSPARSE_ORDER_ROW The matrix is stored in row-major CUSPARSE_ORDER_COL The matrix is stored in column-major 6.1.3.
cusparseIndexType_t  This type indicates the index type for representing the sparse matrix indices.
Value Meaning CUSPARSE_INDEX_32I 32-bit signed integer [1, 2^31 - 1] CUSPARSE_INDEX_64I 64-bit signed integer [1, 2^63 - 1] 6.2.
Dense Vector APIs  The cuSPARSE helper functions for dense vector descriptor are described in this section.
See the Dense Vector Format section for the detailed description of the storage format. 6.2.1. cusparseCreateDnVec()  cusparseStatus_t cusparseCreateDnVec ( cusparseDnVecDescr_t * dnVecDescr , int64_t size , void * values , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstDnVec ( cusparseConstDnVecDescr_t * dnVecDescr , int64_t size , const void * values , cudaDataType valueType ) This function initializes the dense vector descriptor dnVecDescr .
Memory In/out Meaning dnVecDescr HOST OUT Dense vector descriptor size HOST IN Size of the dense vector values DEVICE IN Values of the dense vector.
Array with size elements valueType HOST IN Enumerator specifying the datatype of values cusparseCreateDnVec() has the following constraints: values must be aligned to the size of the datatype specified by valueType .
See cudaDataType_t for the description of the datatypes. 6.2.2. cusparseDestroyDnVec()  cusparseStatus_t cusparseDestroyDnVec ( cusparseConstDnVecDescr_t dnVecDescr )   non-const descriptor supported This function releases the host memory allocated for the dense vector descriptor dnVecDescr .
Memory In/out Meaning dnVecDescr HOST IN Dense vector descriptor See cusparseStatus_t for the description of the return status. 6.2.3. cusparseDnVecGet()  cusparseStatus_t cusparseDnVecGet ( cusparseDnVecDescr_t dnVecDescr , int64_t * size , void ** values , cudaDataType * valueType ) cusparseStatus_t cusparseConstDnVecGet ( cusparseConstDnVecDescr_t dnVecDescr , int64_t * size , const void ** values , cudaDataType * valueType ) This function returns the fields of the dense vector descriptor dnVecDescr .
Memory In/out Meaning dnVecDescr HOST IN Dense vector descriptor size HOST OUT Size of the dense vector values DEVICE OUT Values of the dense vector.
Array with nnz elements valueType HOST OUT Enumerator specifying the datatype of values See cusparseStatus_t for the description of the return status. 6.2.4. cusparseDnVecGetValues()  cusparseStatus_t cusparseDnVecGetValues ( cusparseDnVecDescr_t dnVecDescr , void ** values ) cusparseStatus_t cusparseConstDnVecGetValues ( cusparseConstDnVecDescr_t dnVecDescr , const void ** values ) This function returns the values field of the dense vector descriptor dnVecDescr .
Memory In/out Meaning dnVecDescr HOST IN Dense vector descriptor values DEVICE OUT Values of the dense vector See cusparseStatus_t for the description of the return status. 6.2.5. cusparseDnVecSetValues()  cusparseStatus_t cusparseDnVecSetValues ( cusparseDnVecDescr_t dnVecDescr , void * values ) This function set the values field of the dense vector descriptor dnVecDescr .
Memory In/out Meaning dnVecDescr HOST IN Dense vector descriptor values DEVICE IN Values of the dense vector.
Array with size elements cusparseDnVecSetValues() has the following constraints: values must be aligned to the size of the datatype specified in dnVecDescr . 6.3. Sparse Vector APIs  The cuSPARSE helper functions for sparse vector descriptor are described in this section.
See the Sparse Vector Format section for the detailed description of the storage format. 6.3.1. cusparseCreateSpVec()  cusparseStatus_t cusparseCreateSpVec ( cusparseSpVecDescr_t * spVecDescr , int64_t size , int64_t nnz , void * indices , void * values , cusparseIndexType_t idxType , cusparseIndexBase_t idxBase , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstSpVec ( cusparseConstSpVecDescr_t * spVecDescr , int64_t size , int64_t nnz , const void * indices , const void * values , cusparseIndexType_t idxType , cusparseIndexBase_t idxBase , cudaDataType valueType ) This function initializes the sparse matrix descriptor spVecDescr .
Memory In/out Meaning spVecDescr HOST OUT Sparse vector descriptor size HOST IN Size of the sparse vector nnz HOST IN Number of non-zero entries of the sparse vector indices DEVICE IN Indices of the sparse vector.
Array with nnz elements idxType HOST IN Enumerator specifying the data type of indices idxBase HOST IN Enumerator specifying the the index base of indices valueType HOST IN Enumerator specifying the datatype of values cusparseCreateSpVec() has the following constraints: indices and values must be aligned to the size of the datatypes specified by idxType and valueType , respectively. 6.3.2. cusparseDestroySpVec()  cusparseStatus_t cusparseDestroySpVec ( cusparseConstSpVecDescr_t spVecDescr )   non-const descriptor supported This function releases the host memory allocated for the sparse vector descriptor spVecDescr .
Memory In/out Meaning spVecDescr HOST IN Sparse vector descriptor See cusparseStatus_t for the description of the return status. 6.3.3. cusparseSpVecGet()  cusparseStatus_t cusparseSpVecGet ( cusparseSpVecDescr_t spVecDescr , int64_t * size , int64_t * nnz , void ** indices , void ** values , cusparseIndexType_t * idxType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) cusparseStatus_t cusparseConstSpVecGet ( cusparseConstSpVecDescr_t spVecDescr , int64_t * size , int64_t * nnz , const void ** indices , const void ** values , cusparseIndexType_t * idxType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) This function returns the fields of the sparse vector descriptor spVecDescr .
Memory In/out Meaning spVecDescr HOST IN Sparse vector descriptor size HOST OUT Size of the sparse vector nnz HOST OUT Number of non-zero entries of the sparse vector indices DEVICE OUT Indices of the sparse vector.
Array with nnz elements idxType HOST OUT Enumerator specifying the data type of indices idxBase HOST OUT Enumerator specifying the the index base of indices valueType HOST OUT Enumerator specifying the datatype of values See cusparseStatus_t for the description of the return status. 6.3.4. cusparseSpVecGetIndexBase()  cusparseStatus_t cusparseSpVecGetIndexBase ( cusparseConstSpVecDescr_t spVecDescr ,   non-const descriptor supported cusparseIndexBase_t * idxBase ) This function returns the idxBase field of the sparse vector descriptor spVecDescr .
Memory In/out Meaning spVecDescr HOST IN Sparse vector descriptor idxBase HOST OUT Enumerator specifying the the index base of indices See cusparseStatus_t for the description of the return status. 6.3.5. cusparseSpVecGetValues()  cusparseStatus_t cusparseSpVecGetValues ( cusparseSpVecDescr_t spVecDescr , void ** values ) cusparseStatus_t cusparseConstSpVecGetValues ( cusparseConstSpVecDescr_t spVecDescr , const void ** values ) This function returns the values field of the sparse vector descriptor spVecDescr .
Memory In/out Meaning spVecDescr HOST IN Sparse vector descriptor values DEVICE OUT Values of the sparse vector.
Array with nnz elements See cusparseStatus_t for the description of the return status. 6.3.6. cusparseSpVecSetValues()  cusparseStatus_t cusparseSpVecSetValues ( cusparseSpVecDescr_t spVecDescr , void * values ) This function set the values field of the sparse vector descriptor spVecDescr .
Memory In/out Meaning spVecDescr HOST IN Sparse vector descriptor values DEVICE IN Values of the sparse vector.
Array with nnz elements cusparseDnVecSetValues() has the following constraints: values must be aligned to the size of the datatype specified in spVecDescr . 6.4. Dense Matrix APIs  The cuSPARSE helper functions for dense matrix descriptor are described in this section.
See the Dense Matrix Format section for the detailed description of the storage format. 6.4.1. cusparseCreateDnMat()  cusparseStatus_t cusparseCreateDnMat ( cusparseDnMatDescr_t * dnMatDescr , int64_t rows , int64_t cols , int64_t ld , void * values , cudaDataType valueType , cusparseOrder_t order ) cusparseStatus_t cusparseCreateConstDnMat ( cusparseConstDnMatDescr_t * dnMatDescr , int64_t rows , int64_t cols , int64_t ld , const void * values , cudaDataType valueType , cusparseOrder_t order ) The function initializes the dense matrix descriptor dnMatDescr .
Memory In/out Meaning dnMatDescr HOST OUT Dense matrix descriptor rows HOST IN Number of rows of the dense matrix cols HOST IN Number of columns of the dense matrix ld HOST IN Leading dimension of the dense matrix values DEVICE IN Values of the dense matrix.
Array with size elements valueType HOST IN Enumerator specifying the datatype of values order HOST IN Enumerator specifying the memory layout of the dense matrix cusparseCreateDnMat() has the following constraints: values must be aligned to the size of the datatype specified by valueType . 6.4.2. cusparseDestroyDnMat()  cusparseStatus_t cusparseDestroyDnMat ( cusparseConstDnMatDescr_t dnMatDescr )   non-const descriptor supported This function releases the host memory allocated for the dense matrix descriptor dnMatDescr .
Memory In/out Meaning dnMatDescr HOST IN Dense matrix descriptor See cusparseStatus_t for the description of the return status. 6.4.3. cusparseDnMatGet()  cusparseStatus_t cusparseDnMatGet ( cusparseDnMatDescr_t dnMatDescr , int64_t * rows , int64_t * cols , int64_t * ld , void ** values , cudaDataType * type , cusparseOrder_t * order ) cusparseStatus_t cusparseConstDnMatGet ( cusparseConstDnMatDescr_t dnMatDescr , int64_t * rows , int64_t * cols , int64_t * ld , const void ** values , cudaDataType * type , cusparseOrder_t * order ) This function returns the fields of the dense matrix descriptor dnMatDescr .
Memory In/out Meaning dnMatDescr HOST IN Dense matrix descriptor rows HOST OUT Number of rows of the dense matrix cols HOST OUT Number of columns of the dense matrix ld HOST OUT Leading dimension of the dense matrix values DEVICE OUT Values of the dense matrix.
Array with ld * cols elements valueType HOST OUT Enumerator specifying the datatype of values order HOST OUT Enumerator specifying the memory layout of the dense matrix See cusparseStatus_t for the description of the return status. 6.4.4. cusparseDnMatGetValues()  cusparseStatus_t cusparseDnMatGetValues ( cusparseDnMatDescr_t dnMatDescr , void ** values ) cusparseStatus_t cusparseConstDnMatGetValues ( cusparseConstDnMatDescr_t dnMatDescr , const void ** values ) This function returns the values field of the dense matrix descriptor dnMatDescr .
Memory In/out Meaning dnMatDescr HOST IN Dense matrix descriptor values DEVICE OUT Values of the dense matrix.
Array with ld * cols elements See cusparseStatus_t for the description of the return status. 6.4.5. cusparseDnMatSetValues()  cusparseStatus_t cusparseDnMatSetValues ( cusparseDnMatDescr_t dnMatDescr , void * values ) This function sets the values field of the dense matrix descriptor dnMatDescr .
Memory In/out Meaning dnMatDescr HOST IN Dense matrix descriptor values DEVICE IN Values of the dense matrix.
Array with ld * cols elements cusparseDnMatSetValues() has the following constraints: values must be aligned to the size of the datatype specified in dnMatDescr . 6.4.6. cusparseDnMatGetStridedBatch()  cusparseStatus_t cusparseDnMatGetStridedBatch ( cusparseConstDnMatDescr_t dnMatDescr ,   non-const descriptor supported int * batchCount , int64_t * batchStride ) The function returns the number of batches and the batch stride of the dense matrix descriptor dnMatDescr .
Memory In/out Meaning dnMatDescr HOST IN Dense matrix descriptor batchCount HOST OUT Number of batches of the dense matrix batchStride HOST OUT Address offset between a matrix and the next one in the batch See cusparseStatus_t for the description of the return status. 6.4.7. cusparseDnMatSetStridedBatch()  cusparseStatus_t cusparseDnMatSetStridedBatch ( cusparseDnMatDescr_t dnMatDescr , int batchCount , int64_t batchStride ) The function sets the number of batches and the batch stride of the dense matrix descriptor dnMatDescr .
Memory In/out Meaning dnMatDescr HOST IN Dense matrix descriptor batchCount HOST IN Number of batches of the dense matrix batchStride HOST IN Address offset between a matrix and the next one in the batch.
batchStride ≥ ld * cols if the matrix uses column-major layout, batchStride ≥ ld * rows otherwise See cusparseStatus_t for the description of the return status. 6.5. Sparse Matrix APIs  The cuSPARSE helper functions for sparse matrix descriptor are described in this section.
See the COO , CSR , CSC , SELL , BSR , Blocked-Ell sections for the detailed description of the storage formats. 6.5.1. Coordinate (COO)  6.5.1.1.
cusparseCreateCoo()  cusparseStatus_t cusparseCreateCoo ( cusparseSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t nnz , void * cooRowInd , void * cooColInd , void * cooValues , cusparseIndexType_t cooIdxType , cusparseIndexBase_t idxBase , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstCoo ( cusparseConstSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t nnz , const void * cooRowInd , const void * cooColInd , const void * cooValues , cusparseIndexType_t cooIdxType , cusparseIndexBase_t idxBase , cudaDataType valueType ) This function initializes the sparse matrix descriptor spMatDescr in the COO format (Structure of Arrays layout).
Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor rows HOST IN Number of rows of the sparse matrix cols HOST IN Number of columns of the sparse matrix nnz HOST IN Number of non-zero entries of the sparse matrix cooRowInd DEVICE IN Row indices of the sparse matrix.
Array with nnz elements cooIdxType HOST IN Data type of cooRowInd and cooColInd idxBase HOST IN Index base of cooRowInd and cooColInd valueType HOST IN Datatype of cooValues cusparseCreateCoo() has the following constraints: cooRowInd , cooColInd , and cooValues must be aligned to the size of the datatypes specified by cooIdxType , cooIdxType , and valueType .
respectively. 6.5.1.2. cusparseCooGet()  cusparseStatus_t cusparseCooGet ( cusparseSpMatDescr_t spMatDescr , int64_t * rows , int64_t * cols , int64_t * nnz , void ** cooRowInd , void ** cooColInd , void ** cooValues , cusparseIndexType_t * idxType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) cusparseStatus_t cusparseConstCooGet ( cusparseConstSpMatDescr_t spMatDescr , int64_t * rows , int64_t * cols , int64_t * nnz , const void ** cooRowInd , const void ** cooColInd , const void ** cooValues , cusparseIndexType_t * idxType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) This function returns the fields of the sparse matrix descriptor spMatDescr stored in COO format (Array of Structures layout).
Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor rows HOST OUT Number of rows of the sparse matrix cols HOST OUT Number of columns of the sparse matrix nnz HOST OUT Number of non-zero entries of the sparse matrix cooRowInd DEVICE OUT Row indices of the sparse matrix.
Array nnz elements cooIdxType HOST OUT Data type of cooRowInd and cooColInd idxBase HOST OUT Index base of cooRowInd and cooColInd valueType HOST OUT Datatype of cooValues See cusparseStatus_t for the description of the return status. 6.5.1.3. cusparseCooSetPointers()  cusparseStatus_t cusparseCooSetPointers ( cusparseSpMatDescr_t spMatDescr , void * cooRows , void * cooColumns , void * cooValues ) This function sets the pointers of the sparse matrix descriptor spMatDescr .
Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor cooRows DEVICE IN Row indices of the sparse matrix.
Array with nnz elements cusparseCooSetPointers() has the following constraints: cooRows , cooColumns , and cooValues must be aligned to the size of their corresponding datatypes specified in spMatDescr . 6.5.1.4. cusparseCooSetStridedBatch()  cusparseStatus_t cusparseCooSetStridedBatch ( cusparseSpMatDescr_t spMatDescr , int batchCount , int64_t batchStride ) This function sets the batchCount and the batchStride fields of the sparse matrix descriptor spMatDescr .
Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor batchCount HOST IN Number of batches of the sparse matrix batchStride HOST IN address offset between consecutive batches See cusparseStatus_t for the description of the return status. 6.5.2. Compressed Sparse Row (CSR)  6.5.2.1.
cusparseCreateCsr()  cusparseStatus_t cusparseCreateCsr ( cusparseSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t nnz , void * csrRowOffsets , void * csrColInd , void * csrValues , cusparseIndexType_t csrRowOffsetsType , cusparseIndexType_t csrColIndType , cusparseIndexBase_t idxBase , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstCsr ( cusparseConstSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t nnz , const void * csrRowOffsets , const void * csrColInd , const void * csrValues , cusparseIndexType_t csrRowOffsetsType , cusparseIndexType_t csrColIndType , cusparseIndexBase_t idxBase , cudaDataType valueType ) This function initializes the sparse matrix descriptor spMatDescr in the CSR format.
Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor rows HOST IN Number of rows of the sparse matrix cols HOST IN Number of columns of the sparse matrix nnz HOST IN Number of non-zero entries of the sparse matrix csrRowOffsets DEVICE IN Row offsets of the sparse matrix.
Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor rows HOST OUT Number of rows of the sparse matrix cols HOST OUT Number of columns of the sparse matrix nnz HOST OUT Number of non-zero entries of the sparse matrix csrRowOffsets DEVICE OUT Row offsets of the sparse matrix.
Array with nnz elements csrRowOffsetsType HOST OUT Data type of csrRowOffsets csrColIndType HOST OUT Data type of csrColInd idxBase HOST OUT Index base of csrRowOffsets and csrColInd valueType HOST OUT Datatype of csrValues See cusparseStatus_t for the description of the return status. 6.5.2.3. cusparseCsrSetPointers()  cusparseStatus_t cusparseCsrSetPointers ( cusparseSpMatDescr_t spMatDescr , void * csrRowOffsets , void * csrColInd , void * csrValues ) This function sets the pointers of the sparse matrix descriptor spMatDescr .
Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor csrRowOffsets DEVICE IN Row offsets of the sparse matrix.
Array with nnz elements cusparseCsrSetPointers() has the following constraints: csrRowOffsets , csrColInd , and csrValues must be aligned to the size of their corresponding datatypes specified in spMatDescr . 6.5.2.4. cusparseCsrSetStridedBatch()  cusparseStatus_t cusparseCsrSetStridedBatch ( cusparseSpMatDescr_t spMatDescr , int batchCount , int64_t offsetsBatchStride , int64_t columnsValuesBatchStride ) This function sets the batchCount and the batchStride fields of the sparse matrix descriptor spMatDescr .
Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor batchCount HOST IN Number of batches of the sparse matrix offsetsBatchStride HOST IN Address offset between consecutive batches for the row offset array columnsValuesBatchStride HOST IN Address offset between consecutive batches for the column and value arrays See cusparseStatus_t for the description of the return status. 6.5.3. Compressed Sparse Column (CSC)  6.5.3.1.
cusparseCreateCsc()  cusparseStatus_t cusparseCreateCsc ( cusparseSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t nnz , void * cscColOffsets , void * cscRowInd , void * cscValues , cusparseIndexType_t cscColOffsetsType , cusparseIndexType_t cscRowIndType , cusparseIndexBase_t idxBase , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstCsc ( cusparseConstSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t nnz , const void * cscColOffsets , const void * cscRowInd , const void * cscValues , cusparseIndexType_t cscColOffsetsType , cusparseIndexType_t cscRowIndType , cusparseIndexBase_t idxBase , cudaDataType valueType ) This function initializes the sparse matrix descriptor spMatDescr in the CSC format.
Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor rows HOST IN Number of rows of the sparse matrix cols HOST IN Number of columns of the sparse matrix nnz HOST IN Number of non-zero entries of the sparse matrix cscColOffsets DEVICE IN Column offsets of the sparse matrix.
Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor rows HOST OUT Number of rows of the sparse matrix cols HOST OUT Number of columns of the sparse matrix nnz HOST OUT Number of non-zero entries of the sparse matrix cscColOffsets DEVICE OUT Col offsets of the sparse matrix.
Array with nnz elements cscColOffsetsType HOST OUT Data type of cscColOffsets cscRowIndType HOST OUT Data type of cscRowInd idxBase HOST OUT Index base of cscColOffsets and cscRowInd valueType HOST OUT Datatype of cscValues See cusparseStatus_t for the description of the return status. 6.5.3.3. cusparseCscSetPointers()  cusparseStatus_t cusparseCscSetPointers ( cusparseSpMatDescr_t spMatDescr , void * cscColOffsets , void * cscRowInd , void * cscValues ) This function sets the pointers of the sparse matrix descriptor spMatDescr .
Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor cscColOffsets DEVICE IN Col offsets of the sparse matrix.
Array with nnz elements cusparseCscSetPointers() has the following constraints: cscColOffsets , cscRowInd , and cscValues must be aligned to the size of their corresponding datatypes specified in spMatDescr . 6.5.4. Blocked-Ellpack (Blocked-ELL)  6.5.4.1.
cusparseCreateBlockedEll()  cusparseStatus_t cusparseCreateBlockedEll ( cusparseSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t ellBlockSize , int64_t ellCols , void * ellColInd , void * ellValue , cusparseIndexType_t ellIdxType , cusparseIndexBase_t idxBase , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstBlockedEll ( cusparseConstSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t ellBlockSize , int64_t ellCols , const void * ellColInd , const void * ellValue , cusparseIndexType_t ellIdxType , cusparseIndexBase_t idxBase , cudaDataType valueType ) This function initializes the sparse matrix descriptor spMatDescr for the Blocked-Ellpack (ELL) format.
Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor rows HOST IN Number of rows of the sparse matrix cols HOST IN Number of columns of the sparse matrix ellBlockSize HOST IN Size of the ELL-Block ellCols HOST IN Actual number of columns of the Blocked-Ellpack format ( ellValue columns) ellColInd DEVICE IN Blocked-ELL Column indices.
Array with [ellCols / ellBlockSize][rows / ellBlockSize] elements ellValue DEVICE IN Values of the sparse matrix.
Array with rows * ellCols elements ellIdxType HOST IN Data type of ellColInd idxBase HOST IN Index base of ellColInd valueType HOST IN Data type of ellValue Blocked-ELL Column indices ( ellColInd ) are in the range [0, cols / ellBlockSize -1] .
The array can contain -1 values for indicating empty blocks. 6.5.4.2. cusparseBlockedEllGet()  cusparseStatus_t cusparseBlockedEllGet ( cusparseSpMatDescr_t spMatDescr , int64_t * rows , int64_t * cols , int64_t * ellBlockSize , int64_t * ellCols , void ** ellColInd , void ** ellValue , cusparseIndexType_t * ellIdxType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) cusparseStatus_t cusparseConstBlockedEllGet ( cusparseConstSpMatDescr_t spMatDescr , int64_t * rows , int64_t * cols , int64_t * ellBlockSize , int64_t * ellCols , const void ** ellColInd , const void ** ellValue , cusparseIndexType_t * ellIdxType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) This function returns the fields of the sparse matrix descriptor spMatDescr stored in Blocked-Ellpack (ELL) format.
Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor rows HOST OUT Number of rows of the sparse matrix cols HOST OUT Number of columns of the sparse matrix ellBlockSize HOST OUT Size of the ELL-Block ellCols HOST OUT Actual number of columns of the Blocked-Ellpack format ellColInd DEVICE OUT Column indices for the ELL-Block.
Array with [cols / ellBlockSize][rows / ellBlockSize] elements ellValue DEVICE OUT Values of the sparse matrix.
Array with rows * ellCols elements ellIdxType HOST OUT Data type of ellColInd idxBase HOST OUT Index base of ellColInd valueType HOST OUT Datatype of ellValue See cusparseStatus_t for the description of the return status. 6.5.5. Sliced-Ellpack (SELL)  6.5.5.1.
Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor rows HOST IN Number of rows of the sparse matrix cols HOST IN Number of columns of the sparse matrix nnz HOST IN Number of nonzero elements in the sparse matrix sellValuesSize HOST IN Total number of elements in sellValues array (nonzero and padding) sliceSize HOST IN The number of rows per slice sellSliceOffsets DEVICE IN Slice offsets of the sparse matrix.
Array of size \(\left \lceil{\frac{rows}{sliceSize}} ight  ceil + 1\) sellColInd DEVICE IN Column indexes of the sparse matrix.
Array of size sellValuesSize elements sellSliceOffsetsType HOST IN Data type of sellSliceOffsets sellColIndType HOST IN Data type of sellColInd idxBase HOST IN Index base of sellColInd valueType HOST IN Data type of sellValues Note Sliced Ellpack Column array sellColInd contains -1 values for indicating padded entries.
cusparseCreateSlicedEll() has the following constraints: sellSliceOffsets , sellColInd , and sellValues must be aligned to the size of the datatypes specified by sellSliceOffsetsType , sellColIndType , and valueType , respectively. 6.5.6. Block Sparse Row (BSR)  6.5.6.1.
Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor brows HOST IN Number of block rows of the sparse matrix bcols HOST IN Number of block columns of the sparse matrix bnnz HOST IN Number of blocks of the sparse matrix rowBlockSize HOST IN Number of rows of each block colBlockSize HOST IN Number of columns of each block bsrRowOffsets DEVICE IN Block row offsets of the sparse matrix.
Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor batchCount HOST IN Number of batches of the sparse matrix offsetsBatchStride HOST IN Address offset between consecutive batches for the row offset array columnsBatchStride HOST IN Address offset between consecutive batches for the column array valuesBatchStride HOST IN Address offset between consecutive batches for the values array See cusparseStatus_t for the description of the return status. 6.5.7. All Sparse Formats  6.5.7.1.
cusparseDestroySpMat()  cusparseStatus_t cusparseDestroySpMat ( cusparseConstSpMatDescr_t spMatDescr )   non-const descriptor supported This function releases the host memory allocated for the sparse matrix descriptor spMatDescr .
Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor See cusparseStatus_t for the description of the return status. 6.5.7.2. cusparseSpMatGetSize()  cusparseStatus_t cusparseSpMatGetSize ( cusparseConstSpMatDescr_t spMatDescr ,   non-const descriptor supported int64_t * rows , int64_t * cols , int64_t * nnz ) This function returns the sizes of the sparse matrix spMatDescr .
Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor rows HOST OUT Number of rows of the sparse matrix cols HOST OUT Number of columns of the sparse matrix nnz HOST OUT Number of non-zero entries of the sparse matrix See cusparseStatus_t for the description of the return status. 6.5.7.3. cusparseSpMatGetFormat()  cusparseStatus_t cusparseSpMatGetFormat ( cusparseConstSpMatDescr_t spMatDescr ,   non-const descriptor supported cusparseFormat_t * format ) This function returns the format field of the sparse matrix descriptor spMatDescr .
Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor format HOST OUT Storage format of the sparse matrix See cusparseStatus_t for the description of the return status. 6.5.7.4. cusparseSpMatGetIndexBase()  cusparseStatus_t cusparseSpMatGetIndexBase ( cusparseConstSpMatDescr_t spMatDescr ,   non-const descriptor supported cusparseIndexBase_t * idxBase ) This function returns the idxBase field of the sparse matrix descriptor spMatDescr .
Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor idxBase HOST OUT Index base of the sparse matrix See cusparseStatus_t for the description of the return status. 6.5.7.5. cusparseSpMatGetValues()  cusparseStatus_t cusparseSpMatGetValues ( cusparseSpMatDescr_t spMatDescr , void ** values ) cusparseStatus_t cusparseConstSpMatGetValues ( cusparseConstSpMatDescr_t spMatDescr , const void ** values ) This function returns the values field of the sparse matrix descriptor spMatDescr .
Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor values DEVICE OUT Values of the sparse matrix. 6.5.7.6. cusparseSpMatSetValues()  cusparseStatus_t cusparseSpMatSetValues ( cusparseSpMatDescr_t spMatDescr , void * values ) This function sets the values field of the sparse matrix descriptor spMatDescr .
Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor values DEVICE IN Values of the sparse matrix.
Array with nnz elements cusparseSpMatSetValues() has the following constraints: values must be aligned to the size of its corresponding datatype specified in spMatDescr . 6.5.7.7. cusparseSpMatGetStridedBatch()  cusparseStatus_t cusparseSpMatGetStridedBatch ( cusparseConstSpMatDescr_t spMatDescr ,   non-const descriptor supported int * batchCount ) This function returns the batchCount field of the sparse matrix descriptor spMatDescr .
Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor batchCount HOST OUT Number of batches of the sparse matrix See cusparseStatus_t for the description of the return status. 6.5.7.8. cusparseSpMatGetAttribute()  cusparseStatus_t cusparseSpMatGetAttribute ( cusparseConstSpMatDescr_t spMatDescr ,   non-const descriptor supported cusparseSpMatAttribute_t attribute , void * data , size_t dataSize ) The function gets the attributes of the sparse matrix descriptor spMatDescr .
Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor attribute HOST IN Attribute enumerator data HOST OUT Attribute value dataSize HOST IN Size of the attribute in bytes for safety Attribute Meaning Possible Values CUSPARSE_SPMAT_FILL_MODE Indicates if the lower or upper part of a matrix is stored in sparse storage CUSPARSE_FILL_MODE_LOWER CUSPARSE_FILL_MODE_UPPER CUSPARSE_SPMAT_DIAG_TYPE Indicates if the matrix diagonal entries are unity CUSPARSE_DIAG_TYPE_NON_UNIT CUSPARSE_DIAG_TYPE_UNIT See cusparseStatus_t for the description of the return status. 6.5.7.9. cusparseSpMatSetAttribute()  cusparseStatus_t cusparseSpMatSetAttribute ( cusparseSpMatDescr_t spMatDescr , cusparseSpMatAttribute_t attribute , const void * data , size_t dataSize ) The function sets the attributes of the sparse matrix descriptor spMatDescr Param.
Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor attribute HOST IN Attribute enumerator data HOST IN Attribute value dataSize HOST IN Size of the attribute in bytes for safety Attribute Meaning Possible Values CUSPARSE_SPMAT_FILL_MODE Indicates if the lower or upper part of a matrix is stored in sparse storage CUSPARSE_FILL_MODE_LOWER CUSPARSE_FILL_MODE_UPPER CUSPARSE_SPMAT_DIAG_TYPE Indicates if the matrix diagonal entries are unity CUSPARSE_DIAG_TYPE_NON_UNIT CUSPARSE_DIAG_TYPE_UNIT See cusparseStatus_t for the description of the return status. 6.6. Generic API Functions  6.6.1.
cusparseAxpby()  cusparseStatus_t cusparseAxpby ( cusparseHandle_t handle , const void * alpha , cusparseConstSpVecDescr_t vecX ,   non-const descriptor supported const void * beta , cusparseDnVecDescr_t vecY ) The function computes the sum of a sparse vector vecX and a dense vector vecY .
\(\mathbf{Y} = \alpha\mathbf{X} + \beta\mathbf{Y}\) In other words, for i = 0 to n -1 Y [ i ] = beta * Y [ i ] for i = 0 to nnz -1 Y [ X_indices [ i ]] += alpha * X_values [ i ] Param.
Please visit cuSPARSE Library Samples - cusparseAxpby for a code example. 6.6.2. cusparseGather()  cusparseStatus_t cusparseGather ( cusparseHandle_t handle , cusparseConstDnVecDescr_t vecY ,   non-const descriptor supported cusparseSpVecDescr_t vecX ) The function gathers the elements of the dense vector vecY into the sparse vector vecX In other words, for i = 0 to nnz -1 X_values [ i ] = Y [ X_indices [ i ]] Param.
Please visit cuSPARSE Library Samples - cusparseGather for a code example. 6.6.3. cusparseScatter()  cusparseStatus_t cusparseScatter ( cusparseHandle_t handle , cusparseConstSpVecDescr_t vecX ,   non-const descriptor supported cusparseDnVecDescr_t vecY ) The function scatters the elements of the sparse vector vecX into the dense vector vecY In other words, for i = 0 to nnz -1 Y [ X_indices [ i ]] = X_values [ i ] Param.
Please visit cuSPARSE Library Samples - cusparseScatter for a code example. 6.6.4. cusparseRot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseRot ( cusparseHandle_t handle , const void * c_coeff , const void * s_coeff , cusparseSpVecDescr_t vecX , cusparseDnVecDescr_t vecY ) The function computes the Givens rotation matrix \(G = \begin{bmatrix} c & s \\ {- s} & c \\ \end{bmatrix}\) to a sparse vecX and a dense vector vecY In other words, for i = 0 to nnz -1 Y [ X_indices [ i ]] = c * Y [ X_indices [ i ]] - s * X_values [ i ] X_values [ i ] = c * X_values [ i ] + s * Y [ X_indices [ i ]] Param.
cusparseSpMV() supports the following algorithms: Algorithm Notes CUSPARSE_SPMV_ALG_DEFAULT Default algorithm for any sparse matrix format.
May produce slightly different results during different runs with the same input parameters.
Performance notes: CUSPARSE_SPMV_COO_ALG1 and CUSPARSE_SPMV_CSR_ALG1 provide higher performance than CUSPARSE_SPMV_COO_ALG2 and CUSPARSE_SPMV_CSR_ALG2 .
In general, opA == CUSPARSE_OPERATION_NON_TRANSPOSE is 3x faster than opA != CUSPARSE_OPERATION_NON_TRANSPOSE .
It is beneficial when we need to run cusparseSpMV() multiple times with a same matrix ( cusparseSpMV_preprocess() is executed only once).
cusparseSpMV() has the following properties: The routine requires extra storage for CSR/CSC format (all algorithms) and for COO format with CUSPARSE_SPMV_COO_ALG2 algorithm.
Provides deterministic (bit-wise) results for each run only for CUSPARSE_SPMV_COO_ALG2 and CUSPARSE_SPMV_CSR_ALG2 algorithms, and opA == CUSPARSE_OPERATION_NON_TRANSPOSE .
cusparseSpMV() supports the following optimizations : CUDA graph capture Hardware Memory Compression See cusparseStatus_t for the description of the return status.
The function cusparseSpSV_analysis() performs the analysis phase, while cusparseSpSV_solve() executes the solve phase for a sparse triangular linear system.
The routine supports arbitrary sparsity for the input matrix, but only the upper or lower triangular part is taken into account in the computation.
NOTE: all parameters must be consistent across cusparseSpSV API calls and the matrix descriptions and externalBuffer must not be modified between cusparseSpSV_analysis() and cusparseSpSV_solve() .
The function cusparseSpSV_updateMatrix() can be used to update the values on the sparse matrix stored inside the opaque data structure spsvDescr Param.
Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context opA HOST IN Operation op(A) alpha HOST or DEVICE IN \(\alpha\) scalar used for multiplication of type computeType matA HOST IN Sparse matrix A vecX HOST IN Dense vector X vecY HOST IN/OUT Dense vector Y computeType HOST IN Datatype in which the computation is executed alg HOST IN Algorithm for the computation bufferSize HOST OUT Number of bytes of workspace needed by cusparseSpSV_analysis() and cusparseSpSV_solve() externalBuffer DEVICE IN/OUT Pointer to a workspace buffer of at least bufferSize bytes.
This functions supports the following update strategies ( updatePart ): Strategy Notes CUSPARSE_SPSV_UPDATE_GENERAL Updates the sparse matrix values with values of newValues array CUSPARSE_SPSV_UPDATE_DIAGONAL Updates the diagonal part of the matrix with diagonal values stored in newValues array.
That is, newValues has the new diagonal values only See cusparseStatus_t for the description of the return status.
\(\mathbf{C} = \alpha op\left( \mathbf{A}  ight) \cdot op\left( \mathbf{B}  ight) + \beta\mathbf{C}\) where op(A) is a sparse matrix of size \(m \times k\) op(B) is a dense matrix of size \(k \times n\) C is a dense matrix of size \(m \times n\) \(\alpha\) and \(\beta\) are scalars The routine can be also used to perform the multiplication of a dense matrix and a sparse matrix by switching the dense matrices layout: \(\begin{array}{l} \left.
The function cusparseSpMM_bufferSize() returns the size of the workspace needed by cusparseSpMM() The function cusparseSpMM_preprocess() can be called before cusparseSpMM to speedup the actual computation.
It is useful when cusparseSpMM is called multiple times with the same sparsity pattern ( matA ).
It provides performance advantages is used with CUSPARSE_SPMM_CSR_ALG1 or CUSPARSE_SPMM_CSR_ALG3 .
cusparseSpMM() has the following properties: The routine requires no extra storage for CUSPARSE_SPMM_COO_ALG1 , CUSPARSE_SPMM_COO_ALG3 , CUSPARSE_SPMM_COO_ALG4 , CUSPARSE_SPMM_BSR_ALG1 The routine supports asynchronous execution Provides deterministic (bit-wise) results for each run only for CUSPARSE_SPMM_COO_ALG2 , CUSPARSE_SPMM_CSR_ALG3 , and CUSPARSE_SPMM_BSR_ALG1 algorithms compute-sanitizer could report false race conditions for this routine.
This is for optimization purposes and does not affect the correctness of the computation The routine allows the indices of matA to be unsorted cusparseSpMM() supports the following optimizations : CUDA graph capture Hardware Memory Compression Please visit cuSPARSE Library Samples - cusparseSpMM CSR and cusparseSpMM COO for a code example.
For batched computation please visit cusparseSpMM CSR Batched and cusparseSpMM COO Batched .
The function has the following limitations: The pointer mode must be equal to CUSPARSE_POINTER_MODE_HOST Only opA == CUSPARSE_OPERATION_NON_TRANSPOSE is supported.
Experimental : The function performs the multiplication of a sparse matrix matA and a dense matrix matB with custom operators.
\({C^{\prime}}_{ij} = \text{epilogue}\left( {\sum_{k}^{\oplus}{op\left( A_{ik}  ight) \otimes op\left( B_{kj}  ight),C_{ij}}}  ight)\) where op(A) is a sparse matrix of size \(m \times k\) op(B) is a dense matrix of size \(k \times n\) C is a dense matrix of size \(m \times n\) \(\oplus\) , \(\otimes\) , and \(\text{epilogue}\) are custom add , mul , and epilogue operators respectively.
Also, for matrix A and B \(\text{op}(A) = \begin{cases} A & \text{if op(A) == CUSPARSE_OPERATION_NON_TRANSPOSE} \\ A^{T} & \text{if op(A) == CUSPARSE_OPERATION_TRANSPOSE} \\ \end{cases}\) \(\text{op}(B) = \begin{cases} B & {\text{if op(}B\text{) == CUSPARSE_OPERATION_NON_TRANSPOSE}} \\ B^{T} & {\text{if op(}B\text{) == CUSPARSE_OPERATION_TRANSPOSE}} \\ \end{cases}\) Only opA == CUSPARSE_OPERATION_NON_TRANSPOSE is currently supported The function cusparseSpMMOp_createPlan() returns the size of the workspace and the compiled kernel needed by cusparseSpMMOp() Param.
The function cusparseSpSM_analysis() performs the analysis phase, while cusparseSpSM_solve() executes the solve phase for a sparse triangular linear system.
cusparseSpSM_bufferSize() requires a buffer size for the analysis phase which is proportional to number of non-zero entries of the sparse matrix The externalBuffer is stored into spsmDescr and used by cusparseSpSM_solve() .
For this reason, the device memory buffer must be deallocated only after cusparseSpSM_solve() NOTE: all parameters must be consistent across cusparseSpSM API calls and the matrix descriptions and externalBuffer must not be modified between cusparseSpSM_analysis() and cusparseSpSM_solve() Param.
Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context opA HOST IN Operation op(A) opB HOST IN Operation op(B) alpha HOST or DEVICE IN \(\alpha\) scalar used for multiplication of type computeType matA HOST IN Sparse matrix A matB HOST IN Dense matrix B matC HOST IN/OUT Dense matrix C computeType HOST IN Datatype in which the computation is executed alg HOST IN Algorithm for the computation bufferSize HOST OUT Number of bytes of workspace needed by cusparseSpSM_analysis() and cusparseSpSM_solve() externalBuffer DEVICE IN/OUT Pointer to a workspace buffer of at least bufferSize bytes.
The same device pointer must be provided to the values parameter of the dense matrices matB and matC .
All other dense matrix descriptor parameters (e.g., order ) can be set independently cusparseSpSM_bufferSize() and cusparseSpSM_analysis() routines accept descriptors of NULL values for matB and matC .
These two routines do not accept NULL descriptors The routine allows the indices of matA to be unsorted cusparseSpSM() supports the following optimizations : CUDA graph capture Hardware Memory Compression cusparseSpSM_updateMatrix() updates the sparse matrix after calling the analysis phase.
This functions supports the following update strategies ( updatePart ): Strategy Notes CUSPARSE_SPSM_UPDATE_GENERAL Updates the sparse matrix values with values of newValues array CUSPARSE_SPSM_UPDATE_DIAGONAL Updates the diagonal part of the matrix with diagonal values stored in newValues array.
The function cusparseSDDMM_preprocess() can be called before cusparseSDDMM to speedup the actual computation.
It is useful when cusparseSDDMM is called multiple times with the same sparsity pattern ( matC ).
cusparseSDDMM() for CUSPASRE_FORMAT_BSR supports block sizes of 2, 4, 8, 16, 32, 64 and 128.
cusparseSDDMM() supports the following algorithms: Algorithm Notes CUSPARSE_SDDMM_ALG_DEFAULT Default algorithm.
cusparseSDDMM() has the following properties: The routine requires no extra storage Provides deterministic (bit-wise) results for each run The routine supports asynchronous execution The routine allows the indices of matC to be unsorted cusparseSDDMM() supports the following optimizations : CUDA graph capture Hardware Memory Compression See cusparseStatus_t for the description of the return status.
\(\mathbf{C^{\prime}} = \alpha op\left( \mathbf{A}  ight) \cdot op\left( \mathbf{B}  ight) + \beta\mathbf{C}\) where \(\alpha,\) \(\beta\) are scalars, and \(\mathbf{C},\) \(\mathbf{C^{\prime}}\) have the same sparsity pattern.
The functions cusparseSpGEMM_workEstimation() , cusparseSpGEMM_estimateMemory() , and cusparseSpGEMM_compute() are used for both determining the buffer size and performing the actual computation.
Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context opA HOST IN Operation op(A) opB HOST IN Operation op(B) alpha HOST or DEVICE IN \(\alpha\) scalar used for multiplication matA HOST IN Sparse matrix A matB HOST IN Sparse matrix B beta HOST or DEVICE IN \(\beta\) scalar used for multiplication matC HOST IN/OUT Sparse matrix C computeType HOST IN Enumerator specifying the datatype in which the computation is executed alg HOST IN Enumerator specifying the algorithm for the computation spgemmDescr HOST IN/OUT Opaque descriptor for storing internal data used across the three steps num_prods HOST OUT Pointer to a 64-bit integer that stores the number of intermediate products calculated by cusparseSpGEMM_workEstimation chunk_fraction HOST IN The fraction of total intermediate products being computed in a chunk.
If it is not sufficient, the routine will returns CUSPARSE_STATUS_INSUFFICIENT_RESOURCES status.
CUSPARSE_SPGEMM_ALG2 Algorithm 2 Invokes cusparseSpGEMM_estimateMemory to get the amount of the memory required for the computation.
CUSPARSE_SPGEMM_ALG3 Algorithm 3 Computes the intermediate products in chunks, one chunk at a time.
Invokes cusparseSpGEMM_estimateMemory to get the amount of the memory required for the computation.
The user can control the amount of required memory by changing the chunk size via chunk_fraction .
The chunk size is a fraction of total intermediate products: chunk_fraction * (*num_prods) .
cusparseSpGEMM() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine allows the indices of matA and matB to be unsorted The routine guarantees the indices of matC to be sorted cusparseSpGEMM() supports the following optimizations : CUDA graph capture Hardware Memory Compression See cusparseStatus_t for the description of the return status.
\(\mathbf{C^{\prime}} = \alpha op\left( \mathbf{A}  ight) \cdot op\left( \mathbf{B}  ight) + \beta\mathbf{C}\) where \(\alpha\) and \(\beta\) are scalars.
The functions cusparseSpGEMMreuse_workEstimation() , cusparseSpGEMMreuse_nnz() , and cusparseSpGEMMreuse_copy() are used for determining the buffer size and performing the actual computation.
MEMORY REQUIREMENT: cusparseSpGEMMreuse requires to keep in memory all intermediate products to reuse the structure of the output matrix.
On the other hand, the number of intermediate products is orders of magnitude higher than the number of non-zero entries in general.
In order to minimize the memory requirements, the routine uses multiple buffers that can be deallocated after they are no more needed.
If the number of intermediate product exceeds 2^31-1 , the routine will returns CUSPARSE_STATUS_INSUFFICIENT_RESOURCES status.
Currently, the function has the following limitations: Only 32-bit indices CUSPARSE_INDEX_32I is supported Only CSR format CUSPARSE_FORMAT_CSR is supported Only opA , opB equal to CUSPARSE_OPERATION_NON_TRANSPOSE are supported The data types combinations currently supported for cusparseSpGEMMreuse are listed below.
Uniform-precision computation: A / B / C / computeType CUDA_R_32F CUDA_R_64F CUDA_C_16F [DEPRECATED] CUDA_C_16BF [DEPRECATED] CUDA_C_32F CUDA_C_64F Mixed-precision computation: [DEPRECATED] A / B C computeType CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_32F cusparseSpGEMMreuse routine runs for the following algorithm: Algorithm Notes CUSPARSE_SPGEMM_DEFAULT CUSPARSE_SPGEMM_CSR_ALG_NONDETERMINITIC Default algorithm.
Provides deterministic (bit-wise) structure for the output matrix for each run, while value computation is not deterministic.
CUSPARSE_SPGEMM_CSR_ALG_DETERMINITIC Provides deterministic (bit-wise) structure for the output matrix and value computation for each run.
cusparseSpGEMMreuse() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine allows the indices of matA and matB to be unsorted The routine guarantees the indices of matC to be sorted cusparseSpGEMMreuse() supports the following optimizations : CUDA graph capture Hardware Memory Compression Refer to cusparseStatus_t for the description of the return status.
Please visit cuSPARSE Library Samples - cusparseSpGEMMreuse for a code example. 6.6.14. cusparseSparseToDense()  cusparseStatus_t cusparseSparseToDense_bufferSize ( cusparseHandle_t handle , cusparseConstSpMatDescr_t matA ,   non-const descriptor supported cusparseDnMatDescr_t matB , cusparseSparseToDenseAlg_t alg , size_t * bufferSize ) cusparseStatus_t cusparseSparseToDense ( cusparseHandle_t handle , cusparseConstSpMatDescr_t matA ,   non-const descriptor supported cusparseDnMatDescr_t matB , cusparseSparseToDenseAlg_t alg , void * buffer ) The function converts the sparse matrix matA in CSR, CSC, or COO format into its dense representation matB .
The function cusparseSparseToDense_bufferSize() returns the size of the workspace needed by cusparseSparseToDense() .
The function cusparseDenseToSparse_bufferSize() returns the size of the workspace needed by cusparseDenseToSparse_analysis() .
The function cusparseDenseToSparse_analysis() updates the number of non-zero elements in the sparse matrix descriptor matB .
The user is responsible to allocate the memory required by the sparse matrix: Row/Column indices and value arrays for CSC and CSR respectively Row, column, value arrays for COO Column ( ellColInd ), value ( ellValue ) arrays for Blocked-ELL Finally, we call cusparseDenseToSparse_convert() for filling the arrays allocated in the previous step.
Please visit cuSPARSE Library Samples - cusparseDenseToSparse (CSR) and cuSPARSE Library Samples - cusparseDenseToSparse (Blocked-ELL) for code examples. 7. cuSPARSE Fortran Bindings  The cuSPARSE library is implemented using the C-based CUDA toolchain, and it thus provides a C-style API that makes interfacing to applications written in C or C++ trivial.
There are also many applications implemented in Fortran that would benefit from using cuSPARSE, and therefore a cuSPARSE Fortran interface has been developed.
Unfortunately, Fortran-to-C calling conventions are not standardized and differ by platform and toolchain.
In particular, differences may exist in the following areas: Symbol names (capitalization, name decoration) Argument passing (by value or reference) Passing of pointer arguments (size of the pointer) To provide maximum flexibility in addressing those differences, the cuSPARSE Fortran interface is provided in the form of wrapper functions, which are written in C and are located in the file cusparse_fortran.c .
This file also contains a few additional wrapper functions (for cudaMalloc() , cudaMemset , and so on) that can be used to allocate memory on the GPU.
The cuSPARSE Fortran wrapper code is provided as an example only and needs to be compiled into an application for it to call the cuSPARSE API functions.
Providing this source code allows users to make any changes necessary for a particular platform and toolchain.
The cuSPARSE Fortran wrapper code has been used to demonstrate interoperability with the compilers g95 0.91 (on 32-bit and 64-bit Linux) and g95 0.92 (on 32-bit and 64-bit Mac OS X).
In order to use other compilers, users have to make any changes to the wrapper code that may be required.
The direct wrappers, intended for production code, substitute device pointers for vector and matrix arguments in all cuSPARSE functions.
To use these interfaces, existing applications need to be modified slightly to allocate and deallocate data structures in GPU memory space (using CUDA_MALLOC() and CUDA_FREE() ) and to copy data between GPU and CPU memory spaces (using the CUDA_MEMCPY() routines).
The sample wrappers provided in cusparse_fortran.c map device pointers to the OS-dependent type size_t , which is 32 bits wide on 32-bit platforms and 64 bits wide on a 64-bit platforms.
One approach to dealing with index arithmetic on device pointers in Fortran code is to use C-style macros and to use the C preprocessor to expand them.
On Linux and Mac OS X, preprocessing can be done by using the option '-cpp' with g95 or gfortran.
The function GET_SHIFTED_ADDRESS() , provided with the cuSPARSE Fortran wrappers, can also be used, as shown in example B.
This example should be compiled with ARCH_64 defined as 1 on a 64-bit OS system and as undefined on a 32-bit OS system.
$ (cudaStat6 /= 0)) then write(*,*) "Device malloc failed" write(*,*) "cudaStat1=",cudaStat1 write(*,*) "cudaStat2=",cudaStat2 write(*,*) "cudaStat3=",cudaStat3 write(*,*) "cudaStat4=",cudaStat4 write(*,*) "cudaStat5=",cudaStat5 write(*,*) "cudaStat6=",cudaStat6 stop 2 endif cudaStat1 = cuda_memcpy_fort2c_int(cooRowIndex,cooRowIndexHostPtr, $ nnz*4,1) cudaStat2 = cuda_memcpy_fort2c_int(cooColIndex,cooColIndexHostPtr, $ nnz*4,1) cudaStat3 = cuda_memcpy_fort2c_real(cooVal, cooValHostPtr, $ nnz*8,1) cudaStat4 = cuda_memcpy_fort2c_real(y, yHostPtr, $ 2*n*8,1) cudaStat5 = cuda_memcpy_fort2c_int(xInd, xIndHostPtr, $ nnz_vector*4,1) cudaStat6 = cuda_memcpy_fort2c_real(xVal, xValHostPtr, $ nnz_vector*8,1) if ((cudaStat1 /= 0) .OR.
epsilon)) then write(*,*) "fortran example test FAILED" else write(*,*) "fortran example test PASSED" endif c deallocate GPU memory and exit call cuda_free(cooRowIndex) call cuda_free(cooColIndex) call cuda_free(cooVal) call cuda_free(xInd) call cuda_free(xVal) call cuda_free(y) call cuda_free(z) call cuda_free(csrRowPtr) call cusparse_destroy_mat_descr(descrA) call cusparse_destroy(handle) stop 0 end 8.
Acknowledgements  NVIDIA would like to thank the following individuals and institutions for their contributions: The cusparsegtsv implementation is derived from a version developed by Li-Wen Chang from the University of Illinois.
The cusparsegtsvInterleavedBatch adopts cuThomasBatch developed by Pedro Valero-Lara and Ivan Martínez-Pérez from Barcelona Supercomputing Center and BSC/UPC NVIDIA GPU Center of Excellence.
This product includes {fmt} - A modern formatting library https: fmt.dev Copyright (c) 2012 - present, Victor Zverovich. 9. Bibliography  [1] N.
Garland, “Implementing Sparse Matrix-Vector Multiplication on Throughput-Oriented Processors” , Supercomputing, 2009.
Young, “ITPACK 2.0 User’s Guide”, Technical Report CNA-150, Center for Numerical Analysis, University of Texas, 1979.
Naumov, “Incomplete-LU and Cholesky Preconditioned Iterative Methods Using cuSPARSE and cuBLAS” , Technical Report and White Paper, 2011.
[4] Pedro Valero-Lara, Ivan Martínez-Pérez, Raül Sirvent, Xavier Martorell, and Antonio J.
In Parallel Processing and Applied Mathematics - 12th International Conference (PPAM), 2017. 10. Notices  10.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 10.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 10.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit v12.5.1 cuRAND Introduction 1.
Acknowledgements Search Results cuRAND ( PDF ) - v12.5.1 ( older ) - Last updated July 1, 2024 - Send Feedback cuRAND The API reference guide for cuRAND, the CUDA random number generation library.
GDS enables a direct data path for direct memory access (DMA) transfers between GPU memory and storage, which avoids a bounce buffer through the CPU.
This direct path increases system bandwidth and decreases the latency and utilization load on the CPU.
This document provides information about the cuFile APIs that are used in applications and frameworks to leverage GDS technology and describes the intent, context, and operation of those APIs which are part of the GDS technology.
Note: The APIs and descriptions are subject to change without notice. 2. Usage This section describes the operation of the cuFile APIs.
Because the functionality is part of the CUDA Driver C API, the APIs use the cuFile prefix and camel case motif of the CUDA Driver.
The APIs with GPU buffers should be called in a valid CUDA context and stream if applicable.
Note: Starting from CUDA toolkit 12.2 (GDS version 1.7.x) release cuFile APIs support memory allocated on GPU device as well as host memory.
peer to peer transfer using GPUDirect™ is supported to and from device memory on supported file system and hardware configurations.
The APIs will refer to this memory address as buffer pointer unless the API specifically applies to a particular type of memory. 2.1. Dynamic Interactions The following describes the dynamic interactions between the cuFile APIs.
If they are not called proactively, their actions will occur reactively: If cuFile{DriverOpen, HandleRegister, BufRegister} is called on a driver, file, or buffer, respectively that has been opened or registered by a previous cuFile * API call, this will result in an error.
Calling cuFile{BufDeregister, HandleDeregister, DriverClose} on a buffer, file, or driver, respectively that has never been opened or registered by a previous cuFile * API call results in an error.
For these errors, the output parameters of the APIs are left in an undefined state, and there are no other side effects.
If it is not used, driver initialization happens implicitly at the first use of the cuFile{HandleRegister, Read, Write, BufRegister} APIs.
(Mandatory) cuFileHandleRegister turns an OS-specific file descriptor into a CUfileHandle_t and performs checking on the GDS supportability based on the mount point and the way that the file was opened.
If this API is not called, an internal registered memory is used if required on the first time the buffer is used, for example, in cuFile{Read, Write} .
cuFile{BufDeregister, HandleDeregister} explicitly frees a buffer and file resources, respectively.
If this API is not called, the buffer and resources are implicitly freed when the driver is closed using cuFileDriverClose .
If this API is not called, the driver resources are implicitly freed when dlclose() is performed on the library handle or when the process is terminated. 2.2. Driver, File, and Buffer Management This section describes the overall workflow to manage the driver, the file, and buffer management: Call cuFileDriverOpen() to initialize the state of the critical performance path.
Allocate GPU memory with cudaMalloc, cudaMallocManaged , cuMem* APIs or host memory using cudaMallocHost , malloc or mmap .
To register the buffer, call cuFileBufRegister to initialize the buffer state of the critical performance path.
Call cuFileHandleRegister to wrap an existing file descriptor in an OS-agnostic CUfileHandle_t .
This step evaluates the suitability of the file state and the file mount for GDS and initializes the file state of the critical performance path.
Call IO APIs such as cuFileRead / cuFileWrite on an existing cuFile handle and existing buffer.
If the cuFileBufRegister has not been previously called on the buffer pointer, cuFileRead/cuFileWrite will use internal registered buffers when required.
Note: Not using the cuFileDeregister and cuFileDriverClose APIs (steps 5, 6, and 7) might unnecessarily consume resources, as shown by tools such as valgrind.
The best practice is to always call these APIs in the application cleanup paths. 2.3. cuFile Compatibility Mode Use Cases cuFile APIs can be used in different scenarios: Developers building GPUDirect Storage applications with cuFile APIs, but don’t have the supported hardware configurations.
Developers building applications running on GPU cards that have CUDA compute capability > 6, but don’t have BAR space exposed.
Behavior The cuFile library provides a mechanism for cuFile reads and writes to use compatibility mode using POSIX pread , pwrite , and aio_submit APIS respectively to host memory and copying to GPU memory when applicable.
The behavior of compatibility mode with cuFile APIs is determined by the following configuration parameters.
Configuration Option (default) cuFile IO Behavior “allow_compat_mode": true If true , falls back to using compatibility mode when the library detects that the buffer file descriptor opened cannot use GPUDirect Storage.
“force_compat_mode": false If true , this option can be used to force all IO to use compatibility mode.
Alternatively the admin can unload the nvidia_fs.ko or not expose the character devices in the docker container environment.
“gds_rdma_write_support": true If false , forces compatibility mode to be used for writes even when the underlying file system is capable of performing GPUDirect Storage writes.
Note: If the option is “false”, this option will override and disable any filesystem-specific option to enable RDMA writes.
“posix_unaligned_writes” : false If true , forces compatibility mode to be used for writes where the file offset and/or IO size is not aligned to Page Boundary (4KB).
“lustre:posix_gds_min_kb” : 0 For a lustre filesystem, if greater than 0 , compatibility mode is used for IO sizes between [1 - posix_gds_min_kb ] specified in kB.
“weka:rdma_write_support” : false If this option is false , all writes to WekaFS will use compatibility mode.
Note: If the option is set to “false” , cuFile library will use the posix path even if the allow_compat_mode option is true or false .
“gpfs:gds_write_support” : false If this option is false, all writes to IBM Spectrum Scale will use compatibility mode.
Note: If the option is set to “false” , cuFile library will use the posix path even if the allow_compat_mode option is true or false.
“rdma_dynamic_routing": false, “rdma_dynamic_routing_order": [ " “SYS_MEM” ] If rdma_dynamic_routing is set to true and rdma_dynamic_routing_order is set to [“SYS_MEM”] , then all IO for DFS will use compatibility mode.
In addition to the above configuration options, compatibility mode will be used as a fallback option for following use cases.
For wekaFS or IBM Spectrum Scale mounts: If there are no rdma_dev_addr_list specified, or failure to register MR with ib device.
For WekaFS and IBM Spectrum Scale: If the kernel returns -ENOTSUP for GPUDirect Storage read/write.
cuFile Stream and cuFile Batch APIs on IBM Spectrum Scale or WekaFS All Async and batch operations will internally use compatibility mode IO.
Limitations Compatible mode does not work in cases where the GPUs have CUDA compute capability less than 6.
It has not been tested to work on all other filesystems. 3. cuFile API Specification This section provides information about the cuFile APIs that are used from the CPU to enable applications and frameworks.
Introduction v12.5 | PDF | Archive cuFFT API Reference The API reference guide for cuFFT, the CUDA Fast Fourier Transform library.
Introduction  This document describes cuFFT, the NVIDIA® CUDA® Fast Fourier Transform (FFT) product.
The cuFFTW library is provided as a porting tool to enable users of FFTW to start using NVIDIA GPUs with a minimum amount of effort.
The FFT is a divide-and-conquer algorithm for efficiently computing discrete Fourier transforms of complex or real-valued data sets.
It is one of the most important and widely used numerical algorithms in computational physics and general signal processing.
The cuFFT library provides a simple interface for computing FFTs on an NVIDIA GPU, which allows users to quickly leverage the floating-point power and parallelism of the GPU in a highly optimized and tested FFT library.
The cuFFT product supports a wide range of FFT inputs and options efficiently on NVIDIA GPUs.
This version of the cuFFT library supports the following features: Algorithms highly optimized for input sizes that can be written in the form \(2^{a} \times 3^{b} \times 5^{c} \times 7^{d}\) .
In general the smaller the prime factor, the better the performance, i.e., powers of two are fastest.
An \(O\left( n\log n  ight)\) algorithm for every input data size Half-precision (16-bit floating point), single-precision (32-bit floating point) and double-precision (64-bit floating point).
Real valued input or output require less computations and data than complex values and often have faster time to solution.
Types supported are: C2C - Complex input to complex output R2C - Real input to complex output C2R - Symmetric complex input to real output 1D, 2D and 3D transforms Execution of multiple 1D, 2D and 3D transforms simultaneously.
In-place and out-of-place transforms Arbitrary intra- and inter-dimension element strides (strided layout) FFTW compatible data layout Execution of transforms across multiple GPUs Streamed execution, enabling asynchronous computation and data movement The cuFFTW library provides the FFTW3 API to facilitate porting of existing FFTW applications.
See Deprecated Functionality . 2. Using the cuFFT API  This chapter provides a general overview of the cuFFT library API.
Users are encouraged to read this chapter before continuing with more detailed descriptions.
The Discrete Fourier transform (DFT) maps a complex-valued vector \(x_{k}\) ( time domain ) into its frequency domain representation given by: \(X_{k} = \sum\limits_{n = 0}^{N - 1}x_{n}e^{-2\pi i\frac{kn}{N}}\) where \(X_{k}\) is a complex-valued vector of the same size.
If the sign on the exponent of e is changed to be positive, the transform is an inverse transform.
The cuFFT API is modeled after FFTW , which is one of the most popular and efficient CPU-based FFT libraries.
cuFFT provides a simple configuration mechanism called a plan that uses internal building blocks to optimize the transform for the given configuration and the particular GPU hardware selected.
Then, when the execution function is called, the actual transform takes place following the plan of execution.
The advantage of this approach is that once the user creates a plan, the library retains whatever state is needed to execute the plan multiple times without recalculation of the configuration.
This model works well for cuFFT because different kinds of FFTs require different thread configurations and GPU resources, and the plan interface provides a simple way of reusing configurations.
Computing a number BATCH of one-dimensional DFTs of size NX using cuFFT will typically look like this: #define NX 256 #define BATCH 10 #define RANK 1 ...
cudaMalloc (( void ** ) & data , sizeof ( cufftComplex ) * NX * BATCH ); cufftPlanMany ( & plan , RANK , NX , & iembed , istride , idist , & oembed , ostride , odist , CUFFT_C2C , BATCH ); ...
They consist of compiled programs ready for users to incorporate into applications with the compiler and linker.
By selecting Download CUDA Production Release users are all able to install the package containing the CUDA Toolkit, SDK code samples and development drivers.
The Linux release for simplecuFFT assumes that the root install directory is /usr/local/cuda and that the locations of the products are contained there as follows.
Product Location and name Include file nvcc compiler /bin/nvcc cuFFT library {lib, lib64}/libcufft.so inc/cufft.h cuFFT library with Xt functionality {lib, lib64}/libcufft.so inc/cufftXt.h cuFFTW library {lib, lib64}/libcufftw.so inc/cufftw.h The most common case is for developers to modify an existing CUDA routine (for example, filename.cu ) to call cuFFT routines.
In this case the include file cufft.h or cufftXt.h should be inserted into filename.cu file and the library included in the link line.
A single compile and link line might appear as /usr/local/cuda/bin/nvcc [options] filename.cu … -I/usr/local/cuda/inc -L/usr/local/cuda/lib -lcufft Of course there will typically be many compile lines and the compiler g++ may be used for linking so long as the library path is set correctly.
Users of the FFTW interface (see FFTW Interface to cuFFT ) should include cufftw.h and link with both cuFFT and cuFFTW libraries.
This means any memory allocated by cudaMalloc , cudaMallocHost and cudaMallocManaged or registered with cudaHostRegister can be used as input, output or plan work area with cuFFT and cuFFTW functions.
For the best performance input data, output data and plan work area should reside in device memory.
cuFFTW library also supports input data and output data that is not GPU visible. 2.2. Fourier Transform Setup  The first step in using the cuFFT Library is to create a plan using one of the following: cufftPlan1D() / cufftPlan2D() / cufftPlan3D() - Create a simple plan for a 1D/2D/3D transform respectively.
cufftXtMakePlanMany() - Creates a plan supporting batched input and strided data layouts for any supported precision.
Among the plan creation functions, cufftPlanMany() allows use of more complicated data layouts and batched executions.
Execution of a transform of a particular size and type may take several stages of processing.
When a plan for the transform is generated, cuFFT derives the internal steps that need to be taken.
In addition, all the intermediate buffer allocations (on CPU/GPU memory) take place during planning.
In the worst case, the cuFFT Library allocates space for 8*batch*n[0]*..*n[rank-1] cufftComplex or cufftDoubleComplex elements (where batch denotes the number of transforms that will be executed in parallel, rank is the number of dimensions of the input data (see Multidimensional Transforms ) and n[] is the array of transform dimensions) for single and double-precision transforms respectively.
In some specific cases, the temporary space allocations can be as low as 1*batch*n[0]*..*n[rank-1] cufftComplex or cufftDoubleComplex elements.
This temporary space is allocated separately for each individual plan when it is created (i.e., temporary space is not shared between the plans).
The next step in using the library is to call an execution function such as cufftExecC2C() (see Parameter cufftType ) which will perform the transform with the specifications defined at planning.
One can create a cuFFT plan and perform multiple transforms on different data sets by providing different input and output pointers.
Once the plan is no longer needed, the cufftDestroy() function should be called to release the resources allocated for the plan. 2.2.1. Free Memory Requirement  The first program call to any cuFFT function causes the initialization of the cuFFT kernels.
by creating a plan) and then allocating memory. 2.2.2. Plan Initialization Time  During plan initialization, cuFFT conducts a series of steps, including heuristics to determine which kernels to be used as well as kernel module loads.
Starting from CUDA 12.0, cuFFT delivers a larger portion of kernels using the CUDA Parallel Thread eXecution assembly form (PTX code), instead of the binary form (cubin object).
The PTX code of cuFFT kernels are loaded and compiled further to the binary code by the CUDA device driver at runtime when a cuFFT plan is initialized.
JIT compilation slightly increases cuFFT plan initialization time, depending on the transform size and the speed of the host CPU (see Module load driver API ) .
But the JIT overhead occurs only when a binary code is generated for the first time during plan initialization using one of the plan creation functions .
The device driver automatically caches a copy of the generated binary code to avoid repeating the compilation in subsequent invocations.
If necessary, CUDA_CACHE_PATH or CUDA_CACHE_MAXSIZE can be customized to set the cache folder and max size (see detail in CUDA Environmental Variables ), but the default settings are fine in general. 2.3. Fourier Transform Types  Apart from the general complex-to-complex (C2C) transform, cuFFT implements efficiently two other types: real-to-complex (R2C) and complex-to-real (C2R).
It can be easily shown that in this case the output satisfies Hermitian symmetry ( \(X_{k} = X_{N - k}^{\ast}\) , where the star denotes complex conjugation).
The converse is also true: for complex-Hermitian input the inverse transform will be purely real-valued.
cuFFT takes advantage of this redundancy and works only on the first half of the Hermitian vector.
Transform execution functions for single and double-precision are defined separately as: cufftExecC2C() / cufftExecZ2Z() - complex-to-complex transforms for single/double precision.
cufftExecR2C() / cufftExecD2Z() - real-to-complex forward transform for single/double precision.
cufftExecC2R() / cufftExecZ2D() - complex-to-real inverse transform for single/double precision.
Each of those functions demands different input data layout (see Data Layout for details).
For one-dimensional signals, this requires the 0th element (and the \(\frac{N}{2}\) th input if N is even) to be real-valued, i.e.
For d-dimension signals, this means \(x_{(n_{1},n_{2},\ldots,n_{d})} = x_{(N_{1} - n_{1},N_{2} - n_{2},\ldots,N_{d} - n_{d})}^{\ast}\) .
Functions cufftXtExec() and cufftXtExecDescriptor() can perform transforms on any of the supported types. 2.3.1. Half-precision cuFFT Transforms  Half-precision transforms have the following limitations: Minimum GPU architecture is SM_53 Sizes are restricted to powers of two only Strides on the real part of real-to-complex and complex-to-real transforms are not supported More than one GPU is not supported Transforms spanning more than 4 billion elements are not supported Please refer to cufftXtMakePlanMany function for plan creation details.
The CUDA Toolkit provides the cuda_fp16.h header with types and intrinsic functions for handling half-precision arithmetic. 2.3.2. Bfloat16-precision cuFFT Transforms  cuFFT supports bfloat16 precision using the nv_bfloat16 data type.
Please note that cuFFT utilizes a combination of single- and bfloat16-precision arithmetic operations when computing the FFT in bfloat16 precision.
Bfloat16-precision transforms have similar limitations to half-precision transforms: Minimum GPU architecture is SM_80 Sizes are restricted to powers of two only Strides on the real part of real-to-complex and complex-to-real transforms are not supported More than one GPU is not supported Transforms spanning more than 4 billion elements are not supported Please refer to cufftXtMakePlanMany function for plan creation details.
The CUDA Toolkit provides the cuda_bf16.h header with types and intrinsic functions for handling bfloat16-precision arithmetic. 2.4. Data Layout  In the cuFFT Library, data layout depends strictly on the configuration and the transform type.
In the case of general complex-to-complex transform both the input and output data shall be a cufftComplex / cufftDoubleComplex array in single- and double-precision modes respectively.
In C2R mode an input array \((x_{1},x_{2},\ldots,x_{\lfloor\frac{N}{2} floor + 1})\) of only non-redundant complex elements is required.
The output array \((X_{1},X_{2},\ldots,X_{N})\) consists of cufftReal / cufftDouble elements in this mode.
Finally, R2C demands an input array \((X_{1},X_{2},\ldots,X_{N})\) of real values and returns an array \((x_{1},x_{2},\ldots,x_{\lfloor\frac{N}{2} floor + 1})\) of non-redundant complex elements.
In real-to-complex and complex-to-real transforms the size of input data and the size of output data differ.
Therefore input data for real-to-complex and output data for complex-to-real must be padded.
Expected sizes of input/output data for 1-d transforms are summarized in the table below: FFT type input data size output data size C2C \(x\) cufftComplex \(x\) cufftComplex C2R \(\left\lfloor \frac{x}{2}  ight floor + 1\) cufftComplex \(x\) cufftReal R2C* \(x\) cufftReal \(\left\lfloor \frac{x}{2}  ight floor + 1\) cufftComplex The real-to-complex transform is implicitly a forward transform.
For an in-place real-to-complex transform where FFTW compatible output is desired, the input size must be padded to \(\left( {\lfloor\frac{N}{2} floor + 1}  ight)\) complex elements.
For out-of-place transforms, input and output sizes match the logical transform size \(N\) and the non-redundant size \(\lfloor\frac{N}{2} floor + 1\) , respectively.
For in-place complex-to-real FFTs where FFTW compatible output is selected (default padding mode), the input size is assumed to be \(\lfloor\frac{N}{2} floor + 1\) cufftComplex elements.
Note that in-place complex-to-real FFTs may overwrite arbitrary imaginary input point values when non-unit input and output strides are chosen.
Similar to the one-dimensional case, the frequency domain representation of real-valued input data satisfies Hermitian symmetry, defined as: \(x_{(n_{1},n_{2},\ldots,n_{d})} = x_{(N_{1} - n_{1},N_{2} - n_{2},\ldots,N_{d} - n_{d})}^{\ast}\) .
C2R and R2C algorithms take advantage of this fact by operating only on half of the elements of signal array, namely on: \(x_{\mathbf{n}}\) for \(\mathbf{n} \in \{ 1,\ldots,N_{1}\} \times \ldots \times \{ 1,\ldots,N_{d - 1}\} \times \{ 1,\ldots,\lfloor\frac{N_{d}}{2} floor + 1\}\) .
The general rules of data alignment described in Data Layout apply to higher-dimensional transforms.
Advanced Data Layout  The advanced data layout feature allows transforming only a subset of an input array, or outputting to only a portion of a larger data structure.
It can be set by calling function: cufftResult cufftPlanMany ( cufftHandle * plan , int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch ); Passing inembed or onembed set to NULL is a special case and is equivalent to passing n for each.
This is same as the basic data layout and other advanced parameters such as istride are ignored.
If the advanced parameters are to be used, then all of the advanced interface parameters must be specified correctly.
Advanced parameters are defined in units of the relevant data type ( cufftReal , cufftDoubleReal , cufftComplex , or cufftDoubleComplex ).
Advanced layout can be perceived as an additional layer of abstraction above the access to input/output data arrays.
An element of coordinates [z][y][x] in signal number b in the batch will be associated with the following addresses in the memory: 1D input[ b * idist + x * istride ] output[ b * odist + x * ostride ] 2D input[ b * idist` + (x * inembed[1] + y) * istride ] output[ b * odist + (x * onembed[1] + y) * ostride ] 3D input[ b * idist + ((x * inembed[1] + y) * inembed[2] + z) * istride ] output[ b * odist + ((x * onembed[1] + y) * onembed[2] + z) * ostride ] The istride and ostride parameters denote the distance between two successive input and output elements in the least significant (that is, the innermost) dimension respectively.
In a single 1D transform, if every input element is to be used in the transform, istride should be set to \(1\) ; if every other input element is to be used in the transform, then istride should be set to \(2\) .
Similarly, in a single 1D transform, if it is desired to output final elements one after another compactly, ostride should be set to \(1\) ; if spacing is desired between the least significant dimension output data, ostride should be set to the distance between the elements.
The inembed and onembed parameters define the number of elements in each dimension in the input array and the output array respectively.
The inembed[rank-1] contains the number of elements in the least significant (innermost) dimension of the input data excluding the istride elements; the number of total elements in the least significant dimension of the input array is then istride*inembed[rank-1] .
The inembed[0] or onembed[0] corresponds to the most significant (that is, the outermost) dimension and is effectively ignored since the idist or odist parameter provides this information instead.
Note that the size of each dimension of the transform should be less than or equal to the inembed and onembed values for the corresponding dimension, that is n[i] ≤ inembed[i] , n[i] ≤ onembed[i] , where \(i \in \{ 0,\ldots,rank - 1\}\) .
The idist and odist parameters indicate the distance between the first element of two consecutive batches in the input and output data. 2.7. Streamed cuFFT Transforms  Every cuFFT plan may be associated with a CUDA stream.
Once so associated, all launches of the internal stages of that plan take place through the specified stream.
Streaming of cuFFT execution allows for potential overlap between transforms and memory copies.
(See the NVIDIA CUDA Programming Guide for more information on streams.) If no stream is associated with a plan, launches take place in stream(0) , the default CUDA stream.
cuFFT uses private streams internally to sort operations, including event syncrhonization.
cuFFT does not guarantee ordering of internal operations, and the order is only preserved with respect to the streams set by the user.
However, calls to cufftXtMemcpy() are still synchronous across multiple GPUs when using streams.
In previous versions of cuFFT, cufftSetStream() returns an error in the multiple GPU case.
Likewise, calling certain multi-GPU functions such as cufftXtSetCallback() after setting a stream with cufftSetStream() will result in an error (see API functions for more details).
Please note that in order to overlap plans using single plan handle user needs to manage work area buffers.
Work area can be set by cufftSetWorkArea function. 2.8. Multiple GPU cuFFT Transforms  cuFFT supports using up to sixteen GPUs connected to a CPU to perform Fourier Transforms whose calculations are distributed across the GPUs.
An API has been defined to allow users to write new code or modify existing code to use this functionality.
Some existing functions such as the creation of a plan using cufftCreate() also apply in the multiple GPU case.
The memory on the GPUs is managed by helper functions cufftXtMalloc()/cufftXtFree() and cufftXtMemcpy() using the cudaLibXtDesc descriptor.
Performance is a function of the bandwidth between the GPUs, the computational ability of the individual GPUs, and the type and number of FFT to be performed.
The highest performance is obtained using NVLink interconnect ( https: www.nvidia.com/object/nvlink.html ).
The second best option is using PCI Express 3.0 between the GPUs and ensuring that both GPUs are on the same switch.
Note that multiple GPU execution is not guaranteed to solve a given size problem in a shorter time than single GPU execution.
The general steps in defining and executing a transform with this API are: cufftCreate() - create an empty plan, as in the single GPU case cufftXtSetGPUs() - define which GPUs are to be used Optional: cufftEstimate{1d,2d,3d,Many}() - estimate the sizes of the work areas required.
These are the same functions used in the single GPU case although the definition of the argument workSize reflects the number of GPUs used.
Optional: cufftGetSize{1d,2d,3d,Many}() - refined estimate of the sizes of the work areas required.
This is the same function used in the single GPU case although the definition of the argument workSize reflects the number of GPUs used.
cufftXtMalloc() - allocate descriptor and data on the GPUs cufftXtMemcpy() - copy data to the GPUs cufftXtExecDescriptorC2C()/cufftXtExecDescriptorZ2Z() - execute the plan cufftXtMemcpy() - copy data from the GPUs cufftXtFree() - free any memory allocated with cufftXtMalloc() cufftDestroy() - free cuFFT plan resources 2.8.1.
Plan Specification and Work Areas  In the single GPU case a plan is created by a call to cufftCreate() followed by a call to cufftMakePlan*() .
For multiple GPUs, the GPUs to use for execution are identified by a call to cufftXtSetGPUs() and this must occur after the call to cufftCreate() and prior to the call to cufftMakePlan*() .
Also the strides and batches apply to the entire plan across all GPUs associated with the plan.
Once a plan is locked by a call to cufftMakePlan*() , different descriptors may be specified in calls to cufftXtExecDescriptor*() to execute the plan on different data sets, but the new descriptors must use the same GPUs in the same order.
As in the single GPU case, cufftEstimateSize{Many,1d,2d,3d}() and cufftGetSize{Many,1d,2d,3d}() give estimates of the work area sizes required for a multiple GPU plan and in this case workSize points to a size_t array, one entry per GPU.
Similarly the actual work size returned by cufftGetSize() is a size_t array, one entry per GPU in the multiple GPU case. 2.8.2. Helper Functions  Multiple GPU cuFFT execution functions assume a certain data layout in terms of what input data has been copied to which GPUs prior to execution, and what output data resides in which GPUs post execution.
On a single GPU users may call cudaMalloc() and cudaFree() to allocate and free GPU memory.
To provide similar functionality in the multiple GPU case, cuFFT includes cufftXtMalloc() and cufftXtFree() functions.
The function cufftXtMalloc() returns a descriptor which specifies the location of these memories.
To provide similar functionality in the multiple GPU case, cuFFT includes cufftXtMemcpy() which allows users to copy between host and multiple GPU memories or even between the GPU memories.
All single GPU cuFFT FFTs return output the data in natural order, that is the ordering of the result is the same as if a DFT had been performed on the data.
Some Fast Fourier Transforms produce intermediate results where the data is left in a permutation of the natural output.
When cufftXtMemcpy() is used to copy data from GPU memory back to host memory, the results are in natural order regardless of whether the data on the GPUs is in natural order or permuted.
Using CUFFT_COPY_DEVICE_TO_DEVICE allows users to copy data from the permuted data format produced after a single transform to the natural order on GPUs. 2.8.3. Multiple GPU 2D and 3D Transforms on Permuted Input  For single 2D or 3D transforms on multiple GPUs, when cufftXtMemcpy() distributes the data to the GPUs, the array is divided on the X axis.
for two GPUs half of the X dimenson points, for all Y (and Z) values, are copied to each of the GPUs.
When the transform is computed, the data are permuted such that they are divided on the Y axis.
When cuFFT creates a 2D or 3D plan for a single transform on multiple GPUs, it actually creates two plans.
This is done because many algorithms compute a forward FFT, then perform some point-wise operation on the result, and then compute the inverse FFT.
To avoid this, cufftXtMemcpy and cufftXtExecDescriptor() keep track of the data ordering so that the correct operation is used.
The ability of cuFFT to process data in either order makes the following sequence possible.
cufftCreate() - create an empty plan, as in the single GPU case cufftXtSetGPUs() - define which GPUs are to be used cufftMakePlan{1d,2d,3d,Many}() - create the plan.
cufftXtMalloc() - allocate descriptor and data on the GPUs cufftXtMemcpy() - copy data to the GPUs cufftXtExecDescriptorC2C()/cufftXtExecDescriptorZ2Z() - compute the forward FFT userFunction() - modify the data in the frequency domain cufftXtExecDescriptorC2C()/cufftXtExecDescriptorZ2Z() - compute the inverse FFT Note that it was not necessary to copy/permute the data between execute calls cufftXtMemcpy() - copy data to the host cufftXtFree() - free any memory allocated with cufftXtMalloc() cufftDestroy() - free cuFFT plan resources 2.8.4.
Supported Functionality  Starting with cuFFT version 7.0, a subset of single GPU functionality is supported for multiple GPU execution.
Requirements and limitations: All GPUs must have the same CUDA architecture level and support Unified Virtual Address Space.
For an application that uses the CUDA Driver API, running cuFFT on multiple GPUs is only compatible with applications using the primary context on each GPU.
Running cuFFT on more than 8 GPUs (16 GPUs is max) is supported on machines with NVLink only.
While transforms with batch count greater than one do not impose additional constraints, those with a single batch have some restrictions.
Single-batch FFTs support only in-place mode, and have additional constraints depending on the FFT type.
For batch size m on n GPUs : The first m % n GPUs execute \(\left\lfloor \frac{m}{n}  ight floor+\ 1\) transforms.
Batch size output differences: Single GPU cuFFT results are always returned in natural order.
When multiple GPUs are used to perform more than one transform, the results are also returned in natural order.
When multiple GPUs are used to perform a single transform the results are returned in a permutation of the normal results to reduce communication time.
This behavior is summarized in the following table: Number of GPUs Number of transforms Output Order on GPUs One One or multiple transforms Natural order Multiple One Permuted results Multiple Multiple Natural order To produce natural order results in GPU memory for multi-GPU runs in the 1D single transform case, requires calling cufftXtMemcpy() with CUFFT_COPY_DEVICE_TO_DEVICE .
2D and 3D multi-GPU transforms support execution of a transform given permuted order results as input.
It is also possible to use cufftXtMemcpy() with CUFFT_COPY_DEVICE_TO_DEVICE to return 2D or 3D data to natural order.
See the cuFFT Code Examples section for single GPU and multiple GPU examples. 2.9. cuFFT Callback Routines  Callback routines are user-supplied kernel routines that cuFFT will call when loading or storing data.
Note Starting from CUDA 11.4, support for callback functionality using separately compiled device code is deprecated on all GPU architectures.
Callback functionality will continue to be supported for all GPU architectures. 2.9.1. Overview of the cuFFT Callback Routine Feature  cuFFT provides a set of APIs that allow the cuFFT user to provide CUDA functions that re-direct or manipulate the data as it is loaded prior to processing the FFT, or stored once the FFT has been done.
For the load callback, cuFFT passes the callback routine the address of the input data and the offset to the value to be loaded from device memory, and the callback routine returns the value it wishes cuFFT to use instead.
For the store callback, cuFFT passes the callback routine the value it has computed, along with the address of the output data and the offset to the value to be written to device memory, and the callback routine modifies the value and stores the modified result.
In order to provide a callback to cuFFT, a plan is created and configured normally using the extensible plan APIs.
After the call to cufftCreate and cufftMakePlan , the user may associate a load callback routine, or a store callback routine, or both, with the plan, by calling cufftXtSetCallback .
The caller also has the option to specify a device pointer to an opaque structure they wish to associate with the plan.
The caller may use this structure to remember plan dimensions and strides, or have a pointer to auxiliary data, etc.
With some restrictions, the callback routine is allowed to request shared memory for its own use.
If the requested amount of shared memory is available, cufft will pass a pointer to it when it calls the callback routine.
CUFFT allows for 8 types of callback routine, one for each possible combination of: load or store, real or complex, single precision or double.
It is the caller’s responsibility to provide a routine that matches the function prototype for the type of routine specified.
If there is already a callback of the specified type associated with the plan, the set callback function will replace it with the new one.
The general steps in defining and executing a transform with callbacks are: cufftCreate() - create an empty plan, as in the single GPU case cufftMakePlan{1d,2d,3d,Many}() - create the plan.
cufftXtSetCallback() - called for load and/or store callback for this plan cufftExecC2C() etc.
- execute the plan cufftDestroy() - free cuFFT plan resources Callback functions are not supported on transforms with a dimension size that does not factor into primes smaller than 127.
Callback functions on plans whose dimensions’ prime factors are limited to 2, 3, 5, and 7 can safely call __syncthreads() .
Note The callback API is available in the statically linked cuFFT library only, and only on 64 bit LINUX operating systems. 2.9.2. Specifying Load and Store Callback Routines  In order to associate a callback routine with a plan, it is necessary to obtain a device pointer to the callback routine.
As an example, if the user wants to specify a load callback for an R2C transform, they would write the device code for the callback function, and define a global device variable that contains a pointer to the function: __device__ cufftReal myOwnCallback ( void * dataIn , size_t offset , void * callerInfo , void * sharedPtr ) { cufftReal ret ;   use offset, dataIn, and optionally callerInfo to   compute the return value return ret ; } __device__ cufftCallbackLoadR myOwnCallbackPtr = myOwnCallback ; From the host side, the user then has to get the address of the callback routine, which is stored in myOwnCallbackPtr .
This is done with cudaMemcpyFromSymbol , as follows: cufftCallbackLoadR hostCopyOfCallbackPtr ; cudaMemcpyFromSymbol ( & hostCopyOfCallbackPtr , myOwnCallbackPtr , sizeof ( hostCopyOfCallbackPtr )); hostCopyOfCallbackPtr then contains the device address of the callback routine, that should be passed to cufftXtSetCallback .
Note that, for multi-GPU transforms, hostCopyOfCallbackPtr will need to be an array of pointers, and the cudaMemcpyFromSymbol will have to be invoked for each GPU.
Please note that __managed__ variables are not suitable to pass to cufftSetCallback due to restrictions on variable usage (See the NVIDIA CUDA Programming Guide for more information about __managed__ variables). 2.9.3. Callback Routine Function Details  Below are the function prototypes, and typedefs for pointers to the user supplied callback routines that cuFFT calls to load data prior to the transform.
typedef cufftComplex ( * cufftCallbackLoadC )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef cufftDoubleComplex ( * cufftCallbackLoadZ )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef cufftReal ( * cufftCallbackLoadR )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef cufftDoubleReal ( * cufftCallbackLoadD )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); Parameters for all of the load callbacks are defined as below: offset : offset of the input element from the start of output data.
dataIn : device pointer to the start of the input array that was passed in the cufftExecute call.
callerInfo : device pointer to the optional caller specified data passed in the cufftXtSetCallback call.
sharedPointer : pointer to shared memory, valid only if the user has called cufftXtSetCallbackSharedSize() .
Below are the function prototypes, and typedefs for pointers to the user supplied callback routines that cuFFT calls to store data after completion of the transform.
This is because a store callback function is responsible not only for transforming the data as desired, but also for writing the data to the desired location.
This allows the store callback to rearrange the data, for example to shift the zero frequency result to the center of the ouput.
typedef void ( * cufftCallbackStoreC )( void * dataOut , size_t offset , cufftComplex element , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreZ )( void * dataOut , size_t offset , cufftDoubleComplex element , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreR )( void * dataOut , size_t offset , cufftReal element , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreD )( void * dataOut , size_t offset , cufftDoubleReal element , void * callerInfo , void * sharedPointer ); Parameters for all of the store callbacks are defined as below: offset : offset of the output element from the start of output data.
dataOut : device pointer to the start of the output array that was passed in the cufftExecute call.
element : the real or complex result computed by CUFFT for the element specified by the offset argument. 2.9.4. Coding Considerations for the cuFFT Callback Routine Feature  cuFFT supports callbacks on all types of transforms, dimension, batch, stride between elements or number of GPUs.
cuFFT supports a wide range of parameters, and based on those for a given plan, it attempts to optimize performance.
The number of kernels launched, and for each of those, the number of blocks launched and the number of threads per block, will vary depending on how cuFFT decomposes the transform.
For some configurations, cuFFT will load or store (and process) multiple inputs or outputs per thread.
For some configurations, threads may load or store inputs or outputs in any order, and cuFFT does not guarantee that the inputs or outputs handled by a given thread will be contiguous.
cuFFT will call the load callback routine, for each point in the input, once and only once.
Similarly it will call the store callback routine, for each point in the output, once and only once.
the input and output data are in the same memory location) the store callback for a given element cannot overwrite other elements.
It can either overwrite the given element, or write in a completely distinct output buffer.
When more than one kernel are used to implement a transform, the thread and block structure of the first kernel (the one that does the load) is often different from the thread and block structure of the last kernel (the one that does the store).
One common use of callbacks is to reduce the amount of data read or written to memory, either by selective filtering or via type conversions.
When more than one kernel are used to implement a transform, cuFFT alternates using the workspace and the output buffer to write intermediate results.
This means that the output buffer must always be large enough to accommodate the entire transform.
For multi-GPU transforms, the index passed to the callback routine is the element index from the start of data on that GPU , not from the start of the entire input or output data array.
For transforms whose dimensions can be factored into powers of 2, 3, 5, or 7, cuFFT guarantees that it will call the load and store callback routines from points in the kernel that is safe to call __syncthreads function from within callback routine.
Caller is responsible for guaranteeing that the callback routine is at a point where the callback code has converged, to avoid deadlock.
For plans whose dimensions are factored into higher primes, results of a callback routine calling __syncthreads are not defined. 2.9.4.1. No Ordering Guarantees Within a Kernel  Note that there are no guarantees on the relative order of execution of blocks within a grid.
For instance, reordering data (such as an FFT-shift) could rely on the order of execution of the blocks.
Results in this case would be undefined. 2.10. Thread Safety  cuFFT APIs are thread safe as long as different host threads execute FFTs using different plans and the output data are disjoint.
2.11. CUDA Graphs Support  Using CUDA Graphs with cuFFT is supported on single GPU plans.
The stream associated with a cuFFT plan must meet the requirements stated in Creating a Graph Using Stream Capture .
Note Starting from CUDA 11.8 (including CUDA 12.0 onward), CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms.
An upcoming release will update the cuFFT callback implementation, removing this limitation.
cuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11.4. 2.12. Static Library and Callback Support  Starting with release 6.5, the cuFFT libraries are also delivered in a static form as libcufft_static.a and libcufftw_static.a on Linux and Mac.
The static cufft and cufftw libraries depend on thread abstraction layer library libculibos.a .
For example, on linux, to compile a small application using cuFFT against the dynamic library, the following command can be used: nvcc mCufftApp.c -lcufft -o myCufftApp For cufftw on Linux, to compile a small application against the dynamic library, the following command can be used: nvcc mCufftwApp.c -lcufftw -lcufft -o myCufftwApp Whereas to compile against the static cuFFT library, extra steps need to be taken.
To determine if a specific SM is included in the cuFFT library, one may use cuobjdump utility.
For example, if you wish to know if SM_50 is included, the command to run is cuobjdump -arch sm_50 libcufft_static.a .
It is also possible to use the native Host C++ compiler and perform device link as a separate step.
Depending on the Host Operating system, some additional libraries like pthread or dl might be needed on the linking line.
In the case of a system which does not have the CUDA driver installed, this allows the application to gracefully manage this issue and potentially run if a CPU-only path is available.
The callback routines are CUDA device code, and must be separately compiled with NVCC and linked with the cuFFT library.
If you specify an SM when compiling your callback functions, you must specify one of the SM’s cuFFT includes. 2.12.1. Static library without callback support  Starting with cuFFT version 9.2, a new variant of the cuFTT static library, libcufft_static_nocallback.a , was added.
This new version does not contain callback functionality and can be linked using the host compiler only. 2.13. Accuracy and Performance  A DFT can be implemented as a matrix vector multiplication that requires \(O(N^{2})\) operations.
However, the cuFFT Library employs the Cooley-Tukey algorithm to reduce the number of required operations to optimize the performance of particular transform sizes.
The cuFFT Library implements the following building blocks: radix-2, radix-3, radix-5, and radix-7.
Hence the performance of any transform size that can be factored as \(2^{a} \times 3^{b} \times 5^{c} \times 7^{d}\) (where a , b , c , and d are non-negative integers) is optimized in the cuFFT library.
The nvJitLink library is loaded dynamically, and should be present in the system’s dynamic linking path (e.g.
LD_LIBRARY_PATH on Unix systems, or PATH on Windows systems). 2.15.1. 3. cuFFT API Reference  This chapter specifies the behavior of the cuFFT library functions by describing their input/output parameters, data types, and error codes.
The cuFFT library is initialized upon the first invocation of an API function, and cuFFT shuts down automatically when all user-created FFT plans are destroyed. 3.1. Return value cufftResult  All cuFFT Library return values except for CUFFT_SUCCESS indicate that the current API call failed and the user should reconfigure to correct the problem.
} cufftResult ; Users are encouraged to check return values from cuFFT functions for errors as shown in cuFFT Code Examples . 3.2. cuFFT Basic Plans  3.2.1.
cufftPlan1d()  cufftResult cufftPlan1d ( cufftHandle * plan , int nx , cufftType type , int batch ) ;  Creates a 1D FFT plan configuration for a specified signal size and data type.
type[In] – The transform data type (e.g., CUFFT_C2C for single precision complex to complex).
CUFFT_INVALID_SIZE – The nx or batch parameter is not a supported size. 3.2.2. cufftPlan2d()  cufftResult cufftPlan2d ( cufftHandle * plan , int nx , int ny , cufftType type ) ;  Creates a 2D FFT plan configuration according to specified signal sizes and data type.
nx[In] – The transform size in the x dimension This is slowest changing dimension of a transform (strided in memory).
type[In] – The transform data type (e.g., CUFFT_C2R for single precision complex to real).
CUFFT_INVALID_SIZE – Either or both of the nx or ny parameters is not a supported size. 3.2.3. cufftPlan3d()  cufftResult cufftPlan3d ( cufftHandle * plan , int nx , int ny , int nz , cufftType type ) ;  Creates a 3D FFT plan configuration according to specified signal sizes and data type.
This function is the same as cufftPlan2d() except that it takes a third size parameter nz .
type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex).
CUFFT_INVALID_SIZE – One or more of the nx , ny , or nz parameters is not a supported size. 3.2.4. cufftPlanMany()  cufftResult cufftPlanMany ( cufftHandle * plan , int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch ) ;  Creates a FFT plan configuration of dimension rank , with sizes specified in the array n .
The cufftPlanMany() API supports more complicated input and output data layouts via the advanced data layout parameters: inembed , istride , idist , onembed , ostride , and odist .
If inembed and onembed are set to NULL , all other stride information is ignored, and default strides are used.
Please note that behavior of cufftPlanMany function when inembed and onembed is NULL is different than corresponding function in FFTW library fftw_plan_many_dft .
n[In] – Array of size rank , describing the size of each dimension, n[0] being the size of the outermost and n[rank-1] innermost (contiguous) dimension of a transform.
inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory.
istride[In] – Indicates the distance between two successive input elements in the least significant (i.e., innermost) dimension.
idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data.
onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory.
ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i.e., innermost) dimension.
odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data.
CUFFT_INVALID_SIZE – One or more of the parameters is not a supported size. 3.3. cuFFT Extensible Plans  This API separates handle creation from plan generation.
This makes it possible to change plan settings, which may alter the outcome of the plan generation phase, before the plan is actually generated. 3.3.1. cufftCreate()  cufftResult cufftCreate ( cufftHandle * plan )  Creates only an opaque handle, and allocates small data structures on the host.
CUFFT_ALLOC_FAILED – The allocation of resources for the plan failed. 3.3.2. cufftDestroy()  cufftResult cufftDestroy ( cufftHandle plan )  Frees all GPU resources associated with a cuFFT plan and destroys the internal plan data structure.
This function should be called once a plan is no longer needed, to avoid wasting GPU memory.
Return values CUFFT_SUCCESS – cuFFT successfully destroyed the FFT plan. 3.3.3. cufftMakePlan1d()  cufftResult cufftMakePlan1d ( cufftHandle plan , int nx , cufftType type , int batch , size_t * workSize ) ;  Following a call to cufftCreate() makes a 1D FFT plan configuration for a specified signal size and data type.
If cufftXtSetGPUs() was called prior to this call with multiple GPUs, then workSize will contain multiple sizes.
type[In] – The transform data type (e.g., CUFFT_C2C for single precision complex to complex).
CUFFT_SETUP_FAILED` – The cuFFT library failed to initialize. 3.3.4. cufftMakePlan2d()  cufftResult cufftMakePlan2d ( cufftHandle plan , int nx , int ny , cufftType type , size_t * workSize ) ;  Following a call to cufftCreate() makes a 2D FFT plan configuration according to specified signal sizes and data type.
type[In] – The transform data type (e.g., CUFFT_C2R for single precision complex to real).
workSize[In] – Pointer to the size(s), in bytes, of the work areas. 3.3.5. cufftMakePlan3d()  cufftResult cufftMakePlan3d ( cufftHandle plan , int nx , int ny , int nz , cufftType type , size_t * workSize ) ;  Following a call to cufftCreate() makes a 3D FFT plan configuration according to specified signal sizes and data type.
type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex).
*workSize[Out] – Pointer to the size(s) of the work area(s). 3.3.6. cufftMakePlanMany()  cufftResult cufftMakePlanMany ( cufftHandle plan , int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch , size_t * workSize ) ;  Following a call to cufftCreate() makes a FFT plan configuration of dimension rank , with sizes specified in the array n .
rank[In] – Dimensionality of the transform (1, 2, or 3) n[In] – Array of size rank , describing the size of each dimension, n[0] being the size of the outermost and n[rank-1] innermost (contiguous) dimension of a transform.
For multiple GPUs and rank equal to 2 or 3, the sizes must be factorable into primes less than or equal to 127.
inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory, inembed[0] being the storage dimension of the outermost dimension.
istride[In] – Indicates the distance between two successive input elements in the least significant (i.e., innermost) dimension idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory, onembed[0] being the storage dimension of the outermost dimension.
ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i.e., innermost) dimension odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data type[In] – The transform data type (e.g., CUFFT_R2C for single precision real to complex).
For 2 GPUs this must be a complex to complex transform. 3.3.7. cufftMakePlanMany64()  cufftResult cufftMakePlanMany64 ( cufftHandle plan , int rank , long long int * n , long long int * inembed , long long int istride , long long int idist , long long int * onembed , long long int ostride , long long int odist , cufftType type , long long int batch , size_t * workSize ) ;  Following a call to cufftCreate() makes a FFT plan configuration of dimension rank , with sizes specified in the array n .
This API is identical to cufftMakePlanMany except that the arguments specifying sizes and strides are 64 bit integers.
cuFFT planning selects 32 bit kernels whenever possible to avoid any overhead due to 64 bit arithmetic.
For transforms whose size exceeds 4G elements, the dimensions specified in the array n must be factorable into primes that are less than or equal to 127.
For real to complex and complex to real transforms whose size exceeds 4G elements, the fastest changing dimension must be even.
The cufftPlanMany64() API supports more complicated input and output data layouts via the advanced data layout parameters: inembed , istride , idist , onembed , ostride , and odist .
For multiple GPUs and rank equal to 2 or 3, the sizes must be factorable into primes less than or equal to 127.
inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory. 3.3.8. cufftXtMakePlanMany()  cufftResult cufftXtMakePlanMany ( cufftHandle plan , int rank , long long int * n , long long int * inembed , long long int istride , long long int idist , cudaDataType inputtype , long long int * onembed , long long int ostride , long long int odist , cudaDataType outputtype , long long int batch , size_t * workSize , cudaDataType executiontype ) ;  Following a call to cufftCreate() makes an FFT plan configuration of dimension rank , with sizes specified in the array n .
Type specifiers inputtype , outputtype and executiontype dictate type and precision of transform to be performed.
Parameters inputtype and outputtype need to match transform type complex-to-complex, real-to-complex or complex-to-real.
Example: for a half-precision real-to-complex transform, parameters inputtype , outputtype and executiontype would have values of CUDA_R_16F , CUDA_C_16F and CUDA_C_16F respectively.
Similarly, a bfloat16 complex-to-real transform would use CUDA_C_16BF for inputtype and executiontype , and CUDA_R_16BF for outputtype .
The cufftXtMakePlanMany() API supports more complicated input and output data layouts via the advanced data layout parameters: inembed , istride , idist , onembed , ostride , and odist .
onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory, onembed[0] being the storage dimension of the outermost dimension.
Handle is not valid when multi-GPU restrictions are not met. 3.4. cuFFT Plan Properties  Users can further customize cuFFT plans using plan properties.
These properties can be set, queried and reset on a per-plan basis as needed, using the routines listed in this section.
The current supported properties are listed below: Property Underlying Type Description Behavior NVFFT_PLAN_PROPERTY_INT64_PATIENT_JIT long long int Runtime LTO kernels are enabled when set to not-zero value.
See Link-Time Optimized Kernels Runtime LTO kernles are disabled when set to zero (default) Can be set / reset before planning Cannot be set / reset after planning 3.4.1.
cufftSetPlanPropertyInt64()  cufftResult cufftSetPlanPropertyInt64 ( cufftHandle plan , cufftProperty property , const long long int propertyValueInt64 ) ;  Associates a cuFFT plan with a property identified by the key property .
The value for the property is given by value propertyValueInt64 , which is a signed long long integer.
CUFFT_NOT_SUPPORTED – The property is not supported, or it cannot be set at the time (e.g.
some properties cannot be set after calling a planning routine for the plan, see cuFFT Plan Properties ).
cufftGetPlanPropertyInt64()  cufftResult cufftGetPlanPropertyInt64 ( cufftHandle plan , cufftProperty property , long long int * propertyValueInt64 ) ;  Retrieves the property value identified by the key property associated with the cuFFT plan plan .
The value for the property, which is a signed long long integer, is set in the address space pointed by propertyValueInt64 .
cufftResetPlanProperty()  cufftResult cufftResetPlanProperty ( cufftHandle plan , cufftProperty property ) ;  Resets the value of the property identified by the key property , associated with the cuFFT plan plan , to its default value.
CUFFT_NOT_SUPPORTED – The property is not supported for plan , or cannot be reset at present time (see Behavior column on cuFFT Plan Properties ).
cuFFT Estimated Size of Work Area  During plan execution, cuFFT requires a work area for temporary storage of intermediate results.
The cufftEstimate*() calls return an estimate for the size of the work area required, given the specified parameters, and assuming default plan settings.
Large prime numbers, however, use different algorithms and may need up to the eight times that of a similarly sized power of 2.
These routines return estimated workSize values which may still be smaller than the actual values needed especially for values of n that are not multiples of powers of 2, 3, 5 and 7.
More refined values are given by the cufftGetSize*() routines, but these values may still be conservative. 3.5.1. cufftEstimate1d()  cufftResult cufftEstimate1d ( int nx , cufftType type , int batch , size_t * workSize ) ;  During plan execution, cuFFT requires a work area for temporary storage of intermediate results.
This call returns an estimate for the size of the work area required, given the specified parameters, and assuming default plan settings.
CUFFT_INVALID_SIZE – The nx parameter is not a supported size. 3.5.2. cufftEstimate2d()  cufftResult cufftEstimate2d ( int nx , int ny , cufftType type , size_t * workSize ) ;  During plan execution, cuFFT requires a work area for temporary storage of intermediate results.
ny[In] – The transform size in the y dimension (number of columns). 3.5.3. cufftEstimate3d()  cufftResult cufftEstimate3d ( int nx , int ny , int nz , cufftType type , size_t * workSize ) ;  During plan execution, cuFFT requires a work area for temporary storage of intermediate results.
Parameters nx[In] – The transform size in the x dimension. 3.5.4. cufftEstimateMany()  cufftResult cufftEstimateMany ( int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch , size_t * workSize ) ;  During plan execution, cuFFT requires a work area for temporary storage of intermediate results.
The cufftEstimateMany() API supports more complicated input and output data layouts via the advanced data layout parameters: inembed , istride , idist , onembed , ostride , and odist .
*workSize[Out] – Pointer to the size of the work space Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space. 3.6. cuFFT Refined Estimated Size of Work Area  The cufftGetSize*() routines give a more accurate estimate of the work area size required for a plan than the cufftEstimate*() routines as they take into account any plan settings that may have been made.
As discussed in the section cuFFT Estimated Size of Work Area , the workSize value(s) returned may be conservative especially for values of n that are not multiples of powers of 2, 3, 5 and 7. 3.6.1. cufftGetSize1d()  cufftResult cufftGetSize1d ( cufftHandle plan , int nx , cufftType type , int batch , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimate1d() , given the specified parameters, and taking into account any plan settings that may have been made.
Please consider using cufftGetSizeMany for multiple transforms. 3.6.2. cufftGetSize2d()  cufftResult cufftGetSize2d ( cufftHandle plan , int nx , int ny , cufftType type , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimate2d() , given the specified parameters, and taking into account any plan settings that may have been made.
nx[In] – The transform size in the x dimension (number of rows). 3.6.3. cufftGetSize3d()  cufftResult cufftGetSize3d ( cufftHandle plan , int nx , int ny , int nz , cufftType type , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimate3d() , given the specified parameters, and taking into account any plan settings that may have been made.
3.6.4. cufftGetSizeMany()  cufftResult cufftGetSizeMany ( cufftHandle plan , int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimateSizeMany() , given the specified parameters, and taking into account any plan settings that may have been made.
*workSize[Out] – Pointer to the size of the work area. 3.6.5. cufftGetSizeMany64()  cufftResult cufftGetSizeMany64 ( cufftHandle plan , int rank , long long int * n , long long int * inembed , long long int istride , long long int idist , long long int * onembed , long long int ostride , long long int odist , cufftType type , long long int batch , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimateSizeMany() , given the specified parameters, and taking into account any plan settings that may have been made.
For transforms whose total size exceeds 4G elements, the dimensions specified in the array n must be factorable into primes that are less than or equal to 127.
For real to complex and complex to real transforms whose total size exceeds 4G elements, the fastest changing dimension must be even. 3.6.6. cufftXtGetSizeMany()  cufftResult cufftXtGetSizeMany ( cufftHandle plan , int rank , long long int * n , long long int * inembed , long long int istride , long long int idist , cudaDataType inputtype , long long int * onembed , long long int ostride , long long int odist , cudaDataType outputtype , long long int batch , size_t * workSize , cudaDataType executiontype ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimateSizeMany() , given the specified parameters that match signature of cufftXtMakePlanMany function, and taking into account any plan settings that may have been made.
For more information about valid combinations of inputtype , outputtype and executiontype parameters please refer to documentation of cufftXtMakePlanMany function.
executiontype[In] ( cudaDataType ) – Type of data to be used for computations. 3.7. cufftGetSize()  cufftResult cufftGetSize ( cufftHandle plan , size_t * workSize ) ;  Once plan generation has been done, either with the original API or the extensible API, this call returns the actual size of the work area required to support the plan.
Callers who choose to manage work area allocation within their application must use this call after plan generation, and after any cufftSet*() calls subsequent to plan generation, if those calls might alter the required work space size. 3.8. cuFFT Caller Allocated Work Area Support  3.8.1.
cufftSetAutoAllocation()  cufftResult cufftSetAutoAllocation ( cufftHandle plan , int autoAllocate ) ;  cufftSetAutoAllocation() indicates that the caller intends to allocate and manage work areas for plans that have been generated.
If cufftSetAutoAllocation() has been called with autoAllocate set to 0 (“false”) prior to one of the cufftMakePlan*() calls, cuFFT does not allocate the work area.
autoAllocate[In] – Indicates whether to allocate work area. 3.8.2. cufftSetWorkArea()  cufftResult cufftSetWorkArea ( cufftHandle plan , void * workArea ) ;  cufftSetWorkArea() overrides the work area pointer associated with a plan.
The cufftExecute*() calls assume that the work area pointer is valid and that it points to a contiguous region in device memory that does not overlap with any other work area.
For multiple GPUs, multiple work area pointers must be given. 3.8.3. cufftXtSetWorkAreaPolicy()  cufftResult cufftXtSetWorkAreaPolicy ( cufftHandle plan , cufftXtWorkAreaPolicy policy , size_t * workSize ) ;  cufftXtSetWorkAreaPolicy() indicates that the caller intends to change work area size for a given plan handle.
cuFFT’s default behavior is to allocate the work area at plan generation time with a default size that depends on the plan type and other parameters.
If cufftXtSetWorkAreaPolicy() has been called with the policy parameter set to CUFFT_WORKAREA_MINIMAL , cuFFT will attempt to re-plan the handle to use zero bytes of work area memory.
If the cufftXtSetWorkAreaPolicy() call is successful the auto-allocated work area memory is released.
Currently the policies CUFFT_WORKAREA_PERFORMANCE , CUFFT_WORKAREA_USER and the workSize parameter are not supported and reserved for use in future cuFFT releases.
CUFFT_INVALID_SIZE – FFT size does not allow use of the selected policy. 3.9. cuFFT Execution  3.9.1.
cufftExecC2C() and cufftExecZ2Z()  cufftResult cufftExecC2C ( cufftHandle plan , cufftComplex * idata , cufftComplex * odata , int direction ) ;  cufftResult cufftExecZ2Z ( cufftHandle plan , cufftDoubleComplex * idata , cufftDoubleComplex * odata , int direction ) ;  cufftExecC2C() ( cufftExecZ2Z() ) executes a single-precision (double-precision) complex-to-complex transform plan in the transform direction as specified by direction parameter.
CUFFT_INVALID_VALUE – At least one of the parameters idata , odata , and direction is not valid.
CUFFT_EXEC_FAILED – cuFFT failed to execute the transform on the GPU. 3.9.2. cufftExecR2C() and cufftExecD2Z()  cufftResult cufftExecR2C ( cufftHandle plan , cufftReal * idata , cufftComplex * odata ) ;  cufftResult cufftExecD2Z ( cufftHandle plan , cufftDoubleReal * idata , cufftDoubleComplex * odata ) ;  cufftExecR2C() ( cufftExecD2Z() ) executes a single-precision (double-precision) real-to-complex, implicitly forward, cuFFT transform plan.
Pointers to idata and odata are both required to be aligned to cufftComplex data type in single-precision transforms and cufftDoubleComplex data type in double-precision transforms.
Note the data layout differences between in-place and out-of-place transforms as described in Parameter cufftType .
CUFFT_INVALID_VALUE – At least one of the parameters idata and odata is not valid. 3.9.3. cufftExecC2R() and cufftExecZ2D()  cufftResult cufftExecC2R ( cufftHandle plan , cufftComplex * idata , cufftReal * odata ) ;  cufftResult cufftExecZ2D ( cufftHandle plan , cufftDoubleComplex * idata , cufftDoubleReal * odata ) ;  cufftExecC2R() ( cufftExecZ2D() ) executes a single-precision (double-precision) complex-to-real, implicitly inverse, cuFFT transform plan.
and pointers are both required to be aligned to cufftComplex data type in single-precision transforms and cufftDoubleComplex type in double-precision transforms.
Return values CUFFT_SUCCESS – cuFFT successfully executed the FFT plan. 3.9.4. cufftXtExec()  cufftResult cufftXtExec ( cufftHandle plan , void * input , void * output , int direction ) ;  Function cufftXtExec executes any cuFFT transform regardless of precision and type.
output[Out] – Contains the complex Fourier coefficients. 3.9.5. cufftXtExecDescriptor()  cufftResult cufftXtExecDescriptor ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output , int direction ) ;  Function cufftXtExecDescriptor() executes any cuFFT transform regardless of precision and type.
cuFFT uses the GPU memory pointed to by cudaLibXtDesc *input descriptor as input data and cudaLibXtDesc *output as output data.
CUFFT_INVALID_DEVICE – An invalid GPU index was specified in a descriptor. 3.10. cuFFT and Multiple GPUs  3.10.1.
cufftXtSetGPUs()  cufftResult cufftXtSetGPUs ( cufftHandle plan , int nGPUs , int * whichGPUs ) ;  cufftXtSetGPUs() identifies which GPUs are to be used with the plan.
As in the single GPU case cufftCreate() creates a plan and cufftMakePlan*() does the plan generation.
In cuFFT prior to 10.4.0, this call will return an error if a non-default stream has been associated with the plan.
Note that the call to cufftXtSetGPUs() must occur after the call to cufftCreate() and prior to the call to cufftMakePlan*() .
Parameter whichGPUs of cufftXtSetGPUs() function determines ordering of the GPUs with respect to data decomposition (first data chunk is placed on GPU denoted by first element of whichGPUs ).
CUFFT_INVALID_PLAN – The plan parameter is not a valid handle, or a non-default stream has been associated with the plan in cuFFT prior to 10.4.0 .
CUFFT_INVALID_SIZE – Transform size that plan was created for does not meet minimum size criteria. 3.10.2. cufftXtSetWorkArea()  cufftResult cufftXtSetWorkArea ( cufftHandle plan , void * * workArea ) ;  cufftXtSetWorkArea() overrides the work areas associated with a plan.
The cufftXtExec*() calls assume that the work area is valid and that it points to a contiguous region in each device memory that does not overlap with any other work area.
CUFFT_INVALID_DEVICE – A GPU associated with the plan could not be selected. 3.10.3. cuFFT Multiple GPU Execution  3.10.3.1.
cufftXtExecDescriptorC2C() and cufftXtExecDescriptorZ2Z()  cufftResult cufftXtExecDescriptorC2C ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output , int direction ) ;  cufftResult cufftXtExecDescriptorZ2Z ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output , int direction ) ;  cufftXtExecDescriptorC2C() ( cufftXtExecDescriptorZ2Z() ) executes a single-precision (double-precision) complex-to-complex transform plan in the transform direction as specified by direction parameter.
Since only in-place multiple GPU functionality is supported, this function also stores the result in the cudaLibXtDesc *input arrays.
CUFFT_INVALID_VALUE – At least one of the parameters input and direction is not valid. 3.10.3.2. cufftXtExecDescriptorR2C() and cufftXtExecDescriptorD2Z()  cufftResult cufftXtExecDescriptorR2C ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output ) ;  cufftResult cufftXtExecDescriptorD2Z ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output ) ;  cufftXtExecDescriptorR2C() ( cufftXtExecDescriptorD2Z() ) executes a single-precision (double-precision) real-to-complex transform plan.
input[Out] – Contains the complex Fourier coefficients Return values CUFFT_SUCCESS – cuFFT successfully executed the FFT plan. 3.10.3.3. cufftXtExecDescriptorC2R() and cufftXtExecDescriptorZ2D()  cufftResult cufftXtExecDescriptorC2R ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output ) ;  cufftResult cufftXtExecDescriptorZ2D ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output ) ;  cufftXtExecDescriptorC2R() ( cufftXtExecDescriptorZ2D() ) executes a single-precision (double-precision) complex-to-real transform plan in the transform direction as specified by direction parameter.
3.10.4. Memory Allocation and Data Movement Functions  Multiple GPU cuFFT execution functions assume a certain data layout in terms of what input data has been copied to which GPUs prior to execution, and what output data resides in which GPUs post execution.
They must be called after the call to cufftMakePlan*() . 3.10.4.1. cufftXtMalloc()  cufftResult cufftXtMalloc ( cufftHandle plan , cudaLibXtDesc * * descriptor , cufftXtSubFormat format ) ;  cufftXtMalloc() allocates a descriptor, and all memory for data in GPUs associated with the plan, and returns a pointer to the descriptor.
Note the descriptor contains an array of device pointers so that the application may preprocess or postprocess the data on the GPUs.
The enumerated parameter cufftXtSubFormat_t indicates if the buffer will be used for input or output.
Return values CUFFT_SUCCESS – cuFFT successfully allows user to allocate descriptor and GPU memory.
CUFFT_INVALID_PLAN – The plan parameter is not a valid handle or it is not a multiple GPU plan .
CUFFT_INVALID_DEVICE – An invalid GPU index was specified in the descriptor. 3.10.4.1.1. Parameter cufftXtSubFormat  cufftXtSubFormat_t is an enumerated type that indicates if the buffer will be used for input or output and the ordering of the data.
typedef enum cufftXtSubFormat_t { CUFFT_XT_FORMAT_INPUT ,  by default input is in linear order across GPUs CUFFT_XT_FORMAT_OUTPUT ,  by default output is in scrambled order depending on transform CUFFT_XT_FORMAT_INPLACE ,  by default inplace is input order, which is linear across GPUs CUFFT_XT_FORMAT_INPLACE_SHUFFLED ,  shuffled output order after execution of the transform CUFFT_FORMAT_UNDEFINED } cufftXtSubFormat ; 3.10.4.2.
cufftXtFree()  cufftResult cufftXtFree ( cudaLibXtDesc * descriptor ) ;  cufftXtFree() frees the descriptor and all memory associated with it.
Return values CUFFT_SUCCESS – cuFFT successfully allows user to free descriptor and associated GPU memory. 3.10.4.3. cufftXtMemcpy()  cufftResult cufftXtMemcpy ( cufftHandle plan , void * dstPointer , void * srcPointer , cufftXtCopyType type ) ;  cufftXtMemcpy() copies data between buffers on the host and GPUs or between GPUs.
Calling cufftXtMemcpy function for multi-GPU batched FFT plans with CUFFT_COPY_DEVICE_TO_DEVICE transfer type is not supported.
Note that starting from CUDA 11.2 (cuFFT 10.4.0), cufftSetStream() is supported on multi-GPU plans.
When associating a stream with a plan, cufftXtMemcpy() remains synchronous across the multiple GPUs.
Return values CUFFT_SUCCESS – cuFFT successfully allows user to copy memory between host and GPUs or between GPUs. 3.10.4.3.1. Parameter cufftXtCopyType  cufftXtCopyType_t is an enumerated type for multiple GPU functions that specifies the type of copy for cufftXtMemcpy() .
CUFFT_COPY_HOST_TO_DEVICE copies data from a contiguous host buffer to multiple device buffers, in the layout cuFFT requires for input data.
dstPointer must point to a cudaLibXtDesc structure, and srcPointer must point to a host memory buffer.
CUFFT_COPY_DEVICE_TO_HOST copies data from multiple device buffers, in the layout cuFFT produces for output data, to a contiguous host buffer.
dstPointer must point to a host memory buffer, and srcPointer must point to a cudaLibXtDesc structure.
CUFFT_COPY_DEVICE_TO_DEVICE copies data from multiple device buffers, in the layout cuFFT produces for output data, to multiple device buffers, in the layout cuFFT requires for input data.
dstPointer and srcPointer must point to different cudaLibXtDesc structures (and therefore memory locations).
typedef enum cufftXtCopyType_t { CUFFT_COPY_HOST_TO_DEVICE , CUFFT_COPY_DEVICE_TO_HOST , CUFFT_COPY_DEVICE_TO_DEVICE } cufftXtCopyType ; 3.10.5.
cudaXtDesc  A descriptor type used in multiple GPU routines that contains information about the GPUs and their memory locations.
struct cudaXtDesc_t { int version ;  descriptor version int nGPUs ;  number of GPUs int GPUs [ MAX_CUDA_DESCRIPTOR_GPUS ];  array of device IDs void * data [ MAX_CUDA_DESCRIPTOR_GPUS ];  array of pointers to data, one per GPU size_t size [ MAX_CUDA_DESCRIPTOR_GPUS ];  array of data sizes, one per GPU void * cudaXtState ;  opaque CUDA utility structure }; typedef struct cudaXtDesc_t cudaXtDesc ; 3.10.5.2.
cudaLibXtDesc  A descriptor type used in multiple GPU routines that contains information about the library used.
struct cudaLibXtDesc_t { int version ;  descriptor version cudaXtDesc * descriptor ;  multi-GPU memory descriptor libFormat library ;  which library recognizes the format int subFormat ;  library specific enumerator of sub formats void * libDescriptor ;  library specific descriptor e.g.
cufftXtSetCallback()  cufftResult cufftXtSetCallback ( cufftHandle plan , void * * callbackRoutine , cufftXtCallbackType type , void * * callerInfo )  cufftXtSetCallback() specifies a load or store callback to be used with the plan.
This call is valid only after a call to cufftMakePlan*() , which does the plan generation.
If there was already a callback of this type associated with the plan, this new callback routine replaces it.
If the new callback requires shared memory, you must call cufftXtSetCallbackSharedSize with the amount of shared memory it needs.
callerInfo[In] – Optional array of device pointers to caller specific information, one per GPU.
Return values CUFFT_SUCCESS – cuFFT successfully associated the callback function with the plan. 3.11.2. cufftXtClearCallback()  cufftResult cufftXtClearCallback ( cufftHandle plan , cufftXtCallbackType type )  cufftXtClearCallback() instructs cuFFT to stop invoking the specified callback type when executing the plan.
Return values CUFFT_SUCCESS – cuFFT successfully disassociated the callback function with the plan. 3.11.3. cufftXtSetCallbackSharedSize()  cufftResult cufftXtSetCallbackSharedSize ( cufftHandle plan , cufftXtCallbackType type , size_t sharedSize )  cufftXtSetCallbackSharedSize() instructs cuFFT to dynamically allocate shared memory at launch time, for use by the callback.
Return values CUFFT_SUCCESS – cuFFT will invoke the callback routine with a pointer to the requested amount of shared memory.
CUFFT_ALLOC_FAILED – cuFFT will not be able to allocate the requested amount of shared memory. 3.12. cufftSetStream()  cufftResult cufftSetStream ( cufftHandle plan , cudaStream_t stream ) ;  Associates a CUDA stream with a cuFFT plan.
All kernel launches made during plan execution are now done through the associated stream, enabling overlap with activity in other streams (e.g.
The association remains until the plan is destroyed or the stream is changed with another call to cufftSetStream() .
For previous versions of cuFFT, cufftSetStream() will return an error in multiple GPU plans.
Note that starting from CUDA 12.2 (cuFFT 11.0.8), on multi-GPU plans, stream can be associated with any context on any GPU.
However, repeated calls to cufftSetStream() with streams from different contexts incur a small time penalty.
Optimal performance is obtained when repeated calls to cufftSetStream use streams from the same CUDA context.
stream[In] – A valid CUDA stream created with cudaStreamCreate() ; 0 for the default stream.
CUFFT_INVALID_PLAN – The plan parameter is not a valid handle, or plan is multi-gpu in cuFFT version prior to 10.4.0. 3.13. cufftGetVersion()  cufftResult cufftGetVersion ( int * version ) ;  Returns the version number of cuFFT.
Return values CUFFT_SUCCESS – cuFFT successfully returned the version number. 3.14. cufftGetProperty()  cufftResult cufftGetProperty ( libraryPropertyType type , int * value ) ;  Return in *value the number for the property described by type of the dynamically linked CUFFT library.
The cufftType data type is an enumeration of the types of transform data supported by cuFFT.
typedef enum cufftType_t { CUFFT_R2C = 0x2a ,   Real to complex (interleaved) CUFFT_C2R = 0x2c ,   Complex (interleaved) to real CUFFT_C2C = 0x29 ,   Complex to complex (interleaved) CUFFT_D2Z = 0x6a ,   Double to double-complex (interleaved) CUFFT_Z2D = 0x6c ,   Double-complex (interleaved) to double CUFFT_Z2Z = 0x69   Double-complex to double-complex (interleaved) } cufftType ; 3.15.2.
Parameters for Transform Direction  The cuFFT library defines forward and inverse Fast Fourier Transforms according to the sign of the complex exponential term.
#define cuFFTFORWARD -1 #define cuFFTINVERSE 1 cuFFT performs un-normalized FFTs; that is, performing a forward FFT on an input data set followed by an inverse FFT on the resulting set yields data that is equal to the input, scaled by the number of elements.
Scaling either transform by the reciprocal of the size of the data set is left for the user to perform as seen fit. 3.15.3. Type definitions for callbacks  The cuFFT library supports callback funtions for all combinations of single or double precision, real or complex data, load or store.
The user receives a handle after creating a cuFFT plan and uses this handle to execute the plan.
cufftComplex  A single-precision, floating-point complex data type that consists of interleaved real and imaginary components.
cufftDoubleComplex  A double-precision, floating-point complex data type that consists of interleaved real and imaginary components.
cudaDataType  The cudaDataType data type is an enumeration of the types supported by CUDA libraries.
typedef enum cudaDataType_t { CUDA_R_16F = 2 ,   16 bit real CUDA_C_16F = 6 ,   16 bit complex CUDA_R_32F = 0 ,   32 bit real CUDA_C_32F = 4 ,   32 bit complex CUDA_R_64F = 1 ,   64 bit real CUDA_C_64F = 5 ,   64 bit complex CUDA_R_8I = 3 ,   8 bit real as a signed integer CUDA_C_8I = 7 ,   8 bit complex as a pair of signed integers CUDA_R_8U = 8 ,   8 bit real as an unsigned integer CUDA_C_8U = 9   8 bit complex as a pair of unsigned integers } cudaDataType ; 3.16.2.
libraryPropertyType  The libraryPropertyType data type is an enumeration of library property types.
CUDA version X.Y.Z would yield MAJOR_VERSION=X , MINOR_VERSION=Y , PATCH_LEVEL=Z ) typedef enum libraryPropertyType_t { MAJOR_VERSION , MINOR_VERSION , PATCH_LEVEL } libraryPropertyType ; 4.
cuFFT Code Examples  For simple examples of complex and real 1D, 2D, and 3D transforms that use cuFFT to perform forward and inverse FFTs, refer to the cuFFT Library samples on GitHub . 5. Multiple GPU Data Organization  This chapter explains how data are distributed between the GPUs, before and after a multiple GPU transform.
For simplicity, it is assumed in this chapter that the caller has specified GPU 0 and GPU 1 to perform the transform. 5.1. Multiple GPU Data Organization for Batched Transforms  For batches of transforms, each individual transform is executed on a single GPU.
For a batch of size m performed on n GPUs, where m is not divisible by n , the first m % n GPUs will perform \(\left\lfloor \frac{m}{n}  ight floor+\ 1\) transforms.
For example, in a batch of 15 transforms performed on 4 GPUs, the first three GPUs would perform 4 transforms, and the last GPU would perform 3 transforms.
This approach removes the need for data exchange between the GPUs, and results in nearly perfect scaling for cases where the batch size is divisible by the number of GPUs. 5.2. Multiple GPU Data Organization for Single 2D and 3D Transforms  Single transforms performed on multiple GPUs require the data to be divided between the GPUs.
For example with 2 GPUs, for 2D and 3D transforms with even sized dimensions, each GPU does half of the transform in (rank - 1) dimensions.
Since 2D and 3D transforms support sizes other than powers of 2, it is possible that the data can not be evenly distributed among the GPUs.
In general for the case of n GPUs, a dimension of size m that is not a multiple of n would be distributed such that the first m % n GPUs would get one extra row for 2D transforms, one extra plane for 3D transforms.
Take for example, a 2D transform on 4 GPUs, using an array declared in C as data[x][y] , where x is 65 and y is 99.
The surface is distributed prior to the transform such that GPU 0 receives a surface with dimensions [17][99] , and GPUs 1…3 receive surfaces with dimensions [16][99] .
After the transform, each GPU again has a portion of the surface, but divided in the y dimension.
GPU 3 has a surface with dimensions [65][24] For a 3D transform on 4 GPUs consider an array declared in C as data[x][y][z] , where x is 103, y is 122, and z is 64.
The volume is distributed prior to the transform such that each GPUs 0…2 receive volumes with dimensions [26][122][64] , and GPU 3 receives a volume with dimensions [25][122][64] .
GPUs 0 and 1 have a volumes with dimensions [103][31][64] , and GPUs 2 and 3 have volumes with dimensions [103][30][64] . 5.3. Multiple-GPU Data Organization for Single 1D Transforms  By default for 1D transforms, the initial distribution of data to the GPUs is similar to the 2D and 3D cases.
It is possible to perform this redistribution in the copy from host memory, in cases where the application does not need to pre-process the data prior to the transform.
To do this, the application can create the data descriptor with cufftXtMalloc using the sub-format CUFFT_XT_FORMAT_1D_INPUT_SHUFFLED .
cuFFT performs multiple GPU 1D transforms by decomposing the transform size into factors Factor1 and Factor2 , and treating the data as a grid of size Factor1 x Factor2 .
The four steps done to calculate the 1D FFT are: Factor1 transforms of size Factor2 , data exchange between the GPUs, a pointwise twiddle multiplication, and Factor2 transforms of size Factor1 .
To gain efficiency by overlapping computation with data exchange, cuFFT breaks the whole transform into independent segments or strings, which can be processed while others are in flight.
A side effect of this algorithm is that the output of the transform is not in linear order.
The output in GPU memory is in strings, each of which is composed of Factor2 substrings of equal size.
Each substring contains contiguous results starting Factor1 elements subsequent to start of the previous substring.
See the example below: transform size = 1024 number of strings = 8 Factor1 = 64 Factor2 = 16 substrings per string for output layout is Factor2 ( 16 ) string size = 1024 / 8 = 128 substring size = 128 / 16 = 8 stride between substrings = 1024 / 16 = Factor1 ( 64 ) On GPU 0 : string 0 has substrings with indices 0. . .7 64. . .71 128. . .135 ... 960. . .967 string 1 has substrings with indices 8.
. .15 72. . .79 136. . .143 ... 968. . .975 ... On GPU 1 : string 4 has substrings with indices 32.
. .63 120. . .127 184. . .191 ... 1016. . .1023 The cufftXtQueryPlan API allows the caller to retrieve a structure containing the number of strings, the decomposition factors, and (in the case of power of 2 size) some useful mask and shift elements.
It also shows how to translate from an index in the host input array to the corresponding index on the device, and vice versa.
/* * These routines demonstrate the use of cufftXtQueryPlan to get the 1D * factorization and convert between permuted and linear indexes.
factor2 ); cufftDestroy ( plan ); return 0 ; } /* * Given an index into a permuted array, and the GPU index return the * corresponding linear index from the beginning of the input buffer.
whichString = ( linearIx >> factors -> substringShift ) & whichStringMask ;   the first stringCount/2 strings are in the first GPU,   the rest are in the second.
* GPUIx = whichString / ( factors -> stringCount / 2 );   next determine which substring within the string has our index   the substring index is in the next higher order bits of the index whichSubstring = ( linearIx >> ( factors -> substringShift + whichStringShift )) & factors -> factor2Mask ;   now we can re-assemble the index * permutedIx = indexInSubstring ; * permutedIx += whichSubstring substringShift ; if ( ! * GPUIx ) { * permutedIx += whichString stringShift ; } else { * permutedIx += ( whichString - ( factors -> stringCount / 2 ) ) stringShift ; } return CUFFT_SUCCESS ; } 6.
FFTW Conversion Guide  cuFFT differs from FFTW in that FFTW has many plans and a single execute function while cuFFT has fewer plans, but multiple execute functions.
The cuFFT execute functions determine the precision (single or double) and whether the input is complex or real valued.
FFTW function cuFFT function fftw_plan_dft_1d(), fftw_plan_dft_r2c_1d(), fftw_plan_dft_c2r_1d() cufftPlan1d() fftw_plan_dft_2d(), fftw_plan_dft_r2c_2d(), fftw_plan_dft_c2r_2d() cufftPlan2d() fftw_plan_dft_3d(), fftw_plan_dft_r2c_3d(), fftw_plan_dft_c2r_3d() cufftPlan3d() fftw_plan_dft(), fftw_plan_dft_r2c(), fftw_plan_dft_c2r() cufftPlanMany() fftw_plan_many_dft(), fftw_plan_many_dft_r2c(), fftw_plan_many_dft_c2r() cufftPlanMany() fftw_execute() cufftExecC2C(), cufftExecZ2Z(), cufftExecR2C(), cufftExecD2Z(), cufftExecC2R(), cufftExecZ2D() fftw_destroy_plan() cufftDestroy() 7.
This allows applications using FFTW to use NVIDIA GPUs with minimal modifications to program source code.
To use the interface first do the following two steps It is recommended that you replace the include file fftw3.h with cufftw.h Instead of linking with the double/single precision libraries such as fftw3/fftw3f libraries, link with both the cuFFT and cuFFTW libraries Ensure the search path includes the directory containing cuda_runtime_api.h After an application is working using the FFTW3 interface, users may want to modify their code to move data to and from the GPU and use the routines documented in the FFTW Conversion Guide for the best performance.
Section in FFTW manual Supported Unsupported Complex numbers fftw_complex, fftwf_complex types Precision double fftw3 , single fftwf3 long double fftw3l , quad precision fftw3q are not supported since CUDA functions operate on double and single precision floating-point quantities Memory Allocation fftw_malloc(), fftw_free(), fftw_alloc_real(), fftw_alloc_complex(), fftwf_alloc_real(), fftwf_alloc_complex() Multi-threaded FFTW fftw3_threads, fftw3_omp are not supported Distributed-memory FFTW with MPI fftw3_mpi,fftw3f_mpi are not supported Note that for each of the double precision functions below there is a corresponding single precision version with the letters fftw replaced by fftwf .
Other wisdom functions do not have entry points in the library. 8. Deprecated Functionality  Starting from CUDA 12.0: GPU architectures SM35 and SM37 are no longer supported.
Starting from CUDA 11.8: CUDA Graphs capture is no longer supported for callback routines that load data in out-of-place mode transforms.
Starting from CUDA 11.4: Support for callback functionality using separately compiled device code is deprecated on all GPU architectures.
Support for GPU architectures SM35, SM37 (Kepler), and SM50, SM52 (Maxwell) is deprecated.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 9.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 9.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
1.
nvJPEG Decoder  The nvJPEG library provides high-performance, GPU accelerated JPEG decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications.
The library offers single and batched JPEG decoding capabilities which efficiently utilize the available GPU resources for optimum performance; and the flexibility for users to manage the memory allocation needed for decoding.
The nvJPEG library enables the following functions: use the JPEG image data stream as input; retrieve the width and height of the image from the data stream, and use this retrieved information to manage the GPU memory allocation and the decoding.
A dedicated API is provided for retrieving the image information from the raw JPEG image data stream.
The nvJPEG library supports the following: JPEG options: Baseline and Progressive JPEG decoding/encoding 8 bits per pixel Huffman bitstream decoding Upto 4 channel JPEG bitstreams 8- and 16-bit quantization tables The following chroma subsampling for the 3 color channels Y, Cb, Cr (Y, U, V): 4:4:4 4:2:2 4:2:0 4:4:0 4:1:1 4:1:0 Features: Hybrid decoding using both the CPU (i.e., host) and the GPU (i.e., device).
User-provided memory manager for the device and pinned host memory allocations. 1.2. nvJPEG Encoder  The encoding functions of the nvJPEG library perform GPU-accelerated compression of user’s image data to the JPEG bitstream.
User can provide input data in a number of formats and colorspaces, and control the encoding process with parameters.
Encoding functionality will allocate temporary buffers using user-provided memory allocator.
Before calling the encoding functions the user should perform a few prerequisite steps using the helper functions described in nvJPEG Encoder Helper API Reference . 1.3. Thread Safety  Not all nvJPEG types are thread safe.
When using decoder APIs across multiple threads, the following decoder types should be instantiated separately for each thread: nvjpegJpegStream_t , nvjpegJpegState_t , nvjpegBufferDevice_t , nvjpegBufferPinned_t When using encoder APIs across multiple threads, nvjpegEncoderState_t should be instantiated separately for each thread.
For user-provided allocators (inputs to nvJPEGCreateEx() ), the user needs to ensure thread safety. 1.4. Multi-GPU support  The nvJPEG states and handles are bound to the device that was set as current during their creation.
The user is responsible of keeping track of the current device. 1.5. Hardware Acceleration  Hardware accelerated JPEG decode is available on the following GPUs - A100, A30, H100.
Platforms which support hardware accelerated JPEG decode: Windows Linux (x86_64, PowerPC, ARM64) 2.
Using JPEG Decoding  ​The nvJPEG library provides functions for both the decoding of a single image, and batched decoding of multiple images. 2.1.1. Single Image Decoding  For single-image decoding you provide the data size and a pointer to the file data, and the decoded image is placed in the output buffer.
Create nvJPEG library handle with one of the helper functions nvjpegCreateSimple() or nvjpegCreateEx() .
The following helper functions are available in the nvJPEG library: nvjpegStatus_t nvjpegGetProperty(libraryPropertyType type, int *value); [DEPRECATED] nvjpegStatus_t nvjpegCreate(nvjpegBackend_t backend, nvjpegHandle_t *handle , nvjpeg_dev_allocator allocator); nvjpegStatus_t nvjpegCreateSimple(nvjpegHandle_t *handle); nvjpegStatus_t nvjpegCreateEx(nvjpegBackend_t backend, nvjpegDevAllocator_t *dev_allocator, nvjpegPinnedAllocator_t *pinned_allocator, unsigned int flags, nvjpegHandle_t *handle); nvjpegStatus_t nvjpegDestroy(nvjpegHandle_t handle); nvjpegStatus_t nvjpegJpegStateCreate(nvjpegHandle_t handle, nvjpegJpegState_t *jpeg_handle); nvjpegStatus_t nvjpegJpegStateDestroy(nvjpegJpegState handle); Other helper functions such as nvjpegSet*() and nvjpegGet*() can be used to configure the library functionality on per-handle basis.
Retrieve the width and height information from the JPEG-encoded image by using the nvjpegGetImageInfo() function.
Below is the signature of nvjpegGetImageInfo() function: nvjpegStatus_t nvjpegGetImageInfo ( nvjpegHandle_t handle , const unsigned char * data , size_t length , int * nComponents , nvjpegChromaSubsampling_t * subsampling , int * widths , int * heights ); For each image to be decoded, pass the JPEG data pointer and data length to the above function.
One of the outputs of the above nvjpegGetImageInfo() function is nvjpegChromaSubsampling_t .
This parameter is an enum type, and its enumerator list is composed of the chroma subsampling property retrieved from the JPEG image.
See the signature of this function below: nvjpegStatus_t nvjpegDecode ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * data , size_t length , nvjpegOutputFormat_t output_format , nvjpegImage_t * destination , cudaStream_t stream ); In the above nvjpegDecode() function, the parameters nvjpegOutputFormat_t , nvjpegImage_t , and cudaStream_t can be used to set the output behavior of the nvjpegDecode() function.
You provide the cudaStream_t parameter to indicate the stream to which your asynchronous tasks are submitted.
The ``nvjpegOutputFormat_t`` parameter: The nvjpegOutputFormat_t parameter can be set to one of the output_format settings below: output_format Meaning NVJPEG_OUTPUT_UNCHANGED Return the decoded image planar format.
For example, if output_format is set to NVJPEG_OUTPUT_Y or NVJPEG_OUTPUT_RGBI , or NVJPEG_OUTPUT_BGRI then the output is written only to channel[0] of nvjpegImage_t , and the other channels are not touched.
Alternately, in the case of planar output, the data is written to the corresponding channels of the nvjpegImage_t destination structure.
Finally, in the case of grayscale JPEG and RGB output, the luminance is used to create the grayscale RGB.
The below table explains the combinations of the output formats and the number of channels supported by the library.
No of Channels in bitstream 1 2 3 4 Output Format NVJPEG_OUTPUT_UNCHANGED Yes Yes Yes Yes NVJPEG_OUTPUT_YUV Only the first channel of the output is populated No Yes No NVJPEG_OUTPUT_Y Yes No Yes Yes (a) NVJPEG_OUTPUT_RGB Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_BGR Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_RGBI Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_BGRI Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_UNCHANGEDI_U16 Yes(c) Yes No No NOTES: Must be enabled using nvjpegDecodeParamsSetAllowCMYK() .
As mentioned above, an important benefit of the nvjpegGetImageInfo() function is the ability to utilize the image information retrieved from the the input JPEG image to allocate proper GPU memory for your decoding operation.
The nvjpegGetImageInfo() function returns the widths , heights and nComponents parameters.
nvjpegStatus_t nvjpegGetImageInfo ( nvjpegHandle_t handle , const unsigned char * data , size_t length , int * nComponents , nvjpegChromaSubsampling_t * subsampling , int * widths , int * heights ); You can use the retrieved parameters, widths , heights and nComponents , to calculate the required size for the output buffers, either for a single decoded JPEG, or for every decoded JPEG in a batch.
The nvjpegImage_t structure that holds the output pointers is defined as follows: typedef struct { unsigned char * channel [ NVJPEG_MAX_COMPONENT ]; size_t pitch [ NVJPEG_MAX_COMPONENT ]; } nvjpegImage_t ; NVJPEG_MAX_COMPONENT is the maximum number of color components the nvJPEG library supports in the current release.
For generic images, this is the maximum number of encoded channels that the library is able to decompress.
Finally, when you call the nvjpegDecode() function with the parameters as described above, the nvjpegDecode() function fills the output buffers with the decoded data. 2.1.2. Decode using Decoupled Phases  The nvJPEG library allows further separation of the host and device phases of the decode process.
Below is the sequence of API calls to decode a single image Initialize all the items that are used in the decoding process: Create the library handle using one of the library handle initialization routines.
Choose decoder implementation nvjpegBackend_t , and create decoder using nvjpegDecoderCreate() .
Create the pinned and device buffers used by the decoder using the below APIs respectively.
nvjpegBufferPinnedCreate() nvjpegBufferDeviceCreate() Link the buffers to the JPEG state using the following APIs respectively: nvjpegStateAttachPinnedBuffer() nvjpegStateAttachDeviceBuffer() Create decode parameters using the below API.
This is used to set the output format, and enable ROI decode: nvjpegDecodeParamsCreate() Perform decoding: Parse the jpeg bit-stream using nvjpegJpegStreamParse() Encoded bitstream information, like channel dimensions, can be retrieved using the below API.
nvjpegJpegStreamGetComponentsNum() nvjpegJpegStreamGetComponentDimensions() Call the decode API in the below sequence to decode the image: nvjpegDecodeJpegHost() nvjpegDecodeJpegTransferToDevice() nvjpegDecodeJpegDevice() 2.1.3.
Batched Image Decoding  For the batched image decoding you provide pointers to multiple file data in the memory, and also provide the buffer sizes for each file data.
The nvJPEG library will decode these multiple images, and will place the decoded data in the output buffers that you specified in the parameters. 2.1.3.1. Single Phase  For batched image decoding in single phase, follow these steps: Call nvjpegDecodeBatchedInitialize() function to initialize the batched decoder.
If the size of the batch changes, or if the batch decoding fails, then call the nvjpegDecodeBatchedInitialize() function again. 2.2. nvJPEG Type Declarations  2.2.1.
nvJPEG Backend  typedef enum { NVJPEG_BACKEND_DEFAULT = 0 , NVJPEG_BACKEND_HYBRID = 1 , NVJPEG_BACKEND_GPU_HYBRID = 2 , NVJPEG_BACKEND_HARDWARE = 3 , NVJPEG_BACKEND_GPU_HYBRID_DEVICE = 4 , NVJPEG_BACKEND_HARDWARE_DEVICE = 5 , NVJPEG_BACKEND_LOSSLESS_JPEG = 6 } nvjpegBackend_t ; The nvjpegBackend_t enum is used to select either default back-end by default, or use GPU decoding for baseline JPEG images, or use CPU for Huffman decoding.
nvjpegDecodeBatched will use GPU decoding for baseline JPEG images with interleaved scan when batch size is greater than 50.
Can be used only with batched decode APIs for baseline JPEG images without restart intervals.
NVJPEG_BACKEND_LOSSLESS_JPEG Supports lossless jpeg bitstreams as defined in the jpeg 92 standard.
Bitstreams with up to 2 channels and prediction mode 1 are supported. 2.2.2. nvJPEG Bitstream Handle  struct nvjpegJpegStream ; typedef struct nvjpegJpegStream * nvjpegJpegStream_t ; This handle stores the bit-stream parameters on the host.
This helps retrieve bitstream meta-data using APIs defined in nvJPEG Stream API . 2.2.3. nvJPEG Decode Device Buffer Handle  struct nvjpegBufferDevice ; typedef struct nvjpegBufferDevice * nvjpegBufferDevice_t ; This nvjpegBufferDevice_t is used by decoder states to store the intermediate information in device memory.
2.2.4. nvJPEG Decode Parameter Handle  struct nvjpegDecodeParams ; typedef struct nvjpegDecodeParams * nvjpegDecodeParams_t ; This decoder parameter handle stores the parameters like output format, and the ROI decode parameters that are set using APIs defined in nvJPEG Chroma Subsampling .
2.2.5. nvJPEG Decode Pinned Buffer Handle  struct nvjpegBufferPinned ; typedef struct nvjpegBufferPinned * nvjpegBufferPinned_t ; This nvjpegBufferPinned_t handle is used by decoder states to store the intermediate information on pinned memory.
2.2.6. nvJPEG Decoder Handle  struct nvjpegJpegDecoder ; typedef struct nvjpegJpegDecoder * nvjpegJpegDecoder_t ; This decoder handle stores the intermediate decoder data, which is shared across the decoding stages.
It is used as input to the Decode API—Decoupled Decoding . 2.2.7. nvJPEG Host Pinned Memory Allocator Interface  typedef int ( * tPinnedMalloc )( void ** , size_t , unsigned int flags ); typedef int ( * tPinnedFree )( void * ); typedef struct { tPinnedMalloc pinned_malloc ; tPinnedFree pinned_free ; } nvjpegPinnedAllocator_t ; When the nvjpegPinnedAllocator_t *allocator parameter in the nvjpegCreateEx() function is set as a pointer to the above nvjpegPinnedAllocator_t structure, then this structure will be used for allocating and releasing host pinned memory for copying data to/from device.
The function prototypes for the memory allocation and memory freeing functions are similar to the cudaHostAlloc() and cudaFreeHost() functions.
However, if the nvjpegPinnedAllocator_t *allocator parameter in the nvjpegCreateEx() function is set to NULL, then the default memory allocation functions cudaHostAlloc() and cudaFreeHost() will be used.
When using nvjpegCreate() or nvjpegCreateSimple() function to create library handle, the default host pinned memory allocator will be used. 2.2.8. nvJPEG Extended Host Pinned Memory Allocator Interface  typedef int ( * tPinnedMallocV2 )( void * ctx , void ** ptr , size_t size , cudaStream_t stream ); typedef int ( * tPinnedFreeV2 )( void * ctx , void * ptr , size_t size , cudaStream_t stream ); typedef struct { tPinnedMallocV2 pinned_malloc ; tPinnedFreeV2 pinned_free ; void * pinned_ctx ; } nvjpegPinnedAllocatorV2_t ; Extended pinned allocators support stream ordered allocations along with user defined context information pinned_ctx .
When invoking the allocators, nvJPEG will pass pinned_ctx as input to the extended pinned allocators. 2.2.9. nvJPEG Image  typedef struct { unsigned char * channel [ NVJPEG_MAX_COMPONENT ]; size_t pitch [ NVJPEG_MAX_COMPONENT ]; } nvjpegImage_t ; The nvjpegImage_t structure (or structures, in the case of batched decode) is used to fill with the pointers and pitches of allocated buffers.
Member Description NVJPEG_MAX_COMPONENT Maximum number of color components the nvJPEG library supports. 2.2.10. nvJPEG Device Memory Allocator Interface  typedef int ( * tDevMalloc )( void ** , size_t ); typedef int ( * tDevFree )( void * ); typedef struct { tDevMalloc dev_malloc ; tDevFree dev_free ; } nvjpegDevAllocator_t ; Users can tell the library to use their own device memory allocator.
The function prototypes for the memory allocation and memory freeing functions are similar to the cudaMalloc() and cudaFree() functions.
A pointer to the nvjpegDevAllocator_t structure, with properly filled fields, should be provided to the nvjpegCreate() function.
NULL is accepted, in which case the default memory allocation functions cudaMalloc() and cudaFree() is used.
When the nvjpegDevAllocator_t *allocator parameter in the nvjpegCreate() or nvjpegCreateEx() function is set as a pointer to the above nvjpegDevAllocator_t structure, then this structure is used for allocating and releasing the device memory.
However, if the nvjpegDevAllocator_t *allocator parameter in the nvjpegCreate() or nvjpegCreateEx() function is set to NULL, then the default memory allocation functions cudaMalloc() and cudaFree() will be used.
When using nvjpegCreateSimple() function to create library handle the default device memory allocator will be used. 2.2.11. nvJPEG Extended Device Memory Allocator Interface  typedef int ( * tDevMallocV2 )( void * ctx , void ** ptr , size_t size , cudaStream_t stream ); typedef int ( * tDevFreeV2 )( void * ctx , void * ptr , size_t size , cudaStream_t stream ); typedef struct { tDevMallocV2 dev_malloc ; tDevFreeV2 dev_free ; void * dev_ctx ; } nvjpegDevAllocatorV2_t ; Extended device allocators support stream ordered allocations along with user defined context information dev_ctx .
When invoking the allocators, nvJPEG will pass dev_ctx as input to the extended device allocators. 2.2.12. nvJPEG Opaque JPEG Decoding State Handle  struct nvjpegJpegState ; typedef struct nvjpegJpegState * nvjpegJpegState_t ; The nvjpegJpegState structure stores the temporary JPEG information.
The same JPEG handle should be used across the decoding phases for the same image or batch.
Multiple threads are allowed to share the JPEG state handle only when processing same batch during first phase ( nvjpegDecodePhaseOne ) . 2.2.13. nvJPEG Opaque Library Handle Struct  struct nvjpegHandle ; typedef struct nvjpegHandle * nvjpegHandle_t ; The library handle is used in any consecutive nvJPEG library calls, and should be initialized first.
The library handle is thread safe, and can be used by multiple threads simultaneously. 2.2.14. nvJPEG Output Pointer Struct  typedef struct { unsigned char * channel [ NVJPEG_MAX_COMPONENT ]; size_t pitch [ NVJPEG_MAX_COMPONENT ]; } nvjpegImage_t ; The nvjpegImage_t struct holds the pointers to the output buffers, and holds the corresponding strides of those buffers for the image decoding.
See Single Image Decoding on how to set up the nvjpegImage_t struct. 2.2.15. nvJPEG Jpeg Encoding  typedef enum { NVJPEG_ENCODING_UNKNOWN = 0x0 , NVJPEG_ENCODING_BASELINE_DCT = 0xc0 , NVJPEG_ENCODING_EXTENDED_SEQUENTIAL_DCT_HUFFMAN = 0xc1 , NVJPEG_ENCODING_PROGRESSIVE_DCT_HUFFMAN = 0xc2 , NVJPEG_ENCODING_LOSSLESS_HUFFMAN = 0xc3 } nvjpegJpegEncoding_t ; The nvjpegJpegEncoding_t enum lists the JPEG encoding types that are supported by the nvJPEG library The enum values are based on the markers defined in the JPEG specification Member Description NVJPEG_ENCODING_UNKNOWN This value is returned for all the JPEG markers not supported by the nvJPEG library.
NVJPEG_ENCODING_BASELINE_DCT Corresponds to the JPEG marker 0xc0, refer to the JPEG spec for more details.
NVJPEG_ENCODING_EXTENDED_SEQUENTIAL_DCT_HUFFMAN Corresponds to the JPEG marker 0xc1, refer to the JPEG spec for more details.
NVJPEG_ENCODING_PROGRESSIVE_DCT_HUFFMAN Corresponds to the JPEG marker 0xc2, refer to the JPEG spec for more details.
NVJPEG_ENCODING_LOSSLESS_HUFFMAN Corresponds to the JPEG marker 0xc3, refer to the JPEG spec for more details. 2.2.16. nvJPEG Scale Factor  typedef enum { NVJPEG_SCALE_NONE = 0 , NVJPEG_SCALE_1_BY_2 = 1 , NVJPEG_SCALE_1_BY_4 = 2 , NVJPEG_SCALE_1_BY_8 = 3 } nvjpegScaleFactor_t ; The nvjpegScaleFactor_t enum lists all the scale factors supported by the library.
This feature is supported when nvjpeg handles are intstaniated using NVJPEG_BACKEND_HARDWARE Member Description NVJPEG_SCALE_NONE Decoded output is not scaled NVJPEG_SCALE_1_BY_2 Decoded output width and height are scaled by a factor of 1/2 NVJPEG_SCALE_1_BY_4 Decoded output width and height are scaled by a factor of 1/4 NVJPEG_SCALE_1_BY_8 Decoded output width and height are scaled by a factor of 1/8 2.2.17.
nvJPEG Flags  #define NVJPEG_FLAGS_DEFAULT 0 #define NVJPEG_FLAGS_HW_DECODE_NO_PIPELINE 1 #define NVJPEG_FLAGS_ENABLE_MEMORY_POOLS 2 #define NVJPEG_FLAGS_BITSTREAM_STRICT 4 #define NVJPEG_FLAGS_REDUCED_MEMORY_DECODE 8 #define NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY 16 #define NVJPEG_FLAGS_UPSAMPLING_WITH_INTERPOLATION 32 nvJPEG flags provide additional controls when initializing the library using nvJPEGCreateEx() or nvJPEGCreateExV2() .
NVJPEG_FLAGS_HW_DECODE_NO_PIPELINE To be used when the library is initialized with NVJPEG_BACKEND_HARDWARE.
NVJPEG_FLAGS_ENABLE_MEMORY_POOLS [Deprecated] Starting with CUDA 11.1 this flag will be ignored.
NVJPEG_FLAGS_BITSTREAM_STRICT nvJPEG library will try to decode a bitstream even if it doesn’t strictly follow the JPEG specification.
NVJPEG_FLAGS_REDUCED_MEMORY_DECODE When using NVJPEG_BACKEND_HYBRID or NVJPEG_BACKEND_GPU_HYBRID backends, enabling this flag will reduce the memory usage of the decoding whenever possible.
NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY Using this flag enables zero-copy memory when feasible on supported platforms.
NVJPEG_FLAGS_UPSAMPLING_WITH_INTERPOLATION Using this flag enables the decoder to use interpolation when performing chroma upsampling during the YCbCr to RGB conversion stage. 2.2.18. nvJPEG Exif Orientation  typedef enum { NVJPEG_ORIENTATION_UNKNOWN = 0 , NVJPEG_ORIENTATION_NORMAL = 1 , NVJPEG_ORIENTATION_FLIP_HORIZONTAL = 2 , NVJPEG_ORIENTATION_ROTATE_180 = 3 , NVJPEG_ORIENTATION_FLIP_VERTICAL = 4 , NVJPEG_ORIENTATION_TRANSPOSE = 5 , NVJPEG_ORIENTATION_ROTATE_90 = 6 , NVJPEG_ORIENTATION_TRANSVERSE = 7 , NVJPEG_ORIENTATION_ROTATE_270 = 8 } nvjpegExifOrientation_t ; The nvjpegExifOrientation_t enum represents the exif orientation in a jfif(jpeg) file.
Exif orientation information is typically used to denote the digital camera sensor orientation at the time of image capture.
Member Description NVJPEG_ORIENTATION_UNKNOWN Exif orientation information is not available in the bitstream.
NVJPEG_ORIENTATION_FLIP_HORIZONTAL Decoded output should be mirrored/flipped horizontally.
NVJPEG_ORIENTATION_TRANSPOSE Decoded output should be flipped/mirrored horizontally followed by a 90 degrees counter-clockwise rotation.
NVJPEG_ORIENTATION_ROTATE_90 Decoded output should be rotated 90 degrees counter-clockwise.
NVJPEG_ORIENTATION_TRANSVERSE Decoded output should be flipped/mirrored horizontally followed by a 270 degrees counter-clockwise rotation.
NVJPEG_ORIENTATION_ROTATE_270 Decoded output should be rotated 270 degrees counter-clockwise. 2.3. nvJPEG API Reference  This section describes the nvJPEG decoder API.
nvjpegGetProperty()  Gets the numeric value for the major or minor version, or the patch level, of the nvJPEG library.
Signature: nvjpegStatus_t nvjpegGetProperty ( libraryPropertyType type , int * value ); Parameters: Parameter Input / Output Memory Description libraryPropertyType type Input Host One of the supported libraryPropertyType values, that is, MAJOR_VERSION, MINOR_VERSION or PATCH_LEVEL.
int *value Output Host The numeric value corresponding to the specific libraryPropertyType requested.
Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.2. nvjpegGetCudartProperty()  Gets the numeric value for the major version, minor version, or the patch level of the CUDA toolkit that was used to build nvJPEG library.
Signature: nvjpegStatus_t nvjpegGetCudartProperty ( libraryPropertyType type , int * value ); Parameters: Parameter Input / Output Memory Description libraryPropertyType type Input Host One of the supported libraryPropertyType values, that is, MAJOR_VERSION, MINOR_VERSION or PATCH_LEVEL. 2.3.1.3. nvjpegCreate() [DEPRECATED]  Allocates and initializes the library handle.
Use either nvjpegCreateSimple() or nvjpegCreateEx() functions to create the library handle.
Signature: nvjpegStatus_t nvjpegCreate ( nvjpegBackend_t backend , nvjpegDevAllocator_t * allocator , nvjpegHandle_t * handle ); Parameters: Parameter Input / Output Memory Description nvjpegBackend_t backend Input Host Backend parameter for nvjpegDecodeBatched() API.
If NULL is provided, then the default CUDA runtime cudaMalloc() and cudaFree() functions will be used.
The nvjpegBackend_t parameter is an enum type, with the below enumerated list values: typedef enum { NVJPEG_BACKEND_DEFAULT = 0 , NVJPEG_BACKEND_HYBRID = 1 , } nvjpegBackend_t ; Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.4. nvjpegCreateSimple()  Allocates and initializes the library handle, with default codec implementations selected by library and default memory allocators.
Signature: nvjpegStatus_t nvjpegCreateSimple ( nvjpegHandle_t * handle ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t *handle Input/Output Host The library handle.
Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes . 2.3.1.5. nvjpegCreateEx()  Allocates and initializes the library handle using the provided arguments.
Signature: nvjpegStatus_t nvjpegCreateEx ( nvjpegBackend_t backend , nvjpegDevAllocator_t * dev_allocator , nvjpegPinnedAllocator_t * pinned_allocator , unsigned int flags , nvjpegHandle_t * handle ); Parameters: Parameter Input / Output Memory Description nvjpegBackend_t backend Input Host Backend parameter for nvjpegDecodeBatched() API.
If NULL is provided, then the default CUDA runtime functions cudaMalloc() and cudaFree() will be used.
If NULL is provided, then the default CUDA runtime functions cudaHostAlloc() and cudaFreeHost() will be used.
unsigned int flags Input Host Refer to nvJPEG Flags for details. 2.3.1.6. nvjpegCreateExV2()  Allocates and initializes the library handle using the provided arguments.
Signature: nvjpegStatus_t nvjpegCreateExV2 ( nvjpegBackend_t backend , nvjpegDevAllocatorV2_t * dev_allocator , nvjpegPinnedAllocatorV2_t * pinned_allocator , unsigned int flags , nvjpegHandle_t * handle ); Parameters: Parameter Input / Output Memory Description nvjpegBackend_t backend Input Host Backend parameter for nvjpegDecodeBatched() API.
See nvjpegPinnedAllocatorV2_t structure description. 2.3.1.7. nvjpegDestroy()  Releases the library handle.
Signature: nvjpegStatus_t nvjpegDestroy ( nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input/Output Host The library handle to release. 2.3.1.8. nvjpegSetDeviceMemoryPadding()  Use the provided padding for all device memory allocations with specified library handle.
A large number will help to amortize the need for device memory reallocations when needed.
Signature: nvjpegStatus_t nvjpegSetDeviceMemoryPadding ( size_t padding , nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description size_t padding Input Host Device memory padding to use for all further device memory allocations.
nvjpegHandle_t handle Input/Output Host The library handle. 2.3.1.9. nvjpegGetDeviceMemoryPadding()  Retrieve the device memory padding that is currently used for the specified library handle.
Signature: nvjpegStatus_t nvjpegGetDeviceMemoryPadding ( size_t * padding , nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description size_t *padding Output Host Device memory padding that is currently used for device memory allocations. 2.3.1.10. nvjpegSetPinnedMemoryPadding()  Use the provided padding for all pinned host memory allocations with specified library handle.
A large number will help to amortize the need for pinned host memory reallocations when needed.
Signature: nvjpegStatus_t nvjpegSetPinnedMemoryPadding ( size_t padding , nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description size_t padding Input Host Pinned host memory padding to use for all further pinned host memory allocations. 2.3.1.11. nvjpegGetPinnedMemoryPadding()  Retrieve the pinned host memory padding that is currently used for specified library handle.
Signature: nvjpegStatus_t nvjpegGetPinnedMemoryPadding ( size_t * padding , nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description size_t *padding Output Host Pinned host memory padding that is currently used for pinned host memory allocations. 2.3.1.12. nvjpegGetHardwareDecoderInfo()  Retrieve hardware decoder details such as number of engines and number of cores available in each engine.
Signature: nvjpegStatus_t nvjpegGetHardwareDecoderInfo ( nvjpegHandle_t handle , unsigned int * num_engines , unsigned int * num_cores_per_engine ); Parameters: nvjpegHandle_t handle Input Host The library handle.
unsigned int* num_engines Input/Output Host Retrieves number of engines available for decode.
unsigned int* num_cores_per_engine Input/Output Host Retrieves number of cores per engine. 2.3.1.13. nvjpegJpegStateCreate()  Allocates and initializes the internal structure required for the JPEG processing.
Signature: nvjpegStatus_t nvjpegJpegStateCreate ( nvjpegHandle_t handle , nvjpegJpegState_t * jpeg_handle ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle.
nvjpegJpegState_t *jpeg_handle Input/Output Host The image state handle. 2.3.1.14. nvjpegJpegStateDestroy()  Releases the image internal structure.
Signature: nvjpegStatus_t nvjpegJpegStateDestroy ( nvjpegJpegState handle ); Parameters: Parameter Input / Output Memory Description nvjpegJpegState handle Input/Output Host The image state handle. 2.3.1.15. nvjpegDecoderCreate()  Creates a decoder handle.
Signature: nvjpegStatus_t nvjpegDecoderCreate ( nvjpegHandle_t nvjpeg_handle , nvjpegBackend_t implementation , nvjpegJpegDecoder_t * decoder_handle ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t nvjpeg_handle Input Host Library handle.
nvjpegBackend_t backend Input Host Backend parameter for the decoder_handle.The back end applies to all the functions under the decoupled API , when called with this handle.
nvjpegJpegDecoder_t decoder_handle Input/Output Host Decoder state handle. 2.3.1.16. nvjpegDecoderDestroy()  Destroys the decoder handle.
Signature: nvjpegStatus_t nvjpegDecoderDestroy ( nvjpegJpegDecoder_t decoder_handle ); Parameters: Parameter Input / Output Memory Description nvjpegJpegDecoder_t decoder_handle Input/Output Host Decoder handle. 2.3.1.17. nvjpegDecoderJpegSupported()  Determines whether the decoder_handle is able to handle the bit-stream stored in jpeg_stream .
Signature: nvjpegStatus_t nvjpegDecoderJpegSupported ( nvjpegJpegDecoder_t decoder_handle , nvjpegJpegStream_t jpeg_stream , nvjpegDecodeParams_t decode_params , int * is_supported ); Parameters: Parameter Input / Output Memory Description nvjpegJpegDecoder_t decoder_handle Input Host Decoder state handle nvjpegJpegStream_t jpeg_stream Input Host Bit stream meta-data nvjpegDecodeParams_t decode_params Input Host Decoder output configuration int* is_supported Output Host Return value of 0 indicates bitstream can be decoded by the decoder_handle , non zero value indicates that the bitstream is not supported Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.18. nvjpegDecoderStateCreate()  Creates the decoder_state internal structure.
The decoder_state is associated with the nvjpegBackend_t implementation that was used to create the decoder_handle .
Signature: nvjpegStatus_t nvjpegDecoderStateCreate ( nvjpegHandle_t nvjpeg_handle , nvjpegJpegDecoder_t decoder_handle , nvjpegJpegState_t * decoder_state ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t nvjpeg_handle Input Host Library handle.
nvjpegJpegState_t* decoder_state Input/Output Host nvJPEG Image State Handle. 2.3.1.19. nvjpegJpegStreamCreate()  Creates jpeg_stream that is used to parse the JPEG bitstream and store bitstream parameters.
Signature: nvjpegStatus_t nvjpegJpegStreamCreate ( nvjpegHandle_t handle , nvjpegJpegStream_t * jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle nvjpegJpegStream_t *jpeg_stream Input Host Bitstream handle Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.20. nvjpegJpegStreamDestroy()  Destroys the jpeg_stream structure.
Signature: nvjpegStatus_t nvjpegJpegStreamDestroy ( nvjpegJpegStream_t * jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t *jpeg_stream Input Host Bitstream handle Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes . 2.3.1.21. nvjpegBufferPinnedCreate()  Creates a pinned buffer handle.
Signature: nvjpegStatus_t nvjpegBufferPinnedCreate ( nvjpegHandle_t handle , nvjpegPinnedAllocator_t * pinned_allocator , nvjpegBufferPinned_t * buffer ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle.
nvjpegBufferPinned_t* buffer Input/Output Host nvJPEG pinned buffer object. 2.3.1.22. nvjpegBufferPinnedCreateV2()  Creates a pinned buffer handle using extended allocators.
Signature: nvjpegStatus_t nvjpegBufferPinnedCreateV2 ( nvjpegHandle_t handle , nvjpegPinnedAllocatorV2_t * pinned_allocator , nvjpegBufferPinned_t * buffer ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle.
nvjpegPinnedAllocatorV2_t* pinned_allocator Input Host Extended pinned host memory allocator. 2.3.1.23. nvjpegBufferPinnedDestroy()  Destroys a pinned buffer handle.
Signature: nvjpegStatus_t nvjpegBufferPinnedDestroy ( nvjpegBufferPinned_t buffer ); Parameters: Parameter Input / Output Memory Description nvjpegBufferPinned_t buffer Input Host nvJPEG pinned buffer object. 2.3.1.24. nvjpegStateAttachPinnedBuffer()  Link the nvJPEG pinned buffer handle to decoder_state .
The pinned_buffer is used by the decoder to store the intermediate information that is used across the decoding stages.
Pinned buffer can be attached to different decoder states, which helps to switch between implementations without allocating extra memory.
Signature: nvjpegStatus_t nvjpegStateAttachPinnedBuffer ( nvjpegJpegState_t decoder_state , nvjpegBufferPinned_t pinned_buffer ); Parameters: Parameter Input / Output Memory Description nvjpegJpegState_t decoder_state Input Host nvJPEG decoder state.
nvjpegBufferPinned_t pinned_buffer Input Host nvJPEG pinned buffer container. 2.3.1.25. nvjpegBufferPinnedRetrieve()  Retrieves the pinned memory pointer and size from the nvJPEG pinned buffer handle.
Signature: nvjpegStatus_t nvjpegBufferPinnedRetrieve ( nvjpegBufferPinned_t buffer , size_t * size , void ** ptr ); Parameters: Parameter Input / Output Memory Description nvjpegBufferPinned_t buffer Input Host nvJPEG pinned buffer container.
void** ptr Input/Output Host Pointer to the pinned buffer. 2.3.1.26. nvjpegBufferPinnedResize()  Resize the pinned buffer to the specified size in bytes.
This API can be used to pre-allocate the pinned buffer to a large value and avoid allocator calls during decode.
Signature: nvjpegStatus_t nvjpegBufferPinnedResize ( nvjpegBufferPinned_t buffer , size_t size , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegBufferPinned_t buffer Input Host nvJPEG pinned buffer container.
cudaStream_t stream Input Host CUDA stream to use when nvjpegBufferPinned_t buffer is initialized using stream ordered allocators. 2.3.1.27. nvjpegBufferDeviceCreate()  Creates the device buffer handle.
Signature: nvjpegStatus_t nvjpegBufferDeviceCreate ( nvjpegHandle_t handle , nvjpegDevAllocator_t * device_allocator , nvjpegBufferDevice_t * buffer ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle.
nvjpegBufferDevice_t* buffer Input/Output Host nvJPEG device buffer container. 2.3.1.28. nvjpegBufferDeviceCreateV2()  Creates the device buffer handle using extended allocators.
Signature: nvjpegStatus_t nvjpegBufferDeviceCreateV2 ( nvjpegHandle_t handle , nvjpegDevAllocatorV2_t * device_allocator , nvjpegBufferDevice_t * buffer ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle.
nvjpegDevAllocatorV2_t* device_allocator Input Host Extended device memory allocator. 2.3.1.29. nvjpegBufferDeviceDestroy()  Destroys the device buffer handle.
Signature: nvjpegStatus_t nvjpegBufferDeviceDestroy ( nvjpegBufferDevice_t buffer ); Parameters: Parameter Input / Output Memory Description nvjpegBufferDevice_t buffer Input Host/Device nvJPEG device buffer container.
Device pointers are stored within the host structures. 2.3.1.30. nvjpegStateAttachDeviceBuffer()  Link the nvJPEG device buffer handle to the decoder_state .
The device_buffer is used by the decoder to store the intermediate information that is used across the decoding stages.
Device buffer can be attached to different decoder states, which helps to switch between implementations without allocating extra memory.
Signature: nvjpegStatus_t nvjpegStateAttachDeviceBuffer ( nvjpegJpegState_t decoder_state , nvjpegBufferDevice_t device_buffer ); Parameters: Parameter Input / Output Memory Description nvjpegJpegState_t decoder_state Input Host nvJPEG decoder state.
nvjpegBufferDevice_t device buffer Input Host/Device nvJPEG device buffer container. 2.3.1.31. nvjpegBufferDeviceRetrieve()  Retrieve the device memory pointer and size from the nvJPEG device buffer handle.
Signature: nvjpegStatus_t nvjpegBufferDeviceRetrieve ( nvjpegBufferDevice_t buffer , size_t * size , void ** ptr ); Parameters: Parameter Input / Output Memory Description nvjpegBufferDevice_t buffer Input Host nvJPEG device buffer container.
void** ptr Input/Output Host Pointer to the device buffer. 2.3.1.32. nvjpegBufferDeviceResize()  Resize the device buffer to the specified size in bytes.
This API can be used to pre-allocate the device buffer to a large value and avoid allocator calls during decode.
Signature: nvjpegStatus_t nvjpegBufferDeviceResize ( nvjpegBufferDevice_t buffer , size_t size , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegBufferDevice_t buffer Input Host nvJPEG device buffer container.
cudaStream_t stream Input Host CUDA stream to use when nvjpegBufferDevice_t buffer is initialized using stream ordered allocators. 2.3.1.33. nvjpegDecodeParamsCreate()  Creates a handle for the parameters.
The parameters that can be programmed include: output format, ROI decode, CMYK to RGB conversion.
Signature: nvjpegStatus_t nvjpegDecodeParamsCreate ( nvjpegHandle_t handle , nvjpegDecodeParams_t * decode_params ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle.
nvjpegDecodeParams_t *decode_params Input/Output Host Decode output parameters. 2.3.1.34. nvjpegDecodeParamsDestroy()  Destroys the decode_params handle.
Signature: nvjpegStatus_t nvjpegDecodeParamsDestroy ( nvjpegDecodeParams_t * decode_params ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t *decode_params Input/Output Host Decode output parameters. 2.3.2. Retrieve Encoded Image Information API  The helper functions for retrieving the encoded image information.
2.3.2.1. nvjpegGetImageInfo()  Decodes the JPEG header and retrieves the basic information about the image.
Signature: nvjpegStatus_t nvjpegGetImageInfo ( nvjpegHandle_t handle , const unsigned char * data , size_t length , int * nComponents , nvjpegChromaSubsampling_t * subsampling , int * widths , int * heights ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle.
int *widths Output Host Pointer to the first element of array of size NVJPEG_MAX_COMPONENT, where the width of each channel (up to NVJPEG_MAX_COMPONENT) will be saved.
int *heights Output Host Pointer to the first element of array of size NVJPEG_MAX_COMPONENT, where the height of each channel (up to NVJPEG_MAX_COMPONENT) will be saved. 2.3.2.2. nvJPEG Stream API  These functions store the parsed bit-stream data on the host.
2.3.2.2.1. nvjpegJpegStreamParse()  Parses the bitstream and stores the metadata in the jpeg_stream struct.
Signature: nvjpegStatus_t nvjpegJpegStreamParse ( nvjpegHandle_t handle , const unsigned char * data , size_t length , int save_metadata , int save_stream , nvjpegJpegStream_t jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle.
If not 0, then the JPEG stream metadata (headers, app markers, etc.) will be saved in the internal JpegStream structure for future usage.
int save_stream Input Host If not 0, then the whole jpeg stream will be copied to the internal JpegStream structure, and the pointer to the JPEG file data will not be needed after this call.
If 0, then JpegStream will just save the pointers (to JPEG file data), and these pointers will be used later during the image decoding.
nvjpegJpegStream_t jpeg_stream Input/Output Host/Device The nvJPEG bitstream handle that stores the parsed bitstream information. 2.3.2.2.2. nvjpegJpegStreamParseHeader()  Parses only the header of the bit-stream and stores the header information in the jpeg_stream struct.
Signature: nvjpegStatus_t nvjpegJpegStreamParseHeader ( nvjpegHandle_t handle , const unsigned char * data , size_t length , nvjpegJpegStream_t jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle. 2.3.2.2.3. nvjpegJpegStreamParseTables()  To be used when decoding TIFF files with JPEG compression.
Parses the JPEG tables bitstream and stores the jpeg tables in jpeg_stream Signature: nvjpegStatus_t nvjpegJpegStreamParseHeader ( nvjpegHandle_t handle , const unsigned char * data , size_t length , nvjpegJpegStream_t jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle.
nvjpegJpegStream_t jpeg_stream Input/Output Host The nvJPEG bitstream handle that stores the parsed bitstream information. 2.3.2.2.4. nvjpegJpegStreamGetFrameDimensions()  Extracts the JPEG frame dimensions from the bitstream.
Signature: nvjpegStatus_t nvjpegJpegStreamGetFrameDimensions ( nvjpegJpegStream_t jpeg_stream , unsigned int * width , unsigned int * height ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle.
unsigned int* height Output Host Frame width. 2.3.2.2.5. nvjpegJpegStreamGetComponentsNum()  Extracts the JPEG frame dimensions from the bitstream.
Signature: nvjpegStatus_t nvjpegJpegStreamGetComponentsNum ( nvjpegJpegStream_t jpeg_stream , unsigned int * components_num ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle.
unsigned int* components_num Output Host Number of encoded channels in the input. 2.3.2.2.6. nvjpegJpegStreamGetComponentDimensions()  Extracts the component dimensions from the bitstream.
Signature: nvjpegStatus_t nvjpegJpegStreamGetComponentDimensions ( nvjpegJpegStream_t jpeg_stream , unsigned int component , unsigned int * width , unsigned int * height ) Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle.
unsigned int* height Output Host Component width. 2.3.2.2.7. nvjpegJpegStreamGetChromaSubsampling()  Gets the chroma subsampling from the jpeg_stream .
For 3-channel images it tries to assign one of the known chroma sub-sampling values based on the sampling information present in the bitstream, else it returns NVJPEG_CSS_UNKNOWN.
Signature: nvjpegStatus_t nvjpegJpegStreamGetChromaSubsampling ( nvjpegJpegStream_t jpeg_stream , nvjpegChromaSubsampling_t * chroma_subsampling ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle.
nvjpegChromaSubsampling_t* chroma_subsampling Output Host Chroma subsampling for the 1- or 3- channel encoding. 2.3.2.2.8. nvjpegJpegStreamGetJpegEncoding()  This function obtains the JPEG encoding type from the jpeg_stream .
Signature: nvjpegStatus_t nvjpegJpegStreamGetJpegEncoding ( nvjpegJpegStream_t jpeg_stream , nvjpegJpegEncoding_t * jpeg_encoding ); Parameters: Parameter Input / Output Memory Description jpeg_stream In Host Input bitstream handle.
jpeg_encoding Out Host Encoding type obtained—baseline or progressive. 2.3.2.2.9. nvjpegJpegStreamGetExifOrientation()  Extracts the exif orientation from the bitstream.
Returns NVJPEG_ORIENTATION_UNKNOWN if the exif marker/orientation information is not present.
Signature: nvjpegStatus_t NVJPEGAPI nvjpegJpegStreamGetExifOrientation ( nvjpegJpegStream_t jpeg_stream , nvjpegExifOrientation_t * orientation_flag ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle.
nvjpegExifOrientation_t *orientation_flag Output Host Exif orientation in JPEG stream. 2.3.2.2.10. nvjpegJpegStreamGetSamplePrecision()  Extracts the sample precision(bit depth) from the bitstream.
Signature: nvjpegStatus_t NVJPEGAPI nvjpegJpegStreamGetSamplePrecision ( nvjpegJpegStream_t jpeg_stream , unsigned int * precision ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle.
unsigned int *precision Output Host Sample precision value. 2.3.3. Decode API—Single Phase  Functions for decoding single image or batched images in a single phase.
2.3.3.1. ​nvjpegDecode()  Decodes a single image, and writes the decoded image in the desired format to the output buffers.
From CUDA 11 onwards, nvjpegDecode() picks the best available back-end for a given image, user no longer has control on this.
Signature: nvjpegStatus_t nvjpegDecode ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * data , size_t length , nvjpegOutputFormat_t output_format , nvjpegImage_t * destination , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle.
nvjpegOutputFormat_t output_format Input Host Format in which the decoded output will be saved.
nvjpegImage_t *destination Input/Output Host/Device Pointer to the structure that describes the output destination.
This structure should be on the host (CPU), but the pointers in this structure should be pointing to the device (i.e., GPU) memory.
cudaStream_t stream Input Host The CUDA stream where all of the GPU work will be submitted. 2.3.3.2. ​nvjpegDecodeBatchedInitialize()  This function initializes the batched decoder state.
The initialization parameters include the batch size, the maximum number of CPU threads, and the specific output format in which the decoded image will be saved.
Signature: nvjpegStatus_t nvjpegDecodeBatchedInitialize ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , int batch_size , int max_cpu_threads , nvjpegOutputFormat_t output_format ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle.
int max_cpu_threads Input Host This parameter is no longer used by the library. 2.3.3.3. ​nvjpegDecodeBatched()  Decodes the batch of images, and writes them to the buffers described in the destination parameter in a format provided to nvjpegDecodeBatchedInitialize() function.
Signature: nvjpegStatus_t nvjpegDecodeBatched ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * const * data , const size_t * lengths , nvjpegImage_t * destinations , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle.
const unsigned char *const *data Input Host Pointer to the first element of array of the input data.
The size of the array is assumed to be batch_size provided to nvjpegDecodeBatchedInitialize() batch initialization function.
Size of array is assumed to be batch_size provided to nvjpegDecodeBatchedInitialize() , the batch initialization function.
nvjpegImage_t *destinations Input/Output Host/Device Pointer to the first element of array of output descriptors.
The size of array is assumed to be batch_size provided to nvjpegDecodeBatchedInitialize(), the batch initialization function.
cudaStream_t stream Input Host The CUDA stream where all the GPU work will be submitted. 2.3.3.4. nvjpegDecodeBatchedEx()  This API helps to Decodes the batch of images with ROI, and writes them to the buffers described in the destination parameter in a format provided to nvjpegDecodeBatchedInitialize() function.
Signature: nvjpegStatus_t nvjpegDecodeBatchedEx ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * const * data , const size_t * lengths , nvjpegImage_t * destinations , nvjpegDecodeParams_t * decode_params , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host nvjpeg library handle.
The size of array is assumed to be batch_size provided to nvjpegDecodeBatchedInitialize() , the batch initialization function.
nvjpegDecodeParams_t *decode_params Input Host Setting ROI Decode parameters cudaStream_t stream Input Host The CUDA stream where all the GPU work will be submitted. 2.3.3.5. nvjpegDecodeBatchedSupported()  This API helps determine whether an image can be decoded by nvjpegDecodeBatched .
User can parse the bitstream header using nvjpegJpegStreamParseHeader and then call this API to determine whether the image can be decoded.
Signature: nvjpegStatus_t nvjpegDecodeBatchedSupported ( nvjpegHandle_t handle , nvjpegJpegStream_t jpeg_stream , int * is_supported ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host nvjpeg library handle.
int* is_supported Output Host Return value of 0 indicates bitstream can be decoded by the decoder_handle , non zero value indicates that the bitstream is not supported. 2.3.3.6. nvjpegDecodeBatchedSupportedEx()  This API helps determine whether an image can be decoded by nvjpegDecodeBatchedEx .
User can parse the bitstream header using nvjpegJpegStreamParseHeader and set the ROI in the decode params then call this API to determine whether the image can be decoded.
Signature: nvjpegStatus_t nvjpegDecodeBatchedSupportedEx ( nvjpegHandle_t handle , nvjpegJpegStream_t jpeg_stream , nvjpegDecodeParams_t decode_params , int * is_supported ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host nvjpeg library handle.
int* is_supported Output Host Return value of 0 indicates bitstream can be decoded by the decoder_handle , a non zero value indicates that the bitstream is not supported. 2.3.3.7. nvjpegDecodeBatchedPreAllocate()  This is an experimental API that can be used with nvjpegDecodeBatched .
When decoding images with varying sizes and chroma subsampling, performance is limited by the repeated cuda calls made by the library to free/allocate device memory.
This API attempts to avoid this problem by allocating device memory prior to the actual decoding.
Users have the option to call this API with values that are unlikely to be exceeded when nvjpegDecodeBatched is called.
Note Note: This functionality is available only when the nvjpegHandle_t is instantiated using NVJPEG_BACKEND_HARDWARE.
If the image dimensions at the time of decode exceed what was provided, then the library will resize the device buffers.
If the images being decoded have different chroma subsamplings, then the chroma_subsampling field should be set to NVJPEG_CSS_444 to ensure that the device memory can be reused.
Signature: nvjpegStatus_t nvjpegDecodeBatchedPreAllocate ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , int batch_size , int width , int height , nvjpegChromaSubsampling_t chroma_subsampling , nvjpegOutputFormat_t output_format ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle.
nvjpegChromaSubsampling_t chroma_subsampling Input Host Chroma-subsampling of the images. 2.3.3.8. nvjpegDecodeBatchedParseJpegTables()  To be used along with batched decode APIs when decoding JPEG bitstreams from a TIFF file.
The external Huffman and quantization tables will be applied to all the JPEG bitstreams in the batch.
Signature: nvjpegStatus_t nvjpegDecodeBatchedParseJpegTables ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * data , const size_t length ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle.
Can be set to NULL to reset the jpeg tables. 2.3.4. Decode API—Decoupled Decoding  This set of decoding API works with the bitstream handles, decode parameter handles, pinned and device buffers handles as input, thus decoupling JPEG bitstream parse, buffer management and setting up decoder parameters from the decode process itself.
Multiphase decoupled single image decoding consists of three phases: Host Mixed Device Each of the above decodings is carried on according to its individual semantics.
Phases on different images can be carried out with different decoding state handles simultaneously, while sharing of some helper objects is possible.
The following snippet explains how to use the API to prefetch the host stage of the processing: first do all of the host work on the host, and then submit the rest of decoding work to the device.
If a pinned buffer is attached to the decoder state, then the pinned buffer object will be used to allocate the pinned memory required for the host decoding phase.
There wouldn’t be allocation if the pinned buffer object already handles the required amount of pinned memory.
If a pinned buffer object is not attached, then the state will use heap host memory to allocate the memory required for the host processing.
Hence the device selection, device initialization, and device memory initialization can be done later in the decoding process.
The parsed stream handle that is available after calling the nvjpegJpegStreamParse() function should be provided to this function.
Signature: nnvjpegStatus_t nvjpegDecodeJpegHost ( nvjpegHandle_t handle , nvjpegJpegDecoder_t decoder , nvjpegJpegState_t decoder_state , nvjpegDecodeParams_t decode_params , nvjpegJpegStream_t jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle.
nvjpegJpegStream_t jpeg_stream Input Host Handle to the parsed bitstream data. 2.3.4.2. nvjpegDecodeJpegTransferToDevice()  This phase contains both host and device operations.
This phase should be called only after the host phase with the same decoder handle, decoder state handle and parsed jpeg stream handle.
Device should be initialized and device buffer should be attached to decoder_state handle using nvjpegStateAttachDeviceBuffer() prior to calling this API.
For the host memory buffer, this phase will use whatever was used in the host phase: either the attached pinned buffer or the state’s host memory buffer.
Signature: nvjpegStatus_t nvjpegDecodeJpegTransferToDevice ( nvjpegHandle_t handle , nvjpegJpegDecoder_t decoder , nvjpegJpegState_t decoder_state , nvjpegJpegStream_t jpeg_stream , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle.
cudaStream_t stream Input Host The CUDA stream to which all the GPU tasks will be submitted. 2.3.4.3. nvjpegDecodeJpegDevice()  This phase consists of decode operations that take place mainly on the device (no significant host side computation is done).
This phase should be called after nvjpegDecodeJpegTransferToDevice() for a given decoder_state handle and decoder handle.
In this function call, the host memory buffers are not used, so if the pinned buffer was attached to the state, then it can be reused somewhere else.
Note that at this point the Jpeg stream handle is not needed anymore, since parts that are needed for device decoding will be copied to the device memory in the previous phase.
Signature: nvjpegStatus_t nvjpegDecodeJpegDevice ( nvjpegHandle_t handle , nvjpegJpegDecoder_t decoder , nvjpegJpegState_t decoder_state , nvjpegImage_t * destination , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle.
nvjpegImage_t *destination Input/Output Host/Device Pointer to a structure that describes the output destination.
This structure should be on host, but the pointers in this structure should be pointing to the device memory.
See nvJPEG Image for details. 2.3.4.4. nvjpegDecodeJpeg()  This is a single phase API with the flexibility to select nvJPEG back-end when creating an nvjpegJpegDecoder_t object.
The user has the option to call this API instead of making three separate calls to nvjpegDecodeJpegHost() , nvjpegDecodeJpegTransferToDevice() , and nvjpegDecodeJpegDevice() .
Signature: nvjpegStatus_t nvjpegDecodeJpeg ( nvjpegHandle_t handle , nvjpegJpegDecoder_t decoder , nvjpegJpegState_t decoder_state , nvjpegJpegStream_t jpeg_bitstream , nvjpegImage_t * destination , nvjpegDecodeParams_t decode_params , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle.
This structure should be on the host, but the pointers in this structure should be pointing to the device memory.
nvjpegDecodeParams_t decode_params Input Host The handle which stores the decode output properties. 2.3.5. nvJPEG Decode Parameters  This category of APIs is used to set the decoding parameters.
These APIs should be used with the decode APIs defined in Decode API—Decoupled Decoding . 2.3.5.1. nvjpegDecodeParamsSetOutputFormat()  This function is used to set the decode output format.
The output parameter of nvjpegOutputFormat_t defaults to NVJPEG_OUTPUT_UNCHANGED if not set using this API.
Signature: nvjpegStatus_t nvjpegDecodeParamsSetOutputFormat ( nvjpegDecodeParams_t decode_params , nvjpegOutputFormat_t output_format ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host Decode output parameter handle.
nvjpegOutputFormat_t output_format Input Host See step 6 of Single Image Decoding . 2.3.5.2. nvjpegDecodeParamsSetROI()  This function enables the region of interest-only (ROI-only) decode.
To disable the ROI-only, i.e., to decode the whole image, set: offset_x = 0, offset_y = 0, roi_width = -1, and roi_height = -1.
It is not supported when the nvJPEG decoder handle is created using NVJPEG_BACKEND_HARDWARE.
That is: offset_x cannot be lower than zero, or offset_x + roi_width cannot be larger than the JPEG image width.
If the output format is NVJPEG_OUTPUT_YUV or NVJPEG_OUTPUT_UNCHANGED, then the offset_x and offset_y values have to be multiples of the maximum subsampling factor, as defined in the JPEG standard.
Signature: nvjpegStatus_t nvjpegDecodeParamsSetROI ( nvjpegDecodeParams_t decode_params , int offset_x , int offset_y , int roi_width , int roi_height ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host The decode output parameter handle.
int offset_x Input Host Image offset along the horizontal direction relative to the top left corner.
int offset_y Input Host Image offset along the vertical direction relative to the top left corner.
int roi_height Input Host Image height relative to offset_y . 2.3.5.3. nvjpegDecodeParamsSetAllowCMYK()  If enabled, the nvJPEG library assumes that the JPEG with 4 encoded color components is in CMYK colorspace, and enables the conversion to RGB/YUV colorspace.
The conversion is based on the subtractive scheme—this behavior matches OpenCV’s handling of 4-component JPEGs.
Signature: nvjpegStatus_t nvjpegDecodeParamsSetAllowCMYK ( nvjpegDecodeParams_t decode_params , int allow_cmyk ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host Decode output parameter handle.
int allow_cmyk Input Host Enable CMYK to RGB conversion. 2.3.5.4. nvjpegDecodeParamsSetScaleFactor()  Allows the user to scale decode output.
Note This feature is currently supported only when nvJPEG decoder handle is created using NVJPEG_BACKEND_HARDWARE.
Signature: nvjpegStatus_t nvjpegDecodeParamsSetScaleFactor ( nvjpegDecodeParams_t decode_params , nvjpegScaleFactor_t scale_factor ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host Decode output parameter handle.
When setting a scale factor value, the recommended allocation of the destination parameters is as follows: Use nvjpegGetImageInfo() , or nvjpegJpegStreamGetFrameDimensions() to extract the dimensions of each channel.
Let height[NVJPEG_MAX_COMPONENT] and width[NVJPEG_MAX_COMPONENT] be 2 arrays which store the height and width.
When ExifOrientation is enabled, the output buffers should be allocated based on the rotated dimensions.
If the orientation is set as NVJPEG_ORIENTATION_UNKNOWN , the library will default to NVJPEG_ORIENTATION_HORIZONTAL .
Signature: nvjpegStatus_t nvjpegDecodeParamsSetExifOrientation ( nvjpegDecodeParams_t decode_params , nvjpegExifOrientation_t orientation ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host Decode output parameter handle.
nvjpegExifOrientation_t orientation Input Host Set the exif orientation for the decode output. 2.3.6. nvJPEG API Return Codes  The nvJPEG API adheres to the following return codes and their indicators: typedef enum { NVJPEG_STATUS_SUCCESS = 0 , NVJPEG_STATUS_NOT_INITIALIZED = 1 , NVJPEG_STATUS_INVALID_PARAMETER = 2 , NVJPEG_STATUS_BAD_JPEG = 3 , NVJPEG_STATUS_JPEG_NOT_SUPPORTED = 4 , NVJPEG_STATUS_ALLOCATOR_FAILURE = 5 , NVJPEG_STATUS_EXECUTION_FAILED = 6 , NVJPEG_STATUS_ARCH_MISMATCH = 7 , NVJPEG_STATUS_INTERNAL_ERROR = 8 , NVJPEG_STATUS_IMPLEMENTATION_NOT_SUPPORTED = 9 } nvjpegStatus_t ; Description of the returned error codes: Returned Error (Returned Code) Description NVJPEG_STATUS_SUCCESS (0) The API call has finished successfully.
Note that many of the calls are asynchronous and some of the errors may be seen only after synchronization.
NVJPEG_STATUS_JPEG_NOT_SUPPORTED (4) Attempting to decode a JPEG stream that is not supported by the nvJPEG library.
NVJPEG_STATUS_ALLOCATOR_FAILURE (5) The user-provided allocator functions, for either memory allocation or for releasing the memory, returned a non-zero code.
NVJPEG_STATUS_ARCH_MISMATCH (7) The device capabilities are not enough for the set of input parameters provided (input parameters such as backend, encoded stream parameters, output format).
nvJPEG Chroma Subsampling  One of the outputs of the nvjpegGetImageInfo() API is nvjpegChromaSubsampling_t .
This parameter is an enum type, and its enumerator list comprises of the chroma subsampling property retrieved from the encoded JPEG image.
The nvjpegGetImageInfo() function currently supports the following chroma subsampling types: typedef enum { NVJPEG_CSS_444 , NVJPEG_CSS_422 , NVJPEG_CSS_420 , NVJPEG_CSS_440 , NVJPEG_CSS_411 , NVJPEG_CSS_410 , NVJPEG_CSS_GRAY , NVJPEG_CSS_410V , NVJPEG_CSS_UNKNOWN } nvjpegChromaSubsampling_t ; 2.3.8.
Examples of nvJPEG  nvJPEG Decode sample can be found here: https: github.com/NVIDIA/CUDALibrarySamples/tree/master/nvJPEG/nvJPEG-Decoder 3.
JPEG Encoding  This section describes the encoding functions of the nvJPEG Library. 3.1. Using the Encoder  The user should perform the below prerequisite steps before calling the nvJPEG encoding functions.
See also nvJPEG Encoder Helper API Reference . 3.1.1. Encoding the Parameters  The user should create an encoding parameters structure with nvjpegEncoderParamsCreate() function.
User can use an appropriate nvjpegEncoderParamsSet*() function to set a specific parameter.
The quality parameter can be set, using the nvjpegEncoderParamsSetQuality() function, to an integer value between 1 and 100, and this quality parameter will be used as a base for generating the JPEG quantization tables.
Note Occasionally, when encoding high entropy input data, such as random images, the encoding can fail if the quality parameter is set too high.
This is due to the fact that the compressed bitstream would be larger than the input image.
We recommend restarting the encoding with slightly lower quality factor or using a real-world images if possible.
Note The encoding parameters structure can be reused to compress multiple images simultaneously, but no changes to the parameters should be made during the ongoing encoding, or the encoding result will be undefined. 3.1.2. Encoding the State  The user should create the encoding state structure using nvjpegEncoderStateCreate() function.
Note The encoding state structure can be reused to encode a series of images, but no encoding should be performed on multiple images with the same encoding state at the same time—otherwise the result of the encodings will be undefined. 3.1.3. Encoding the Image  The nvJPEG library provides a few interfaces for compressing the image in different formats and colorspaces.
See below. 3.1.3.1. nvjpegEncodeYUV  Input for this function is an image in YUV colorspace.
If the chroma subsampling in the encoding parameters is the same as input chroma subsampling, then the user’s input data will be directly used in the JPEG compression.
Otherwise chroma will be resampled to match the chroma subsampling of the encoding parameters.
That is, the chrominance image planes should have sizes aligned to the corresponding subsamplings.
For example: Image dimensions: 123x321 Input chroma subsampling: NVJPEG_CSS_410 Chroma subsampling factor for this chroma subsampling: 4x2 Given the above, the encoder library expects the user to provide: Y plane with size: 123 x 321 Cb and Cr plane with size: 31 x 161 3.1.3.2.
Input for this function, i.e., how data should be provided in the source argument, is determined by the input_format argument.
For example, if the user has interleaved the RGB image of size W x H , stored continuously, and the pointer to it is pImage , then source should be: source.channel[0] = pImage source.pitch[0] = W*3 When the same image is stored in planar format, with image planes pointers stored continuously in the array pImage[3] , then source should be: source.channel[0] = pImage[0] source.channel[1] = pImage[1] source.channel[2] = pImage[2] The pitch values for each channel in the source parameter should be set accordingly to the data layout.
The nvJPEG library will perform the color transformation to the YCbCr, and will compress the result. 3.1.4. Retrieving the Compressed Stream  Often it is not feasible to accurately predict the final compressed data size of the final JPEG stream for any input data and parameters.
The nvJPEG library, while encoding, will calculate the size of the final stream, allocate temporary buffer in the encoder state and save the compressed data in the encoding state’s buffer.
In order to get final compressed JPEG stream, the user should provide the memory buffer large enough to store this compressed data.
There are two options for how to do this: Use the upper bound on compressed JPEG stream size for the given parameters and image dimensions: Use the nvjpegEncodeRetrieveBitstream() function to retrieve the maximum possible JPEG stream size at any given time.
Retrieve the compressed JPEG stream from the encoder state after successful encoding, using the nvjpegEncodeRetrieveBitstream() and the allocated buffer.
Wait for the encoding to complete, and retrieve the exact size of required buffer, as below: Encode the image using one of the encoding functions.
Use the nvjpegEncodeRetrieveBitstream() function to retrieve the size in bytes of the compressed JPEG stream.
Use the nvjpegEncodeRetrieveBitstream() function to populate your buffer with the compressed JPEG stream.
Note As the same encoding image state can be reused to compress a series of images, the nvjpegEncodeRetrieveBitstream() function will return the result for the last compressed image. 3.1.5. JPEG Encoding Example  See below the example code, and the block diagram shown in Figure 1 , for encoding with nvJPEG Encoder.
data (), & length , 0 );   write stream to file cudaStreamSynchronize ( stream ); std :: ofstream output_file ( "test.jpg" , std :: ios :: out | std :: ios :: binary ); output_file .
nvJPEG Encoder Type Declarations  This section describes the nvJPEG Encoder Type Declarations. 3.2.1. nvjpegInputFormat_t  typedef enum { NVJPEG_INPUT_RGB = 3 , NVJPEG_INPUT_BGR = 4 , NVJPEG_INPUT_RGBI = 5 , NVJPEG_INPUT_BGRI = 6 } nvjpegInputFormat_t ; The nvjpegInputFormat_t enum is used to select the color model and pixel format of the input image.
Pixel format is interleaved BGR. 3.2.2. nvjpegEncoderState_t  The nvjpegEncoderState_t structure stores intermediate buffers and variables used for compression.
3.2.3. nvjpegEncoderParams_t  The nvjpegEncoderParams_t structure stores JPEG encode parameters.
3.3. nvJPEG Encoder Helper API Reference  The nvJPEG Encoder helper functions are used for initializing.
3.3.1. nvjpegEncoderStateCreate()  Creates encoder state that stores intermediate buffers used in compression.
Signature: nvjpegStatus_t nvjpegEncoderStateCreate ( nvjpegHandle_t handle , nvjpegEncoderState_t * encoder_state , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle encoder_state Output Host Pointer to the encoder state structure, where the new state will be placed.
stream Inputt Host CUDA stream where all the required device operations will be placed. 3.3.2. nvjpegEncoderStateDestroy()  Destroys the encoder state.
Signature: nvjpegStatus_t nvjpegEncoderStateDestroy ( nvjpegEncoderState_t encoder_state ); Parameters: Parameter Input / Output Memory Description encoder_state Input/Output Host Encoder state structure that will be released. 3.3.3. nvjpegEncoderParamsCreate()  Creates the structure that holds the compression parameters.
Signature: nvjpegStatus_t nvjpegEncoderParamsCreate ( nvjpegHandle_t handle , nvjpegEncoderParams_t * encoder_params , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle.
encoder_params Output Host Pointer to the location where the new parameters structure will be placed. 3.3.4. nvjpegEncoderParamsDestroy()  Destroys the encoder parameters structure.
Signature: nvjpegEncoderParamsDestroy ( nvjpegEncoderParams_t encoder_params ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder params structure that will be released. 3.3.5. nvjpegEncoderParamsSetEncoding()  Sets the parameter quality in the encoder parameters structure.
Signature: nvjpegStatus_t nvjpegEncoderParamsSetEncoding ( nvjpegEncoderParams_t encoder_params , nvjpegJpegEncoding_t etype , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameters structure handle.
stream Input Host CUDA stream where all the required device operations will be placed. 3.3.6. nvjpegEncoderParamsSetQuality()  Sets the parameter quality in the encoder parameters structure.
Signature: nvjpegStatus_t nvjpegEncoderParamsSetQuality ( nvjpegEncoderParams_t encoder_params , const int quality , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameterss structure handle.
quality Input Host Integer value of quality between 1 and 100, where 100 is the highest quality.
stream Input Host CUDA stream where all the required device operations will be placed. 3.3.7. nvjpegEncoderParamsSetOptimizedHuffman()  Sets whether or not to use optimized Huffman.
Using optimized Huffman produces smaller JPEG bitstream sizes with the same quality, but with slower performance.
Signature: nvjpegStatus_t nvjpegEncoderParamsSetOptimizedHuffman ( nvjpegEncoderParams_t encoder_params , const int optimized , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameters structure handle.
stream Input Host CUDA stream where all the required device operations will be placed. 3.3.8. nvjpegEncoderParamsSetSamplingFactors()  Sets which chroma subsampling will be used for JPEG compression.
Signature: nvjpegStatus_t nvjpegEncoderParamsSetSamplingFactors ( nvjpegEncoderParams_t encoder_params , const nvjpegChromaSubsampling_t chroma_subsampling , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameters structure handle.
If the input is in YUV color model and chroma_subsampling is different from the subsampling factors of source image, then the NVJPEG library will convert subsampling to the value of chroma_subsampling .
stream Input Host CUDA stream where all the required device operations will be placed. 3.4. nvJPEG Encoder API Reference  This section describes the nvJPEG Encoder API.
3.4.1. nvjpegEncodeGetBufferSize()  Returns the maximum possible buffer size that is needed to store the compressed JPEG stream, for the given input parameters.
Signature: nvjpegStatus_t nvjpegEncodeGetBufferSize ( nvjpegHandle_t handle , const nvjpegEncoderParams_t encoder_params , int image_width , int image_height , size_t * max_stream_length ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle.
image_height Input Host Input image height. 3.4.2. nvjpegEncodeYUV()  Compresses the image in YUV colorspace to JPEG stream using the provided parameters, and stores it in the state structure.
Signature: nvjpegStatus_t nvjpegEncodeYUV ( nvjpegHandle_t handle , nvjpegEncoderState_t encoder_state , const nvjpegEncoderParams_t encoder_params , const nvjpegImage_t * source , nvjpegChromaSubsampling_t chroma_subsampling , int image_width , int image_height , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle.
encoder_state Input/Output Host Internal structure that holds the temporary buffers required for the compression and also stores the final compressed JPEG stream.
source Input Host Pointer to the nvjpeg structure that holds the device pointers to the Y, U(Cb) and V(Cr) image planes and the respective strides.
chroma_subsampling Input Host Chroma subsampling of the input data. 3.4.3. nvjpegEncodeImage()  Compresses the image in the provided format to the JPEG stream using the provided parameters, and stores it in the state structure.
Signature: nvjpegStatus_t nvjpegEncodeImage ( nvjpegHandle_t handle , nvjpegEncoderState_t encoder_state , const nvjpegEncoderParams_t encoder_params , const nvjpegImage_t * source , nvjpegInputFormat_t input_format , int image_width , int image_height , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle.
input_format Input Host Value of nvjpegInputFormat_t type that describes the input data. 3.4.4. nvjpegEncodeRetrieveBitstream()  Retrieves the compressed stream from the encoder state that was previously used in one of the encoder functions.
If data parameter is NULL then the encoder will return compressed stream size in the length parameter.
If data is not NULL then the provided length parameter should contain the data buffer size.
If the provided length is less than compressed stream size, then an error will be returned.
Otherwise the compressed stream will be stored in the data buffer and the actual compressed buffer size will be stored in the length parameter.
Signature: nvjpegStatus_t nvjpegEncodeRetrieveBitstream ( nvjpegHandle_t handle , nvjpegEncoderState_t encoder_state , unsigned char * data , size_t * length , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle.
encoder_state Input/Output Host The encoder_state that was previously used in one of the encoder functions.
data Input/Output Host Pointer to the buffer in the host memory where the compressed stream will be stored.
On return the NVJPEG library will store the actual compressed stream size in this parameter. 3.4.5. nvjpegEncodeRetrieveBitstreamDevice()  Retrieves the compressed stream from the encoder state that was previously used in one of the encoder functions.
data parameter should be on device memory If data parameter is NULL then the encoder will return compressed stream size in the length parameter.
Signature: nvjpegStatus_t nvjpegEncodeRetrieveBitstreamDevice ( nvjpegHandle_t handle , nvjpegEncoderState_t encoder_state , unsigned char * data , size_t * length , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle.
data Input/Output Device Pointer to the buffer in the device memory where the compressed stream will be stored. 4. JPEG Transcoding  This section describes the transcoding functions of the nvJPEG Library.
4.1. nvJPEG Transcoder Helper API Reference  This section describes the nvJPEG Transcoder helper API.
4.1.1. nvjpegEncoderParamsCopyMetadata()  Copies the metadata (JFIF, APP, EXT, and COM markers) from the parsed stream.
Signature: nvjpegStatus_t nvjpegEncoderParamsCopyMetadata ( nvjpegEncoderState_t encoder_state , nvjpegEncoderParams_t encode_params , nvjpegJpegStream_t jpeg_stream , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_state In/Out Host Internal structure that stores the temporary buffers required for the compression.
stream In Host CUDA stream where all the required device operations will be placed. 4.1.2. nvjpegEncoderParamsCopyQuantizationTables()  Copies the quantization tables from the parsed stream.
Signature: nvjpegStatus_t nvjpegEncoderParamsCopyQuantizationTables ( nvjpegEncoderParams_t encode_params , nvjpegJpegStream_t jpeg_stream , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encode_params Out Host Encoder parameters that will be used for compression. 4.1.3. nvjpegEncoderParamsCopyHuffmanTables() [Deprecated]  nvjpegEncoderParamsCopyHuffmanTables() is now deprecated.
Due to precision differences in the JPEG encode/decode process, the input huffman tables may no longer be valid for the image being encoded and may result in corrupt bitstream.
Signature: nvjpegStatus_t nvjpegEncoderParamsCopyHuffmanTables ( nvjpegEncoderState_t encoder_state , nvjpegEncoderParams_t encode_params , nvjpegJpegStream_t jpeg_stream , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_state In/Out Host Internal structure that stores the temporary buffers required for the compression. 4.2. JPEG Transcoding Example  See below the example code.
Known Issues  Decoupled APIs, when initialized with NVJPEG_BACKEND_GPU_HYBRID , may not be able to correctly decode jpeg bitstreams which have out of bound run length codes. 7. Notices  7.1.
Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.
NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.
NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.
This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).
NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.
NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.
It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.
Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.
NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.
Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.
Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 7.2. OpenCL  OpenCL is a trademark of Apple Inc.
used under license to the Khronos Group Inc. 7.3. Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S.
Other company and product names may be trademarks of the respective companies with which they are associated.
Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates.
jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== "undefined"){_satellite.pageBottom();}
