1. Query Enhancing: Currently we are using a LLM to generate some alternate queries and adding some keywords for better retrieval. The pipeline is streamlined however the outputs generated by the models is not upto the mark, thus we either need to use a better model which may add some more time to the query enhancing step or we need to fine-tune the model. 
2. Output Generation: The same issue persists with the output generation model. We have experimented with multiple BERT-based models, original and some others, fine-tuned for Q&A but their performance was not substantial. 
3. Infrastructure unstability: Due to unresolved issues of "AWS instance crashing" we could not use the cuda for output generation. After resolving the issue, the output generation time can be reduced significantly.  
4. Dockerization for Convenience: We will convert the entire model into a Docker image for convenience, ensuring ease of deployment and consistency across different environments.