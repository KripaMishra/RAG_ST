{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded Query: skill information scientific_discipline science datum data_point data\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def expand_query(query):\n",
    "    expanded_terms = set(query.split())\n",
    "    for word in query.split():\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                expanded_terms.add(lemma.name())\n",
    "    return ' '.join(expanded_terms)\n",
    "\n",
    "query = \"data science\"\n",
    "expanded_query = expand_query(query)\n",
    "print(\"Expanded Query:\", expanded_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nDPRQuestionEncoder requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m bm25_scores \u001b[38;5;241m=\u001b[39m bm25\u001b[38;5;241m.\u001b[39mget_scores(expanded_query\u001b[38;5;241m.\u001b[39msplit())\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# DPR retrieval\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m question_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mDPRQuestionEncoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/dpr-question_encoder-single-nq-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m question_tokenizer \u001b[38;5;241m=\u001b[39m DPRQuestionEncoderTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/dpr-question_encoder-single-nq-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m context_encoder \u001b[38;5;241m=\u001b[39m DPRContextEncoder\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/dpr-ctx_encoder-single-nq-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1500\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1500\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1488\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1486\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nDPRQuestionEncoder requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Sample documents\n",
    "documents = [\"Data science is a field that uses scientific methods\", \n",
    "             \"Machine learning is a subfield of artificial intelligence\", \n",
    "             \"Statistics is a branch of mathematics\"]\n",
    "\n",
    "# BM25 retrieval\n",
    "tokenized_docs = [doc.split() for doc in documents]\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "bm25_scores = bm25.get_scores(expanded_query.split())\n",
    "\n",
    "# DPR retrieval\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "# Encode query\n",
    "query_embedding = question_encoder(question_tokenizer(expanded_query, return_tensors=\"pt\")['input_ids']).pooler_output.detach().numpy()\n",
    "\n",
    "# Encode documents\n",
    "context_embeddings = []\n",
    "for doc in documents:\n",
    "    inputs = context_tokenizer(doc, return_tensors=\"pt\")['input_ids']\n",
    "    outputs = context_encoder(inputs).pooler_output.detach().numpy()\n",
    "    context_embeddings.append(outputs)\n",
    "context_embeddings = np.vstack(context_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatIP(query_embedding.shape[1])\n",
    "index.add(context_embeddings)\n",
    "\n",
    "# Retrieve using FAISS\n",
    "_, dpr_scores = index.search(query_embedding, len(documents))\n",
    "\n",
    "# Combine scores\n",
    "combined_scores = bm25_scores + dpr_scores.flatten()\n",
    "\n",
    "# Retrieve top documents\n",
    "top_k = 3\n",
    "top_indices = np.argsort(combined_scores)[::-1][:top_k]\n",
    "retrieved_docs = [documents[i] for i in top_indices]\n",
    "\n",
    "print(\"Retrieved Documents:\", retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
